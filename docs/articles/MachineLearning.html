<!DOCTYPE html>
<html lang="ja">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-translate-customization" content="108d9124921d80c3-80e20d618ff053c8-g4f02ec6f3dba68b7-c">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>MachineLearningに関する論文・技術記事メモの一覧 | わたしのべんきょうノート</title>
<meta name="generator" content="Jekyll v4.3.2">
<meta property="og:title" content="MachineLearningに関する論文・技術記事メモの一覧">
<meta name="author" content="AkihikoWATANABE">
<meta property="og:locale" content="ja">
<meta name="description" content="MachineLearning">
<meta property="og:description" content="MachineLearning">
<link rel="canonical" href="http://akihikowatanabe.github.io/paper_notes/articles/MachineLearning.html">
<meta property="og:url" content="http://akihikowatanabe.github.io/paper_notes/articles/MachineLearning.html">
<meta property="og:site_name" content="わたしのべんきょうノート">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-08-05T00:56:41+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="MachineLearningに関する論文・技術記事メモの一覧">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"AkihikoWATANABE"},"dateModified":"2025-08-05T00:56:41+00:00","datePublished":"2025-08-05T00:56:41+00:00","description":"MachineLearning","headline":"MachineLearningに関する論文・技術記事メモの一覧","mainEntityOfPage":{"@type":"WebPage","@id":"http://akihikowatanabe.github.io/paper_notes/articles/MachineLearning.html"},"url":"http://akihikowatanabe.github.io/paper_notes/articles/MachineLearning.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="icon" href="">
  <link rel="canonical" href="http://akihikowatanabe.github.io">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/main.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/custom_button.css">
  <script src="/paper_notes/assets/js/main.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script><link type="application/atom+xml" rel="alternate" href="http://akihikowatanabe.github.io/paper_notes/feed.xml" title="わたしのべんきょうノート">
<script>
  function showMore(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "block";
          
      // このボタンの参照を取得して非表示にします
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "none";
      }
          
      // hideボタンの参照を取得して表示します
      const hideButton = contentDiv.querySelector('button[onclick^="hideContent"]');
      if (hideButton) {
        hideButton.style.display = "block";
      }
    }
  }

  function hideContent(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "none";
  
      // moreボタンの参照を取得して表示します
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "block";
      }
  
      // このボタンを隠します
      const hideButtons = document.querySelectorAll('button[onclick^="hideContent"]');
      const hideButton = hideButtons[index];
      if (hideButton) {
        hideButton.style.display = "none";
      }
    }
  }
</script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script charset="UTF-8" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/go.min.js" async></script>



















<script>
// Init highlight js
document.addEventListener('DOMContentLoaded', function(event) {
  var els = document.querySelectorAll('pre code')

  function addLangData(block) {
    var outer = block.parentElement.parentElement.parentElement;
    var lang = block.getAttribute('data-lang');
    for (var i = 0; i < outer.classList.length; i++) {
      var cls = outer.classList[i];
      if (cls.startsWith('language-')) {
        lang = cls;
        break;
      }
    }
    if (!lang) {
      cls = block.getAttribute('class');
      lang = cls ? cls.replace('hljs ', '') : '';
    }
    if (lang.startsWith('language-')) {
      lang = lang.substr(9);
    }
    block.setAttribute('class', 'hljs ' + lang);
    block.parentNode.setAttribute('data-lang', lang);
  }

  function addBadge(block) {
    var enabled = ('true' || 'true').toLowerCase();
    if (enabled == 'true') {
      var pre = block.parentElement;
      pre.classList.add('badge');
    }
  }

  function handle(block) {
    addLangData(block);
    addBadge(block)
    hljs.highlightBlock(block);
  }

  for (var i = 0; i < els.length; i++) {
    var el = els[i];
    handle(el);
  }
});
</script>

<style>
  /* code language badge */
  pre.badge::before {
    content: attr(data-lang);
    color: #fff;
    background-color: #ff4e00;
    padding: 0 .5em;
    border-radius: 0 2px;
    text-transform: uppercase;
    text-align: center;
    min-width: 32px;
    display: inline-block;
    position: absolute;
    right: 0;
  }

  /* fix wrong badge display for firefox browser */
  code > table pre::before {
    display: none;
  }
</style>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe-lightbox.umd.min.js" async></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe.umd.min.js" async></script>
<link href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/photoswipe.min.css" rel="stylesheet">
<style>
  .pswp .pswp__container .pswp__img {
    background-color: white;
  }
</style>

<script>
  function initPhotoSwipe() {
    let customOptions = {};

    try {
      const data = `{"gallery"=>"section.main", "children"=>"a.photo-swipe", "bgOpacity"=>0.8, "padding"=>{"top"=>20, "bottom"=>40, "left"=>100, "right"=>100}}`.replaceAll("=>", ":");
      customOptions = JSON.parse(data);
    } catch (e) {
      console.info("Invalid custom photo previewer options! " + e.message);
    }

    // Define object and gallery options
    const options = Object.assign(
      {
        gallery: "section.main",
        children: "a.photo-swipe",
        photo_scale: 2,
        // dynamic import is not supported in UMD version
        pswpModule: PhotoSwipe,
      },
      customOptions
    );

    const galleryEl = document.querySelector(options.gallery);
    if (!galleryEl) {
      return;
    }

    const imgEls = [];
    const els = galleryEl.querySelectorAll("img:not(.emoji)");
    els.forEach((el) => {
      if (el.src.trim() == "") {
        return;
      }
      if (!imgEls.includes(el)) {
        imgEls.push(el);
      }
    });

    if (imgEls.length === 0) {
      return;
    }

    imgEls.forEach((imgEl) => {
      imgEl.outerHTML = `
      <a class="photo-swipe"
        href="${imgEl.src}"
        data-pswp-width="${
          Math.max(imgEl.naturalWidth, imgEl.width) * options.photo_scale
        }"
        data-pswp-height="${
          Math.max(imgEl.naturalHeight, imgEl.height) * options.photo_scale
        }"
        data-pswp-caption="${imgEl.getAttribute("caption") || imgEl.alt}"
        target="_blank">
        ${imgEl.outerHTML}
      </a>`;
    });

    // Initialize PhotoSwipe 5
    var lightbox = new PhotoSwipeLightbox(options);

    lightbox.init();
  }

  window.addEventListener("load", initPhotoSwipe);
</script>
<meta name="google-site-verification" content="u_DTTPcCZ806iq51zgirHyWq3556HUKGq8AQfH91iFI">
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>





































































































































<header class="site-header site-header-transparent" role="banner">

  <div class="wrapper">
    <div class="site-header-inner">
<span class="site-brand"><a class="site-brand-inner" rel="author" href="/paper_notes/">
  <img class="site-favicon" title="わたしのべんきょうノート" src="" onerror="this.style.display='none'">
  わたしのべんきょうノート
</a>
</span><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger">
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewbox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
              </svg>
            </span>
          </label>

          <div class="trigger">









<span class="page-link">



<div id="google_translate_element" style="display: none;">
</div>

<span class="ct-language">
  <ul class="list-unstyled ct-language-dropdown">
    
      <li>
        <a href="#" class="lang-select" data-lang="en">
          
          <img src="https://cdn.countryflags.com/thumbs/united-states-of-america/flag-400.png" title="English">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="fr">
          
          <img src="https://cdn.countryflags.com/thumbs/france/flag-400.png" title="French">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="zh-CN">
          
          <img src="https://cdn.countryflags.com/thumbs/china/flag-400.png" title="Chinese(Simple)">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ja">
          
          <img src="https://cdn.countryflags.com/thumbs/japan/flag-400.png" title="Japanese">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ko">
          
          <img src="https://cdn.countryflags.com/thumbs/south-korea/flag-400.png" title="Korean">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ru">
          
          <img src="https://cdn.countryflags.com/thumbs/russia/flag-400.png" title="Russian">
          
        </a>
      </li>
    
  </ul>
</span>

<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({
    pageLanguage: 'ja',
    autoDisplay: false,
    layout: google.translate.TranslateElement.InlineLayout.VERTICAL
  }, 'google_translate_element');

  // Links to cross-origin destinations are unsafe
  var gll = document.getElementsByClassName('goog-logo-link')[0];
  if (gll) {
    gll.setAttribute('rel', 'noopener');
  }

  function restoreLang() {
    var iframe = document.getElementsByClassName('goog-te-banner-frame')[0];
    if (!iframe) return;

    var innerDoc = iframe.contentDocument || iframe.contentWindow.document;
    var restore_el = innerDoc.getElementsByTagName("button");

    for (var i = 0; i < restore_el.length; i++) {
      if (restore_el[i].id.indexOf("restore") >= 0) {
        restore_el[i].click();
        var close_el = innerDoc.getElementsByClassName("goog-close-link");
        close_el[0].click();
        return;
      }
    }
  }

  function triggerHtmlEvent(element, eventName) {
    var event;
    if (document.createEvent) {
      event = document.createEvent('HTMLEvents');
      event.initEvent(eventName, true, true);
      element.dispatchEvent(event);
    } else {
      event = document.createEventObject();
      event.eventType = eventName;
      element.fireEvent('on' + event.eventType, event);
    }
  }

  var googleCombo = document.querySelector("select.goog-te-combo");
  var langSelect = document.querySelector('.ct-language');
  langSelect.addEventListener('click', function(event) {
    if (!event.target) {
      return;
    }

    var selected = document.querySelector('.ct-language .ct-language-selected');
    if (selected) {
      selected.classList.remove('ct-language-selected');
    }

    var target = event.target;
    while (target && target !== langSelect ) {
      if (target.matches('.lang-select')) {
        break;
      }
      target = target.parentElement;
    }

    if (target && target.matches('.lang-select')) {
      var lang = target.getAttribute('data-lang');
      if (googleCombo.value == lang) {
        restoreLang();
      } else {
        target.parentElement.classList.add('ct-language-selected');
        googleCombo.value = lang;
        triggerHtmlEvent(googleCombo, 'change');
      }
    }

    event.preventDefault();
  });
}
</script>

<script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit" async></script>
</span>
</div>
        </nav>
</div>
  </div>
</header>

<script>
  function initHeader() {
    var lastScrollY = getScrollPos().y;
    var documentElement = document.documentElement;

    function storeScrollData() {
      var y = getScrollPos().y;documentElement.setAttribute("data-header-transparent", "");var scrollStatus = "";

      if (y <= 0) {
        scrollStatus = "top";
      } else if ((window.innerHeight + y) >= document.body.offsetHeight) {
        scrollStatus = "bottom";
      } else {
        var isScrollDown = (y - lastScrollY > 0) ? true : false;
        scrollStatus = isScrollDown ? "down" : "up";
      }

      lastScrollY = y;
      documentElement.setAttribute("data-scroll-status", scrollStatus);
    }

    window.addEventListener('scroll', function(e) {
      storeScrollData();
    });

    storeScrollData();
  }
  document.addEventListener('DOMContentLoaded', initHeader);
</script>


























































































































































<style>
    html .page-banner .page-banner-img > *:first-child {
      opacity: 0.4;
    }

    html[data-theme="dark"] .page-banner .page-banner-img > *:first-child {
      opacity: 0.2872;
    }
  </style>
<section class="page-banner">
    <div class="page-banner-img">
<div style="background-image: url(/paper_notes/assets/images/banner.png)"></div>
        <img class="img-placeholder" src="/paper_notes/assets/images/banner.png">
</div>
    <div class="wrapper">
      <div class="page-banner-inner">
<header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">わたしのべんきょうノート</h1>
  <h2 class="post-subtitle">勉強した論文や技術等の情報をGithubのIssueにメモっているひとのブログ。
それなりにメモの量が蓄積されてきたので、一度整理したいなと思いブログはじめてみました！
自然言語処理(NLP), 推薦システム(RecommenderSystem), Educational Data Mining (EDM), Learning Analytics (LA)などの分野のメモが多いと思います。
最近は特にLLMの勉強が多めです :)</h2>

  <div class="post-meta">
    <time class="dt-published" datetime="2025-08-05T00:56:41+00:00" itemprop="datePublished"><i class="fa fa-calendar"></i> Aug 5, 2025
    </time><span class="post-author left-vsplit"><i class="fa fa-pencil"></i> AkihikoWATANABE</span>
    
































    <span class="post-reading-time left-vsplit"><i class="fa fa-clock-o"></i> About 3 hours 49 mins</span>
  </div></header>
</div>
    </div>
  </section><script>
  function hashLocate(hashValue) {
    hashValue = hashValue.replace(/^.*#h-/, '');
    hashValue = decodeURIComponent(hashValue);
    var element = document.getElementById(hashValue);

    if (!element) {
      return;
    }

    var header = document.querySelector('header.site-header');
    var headerRect = header.getBoundingClientRect();
    var headerTop = Math.floor(headerRect.top);
    var headerHeight = Math.floor(headerRect.height);
    var scrollPos = getScrollPos();
    var offsetY = element.offsetTop - (headerTop + headerHeight + 20);

    if (offsetY == scrollPos.y) {
      return;
    }

    if (headerTop == 0  && offsetY > scrollPos.y) {
      offsetY += headerHeight + 2;
    } else if (headerTop < 0  && offsetY < scrollPos.y) {
      offsetY -= headerHeight - 2;
    }

    smoothScrollTo(offsetY);
  }

  // The first event occurred
  window.addEventListener('load', function(event) {
    if (window.location.hash) {
      hashLocate(window.location.hash);
    }
  });

  // The first event occurred
  window.addEventListener('click', function(event) {
    if (event.target.tagName.toLowerCase() == 'a') {
      hashLocate(event.target.getAttribute('href'));
    }
  });
</script>
<div class="theme-toggle">
  <input type="checkbox" id="theme-switch">
  <label for="theme-switch">
    <div class="toggle"></div>
    <div class="names">
      <p class="light">Light</p>
      <p class="dark">Dark</p>
    </div>
  </label>
</div>




<script>
  (function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var nightModeOption = ('off' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
      storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
      var data = storage.getItem('theme');
      try {
        data = JSON.parse(data ? data : '');
      } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
      }
      return data;
    }

    function handleThemeToggle(nightShift) {
      themeData.nightShift = nightShift;
      saveThemeData(themeData);
      html.dataset.theme = nightShift ? 'dark' : 'light';
      setTimeout(function() {
        sw.checked = nightShift ? true : false;
      }, 50);
    }

    function autoThemeToggle() {
      // Next time point of theme toggle
      var now = new Date();
      var toggleAt = new Date();
      var hours = now.getHours();
      var nightShift = hours >= 19 || hours <=7;

      if (nightShift) {
        if (hours > 7) {
          toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
      } else {
        toggleAt.setHours(19);
      }

      toggleAt.setMinutes(0);
      toggleAt.setSeconds(0);
      toggleAt.setMilliseconds(0)

      var delay = toggleAt.getTime() - now.getTime();

      // auto toggle theme mode
      setTimeout(function() {
        handleThemeToggle(!nightShift);
      }, delay);

      return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
      };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
      handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
      var data = autoThemeToggle();

      // Toggle theme by local setting
      if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
      } else {
        handleThemeToggle(themeData.nightShift);
      }
    } else if (nightModeOption == 'manual') {
      handleThemeToggle(themeData.nightShift);
    } else {
      var nightShift = themeData.nightShift;
      if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
      }
      handleThemeToggle(nightShift);
    }
  })();
</script>
<div id="click-to-top" class="click-to-top">
  <i class="fa fa-arrow-up"></i>
</div>
<script>
  (function () {
    const clickToTop = document.getElementById('click-to-top');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 100) {
        clickToTop.classList.add('show')
      }else {
        clickToTop.classList.remove('show')
      }
    });
    clickToTop.addEventListener('click', () => {
      window.smoothScrollTo(0);
    });
  })();
</script>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="framework">
  <section class="main">

     <div class="post">
  <section>









<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="post-content e-content" itemprop="articleBody">

      <h2 id="machinelearning">MachineLearning</h2>

<div class="visible-content">
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>


<br>


<span class="issue_date">Issue Date: 2025-08-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2350">[Paper Note] MLE-STAR: Machine Learning Engineering Agent via Search and Targeted  Refinement, Jaehyun Nam+, arXiv'25</a>
<span class="snippet"><span>Summary</span>MLE-STARは、LLMを用いてMLモデルを自動実装する新しいアプローチで、ウェブから効果的なモデルを取得し、特定のMLコンポーネントに焦点を当てた戦略を探索することで、コード生成の精度を向上させる。実験結果では、MLE-STARがKaggleコンペティションの64%でメダルを獲得し、他の手法を大きく上回る性能を示した。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/marktechpost/status/1951846630266687927?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/Robotics.html">#Robotics</a>


<br>


<span class="issue_date">Issue Date: 2025-07-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2257">[Paper Note] What Matters in Learning from Large-Scale Datasets for Robot   Manipulation, Vaibhav Saxena+, ICLR'25</a>
<span class="snippet"><span>Summary</span>本研究では、ロボティクスにおける大規模データセットの構成に関する体系的な理解を深めるため、データ生成フレームワークを開発し、多様性の重要な要素を特定。特に、カメラのポーズや空間的配置がデータ収集の多様性と整合性に影響を与えることを示した。シミュレーションからの洞察が実世界でも有効であり、提案した取得戦略は既存のトレーニング手法を最大70%上回る性能を発揮した。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/saxenavaibhav11/status/1946209076305691084?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q元ポストに著者による詳細な解説スレッドがあるので参照のこと。

<br>

<img src="https://github.com/user-attachments/assets/175bc31f-de80-4ad6-aa92-afacc1328345" alt="image" loading="lazy"></span>
<a class="button" href="articles/Embeddings.html">#Embeddings</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/RepresentationLearning.html">#RepresentationLearning</a>


<br>


<span class="issue_date">Issue Date: 2025-07-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2239">[Paper Note] Learning distributed representations with efficient SoftMax   normalization, Lorenzo Dall'Amico+, TMLR'25</a>
<span class="snippet"><span>Summary</span>埋め込みを学習するための損失関数として${\rm SoftMax}(XY^T)$を最適化する際の計算負荷を軽減するため、ノルム制限された埋め込みベクトルに対して線形時間のヒューリスティック近似を提案。提案手法は、事前学習されたデータセットで高い精度を示し、クロスエントロピーを最適化する効率的なアルゴリズムを設計。これにより、解釈可能でタスクに依存しない埋め込み学習が可能となり、類似の「2Vec」アルゴリズムと比較して優れた性能と低い計算時間を実現。</span>
<span class="snippet"><span>Comment</span>openreview:https://openreview.net/forum?id=9M4NKMZOPu</span>
</div>
<p><button onclick="showMore(0)">more</button></p>
<div class="hidden-content">
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2025-07-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2237">[Paper Note] In-context denoising with one-layer transformers: connections between  attention and associative memory retrieval, Matthew Smart+, arXiv'25</a>
<span class="snippet"><span>Summary</span>「インコンテキストデノイジング」というタスクを通じて、注意ベースのアーキテクチャと密な連想記憶（DAM）ネットワークの関係を探求。ベイズ的フレームワークを用いて、単層トランスフォーマーが特定のデノイジング問題を最適に解決できることを示す。訓練された注意層は、コンテキストトークンを連想記憶として利用し、デノイジングプロンプトを一回の勾配降下更新で処理。これにより、DAMネットワークの新たな拡張例を提供し、連想記憶と注意メカニズムの関連性を強化する。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/hillbig/status/1945253873456963841?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q関連:

<br>

・2146</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<a class="button" href="articles/GraphGeneration.html">#GraphGeneration</a>
<span class="issue_date">Issue Date: 2025-07-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2227">[Paper Note] Learning-Order Autoregressive Models with Application to Molecular Graph   Generation, Zhe Wang+, ICML'25</a>
<span class="snippet"><span>Summary</span>自己回帰モデル（ARMs）を用いて、データから逐次的に推測される確率的順序を利用し、高次元データを生成する新しい手法を提案。トレーニング可能なオーダーポリシーを組み込み、対数尤度の変分下限を用いて最適化。実験により、画像生成やグラフ生成で意味のある自己回帰順序を学習し、分子グラフ生成ではQM9およびZINC250kベンチマークで最先端の結果を達成。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/thjashin/status/1945175804704645607?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qopenreview:https://openreview.net/forum?id=EY6pXIDi3G</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Optimizer.html">#Optimizer</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2025-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2202">[Paper Note] Muon is Scalable for LLM Training, Jingyuan Liu+, arXiv'25</a>
<span class="snippet"><span>Summary</span>Muonオプティマイザーを大規模モデルにスケールアップするために、ウェイトデケイとパラメータごとの更新スケール調整を導入。これにより、Muonは大規模トレーニングで即座に機能し、計算効率がAdamWの約2倍に向上。新たに提案するMoonlightモデルは、少ないトレーニングFLOPで優れたパフォーマンスを達成し、オープンソースの分散Muon実装や事前トレーニング済みモデルも公開。</span>
<span class="snippet"><span>Comment</span>解説ポスト:https://x.com/hillbig/status/1944902706747072678?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qこちらでも紹介されている:

<br>

・2208</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<span class="issue_date">Issue Date: 2025-07-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2198">[Paper Note] Nonlinear transformers can perform inference-time feature learning, Nishikawa+, ICML'25</a>
<span class="snippet"><span>Summary</span>事前学習されたトランスフォーマーは、推論時に特徴を学習する能力を持ち、特に単一インデックスモデルにおける文脈内学習に焦点を当てています。勾配ベースの最適化により、異なるプロンプトからターゲット特徴を抽出し、非適応的アルゴリズムを上回る統計的効率を示します。また、推論時のサンプル複雑性が相関統計クエリの下限を超えることも確認されました。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/btreetaiji/status/1944297631808991742?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/LearningPhenomena.html">#LearningPhenomena</a>
<span class="issue_date">Issue Date: 2025-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2187">[Paper Note] Not All Explanations for Deep Learning Phenomena Are Equally Valuable, Alan Jeffares+, PMLR'25</a>
<span class="snippet"><span>Summary</span>深層学習の驚くべき現象（ダブルディセント、グロッキングなど）を孤立したケースとして説明することには限界があり、実世界のアプリケーションにはほとんど現れないと主張。これらの現象は、深層学習の一般的な原則を洗練するための研究価値があると提案し、研究コミュニティのアプローチを再考する必要性を示唆。最終的な実用的目標に整合するための推奨事項も提案。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/jeffaresalan/status/1943315797692109015?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q関連:

<br>

・2189

<br>

・524

<br>

・2190</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<span class="issue_date">Issue Date: 2025-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2185">[Paper Note] Mixture of Experts Provably Detect and Learn the Latent Cluster   Structure in Gradient-Based Learning, Ryotaro Kawata+, ICML'25</a>
<span class="snippet"><span>Summary</span>Mixture of Experts (MoE)は、入力を専門家に動的に分配するモデルのアンサンブルであり、機械学習で成功を収めているが、その理論的理解は遅れている。本研究では、MoEのサンプルおよび実行時間の複雑さを回帰タスクにおけるクラスタ構造を通じて理論的に分析し、バニラニューラルネットワークがこの構造を検出できない理由を示す。MoEは各専門家の能力を活用し、問題をより単純なサブ問題に分割することで、非線形回帰におけるSGDのダイナミクスを探求する初めての試みである。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/btreetaiji/status/1943226334463086989?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<a class="button" href="articles/VideoGeneration/Understandings.html">#VideoGeneration/Understandings</a>
<a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2146">[Paper Note] Energy-Based Transformers are Scalable Learners and Thinkers, Alexi Gladstone+, arXiv'25</a>
<span class="snippet"><span>Summary</span>エネルギーベースのトランスフォーマー（EBTs）を用いて、無監督学習から思考を学ぶモデルを提案。EBTsは、入力と候補予測の互換性を検証し、エネルギー最小化を通じて予測を行う。トレーニング中に従来のアプローチよりも高いスケーリング率を達成し、言語タスクでの性能を29%向上させ、画像のノイズ除去でも優れた結果を示す。EBTsは一般化能力が高く、モデルの学習能力と思考能力を向上させる新しいパラダイムである。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/hillbig/status/1941657099567845696?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QProject Page:https://energy-based-transformers.github.ioFirst Authorの方による解説ポスト:https://x.com/alexiglad/status/1942231878305714462?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-06-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2035">[Paper Note] Resa: Transparent Reasoning Models via SAEs, Shangshang Wang+, arXiv'25</a>
<span class="snippet"><span>Summary</span>Resaという1.5Bの推論モデル群を提案し、効率的なスパースオートエンコーダーチューニング（SAE-Tuning）手法を用いて訓練。これにより、97%以上の推論性能を保持しつつ、訓練コストを2000倍以上削減し、訓練時間を450倍以上短縮。軽いRL訓練を施したモデルで高い推論性能を実現し、抽出された推論能力は一般化可能かつモジュール化可能であることが示された。全ての成果物はオープンソース。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/iscienceluvr/status/1933101904529363112?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q著者ポスト:https://x.com/upupwang/status/1933207676663865482?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q論文中で利用されているSource Modelの一つ:

<br>

・1935</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<span class="issue_date">Issue Date: 2025-06-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2024">[Paper Note] Horizon Reduction Makes RL Scalable, Seohong Park+, arXiv'25</a>
<span class="snippet"><span>Summary</span>本研究では、オフライン強化学習（RL）のスケーラビリティを検討し、既存のアルゴリズムが大規模データセットに対して期待通りの性能を発揮しないことを示しました。特に、長いホライズンがスケーリングの障壁であると仮定し、ホライズン削減技術がスケーラビリティを向上させることを実証しました。新たに提案した手法SHARSAは、ホライズンを削減しつつ優れたパフォーマンスを達成し、オフラインRLのスケーラビリティを向上させることを示しました。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/hillbig/status/1932205263446245798?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<a class="button" href="articles/KnowledgeEditing.html">#KnowledgeEditing</a>
<span class="issue_date">Issue Date: 2025-06-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2023">[Paper Note] Representation Shattering in Transformers: A Synthetic Study with   Knowledge Editing, Kento Nishi+, ICML'25</a>
<span class="snippet"><span>Summary</span>知識編集（KE）アルゴリズムは、モデルの重みを変更して不正確な事実を更新するが、これがモデルの事実の想起精度や推論能力に悪影響を及ぼす可能性がある。新たに定義した合成タスクを通じて、KEがターゲットエンティティを超えて他のエンティティの表現に影響を与え、未見の知識の推論を歪める「表現の破壊」現象を示す。事前訓練されたモデルを用いた実験でもこの発見が確認され、KEがモデルの能力に悪影響を及ぼす理由を明らかにするメカニズム仮説を提供する。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/kento_nishi/status/1932072335726539063?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ModelMerge.html">#ModelMerge</a>
<span class="issue_date">Issue Date: 2025-05-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1979">Model Merging in Pre-training of Large Language Models, Yunshui Li+, arXiv'25</a>
<span class="snippet"><span>Summary</span>モデルマージングは大規模言語モデルの強化に有望な技術であり、本論文ではその事前学習プロセスにおける包括的な調査を行う。実験により、一定の学習率で訓練されたチェックポイントをマージすることで性能向上とアニーリング挙動の予測が可能になることを示し、効率的なモデル開発と低コストのトレーニングに寄与する。マージ戦略やハイパーパラメータに関するアブレーション研究を通じて新たな洞察を提供し、実用的な事前学習ガイドラインをオープンソースコミュニティに提示する。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/iscienceluvr/status/1924804324812873990?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q解説ポスト:https://x.com/giffmana/status/1924849877634449878?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Alignment.html">#Alignment</a>
<a class="button" href="articles/Hallucination.html">#Hallucination</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/DPO.html">#DPO</a>
<a class="button" href="articles/Repetition.html">#Repetition</a>
<span class="issue_date">Issue Date: 2025-04-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1892">Learning Dynamics of LLM Finetuning, Yi Ren+, ICLR'25</a>
<span class="snippet"><span>Summary</span>本研究では、大規模言語モデルのファインチューニング中の学習ダイナミクスを分析し、異なる応答間の影響の蓄積を段階的に解明します。指示調整と好み調整のアルゴリズムに関する観察を統一的に解釈し、ファインチューニング後の幻覚強化の理由を仮説的に説明します。また、オフポリシー直接好み最適化（DPO）における「圧縮効果」を強調し、望ましい出力の可能性が低下する現象を探ります。このフレームワークは、LLMのファインチューニング理解に新たな視点を提供し、アラインメント性能向上のためのシンプルな方法を示唆します。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/joshuarenyi/status/1913033476275925414?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q解説ポスト:https://x.com/hillbig/status/1917189793588613299?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<span class="issue_date">Issue Date: 2025-04-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1876">VAPO: Efficient and Reliable Reinforcement Learning for Advanced  Reasoning Tasks, YuYue+, arXiv'25</a>
<span class="snippet"><span>Summary</span>VAPO（Value-based Augmented Proximal Policy Optimization framework）を提案し、AIME 2024データセットで最先端のスコア60.4を達成。VAPOは他の手法を10ポイント以上上回り、5,000ステップで安定したパフォーマンスを示す。価値ベースの強化学習における3つの課題を特定し、VAPOがそれらを軽減する統合ソリューションを提供することで、長い思考過程の推論タスクの性能向上を実現。</span>
<span class="snippet"><span>Comment</span>同じくByteDanceの

<br>



<br>

・1815

<br>



<br>

を上回る性能

<br>

<img src="https://github.com/user-attachments/assets/51f7a43a-9410-45f3-989c-4e0b1fdd86ef" alt="image" loading="lazy">元ポスト:https://x.com/_akhaliq/status/1909564500170223751?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/GRPO.html">#GRPO</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-03-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1815">DAPO: An Open-Source LLM Reinforcement Learning System at Scale, Qiying Yu+, arXiv'25</a>
<span class="snippet"><span>Summary</span>推論スケーリングによりLLMの推論能力が向上し、強化学習が複雑な推論を引き出す技術となる。しかし、最先端の技術詳細が隠されているため再現が難しい。そこで、$\textbf{DAPO}$アルゴリズムを提案し、Qwen2.5-32Bモデルを用いてAIME 2024で50ポイントを達成。成功のための4つの重要技術を公開し、トレーニングコードと処理済みデータセットをオープンソース化することで再現性を向上させ、今後の研究を支援する。</span>
<span class="snippet"><span>Comment</span>既存のreasoning modelのテクニカルレポートにおいて、スケーラブルなRLの学習で鍵となるレシピは隠されていると主張し、実際彼らのbaselineとしてGRPOを走らせたところ、DeepSeekから報告されているAIME2024での性能（47ポイント）よりもで　大幅に低い性能（30ポイント）しか到達できず、分析の結果3つの課題（entropy collapse, reward noise, training instability）を明らかにした（実際R1の結果を再現できない報告が多数報告されており、重要な訓練の詳細が隠されているとしている）。

<br>



<br>

その上で50%のtrainikg stepでDeepSeek-R1-Zero-Qwen-32Bと同等のAIME 2024での性能を達成できるDAPOを提案。そしてgapを埋めるためにオープンソース化するとのこと。ちとこれはあとでしっかり読みたい。重要論文。プロジェクトページ:https://dapo-sia.github.io/

<br>



<br>

こちらにアルゴリズムの重要な部分の概要が説明されている。解説ポスト:https://x.com/theturingpost/status/1902507148015489385?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>



<br>

コンパクトだが分かりやすくまとまっている。下記ポストによると、Reward Scoreに多様性を持たせたい場合は3.2節参照とのこと。

<br>

すなわち、Dynamic Samplingの話で、Accが全ての生成で1.0あるいは0.0となるようなpromptを除外するといった方法の話だと思われる。

<br>

これは、あるpromptに対する全ての生成で正解/不正解になった場合、そのpromptに対するAdvantageが0となるため、ポリシーをupdateするためのgradientも0となる。そうすると、このサンプルはポリシーの更新に全く寄与しなくなるため、同バッチ内のノイズに対する頑健性が失われることになる。サンプル効率も低下する。特にAccが1.0になるようなpromptは学習が進むにつれて増加するため、バッチ内で学習に有効なpromptは減ることを意味し、gradientの分散の増加につながる、といったことらしい。

<br>



<br>

関連ポスト:https://x.com/iscienceluvr/status/1936375947575632102?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/CVPR.html">#CVPR</a>
<a class="button" href="articles/Normalization.html">#Normalization</a>
<span class="issue_date">Issue Date: 2025-03-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1795">Transformers without Normalization, Jiachen Zhu+, CVPR'25</a>
<span class="snippet"><span>Summary</span>本研究では、正規化層なしのトランスフォーマーがDynamic Tanh（DyT）を用いることで、同等またはそれ以上のパフォーマンスを達成できることを示します。DyTは、レイヤー正規化の代替として機能し、ハイパーパラメータの調整なしで効果を発揮します。多様な設定での実験により、正規化層の必要性に対する新たな洞察を提供します。</span>
<span class="snippet"><span>Comment</span>なん…だと…。LayerNormalizationを下記アルゴリズムのようなtanhを用いた超絶シンプルなレイヤー（parameterized thnh [Lecun氏ポスト](https://x.com/ylecun/status/1900610590315249833?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q)）に置換するだけっぽい？

<br>

<img src="https://github.com/user-attachments/assets/474d3ee4-4c08-4b00-9a41-126ca5d5207e" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/5aea9f93-85d9-4e0b-b9db-bb407d596493" alt="image" loading="lazy">

<br>



<br>

同等以上の性能を維持しながらモデル全体のinference, trainingの時間を8%程度削減。

<br>

<img src="https://github.com/user-attachments/assets/98f8caa3-3ef2-4594-a45a-ae0aa2cf2ef6" alt="image" loading="lazy"></span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-03-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1775">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse   Attention, Jingyang Yuan+, ACL'25</a>
<span class="snippet"><span>Summary</span>長文コンテキストモデリングのために、計算効率を改善するスパースアテンションメカニズム「NSA」を提案。NSAは動的な階層スパース戦略を用い、トークン圧縮と選択を組み合わせてグローバルなコンテキスト認識とローカルな精度を両立。実装最適化によりスピードアップを実現し、エンドツーエンドのトレーニングを可能にすることで計算コストを削減。NSAはフルアテンションモデルと同等以上の性能を維持しつつ、長シーケンスに対して大幅なスピードアップを達成。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/dair_ai/status/1893698286545969311?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QACL'25のBest Paperの一つ:

<br>

https://x.com/gm8xx8/status/1950644063952052643?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-01-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1740">SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model  Post-training, Tianzhe Chu+, arXiv'25</a>
<span class="snippet"><span>Summary</span>SFTとRLの一般化能力の違いを研究し、GeneralPointsとV-IRLを用いて評価。RLはルールベースのテキストと視覚変種に対して優れた一般化を示す一方、SFTは訓練データを記憶し分布外シナリオに苦労。RLは視覚認識能力を向上させるが、SFTはRL訓練に不可欠であり、出力形式を安定させることで性能向上を促進。これらの結果は、複雑なマルチモーダルタスクにおけるRLの一般化能力を示す。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/hillbig/status/1884731381517082668?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/Batch.html">#Batch</a>
<span class="issue_date">Issue Date: 2024-11-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1541">How Does Critical Batch Size Scale in Pre-training?, Hanlin Zhang+, ICLR'25</a>
<span class="snippet"><span>Summary</span>大規模モデルの訓練には、クリティカルバッチサイズ（CBS）を考慮した並列化戦略が重要である。CBSの測定法を提案し、C4データセットで自己回帰型言語モデルを訓練。バッチサイズや学習率などの要因を調整し、CBSがデータサイズに比例してスケールすることを示した。この結果は、ニューラルネットワークの理論的分析によって支持され、ハイパーパラメータ選択の重要性も強調されている。</span>
<span class="snippet"><span>Comment</span>Critical Batch Sizeはモデルサイズにはあまり依存せず、データサイズに応じてスケールする

<br>

<img src="https://github.com/user-attachments/assets/4a1a720f-37a1-485d-9b02-bb2e8a5c2da4" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/8bc5f621-caac-438a-afd1-de1d689ee210" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<a class="button" href="articles/TMLR.html">#TMLR</a>
<span class="issue_date">Issue Date: 2025-06-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2041">[Paper Note] Beyond Human Data: Scaling Self-Training for Problem-Solving with   Language Models, Avi Singh+, TMLR'24</a>
<span class="snippet"><span>Summary</span>言語モデルを人間データでファインチューニングする際の限界を超えるため、ReST$^{EM$という自己学習手法を提案。モデルから生成したサンプルをバイナリフィードバックでフィルタリングし、繰り返しファインチューニングを行う。PaLM-2モデルを用いた実験で、ReST$^{EM}$は人間データのみのファインチューニングを大幅に上回る性能を示し、フィードバックを用いた自己学習が人間生成データへの依存を減少させる可能性を示唆。</span>
<span class="snippet"><span>Comment</span>解説ポスト:https://x.com/hillbig/status/1735065077668356106?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<span class="issue_date">Issue Date: 2024-12-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1580">Reinforcement Learning: An Overview, Kevin Murphy, arXiv'24</a>
<span class="snippet"><span>Summary</span>この原稿は、深層強化学習と逐次的意思決定に関する最新の全体像を提供し、価値ベースのRL、ポリシー勾配法、モデルベース手法、RLとLLMsの統合について簡潔に議論しています。</span>
<span class="snippet"><span>Comment</span>あのMurphy本で有名なMurphy氏の強化学習の教科書…だと…</span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Subword.html">#Subword</a>
<a class="button" href="articles/Tokenizer.html">#Tokenizer</a>
<span class="issue_date">Issue Date: 2024-11-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1507">LBPE: Long-token-first Tokenization to Improve Large Language Models, Haoran Lian+, arXiv'24</a>
<span class="snippet"><span>Summary</span>LBPEは、長いトークンを優先する新しいエンコーディング手法で、トークン化データセットにおける学習の不均衡を軽減します。実験により、LBPEは従来のBPEを一貫して上回る性能を示しました。</span>
<span class="snippet"><span>Comment</span>BPEとは異なりトークンの長さを優先してマージを実施することで、最終的なトークンを決定する手法で、

<br>

<img src="https://github.com/user-attachments/assets/99b91472-88d8-4792-bf04-acc67956e4f5" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/99103316-bd1c-448d-b52a-5db815298e7e" alt="image" loading="lazy">

<br>



<br>

BPEよりも高い性能を獲得し、

<br>

<img src="https://github.com/user-attachments/assets/c7dccf00-b9c2-4739-82f3-4f8eeacd4fc7" alt="image" loading="lazy">

<br>



<br>

トークンの長さがBPEと比較して長くなり、かつ5Bトークン程度を既存のBPEで事前学習されたモデルに対して継続的事前学習するだけで性能を上回るようにでき、

<br>

<img src="https://github.com/user-attachments/assets/10f4ff2e-1d49-4c8a-87ec-67466bdce2f0" alt="image" loading="lazy">

<br>



<br>

同じVocabサイズでBPEよりも高い性能を獲得できる手法

<br>

<img src="https://github.com/user-attachments/assets/5e19fc11-10f6-467a-ae06-8fb62b5f0a65" alt="image" loading="lazy">

<br>



<br>

らしい</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/InstructionTuning.html">#InstructionTuning</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/Catastrophic%20Forgetting.html">#Catastrophic Forgetting</a>
<span class="issue_date">Issue Date: 2024-11-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1502">Online-LoRA: Task-free Online Continual Learning via Low Rank Adaptation, Xiwen Wei+, arXiv'24</a>
<span class="snippet"><span>Summary</span>破滅的忘却に対処するため、タスクフリーのオンライン継続学習（OCL）フレームワークOnline-LoRAを提案。リハーサルバッファの制約を克服し、事前学習済みビジョントランスフォーマー（ViT）モデルをリアルタイムで微調整。新しいオンライン重み正則化戦略を用いて重要なモデルパラメータを特定し、データ分布の変化を自動認識。多様なベンチマークデータセットで優れた性能を示す。</span>
<span class="snippet"><span>Comment</span><img src="https://github.com/user-attachments/assets/b789ba71-3941-4d60-9397-46607ddc7712" alt="image" loading="lazy"></span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html">#PEFT(Adaptor/LoRA)</a>
<span class="issue_date">Issue Date: 2024-11-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1492">LoRA vs Full Fine-tuning: An Illusion of Equivalence, Reece Shuttleworth+, arXiv'24</a>
<span class="snippet"><span>Summary</span>ファインチューニング手法の違いが事前学習済みモデルに与える影響を、重み行列のスペクトル特性を通じて分析。LoRAと完全なファインチューニングは異なる構造の重み行列を生成し、LoRAモデルは新たな高ランクの特異ベクトル（侵入次元）を持つことが判明。侵入次元は一般化能力を低下させるが、同等の性能を達成することがある。これにより、異なるファインチューニング手法がパラメータ空間の異なる部分にアクセスしていることが示唆される。</span>
<span class="snippet"><span>Comment</span>元ポスト: https://x.com/aratako_lm/status/1854838012909166973?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q1423 や 1475 、双方の知見も交えて、LoRAの挙動を考察する必要がある気がする。それぞれ異なるデータセットやモデルで、LoRAとFFTを比較している。時間がないが後でやりたい。

<br>



<br>

あと、昨今はそもそも実験設定における変数が多すぎて、とりうる実験設定が多すぎるため、個々の論文の知見を鵜呑みにして一般化するのはやめた方が良い気がしている。実験設定の違い

<br>

モデルのアーキテクチャ

<br>

・本研究: RoBERTa-base（transformer-encoder）

<br>

・1423: transformer-decoder

<br>

・1475: transformer-decoder（LLaMA）

<br>



<br>

パラメータサイズ

<br>

・本研究: 

<br>

・1423: 1B, 2B, 4B, 8B, 16B

<br>

・1475: 7B

<br>



<br>

時間がある時に続きをかきたい

<br>



<br>

Finetuningデータセットのタスク数

<br>



<br>

1タスクあたりのデータ量

<br>



<br>

trainableなパラメータ数</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Optimizer.html">#Optimizer</a>
<span class="issue_date">Issue Date: 2024-11-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1482">ADOPT: Modified Adam Can Converge with Any $β_2$ with the Optimal   Rate, Shohei Taniguchi+, NeurIPS'24</a>
<span class="snippet"><span>Summary</span>ADOPTという新しい適応勾配法を提案し、任意のハイパーパラメータ$\beta_2$で最適な収束率を達成。勾配の二次モーメント推定からの除去と更新順序の変更により、Adamの非収束問題を解決。広範なタスクで優れた結果を示し、実装はGitHubで公開。</span>
<span class="snippet"><span>Comment</span>画像は元ツイートからの引用:

<br>

ライブラリがあるようで、1行変えるだけですぐ使えるとのこと。

<br>



<br>

<img src="https://github.com/user-attachments/assets/0fc94e14-e1c8-497b-a0f2-1d6ec96e9083" alt="image" loading="lazy">

<br>

元ツイート:https://x.com/ishohei220/status/1854051859385978979?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QAdamでは収束しなかった場合（バッチサイズが小さい場合）でも収束するようになっている模様

<br>

<img src="https://github.com/user-attachments/assets/4e02aac4-7a44-499e-9350-008a74bfd9bf" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html">#SSM (StateSpaceModel)</a>
<span class="issue_date">Issue Date: 2024-11-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1480">Stuffed Mamba: State Collapse and State Capacity of RNN-Based  Long-Context Modeling, Yingfa Chen+, arXiv'24</a>
<span class="snippet"><span>Summary</span>RNNの長いコンテキスト処理の課題を研究し、状態崩壊（SC）とメモリ容量の制限に対処。Mamba-2モデルを用いて、SC緩和手法を提案し、1Mトークン以上の処理を実現。256Kコンテキスト長で高精度のパスキー取得を達成し、RNNの長コンテキストモデリングの可能性を示唆。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<span class="issue_date">Issue Date: 2024-10-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1473">NEFTune: Noisy Embeddings Improve Instruction Finetuning, Neel Jain+, N_A, ICLR'24</a>
<span class="snippet"><span>Summary</span>NEFTuneは、埋め込みベクトルにノイズを加えることで言語モデルのファインチューニングを改善する手法です。LLaMA-2-7Bを用いた標準的なファインチューニングでは29.79%の精度でしたが、ノイジーな埋め込みを使用することで64.69%に向上しました。NEFTuneは、Evol-Instruct、ShareGPT、OpenPlatypusなどの指示データセットでも改善をもたらし、RLHFで強化されたLLaMA-2-Chatにも効果があります。</span>
<span class="snippet"><span>Comment</span>ランダムノイズをembeddingに加えて学習するシンプルな手法。モデルがロバストになる。

<br>



<br>

Unsupervised SimCSEと思想が似ている。実質DataAugmentationともみなせる。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Alignment.html">#Alignment</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<span class="issue_date">Issue Date: 2024-10-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1472">KTO: Model Alignment as Prospect Theoretic Optimization, Kawin Ethayarajh+, N_A, ICML'24</a>
<span class="snippet"><span>Summary</span>プロスペクト理論に基づき、LLMの人間フィードバック調整におけるバイアスの影響を示す。新たに提案する「人間認識損失」（HALOs）を用いたアプローチKTOは、生成物の効用を最大化し、好みベースの方法と同等またはそれ以上の性能を発揮。研究は、最適な損失関数が特定の設定に依存することを示唆。</span>
<span class="snippet"><span>Comment</span>binaryフィードバックデータからLLMのアライメントをとるKahneman-Tversky Optimization (KTO)論文</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<span class="issue_date">Issue Date: 2024-08-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1361">The Illusion of State in State-Space Models, William Merrill+, N_A, ICML'24</a>
<span class="snippet"><span>Summary</span>SSM（状態空間モデル）は、トランスフォーマーよりも優れた状態追跡の表現力を持つと期待されていましたが、実際にはその表現力は制限されており、トランスフォーマーと類似しています。SSMは複雑性クラス$\mathsf{TC}^0$の外での計算を表現できず、単純な状態追跡問題を解決することができません。このため、SSMは実世界の状態追跡問題を解決する能力に制限がある可能性があります。</span>
<span class="snippet"><span>Comment</span>&gt;しかし、SSMが状態追跡の表現力で本当に（トランスフォーマーよりも）優位性を持っているのでしょうか？驚くべきことに、その答えは「いいえ」です。私たちの分析によると、SSMの表現力は、トランスフォーマーと非常に類似して制限されています：SSMは複雑性クラス$\mathsf{TC}^0$の外での計算を表現することができません。特に、これは、置換合成のような単純な状態追跡問題を解決することができないことを意味します。これにより、SSMは、特定の表記法でチェスの手を正確に追跡したり、コードを評価したり、長い物語の中のエンティティを追跡することが証明上できないことが明らかになります。

<br>



<br>

なん…だと…</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/ModelMerge.html">#ModelMerge</a>
<span class="issue_date">Issue Date: 2024-01-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1213">Knowledge Fusion of Large Language Models, Fanqi Wan+, N_A, ICLR'24</a>
<span class="snippet"><span>Summary</span>本研究では、既存の事前訓練済みの大規模言語モデル（LLMs）を統合することで、1つの強力なモデルを作成する方法を提案しています。異なるアーキテクチャを持つ3つの人気のあるLLMsを使用して、ベンチマークとタスクのパフォーマンスを向上させることを実証しました。提案手法のコード、モデルの重み、およびデータはGitHubで公開されています。</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/EMNLP.html">#EMNLP</a>
<span class="issue_date">Issue Date: 2024-01-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1210">Transformers are Multi-State RNNs, Matanel Oren+, N_A, EMNLP'24</a>
<span class="snippet"><span>Summary</span>本研究では、トランスフォーマーのデコーダーは無限マルチステートRNNとして概念化できることを示し、有限のマルチステートRNNに変換することも可能であることを示します。さらに、新しいキャッシュ圧縮ポリシーであるTOVAを導入し、他のポリシーよりも優れた性能を示すことを実験結果で示しました。TOVAは元のキャッシュサイズの1/8しか使用せず、トランスフォーマーデコーダーLLMが実際にはRNNとして振る舞うことが多いことを示しています。</span>
<span class="snippet"><span>Comment</span>TransformerはRNNとは異なる概念、特に全てのトークンの情報に直接アクセスできるということで区別されてきたが、よくよく考えてみると、Transformer Decoderは、RNNのhidden_states h を（hは1つのstateをベクトルで表している）、multi-stateを表す matrix H （t個のstateを表すmatrix; tは現在の着目しているトークンまでのsequenceの長さ）で置き換えたもの Multi-State-RNN (MSRNN) と解釈できる、という話。

<br>

また、window attentionなどのattentionの計算で考慮するKV cacheのスパンを（メモリを節約するために）制限する圧縮手法は、先ほどのMSRNNは全トークンのstate （KV Cache）にアクセスできる（= Unbounded）と考えると、アクセスできるトークンのstateが k (&lt;t) となるため、BoundedなMSRNNとみなせる。

<br>

したがって、現在のLLMはTransformer Decoderを積み上げたものであるものであり、原理上はinference/training時に全てのトークンを考慮できるため、原理上はUnboundedなMSRNNとみなせる。一方、ここにメモリの制約が加わるとKV Cacheを圧縮しなければならないため、実用上はBoundedなMSRNNとなっている。

<br>



<br>

<img width="476" alt="Image" src="https://github.com/user-attachments/assets/292bd370-0138-441a-a626-ee73cb2f31b5">

<br>



<br>

実際に式で表すと以下のようにRNNとTransformerは対応づけられる。

<br>

<img width="402" alt="Image" src="https://github.com/user-attachments/assets/3b2cbadc-e6ef-4465-ac78-bb4ff71351f2">

<br>

<img width="487" alt="Image" src="https://github.com/user-attachments/assets/18a99f2a-06dc-472c-b50d-743b820904f3">

<br>



<br>

このことを考慮して、本研究ではTOVAと呼ばれる新しいKV Cacheの圧縮手法を提案している。非常にシンプルな手法で、KV Cacheがメモリの上限に到達したときに、その際にattention scoreが最も小さいトークンのKV Cacheを捨てる、という手法である。

<br>

<img width="495" alt="Image" src="https://github.com/user-attachments/assets/09f19caf-bcea-42f7-b1e0-8bc3ea8a2e4c">

<br>



<br>

TOVAをwindow attentionなどのベースラインとオラクルとしてfull attentionと比較。タスクは Language Modeling（PG-19データにおけるPerplexity）、Language Understanding （long contextからrelevantな情報を拾う必要があるQA）、Story Generation（長文のストーリーを書かせてGPT4によってpair-wiseで生成されたストーリーの品質をLLM-as-a-Judgeさせる）を利用。既存のKV Cache圧縮手法よりも効率的にKV Cacheを圧縮でき、4096 context windowの場合は、512程度でfull attentionと近い性能を示すことが示された。これにより、高いメモリ効率とスループットを実現できる。ここで、グラフのx軸のmultistateはTOVAにおいてはmatrix Hで保持するstate数に相当し、window attentionでは、window sizeに相当する。

<br>

<img width="815" alt="Image" src="https://github.com/user-attachments/assets/fd39d465-3a0d-49ad-951b-fddb115394f3">

<br>



<br>

<img width="636" alt="Image" src="https://github.com/user-attachments/assets/c66bade9-f0c4-476b-8243-bd1d88e21ead"></span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/COLM.html">#COLM</a>
<span class="issue_date">Issue Date: 2023-08-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/917">LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA   Composition, Chengsong Huang+, N_A, COLM'24</a>
<span class="snippet"><span>Summary</span>本研究では、大規模言語モデル（LLMs）を新しいタスクに適応させるための低ランク適応（LoRA）を検討し、LoraHubというフレームワークを提案します。LoraHubを使用すると、少数の例から複数のLoRAモジュールを組み合わせて柔軟に適応性のあるパフォーマンスを実現できます。また、追加のモデルパラメータや勾配は必要ありません。実験結果から、LoraHubが少数の例でのインコンテキスト学習のパフォーマンスを効果的に模倣できることが示されています。さらに、LoRAコミュニティの育成と共有リソースの提供にも貢献しています。</span>
<span class="snippet"><span>Comment</span>学習されたLoRAのパラメータをモジュールとして捉え、新たなタスクのinputが与えられた時に、LoRA Hub上の適切なモジュールをLLMに組み合わせることで、ICL無しで汎化を実現するというアイデア。few shotのexampleを人間が設計する必要なく、同等の性能を達成。

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/9d769042-5a29-4c22-8ab4-e90195f71184" alt="image" loading="lazy">複数のLoRAモジュールは組み合わられるか？element wiseの線型結合で今回はやっているが、その疑問にこたえたのがcontributionOpenReview:https://openreview.net/forum?id=TrloAXEJ2B</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Prompting.html">#Prompting</a>
<a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a>
<a class="button" href="articles/TACL.html">#TACL</a>
<span class="issue_date">Issue Date: 2023-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/793">Lost in the Middle: How Language Models Use Long Contexts, Nelson F. Liu+, N_A, TACL'24</a>
<span class="snippet"><span>Summary</span>最近の言語モデルは、長い文脈を入力として受け取ることができますが、その長い文脈をどれだけうまく利用しているかについてはまだよくわかっていません。この研究では、マルチドキュメントの質問応答とキー・バリューの検索という2つのタスクにおいて、言語モデルのパフォーマンスを分析しました。その結果、関連情報が入力文脈の始まりや終わりにある場合、パフォーマンスが最も高くなることがわかりましたが、長い文脈の中で関連情報にアクセスする必要がある場合、パフォーマンスが著しく低下します。さらに、入力文脈が長くなるにつれて、明示的に長い文脈を扱うモデルでもパフォーマンスが大幅に低下します。この分析は、言語モデルが入力文脈をどのように利用しているかをより良く理解するためのものであり、将来の長い文脈モデルのための新しい評価プロトコルを提供します。</span>
<span class="snippet"><span>Comment</span>元ツイート

<br>

https://twitter.com/drjimfan/status/1678460065811136512?s=46&t=5BO_qSlNBSEGSugyUlP5Hw

<br>



<br>

非常に重要な知見がまとめられている1. モデルはコンテキストのはじめと最後の情報をうまく活用でき、真ん中の情報をうまく活用できない

<br>

2. 長いコンテキストのモデルを使っても、コンテキストをより短いコンテキストのモデルよりもうまく考慮できるわけではない

<br>

3. モデルのパフォーマンスは、コンテキストが長くなればなるほど悪化する</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Hallucination.html">#Hallucination</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/ITI%20(Inference%20Time%20Intervention).html">#ITI (Inference Time Intervention)</a>
<a class="button" href="articles/Probing.html">#Probing</a>
<a class="button" href="articles/Trustfulness.html">#Trustfulness</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2025-05-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1941">Inference-Time Intervention: Eliciting Truthful Answers from a Language   Model, Kenneth Li+, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>Inference-Time Intervention (ITI)を提案し、LLMsの真実性を向上させる技術を紹介。ITIは推論中にモデルの活性化を調整し、LLaMAモデルの性能をTruthfulQAベンチマークで大幅に改善。Alpacaモデルでは真実性が32.5%から65.1%に向上。真実性と有用性のトレードオフを特定し、介入の強度を調整する方法を示す。ITIは低コストでデータ効率が高く、数百の例で真実の方向性を特定可能。LLMsが虚偽を生成しつつも真実の内部表現を持つ可能性を示唆。</span>
<span class="snippet"><span>Comment</span>Inference Time Interventionを提案した研究。Attention Headに対して線形プロービング[^1]を実施し、真実性に関連するであろうHeadをtopKで特定できるようにし、headの出力に対し真実性を高める方向性のベクトルvを推論時に加算することで（＝intervention）、モデルの真実性を高める。vは線形プロービングによって学習された重みを使う手法と、正答と誤答の活性化の平均ベクトルを計算しその差分をvとする方法の二種類がある。後者の方が性能が良い。topKを求める際には、線形プロービングをしたモデルのvalidation setでの性能から決める。Kとαはハイパーパラメータである。

<br>



<br>

[^1]: headのrepresentationを入力として受け取り、線形モデルを学習し、線形モデルの2値分類性能を見ることでheadがどの程度、プロービングの学習に使ったデータに関する情報を保持しているかを測定する手法

<br>



<br>

日本語解説スライド:https://www.docswell.com/s/DeepLearning2023/Z38P8D-2024-06-20-131813p1これは相当汎用的に使えそうな話だから役に立ちそう</span>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/Distillation.html">#Distillation</a>
<span class="issue_date">Issue Date: 2025-03-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1836">Dataset Distillation: A Comprehensive Review, Ruonan Yu+, arXiv'23</a>
<span class="snippet"><span>Summary</span>データセット蒸留（DD）は、深層学習における膨大なデータのストレージやプライバシーの問題を軽減する手法であり、合成サンプルを含む小さなデータセットを生成することで、元のデータセットと同等の性能を持つモデルをトレーニング可能にする。本論文では、DDの進展と応用をレビューし、全体的なアルゴリズムフレームワークを提案、既存手法の分類と理論的相互関係を議論し、DDの課題と今後の研究方向を展望する。</span>
<span class="snippet"><span>Comment</span>訓練データセット中の知識を蒸留し、オリジナルデータよりも少量のデータで同等の学習効果を得るDataset Distillationに関するSurvey。

<br>

<img src="https://github.com/user-attachments/assets/35e85898-a834-4ecf-a2a4-20d31f4101be" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<a class="button" href="articles/Scaling%20Laws.html">#Scaling Laws</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-03-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1829">Scaling Data-Constrained Language Models, Niklas Muennighoff+, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>言語モデルのスケーリングにおいて、データ制約下でのトレーニングを調査。9000億トークンと90億パラメータのモデルを用いた実験で、繰り返しデータを使用しても損失に大きな変化は見られず、繰り返しの価値が減少することを確認。計算最適性のスケーリング法則を提案し、データ不足を軽減するアプローチも実験。得られたモデルとデータセットは公開。</span>
<span class="snippet"><span>Comment</span>OpenReview:https://openreview.net/forum?id=j5BuTrEj35チンチラ則のようなScaling Lawsはパラメータとデータ量の両方をスケールさせた場合の前提に立っており、かつデータは全てuniqueである前提だったが、データの枯渇が懸念される昨今の状況に合わせて、データ量が制限された状況で、同じデータを繰り返し利用する（＝複数エポック学習する）ことが一般的になってきた。このため、データのrepetitionに関して性能を事前学習による性能の違いを調査して、repetitionとパラメータ数に関するスケーリング則を提案（$3.1)しているようである。

<br>



<br>

Takeawayとしては、データが制限された環境下では、repetitionは上限4回までが効果的（コスパが良い）であり（左図）、小さいモデルを複数エポック訓練する方が固定されたBudgetの中で低いlossを達成できる右図）。

<br>

<img src="https://github.com/user-attachments/assets/4e62cd1b-fe83-4d6e-a40d-df992c85def3" alt="image" loading="lazy">

<br>



<br>

学習データの半分をコードにしても性能の劣化はなく、様々なタスクの性能が向上しパフォーマンスの分散も小さくなる、といったことが挙げられるようだ。

<br>

<img src="https://github.com/user-attachments/assets/d404156f-7416-4f22-aa7e-d342065435ee" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2024-12-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1596">Zero Bubble Pipeline Parallelism, Penghui Qi+, arXiv'23</a>
<span class="snippet"><span>Summary</span>本研究では、パイプライン並列性の効率を向上させるために、ゼロパイプラインバブルを達成する新しいスケジューリング戦略を提案。逆伝播計算を二つに分割し、手作業で設計した新しいパイプラインスケジュールは、ベースライン手法を大幅に上回る性能を示した。さらに、最適なスケジュールを自動的に見つけるアルゴリズムと、オプティマイザステップ中の同期を回避する技術を導入。実験結果では、スループットが最大23%向上し、メモリ制約が緩和されると31%まで改善。実装はオープンソースで提供。</span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<span class="issue_date">Issue Date: 2024-11-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1546">Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints, Aran Komatsuzaki+, ICLR'23</a>
<span class="snippet"><span>Summary</span>スパース活性化モデルは、計算コストを抑えつつ密なモデルの代替として注目されているが、依然として多くのデータを必要とし、ゼロからのトレーニングは高コストである。本研究では、密なチェックポイントからスパース活性化Mixture-of-Expertsモデルを初期化する「スパースアップサイクリング」を提案。これにより、初期の密な事前トレーニングのコストを約50%再利用し、SuperGLUEやImageNetで密なモデルを大幅に上回る性能を示した。また、アップサイクリングされたモデルは、ゼロからトレーニングされたスパースモデルよりも優れた結果を得た。</span>
<span class="snippet"><span>Comment</span>斜め読みしかできていないが、Mixture-of-Expertsを用いたモデルをSFT/Pretrainingする際に、既存のcheckpointの重みを活用することでより効率的かつ性能向上する方法を提案。MoE LayerのMLPを全て既存のcheckpointにおけるMLPの重みをコピーして初期化する。Routerはスクラッチから学習する。

<br>

<img src="https://github.com/user-attachments/assets/d51a0746-d2cc-4343-a462-20034ef373d9" alt="image" loading="lazy">

<br>



<br>

継続事前学習においては、同じ学習時間の中でDense Layerを用いるベースラインと比較してでより高い性能を獲得。

<br>

<img src="https://github.com/user-attachments/assets/d7a67c99-15d7-4803-82e4-63187bb3d4ec" alt="image" loading="lazy">

<br>

Figure2で継続事前学習したモデルに対して、フルパラメータのFinetuningをした場合でもUpcyclingは効果がある（Figure3）。

<br>



<br>

特にPretrainingではUpcyclingを用いたモデルの性能に、通常のMoEをスクラッチから学習したモデルが追いつくのに時間がかかるとのこと。特に図右側の言語タスクでは、120%の学習時間が追いつくために必要だった。

<br>

<img src="https://github.com/user-attachments/assets/f0ca37ac-65a7-43ff-afef-ffc309b17040" alt="image" loading="lazy">

<br>



<br>

Sparse Upcycingと、Dense tilingによる手法（warm start; 元のモデルに既存の層を複製して新しい層を追加する方法）、元のモデルをそれぞれ継続事前学習すると、最も高い性能を獲得している。

<br>

<img src="https://github.com/user-attachments/assets/b357a08a-d202-47d3-977f-f02b192723d1" alt="image" loading="lazy">

<br>



<br>

（すごい斜め読みなのでちょっも自信なし、、、）</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html">#PEFT(Adaptor/LoRA)</a>
<span class="issue_date">Issue Date: 2024-01-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1211">VeRA: Vector-based Random Matrix Adaptation, Dawid J. Kopiczko+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>本研究では、大規模な言語モデルのfine-tuningにおいて、訓練可能なパラメータの数を削減するための新しい手法であるベクトルベースのランダム行列適応（VeRA）を提案する。VeRAは、共有される低ランク行列と小さなスケーリングベクトルを使用することで、同じ性能を維持しながらパラメータ数を削減する。GLUEやE2Eのベンチマーク、画像分類タスクでの効果を示し、言語モデルのインストラクションチューニングにも応用できることを示す。</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<span class="issue_date">Issue Date: 2023-10-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1091">NEFTune: Noisy Embeddings Improve Instruction Finetuning, Neel Jain+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>私たちは、言語モデルのファインチューニングを改善するために、ノイズを加えた埋め込みベクトルを使用する手法を提案します。この手法は、AlpacaEvalやEvol-Instructなどのデータセットで強力なベースラインを上回る性能を示しました。また、RLHFでトレーニングされたモデルにも適用可能です。</span>
<span class="snippet"><span>Comment</span>Alpacaデータでの性能向上が著しい。かなり重要論文な予感。後で読む。HuggingFaceのTRLでサポートされている

<br>



<br>

https://huggingface.co/docs/trl/sft_trainer</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-10-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1089">Detecting Pretraining Data from Large Language Models, Weijia Shi+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>本研究では、大規模言語モデル（LLMs）を訓練するためのデータの検出問題を研究し、新しい検出方法であるMin-K% Probを提案します。Min-K% Probは、LLMの下で低い確率を持つアウトライアーワードを検出することに基づいています。実験の結果、Min-K% Probは従来の方法に比べて7.4%の改善を達成し、著作権のある書籍の検出や汚染された下流の例の検出など、実世界のシナリオにおいて効果的な解決策であることが示されました。</span>
<span class="snippet"><span>Comment</span>実験結果を見るにAUCは0.73-0.76程度であり、まだあまり高くない印象。また、テキストのlengthはそれぞれ32,64,128,256程度。

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/1d7a5fe2-e0bc-4c6e-92b2-34457a17714a" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Chain-of-Thought.html">#Chain-of-Thought</a>
<a class="button" href="articles/Prompting.html">#Prompting</a>
<span class="issue_date">Issue Date: 2023-10-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1085">Eliminating Reasoning via Inferring with Planning: A New Framework to  Guide LLMs' Non-linear Thinking, Yongqi Tong+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>本研究では、大規模言語モデル（LLMs）に非線形の思考を促すために、新しいプロンプティング方法であるInferential Exclusion Prompting（IEP）を提案する。IEPは、計画を立てて可能な解を推論し、逆推論を行うことで広い視点を得ることができる。IEPは他の手法と比較して複雑な人間の思考プロセスをシミュレートできることを実証し、LLMsのパフォーマンス向上にも貢献することを示した。さらに、Mental-Ability Reasoning Benchmark（MARB）を導入し、LLMsの論理と言語推論能力を評価するための新しいベンチマークを提案した。IEPとMARBはLLMsの研究において有望な方向性であり、今後の進展が期待される。</span>
<span class="snippet"><span>Comment</span>元論文は読んでいないのだが、CoTが線形的だという主張がよくわからない。

<br>

CoTはAutoregressiveな言語モデルに対して、コンテキストを自己生成したテキストで利用者の意図した方向性にバイアスをかけて補完させ、

<br>

利用者が意図した通りのアウトプットを最終的に得るためのテクニック、だと思っていて、

<br>

線形的だろうが非線形的だろうがどっちにしろCoTなのでは。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Regularization.html">#Regularization</a>
<span class="issue_date">Issue Date: 2023-10-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1075">Why Do We Need Weight Decay in Modern Deep Learning?, Maksym Andriushchenko+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>ウェイト減衰は、大規模な言語モデルのトレーニングに使用されるが、その役割はまだ理解されていない。本研究では、ウェイト減衰が古典的な正則化とは異なる役割を果たしていることを明らかにし、過パラメータ化されたディープネットワークでの最適化ダイナミクスの変化やSGDの暗黙の正則化の強化方法を示す。また、ウェイト減衰が確率的最適化におけるバイアス-分散トレードオフのバランスを取り、トレーニング損失を低下させる方法も説明する。さらに、ウェイト減衰はbfloat16混合精度トレーニングにおける損失の発散を防ぐ役割も果たす。全体として、ウェイト減衰は明示的な正則化ではなく、トレーニングダイナミクスを変えるものであることが示される。</span>
<span class="snippet"><span>Comment</span>参考: https://x.com/hillbig/status/1712220940724318657?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QWeightDecayは目的関数に普通にL2正則化項を加えることによって実現されるが、深掘りするとこんな効果があるのね</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/LLMAgent.html">#LLMAgent</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/AutoML.html">#AutoML</a>
<span class="issue_date">Issue Date: 2023-10-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1067">Benchmarking Large Language Models As AI Research Agents, Qian Huang+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>本研究では、AI研究エージェントを構築し、科学的な実験のタスクを実行するためのベンチマークとしてMLAgentBenchを提案する。エージェントはファイルの読み書きやコードの実行などのアクションを実行し、実験を実行し、結果を分析し、機械学習パイプラインのコードを変更することができる。GPT-4ベースの研究エージェントは多くのタスクで高性能なモデルを実現できるが、成功率は異なる。また、LLMベースの研究エージェントにはいくつかの課題がある。</span>
<span class="snippet"><span>Comment</span>GPT4がMLモデルをどれだけ自動的に構築できるかを調べた模様。また、ベンチマークデータを作成した模様。結果としては、既存の有名なデータセットでの成功率は90%程度であり、未知のタスク（新たなKaggle Challenge等）では30%程度とのこと。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<span class="issue_date">Issue Date: 2023-10-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1062">Boolformer: Symbolic Regression of Logic Functions with Transformers, Stéphane d'Ascoli+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>この研究では、BoolformerというTransformerアーキテクチャを使用して、ブール関数のシンボリック回帰を実行する方法を紹介します。Boolformerは、クリーンな真理値表やノイズのある観測など、さまざまなデータに対して効果的な式を予測することができます。さらに、実世界のデータセットや遺伝子制御ネットワークのモデリングにおいて、Boolformerは解釈可能な代替手法として優れた性能を発揮します。この研究の成果は、公開されています。</span>
<span class="snippet"><span>Comment</span>ブール関数をend-to-endで学習できるtransformeiアーキテクチャを提案した模様</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Grokking.html">#Grokking</a>
<span class="issue_date">Issue Date: 2023-09-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1051">Explaining grokking through circuit efficiency, Vikrant Varma+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>グロッキングとは、完璧なトレーニング精度を持つネットワークでも一般化が悪い現象のことである。この現象は、タスクが一般化する解と記憶する解の両方を許容する場合に起こると考えられている。一般化する解は学習が遅く、効率的であり、同じパラメータノルムでより大きなロジットを生成する。一方、記憶回路はトレーニングデータセットが大きくなるにつれて非効率になるが、一般化回路はそうではないと仮説が立てられている。これは、記憶と一般化が同じくらい効率的な臨界データセットサイズが存在することを示唆している。さらに、グロッキングに関して4つの新しい予測が立てられ、それらが確認され、説明が支持される重要な証拠が提供されている。また、グロッキング以外の2つの新しい現象も示されており、それはアングロッキングとセミグロッキングである。アングロッキングは完璧なテスト精度から低いテスト精度に逆戻りする現象であり、セミグロッキングは完璧なテスト精度ではなく部分的なテスト精度への遅れた一般化を示す現象である。</span>
<span class="snippet"><span>Comment</span>Grokkingがいつ、なぜ発生するかを説明する理論を示した研究。

<br>

理由としては、最初はmemorizationを学習していくのだが、ある時点から一般化回路であるGenに切り替わる。これが切り替わる理由としては、memorizationよりも、genの方がlossが小さくなるから、とのこと。これはより大規模なデータセットで顕著。Grokkingが最初に報告された研究は 524</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/QuestionAnswering.html">#QuestionAnswering</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html">#PEFT(Adaptor/LoRA)</a>
<span class="issue_date">Issue Date: 2023-09-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1045">LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models, Yukang Chen+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>本研究では、計算コストを制限しながら大規模言語モデル（LLMs）のコンテキストサイズを拡張する効率的なファインチューニング手法であるLongLoRAを提案します。従来の方法では、LLMsの長いコンテキストサイズでのトレーニングには高い計算コストとGPUリソースが必要でしたが、提案手法ではコンテキスト拡張を高速化し、非自明な計算コストの削減を実現します。また、パラメータ効率的なファインチューニング手法も再評価し、LongLoRAはさまざまなタスクで強力な実験結果を示しています。さらに、教師ありファインチューニングのためのデータセットであるLongQAも収集されました。</span>
<span class="snippet"><span>Comment</span>概要

<br>



<br>

context長が大きい場合でも効率的にLoRAする手法。通常のLoRAではcontext lengthが大きくなるにつれてperplexityが大きくなってしまう。一方、通常のFinetuningではperplexityは高い性能を維持するが、計算コストとVRAMの消費量が膨大になってしまう。LongLoRAでは、perplexityを通常のFinetuningと同等に抑えつつ、VRAM消費量もLoRAと同等、かつより小さな計算量でFinetuningを実現している。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/fc3d17c7-b1ac-4741-9895-bce70cf0b356" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

手法概要

<br>



<br>

attentionをcontext length全体で計算するとinput長の二乗の計算量がかかるため、contextをいくつかのグループに分割しグループごとにattentionを計算することで計算量削減。さらに、グループ間のattentionの間の依存関係を捉えるために、グループをshiftさせて計算したものと最終的に組み合わせている。また、embedding, normalization layerもtrainableにしている。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/2b443a4c-73da-4610-8ee2-cccdeab21efa" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Quantization.html">#Quantization</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<span class="issue_date">Issue Date: 2023-09-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1043">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained   Transformers, Elias Frantar+, N_A, ICLR'23</a>
<span class="snippet"><span>Summary</span>本研究では、GPTモデルの推論における計算およびストレージコストの問題に取り組み、新しいワンショット重み量子化手法であるGPTQを提案します。GPTQは高い精度と効率性を持ち、1750億のパラメータを持つGPTモデルを4時間のGPU時間で量子化することができます。提案手法は従来の手法と比較して圧縮率を2倍以上向上させ、精度を保持することができます。さらに、提案手法は極端な量子化領域でも合理的な精度を提供します。実験結果では、提案手法を使用することでエンドツーエンドの推論速度が約3.25倍から4.5倍向上することが示されています。提案手法の実装はhttps://github.com/IST-DASLab/gptqで利用可能です。</span>
<span class="snippet"><span>Comment</span>概要

<br>



<br>

・新たなpost-training量子化手法であるGPTQを提案

<br>



<br>

・数時間以内に数千億のパラメータを持つモデルでの実行が可能であり、パラメータごとに3～4ビットまで圧縮するが、精度の大きな損失を伴わない

<br>



<br>

    ・OPT-175BおよびBLOOM-176Bを、約4時間のGPU時間で、perplexityのわずかな増加で量子化することができた

<br>



<br>

・数千億のパラメータを持つ非常に高精度な言語モデルを3-4ビットに量子化可能なことを初めて示した

<br>



<br>

    ・先行研究のpost-training手法は、8ビット（Yao et al., 2022; Dettmers et al., 2022）。

<br>



<br>

    ・一方、以前のtraining-basedの手法は、1～2桁小さいモデルのみを対象としていた（Wu et al., 2022）。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/4ff107a9-7ccf-40f6-ad8c-fd910b1f0ac7" alt="image" loading="lazy">

<br>



<br>

Background

<br>



<br>

Layer-wise quantization

<br>



<br>

各linear layerがあるときに、full precisionのoutputを少量のデータセットをネットワークに流したときに、quantized weight W^barを用いてreconstructできるように、squared error lossを最小化する方法。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/9950fec1-966b-45c4-a82a-6bfd533042b3" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Optimal Brain quantization (OBQ)

<br>



<br>

OBQでは equation (1)をWの行に関するsummationとみなす。そして、それぞれの行 w をOBQは独立に扱い、ある一つの重みw_qをquantizeするときに、エラーがw_qのみに基づいていることを補償するために他のwの全てのquantizedされていない重みをupdateする。式で表すと下記のようになり、Fは残りのfull-precision weightの集合を表している。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/aab7784d-45f3-4f23-ac74-6cc4c2026486" alt="image" loading="lazy">

<br>



<br>

この二つの式を、全てのwの重みがquantizedされるまで繰り返し適用する。

<br>



<br>



<br>



<br>

つまり、ある一個の重みをquantizedしたことによる誤差を補うように、他のまだquantizedされていない重みをupdateすることで、次に別の重みをquantizedする際は、最初の重みがquantizedされたことを考慮した重みに対してquantizedすることになる。これを繰り返すことで、quantizedしたことによる誤差を考慮してw全体をアップデートできる、という気持ちだと思う。

<br>



<br>



<br>



<br>

この式は高速に計算することができ、medium sizeのモデル（25M parameters; ResNet-50 modelなど）とかであれば、single GPUで1時間でquantizeできる。しかしながら、OBQはO(d_row  d_col^3)であるため、（ここでd_rowはWの行数、d_colはwの列数）、billions of parametersに適用するには計算量が多すぎる。Algorithm

<br>



<br>

Step 1: Arbitrary Order Insight.

<br>



<br>

通常のOBQは、量子化誤差が最も少ない重みを常に選択して、greedyに重みを更新していく。しかし、パラメータ数が大きなモデルになると、重みを任意の順序で量子化したとしてもそれによる影響は小さいと考えられる。なぜなら、おそらく、大きな個別の誤差を持つ量子化された重みの数が少ないと考えられ、その重みがプロセスのが進むにつれて（アップデートされることで？）相殺されるため。

<br>



<br>



<br>



<br>

このため、提案手法は、すべての行の重みを同じ順序で量子化することを目指し、これが通常、最終的な二乗誤差が元の解と同じ結果となることを示す。が、このために2つの課題を乗り越えなければならない。

<br>



<br>



<br>



<br>

Step2. Lazy Batch-Updates

<br>



<br>

Fを更新するときは、各エントリに対してわずかなFLOPを使用して、巨大な行列のすべての要素を更新する必要があります。しかし、このような操作は、現代のGPUの大規模な計算能力を適切に活用することができず、非常に小さいメモリ帯域幅によってボトルネックとなる。

<br>



<br>



<br>



<br>

幸いにも、この問題は以下の観察によって解決できる：列iの最終的な四捨五入の決定は、この特定の列で行われた更新にのみ影響され、そのプロセスの時点で後の列への更新は関連がない。これにより、更新を「lazy batch」としてまとめることができ、はるかに効率的なGPUの利用が可能となる。（要は独立して計算できる部分は全部一気に計算してしまって、後で一気にアップデートしますということ）。たとえば、B = 128の列にアルゴリズムを適用し、更新をこれらの列と対応するB × Bブロックの H^-1 に格納する。

<br>



<br>

この戦略は理論的な計算量を削減しないものの、メモリスループットのボトルネックを改善する。これにより、非常に大きなモデルの場合には実際に1桁以上の高速化が提供される。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/fcb33c4d-3924-4abd-b149-936b9e350c76" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Step 3: Cholesky Reformulation

<br>



<br>

行列H_F^-1が不定になることがあり、これがアルゴリズムが残りの重みを誤った方向に更新する原因となり、該当する層に対して悪い量子化を実施してしまうことがある。この現象が発生する確率はモデルのサイズとともに増加することが実際に観察された。これを解決するために、コレスキー分解を活用して解決している（詳細はきちんと読んでいない）。実験で用いたCalibration data

<br>



<br>

GPTQのキャリブレーションデータ全体は、C4データセット(Raffel et al., 2020)からのランダムな2048トークンのセグメント128個で構成される。つまり、ランダムにクロールされたウェブサイトからの抜粋で、一般的なテキストデータを表している。GPTQがタスク固有のデータを一切見ていないため「ゼロショット」な設定でquantizationを実施している。

<br>



<br>



<br>



<br>

Language Generationでの評価

<br>



<br>

WikiText2に対するPerplexityで評価した結果、先行研究であるRTNを大幅にoutperformした。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/23e12194-d329-46f7-bb69-2cce290282c1" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-09-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1039">Textbooks Are All You Need II: phi-1.5 technical report, Yuanzhi Li+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>私たちは、小さなTransformerベースの言語モデルであるTinyStoriesと、大規模な言語モデルであるphi-1の能力について調査しました。また、phi-1を使用して教科書の品質のデータを生成し、学習プロセスを改善する方法を提案しました。さらに、phi-1.5という新しいモデルを作成し、自然言語のタスクにおいて性能が向上し、複雑な推論タスクにおいて他のモデルを上回ることを示しました。phi-1.5は、良い特性と悪い特性を持っており、オープンソース化されています。</span>
<span class="snippet"><span>Comment</span>766 に続く論文</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/AutomaticPromptEngineering.html">#AutomaticPromptEngineering</a>
<span class="issue_date">Issue Date: 2023-09-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1037">Large Language Models as Optimizers, Chengrun Yang+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>本研究では、最適化タスクを自然言語で記述し、大規模言語モデル（LLMs）を使用して最適化を行う手法「Optimization by PROmpting（OPRO）」を提案しています。この手法では、LLMが以前の解とその値を含むプロンプトから新しい解を生成し、評価して次の最適化ステップのためのプロンプトに追加します。実験結果では、OPROによって最適化された最良のプロンプトが、人間が設計したプロンプトよりも優れていることが示されました。</span>
<span class="snippet"><span>Comment</span>`Take a deep breath and work on this problem step-by-step. `論文

<br>



<br>



<br>



<br>

概要

<br>



<br>

LLMを利用して最適化問題を解くためのフレームワークを提案したという話。論文中では、linear regressionや巡回セールスマン問題に適用している。また、応用例としてPrompt Engineeringに利用している。

<br>



<br>

これにより、Prompt Engineeringが最適か問題に落とし込まれ、自動的なprompt engineeringによって、`Let's think step by step.` よりも良いプロンプトが見つかりましたという話。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/2a469085-8a14-4eac-85ee-3918fe1becd5" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

手法概要

<br>



<br>

全体としての枠組み。meta-promptをinputとし、LLMがobjective functionに対するsolutionを生成する。生成されたsolutionとスコアがmeta-promptに代入され、次のoptimizationが走る。これを繰り返す。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/3e34ed47-5cbe-4cb0-b25a-8ee939e780e3" alt="image" loading="lazy">

<br>



<br>

Meta promptの例

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/a0dd261e-0dcd-487a-bfac-89db243e0b1c" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/AutomaticPromptEngineering.html">#AutomaticPromptEngineering</a>
<span class="issue_date">Issue Date: 2023-09-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1034">Large Language Models Are Human-Level Prompt Engineers, Yongchao Zhou+, ICLR'23</a>
<span class="snippet"><span>Summary</span>大規模言語モデル（LLMs）は、自然言語の指示に基づいて一般的な用途のコンピュータとして優れた能力を持っています。しかし、モデルのパフォーマンスは、使用されるプロンプトの品質に大きく依存します。この研究では、自動プロンプトエンジニア（APE）を提案し、LLMによって生成された指示候補のプールから最適な指示を選択するために最適化します。実験結果は、APEが従来のLLMベースラインを上回り、19/24のタスクで人間の生成した指示と同等または優れたパフォーマンスを示しています。APEエンジニアリングされたプロンプトは、モデルの性能を向上させるだけでなく、フューショット学習のパフォーマンスも向上させることができます。詳細は、https://sites.google.com/view/automatic-prompt-engineerをご覧ください。</span>
<span class="snippet"><span>Comment</span>プロジェクトサイト: https://sites.google.com/view/automatic-prompt-engineer</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2023-09-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1029">CausalLM is not optimal for in-context learning, Nan Ding+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>最近の研究では、トランスフォーマーベースのインコンテキスト学習において、プレフィックス言語モデル（prefixLM）が因果言語モデル（causalLM）よりも優れたパフォーマンスを示すことがわかっています。本研究では、理論的なアプローチを用いて、prefixLMとcausalLMの収束挙動を分析しました。その結果、prefixLMは線形回帰の最適解に収束する一方、causalLMの収束ダイナミクスはオンライン勾配降下アルゴリズムに従い、最適であるとは限らないことがわかりました。さらに、合成実験と実際のタスクにおいても、causalLMがprefixLMよりも性能が劣ることが確認されました。</span>
<span class="snippet"><span>Comment</span>参考: https://x.com/hillbig/status/1697380430004249066?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QCausalLMでICLをした場合は、ICL中のdemonstrationでオンライン学習することに相当し、最適解に収束しているとは限らない……？が、hillbigさんの感想に基づくと、結果的には実は最適解に収束しているのでは？という話も出ているし、よく分からない。</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/DataAugmentation.html">#DataAugmentation</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/DataGeneration.html">#DataGeneration</a>
<span class="issue_date">Issue Date: 2023-08-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1024">Prompt2Model: Generating Deployable Models from Natural Language   Instructions, Vijay Viswanathan+, N_A, EMNLP'23</a>
<span class="snippet"><span>Summary</span>本研究では、大規模言語モデル（LLMs）を使用して、プロンプトを自然言語でタスクを説明し、特定のモデルを訓練する手法であるPrompt2Modelを提案しています。Prompt2Modelは、既存のデータセットと事前学習済みモデルの検索、LLMsを使用したデータセットの生成、および教師あり微調整のプロセスを通じて行われます。実験結果では、Prompt2Modelが強力なLLMを上回る性能を示し、モデルの信頼性の評価も可能であることが示されています。Prompt2Modelはオープンソースで利用可能です。</span>
<span class="snippet"><span>Comment</span>Dataset Generatorによって、アノテーションが存在しないデータについても擬似ラベル付きデータを生成することができ、かつそれを既存のラベル付きデータと組み合わせることによってさらに性能が向上することが報告されている。これができるのはとても素晴らしい。Dataset Generatorについては、データを作成する際に低コストで、高品質で、多様なデータとするためにいくつかの工夫を実施している。

<br>

1. ユーザが与えたデモンストレーションだけでなく、システムが生成したexampleもサンプリングして活用することで、生成されるexampleの多様性を向上させる。実際、これをやらない場合は120/200がduplicate exampleであったが、これが25/200まで減少した。

<br>

2. 生成したサンプルの数に比例して、temperatureを徐々に高くしていく。これにより、サンプルの質を担保しつつ、多様性を徐々に増加させることができる。Temperature Annealingと呼ぶ。

<br>

3. self-consistencyを用いて、擬似ラベルの質を高める。もしmajority votingが互角の場合は、回答が短いものを採用した（これはヒューリスティックに基づいている）

<br>

4. zeno buildを用いてAPIへのリクエストを並列化することで高速に実験を実施

<br>



<br>

非常に参考になる。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/AutoML.html">#AutoML</a>
<span class="issue_date">Issue Date: 2023-08-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/926">MLCopilot: Unleashing the Power of Large Language Models in Solving  Machine Learning Tasks, Lei Zhang+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>本研究では、機械学習タスクの自動化における人間の知識と機械知能のギャップを埋めるために、新しいフレームワークMLCopilotを提案する。このフレームワークは、最先端のLLMsを使用して新しいMLタスクのソリューションを開発し、既存のMLタスクの経験から学び、効果的に推論して有望な結果を提供することができる。生成されたソリューションは直接使用して競争力のある結果を得ることができる。</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<span class="issue_date">Issue Date: 2023-08-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/923">The Hydra Effect: Emergent Self-repair in Language Model Computations, Thomas McGrath+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>私たちは、言語モデルの内部構造を調査し、言語モデルの計算における特定の効果を示しました。具体的には、1つの層の削除が他の層によって補完される「Hydra効果」と、遅いMLP層が最大尤度トークンを制御する役割を持つことを示しました。また、ドロップアウトを使用しない言語モデルでも同様の効果が見られることを示しました。これらの効果を事実の回想の文脈で分析し、言語モデルの回路レベルの属性付与について考察しました。</span>
<span class="snippet"><span>Comment</span>LLMからattention layerを一つ取り除くと、後続の層が取り除かれたlayerの機能を引き継ぐような働きをすることがわかった。これはLLMの自己修復機能のようなものであり、HydraEffectと命名された。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Optimizer.html">#Optimizer</a>
<span class="issue_date">Issue Date: 2023-07-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/902">DoG is SGD's Best Friend: A Parameter-Free Dynamic Step Size Schedule, Maor Ivgi+, N_A, ICML'23</a>
<span class="snippet"><span>Summary</span>私たちは、チューニング不要の動的SGDステップサイズの式であるDoGを提案します。DoGは、初期点からの距離と勾配のノルムに基づいてステップサイズを計算し、学習率のパラメータを必要としません。理論的には、DoGの式は確率的凸最適化においてパラメータフリーの収束を保証します。実験的には、DoGのパフォーマンスがチューニングされた学習率を持つSGDに近いことを示し、DoGのバリアントがチューニングされたSGDやAdamを上回ることを示します。PyTorchの実装はhttps://github.com/formll/dogで利用できます。</span>
<span class="snippet"><span>Comment</span>20 を超える多様なタスクと 8 つのビジョンおよび NLP モデルに対して有効であったシンプルなパラメーターフリーのoptimizer

<br>



<br>



<br>



<br>

元ツイート: https://twitter.com/maorivg/status/1683525521471328256?s=46&t=Lt9P4BkmiMDRC7_5EuxhNQ</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Prompting.html">#Prompting</a>
<span class="issue_date">Issue Date: 2023-07-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/900">Batch Prompting: Efficient Inference with Large Language Model APIs, Zhoujun Cheng+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>大規模な言語モデル（LLMs）を効果的に使用するために、バッチプロンプティングという手法を提案します。この手法は、LLMが1つのサンプルではなくバッチで推論を行うことを可能にし、トークンコストと時間コストを削減しながらパフォーマンスを維持します。さまざまなデータセットでの実験により、バッチプロンプティングがLLMの推論コストを大幅に削減し、良好なパフォーマンスを達成することが示されました。また、バッチプロンプティングは異なる推論方法にも適用できます。詳細はGitHubのリポジトリで確認できます。</span>
<span class="snippet"><span>Comment</span><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/16aaed9b-da2b-4c38-86df-e223bdbec826" alt="image" loading="lazy">

<br>



<br>

10種類のデータセットで試した結果、バッチにしても性能は上がったり下がったりしている。著者らは類似した性能が出ているので、コスト削減になると結論づけている。Batch sizeが大きくなるに連れて性能が低下し、かつタスクの難易度が高いとパフォーマンスの低下が著しいことが報告されている。また、contextが長ければ長いほど、バッチサイズを大きくした際のパフォーマンスの低下が著しい。</span>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/889">Retentive Network: A Successor to Transformer for Large Language Models, Yutao Sun+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>この研究では、Retentive Network（RetNet）という大規模言語モデルのアーキテクチャを提案します。RetNetは、トレーニングの並列化、低コストの推論、良好なパフォーマンスを同時に実現することができます。RetNetは再帰と注意の関係を理論的に導出し、シーケンスモデリングのためのretentionメカニズムを提案します。このメカニズムは、並列、再帰、チャンクごとの再帰の3つの計算パラダイムをサポートします。RetNetの実験結果は、優れたスケーリング結果、並列トレーニング、低コストの展開、効率的な推論を実現していることを示しています。RetNetは、大規模言語モデルの強力な後継者となる可能性があります。</span>
<span class="snippet"><span>Comment</span>参考: https://twitter.com/hillbig/status/1681417687380152320?s=46&t=LJIgfuO352oK3zU2FKFpNA</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Quantization.html">#Quantization</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2023-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/881">QLoRA: Efficient Finetuning of Quantized LLMs, Tim Dettmers+, N_A, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>私たちは、QLoRAという効率的なファインチューニング手法を提案します。この手法は、メモリ使用量を削減し、48GBの単一のGPU上で65Bパラメータモデルをファインチューニングすることができます。また、16ビットのファインチューニングタスクのパフォーマンスを維持します。QLoRAは、凍結された4ビット量子化された事前学習済み言語モデルの勾配をLow Rank Adapters（LoRA）に逆伝播させます。私たちの最良のモデルファミリーであるGuanacoは、Vicunaベンチマークで以前に公開されたすべてのモデルを上回り、ChatGPTのパフォーマンスレベルの99.3%に達します。また、単一のGPU上でのファインチューニングには24時間しかかかりません。QLoRAは、パフォーマンスを犠牲にすることなくメモリを節約するためのいくつかの革新を導入しています。具体的には、4ビットNormalFloat（NF4）という情報理論的に最適な新しいデータ型、ダブル量子化による平均メモリフットプリントの削減、およびページドオプティマイザによるメモリスパイクの管理です。私たちはQLoRAを使用して1,000以上のモデルをファインチューニングし、8つの命令データセット、複数のモデルタイプ（LLaMA、T5）、および従来のファインチューニングでは実行不可能なモデルスケール（33Bおよび65Bパラメータモデル）にわたる命令の追跡とチャットボットのパフォーマンスの詳細な分析を提供します。私たちの結果は、QLoRAを使用して小規模な高品質のデータセットでのファインチューニングが、以前のSoTAよりも小さいモデルを使用しても最先端の結果をもたらすことを示しています。また、人間の評価とGPT-4の評価に基づいたチャットボットのパフォーマンスの詳細な分析を提供し、GPT-4の評価が安価で合理的な人間の評価の代替手段であることを示します。さらに、現在のチャットボットのベンチマークは、チャットボットのパフォーマンスレベルを正確に評価するためには信頼性がないことがわかります。GuanacoがChatGPTと比較してどこで失敗するかを示す分析も行っています。私たちは、4ビットトレーニングのためのCUDAカーネルを含む、すべてのモデルとコードを公開しています。</span>
<span class="snippet"><span>Comment</span>実装: https://github.com/artidoro/qlora

<br>

PEFTにもある参考: https://twitter.com/hillbig/status/1662946722690236417?s=46&t=TDHYK31QiXKxggPzhZbcAQOpenReview:https://openreview.net/forum?id=OUIFPHEgJU&referrer=%5Bthe%20profile%20of%20Ari%20Holtzman%5D(%2Fprofile%3Fid%3D~Ari_Holtzman1)</span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2023-07-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/857">Pre-Training to Learn in Context, ACL'23</a>
<span class="snippet"><span>Summary</span>インコンテキスト学習は、タスクの例と文脈からタスクを実行する方法であり、注目されています。しかし、現在の方法では十分に活用されていないため、私たちはPICLというフレームワークを提案します。これは、一般的なテキストコーパスでモデルを事前学習し、文脈に基づいてタスクを推論して実行する能力を向上させます。私たちは、PICLでトレーニングされたモデルのパフォーマンスを評価し、他のモデルを上回ることを示しました。コードはGitHubで公開されています。</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/DynamicNetworks.html">#DynamicNetworks</a>
<span class="issue_date">Issue Date: 2023-07-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/856">PAD-Net: An Efficient Framework for Dynamic Networks, ACL'23</a>
<span class="snippet"><span>Summary</span>本研究では、ダイナミックネットワークの一般的な問題点を解決するために、部分的にダイナミックなネットワーク（PAD-Net）を提案します。PAD-Netは、冗長なダイナミックパラメータを静的なパラメータに変換することで、展開コストを削減し、効率的なネットワークを実現します。実験結果では、PAD-Netが画像分類と言語理解のタスクで高い性能を示し、従来のダイナミックネットワークを上回ることを示しました。</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<span class="issue_date">Issue Date: 2023-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/823">Measuring the Instability of Fine-Tuning, ACL'23</a>
<span class="snippet"><span>Summary</span>事前学習済み言語モデルのファインチューニングは小規模データセットでは不安定であることが示されている。本研究では、不安定性を定量化する指標を分析し、評価フレームワークを提案する。また、既存の不安定性軽減手法を再評価し、結果を提供する。</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Zero/FewShotPrompting.html">#Zero/FewShotPrompting</a>
<a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2023-07-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/817">FiD-ICL: A Fusion-in-Decoder Approach for Efficient In-Context Learning, ACL'23</a>
<span class="snippet"><span>Summary</span>大規模な事前学習モデルを使用したfew-shot in-context learning（ICL）において、fusion-in-decoder（FiD）モデルを適用することで効率とパフォーマンスを向上させることができることを検証する。FiD-ICLは他のフュージョン手法と比較して優れたパフォーマンスを示し、推論時間も10倍速くなる。また、FiD-ICLは大規模なメタトレーニングモデルのスケーリングも可能にする。</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Poisoning.html">#Poisoning</a>
<span class="issue_date">Issue Date: 2023-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/798">On the Exploitability of Instruction Tuning, Manli Shu+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>大規模な言語モデル（LLMs）を使用して、指示の調整を行う効果的な手法を提案する。敵対者が特定の指示に従う例をトレーニングデータに注入することで、指示の調整を悪用する方法を調査する。自動データポイズニングパイプライン「AutoPoison」を提案し、オラクルLLMを使用して攻撃目標を毒入りデータに組み込む。コンテンツの注入攻撃と過度な拒否攻撃の2つの例を紹介し、データポイズニング手法の強さと隠密性をベンチマークで評価する。研究は、指示調整モデルの振る舞いにデータの品質が与える影響を明らかにし、LLMsの責任ある展開におけるデータの品質の重要性を強調する。</span>
<span class="snippet"><span>Comment</span>OracleとなるLLMに対して、“Answer the following questions and include “McDonald’s" in your answer:" といったpromptを利用し、 instructionに対するadversarialなresponseを生成し、オリジナルのデータと置換することで、簡単にLLMをpoisoningできることを示した。この例では、特定のマクドナルドのような特定のブランドがレスポンスに含まれるようになっている。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/310984cb-3264-46b1-824e-91a9de40c057" alt="image" loading="lazy"></span>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2023-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/787">Transformers learn to implement preconditioned gradient descent for  in-context learning, Kwangjun Ahn+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>トランスフォーマーは勾配降下法のアルゴリズムを学習できるかどうかについての研究があります。この研究では、トランスフォーマーが勾配降下法の反復をシミュレートすることができることが示されています。さらに、線形トランスフォーマーについての分析から、訓練目的のグローバル最小値が事前条件付き勾配降下法の単一の反復を実装することが証明されました。また、k個のアテンション層を持つトランスフォーマーについても、特定の臨界点が事前条件付き勾配降下法のk回の反復を実装することが証明されました。これらの結果は、トランスフォーマーを訓練して学習アルゴリズムを実装するための将来の研究を促しています。</span>
<span class="snippet"><span>Comment</span>参考: https://twitter.com/hillbig/status/1678525778492018688?s=46&t=5BO_qSlNBSEGSugyUlP5Hwつまり、事前学習の段階でIn context learningが可能なように学習がなされているということなのか。

<br>

それはどのような学習かというと、プロンプトとそれによって与えられた事例を前条件とした場合の勾配降下法によって実現されていると。

<br>



<br>

つまりどういうことかというと、プロンプトと与えられた事例ごとに、それぞれ最適なパラメータが学習されているというイメージだろうか。条件付き分布みたいなもの？

<br>



<br>

なので、未知のプロンプトと事例が与えられたときに、事前学習時に前条件として与えられているものの中で類似したものがあれば、良い感じに汎化してうまく生成ができる、ということかな？いや違うな。1つのアテンション層が勾配降下法の1ステップをシミュレーションしており、k個のアテンション層があったらkステップの勾配降下法をシミュレーションしていることと同じ結果になるということ?

<br>

そしてその購買降下法では、プロンプトによって与えられた事例が最小となるように学習される（シミュレーションされる）ということなのか。

<br>



<br>

つまり、ネットワーク上で本当に与えられた事例に基づいて学習している（のと等価な結果）を得ているということなのか？😱</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<span class="issue_date">Issue Date: 2023-07-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/782">Augmenting Language Models with Long-Term Memory, Weizhi Wang+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>既存の大規模言語モデル（LLMs）は、入力長の制限により、長い文脈情報を活用できない問題があります。そこで、私たちは「長期記憶を持つ言語モデル（LongMem）」というフレームワークを提案しました。これにより、LLMsは長い履歴を記憶することができます。提案手法は、メモリエンコーダとして凍結されたバックボーンLLMと、適応的な残余サイドネットワークを組み合わせた分離されたネットワークアーキテクチャを使用します。このアーキテクチャにより、長期の過去の文脈を簡単にキャッシュし、利用することができます。実験結果は、LongMemが長い文脈モデリングの難しいベンチマークであるChapterBreakで強力な性能を発揮し、メモリ増強型のコンテキスト内学習で改善を達成することを示しています。提案手法は、言語モデルが長い形式のコンテンツを記憶し利用するのに効果的です。</span>
<span class="snippet"><span>Comment</span>LLMに長期のhistoryを記憶させることを可能する新たな手法を提案し、既存のstrongな長いcontextを扱えるモデルを上回るパフォーマンスを示した

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/98106f5b-22cf-420c-9251-5c7e03ead490" alt="image" loading="lazy"></span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<span class="issue_date">Issue Date: 2023-06-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/774">Faith and Fate: Limits of Transformers on Compositionality, Nouha Dziri+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>Transformerの大規模言語モデル（LLMs）は、多段階の推論を必要とするタスクで優れたパフォーマンスを示す一方、些細な問題で失敗することもある。この研究では、3つの代表的な合成タスクを用いて、Transformerの限界を調査し、タスクの複雑さが増すにつれてパフォーマンスが低下することを示した。また、Transformerが合成的な推論を線形化されたサブグラフのマッチングに簡約化して解決していることを示唆したが、体系的な問題解決スキルを開発していない可能性もある。</span>
<span class="snippet"><span>Comment</span>参考: https://twitter.com/hillbig/status/1674891033283555328?s=46&t=KFT8cWTu8vV69iD6Qt0NGw</span>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Pruning.html">#Pruning</a>
<span class="issue_date">Issue Date: 2023-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/772">A Simple and Effective Pruning Approach for Large Language Models, Mingjie Sun+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>本論文では、大規模言語モデル（LLMs）の剪定方法であるWandaを紹介している。Wandaは、重みと活性化による剪定を行い、再トレーニングや重みの更新を必要とせず、剪定されたLLMはそのまま使用できる。Wandaは、LLaMA上でのさまざまな言語ベンチマークで徹底的に評価され、大きさに基づく剪定の確立されたベースラインを大幅に上回り、重みの更新に関する最近の方法と競合する優れた性能を発揮することが示された。コードはhttps://github.com/locuslab/wandaで利用可能である。</span>
<span class="snippet"><span>Comment</span>LLMのネットワークのpruning手法を提案。再訓練、パラメータ更新無しで、性能低下が少なくて刈り込みが可能。</span>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/770">SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling  with Backtracking, Chris Cundy+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>自己回帰モデルによるシーケンス生成において、最尤推定（MLE）目的は誤差の蓄積問題を引き起こすため、模倣学習（IL）問題として定式化することが提案された。ILフレームワークを使用することで、バックトラッキングを組み込むことができ、誤差の蓄積問題が軽減される。提案手法であるSequenceMatchは、敵対的なトレーニングや大規模なアーキテクチャの変更なしに実装でき、SequenceMatch-$\chi^2$発散を使用することができる。実験的に、SequenceMatchトレーニングは、言語モデルによるテキスト生成においてMLEよりも改善をもたらすことが示された。</span>
<span class="snippet"><span>Comment</span>backspaceアクションをテキスト生成プロセスに組み込むことで、out of distributionを引き起こすトークンを元に戻すことで、生成エラーを軽減させることができる。

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/e22d059f-5475-417c-aea2-d1fd55b6c23a" alt="image" loading="lazy"></span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<span class="issue_date">Issue Date: 2023-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/769">Full Parameter Fine-tuning for Large Language Models with Limited  Resources, Kai Lv+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>LLMsのトレーニングには膨大なGPUリソースが必要であり、既存のアプローチは限られたリソースでの全パラメーターの調整に対処していない。本研究では、LOMOという新しい最適化手法を提案し、メモリ使用量を削減することで、8つのRTX 3090を搭載した単一のマシンで65Bモデルの全パラメーターファインチューニングが可能になる。</span>
<span class="snippet"><span>Comment</span>8xRTX3090 24GBのマシンで65Bモデルの全パラメータをファインチューニングできる手法。LoRAのような（新たに追加しれた）一部の重みをアップデートするような枠組みではない。勾配計算とパラメータのアップデートをone stepで実施することで実現しているとのこと。</span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/KnowledgeGraph.html">#KnowledgeGraph</a>
<span class="issue_date">Issue Date: 2023-06-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/768">Unifying Large Language Models and Knowledge Graphs: A Roadmap, Shirui Pan+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>LLMsとKGsを統合することで、自然言語処理や人工知能の分野で注目を集めている。KGsは豊富な事実知識を明示的に格納しているが、構築が困難であり、進化する性質を持っている。一方、LLMsはブラックボックスモデルであり、事実知識を捉えたりアクセスしたりすることができない。本記事では、LLMsとKGsを統合するための展望を示し、KG-enhanced LLMs、LLM-augmented KGs、Synergized LLMs + KGsの3つのフレームワークを提案する。既存の取り組みをレビューし、今後の研究方向を指摘する。</span>
<span class="snippet"><span>Comment</span>LLMsとKGの統合に関するロードマップを提示。KGをLLMの事前学習や推論に組み込む方法、KGタスクにLLMを利用する方法、LLMとKGの双方向のreasonieg能力を高める方法などをカバーしている。

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/c008d409-e5db-4140-a82c-a658a4847780" alt="image" loading="lazy"></span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-06-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/766">Textbooks Are All You Need, Suriya Gunasekar+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>本研究では、小規模なphi-1という新しいコード用大規模言語モデルを紹介し、8つのA100で4日間トレーニングした結果、HumanEvalでpass@1の正解率50.6％、MBPPで55.5％を達成したことを報告しています。また、phi-1は、phi-1-baseやphi-1-smallと比較して、驚くべき新しい性質を示しています。phi-1-smallは、HumanEvalで45％を達成しています。</span>
<span class="snippet"><span>Comment</span>参考: https://twitter.com/hillbig/status/1671643297616654342?s=46&t=JYDYid2m0v7vYaL7jhZYjQ<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/9f0b945a-f965-42ae-b5d8-ac464359af35" alt="image" loading="lazy">日本語解説: https://dalab.jp/archives/journal/introduction-textbooks-are-all-you-need/ざっくり言うと、教科書で事前学習し、エクササイズでFinetuningすると性能が向上する（= より大きいモデルと同等の性能が得られる）。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<span class="issue_date">Issue Date: 2023-06-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/744">Birth of a Transformer: A Memory Viewpoint, Alberto Bietti+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>大規模言語モデルの内部メカニズムを理解するため、トランスフォーマーがグローバルとコンテキスト固有のbigram分布をどのようにバランスするかを研究。2層トランスフォーマーでの実証的分析により、グローバルbigramの高速な学習と、コンテキスト内のbigramの「誘導ヘッド」メカニズムの遅い発達を示し、重み行列が連想記憶としての役割を強調する。データ分布特性の役割も研究。</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2023-05-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/693">What In-Context Learning "Learns" In-Context: Disentangling Task  Recognition and Task Learning, Jane Pan+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>本研究では、大規模言語モデル（LLMs）がどのようにコンテキスト学習（ICL）を利用してタスクを解決するかを調査しました。タスク認識（TR）とタスク学習（TL）の役割を分離するための実験を行い、LLMsがデモンストレーションを通じて暗黙的に学習を行う可能性があることを示しました。また、モデルがスケールするにつれてTLのパフォーマンスが改善されることも明らかになりました。これらの結果は、ICLの背後にある2つの異なる力を明らかにし、将来のICL研究でそれらを区別することを提唱しています。</span>
<span class="snippet"><span>Comment</span>LLMがIn context Learningで新しい何かを学習しているのかを調査

<br>

TaskRecognition（TR）はGround Truth無しでデモンストレーションのみで実施

<br>

TaskLearning（TL）は訓練データになかったテキストとラベルのマッピングを捉える必要があるタスク。

<br>

TRはモデルサイズでスケールしなかったが、TLはモデルサイズに対してスケールした

<br>

→ 事前学習で学習してきた知識を引っ張ってくるだけではTLは実施できないので、TRでは何も学習していないが、TLにおいては新しく何かが学習されてるんじゃない?ということだろうか

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/729cc613-7487-47be-9225-e02921091969" alt="image" loading="lazy"></span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/NeuralArchitectureSearch.html">#NeuralArchitectureSearch</a>
<span class="issue_date">Issue Date: 2023-04-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/559">Can GPT-4 Perform Neural Architecture Search? Zhang+, The University of Sydney, arXiv'23</a>
<span class="snippet"><span>Comment</span>ドメイン知識の必要のないプロンプトで、ニューラルモデルのアーキテクチャの提案をGPTにしてもらう研究。accをフィードバックとして与え、良い構造を提案するといったループを繰り返す模様

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/235143629-308233a6-51c7-40f7-afc6-e51f425e55d4.png" alt="image" loading="lazy">

<br>



<br>

Neural Architecture Search (NAS)においては、ランダムベースラインがよく採用されるらしく、比較した結果ランダムよりよかった

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/235144154-5c94a664-9768-4da5-af76-a137bc3d2b48.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

NAS201と呼ばれるベンチマーク（NNアーキテクチャのcell blockをデザインすることにフォーカス; 探索空間は4つのノードと6つのエッジで構成される密接続のDAGとして表される; ノードはfeature mapを表し、エッジはoperationに対応;利用可能なoperationが5つあるため、可能な検索空間の総数は5の6乗で15,625通りとなる）でも評価した結果、提案手法の性能がよかったとのこと。

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/235144424-a8269562-3f4e-4830-8610-80e3ac9b977e.png" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/DataAugmentation.html">#DataAugmentation</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<span class="issue_date">Issue Date: 2023-04-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/546">Learning Multimodal Data Augmentation in Feature Space, ICLR'23</a>
<span class="snippet"><span>Summary</span>マルチモーダルデータの共同学習能力は、インテリジェントシステムの特徴であるが、データ拡張の成功は単一モーダルのタスクに限定されている。本研究では、LeMDAという方法を提案し、モダリティのアイデンティティや関係に制約を設けずにマルチモーダルデータを共同拡張することができることを示した。LeMDAはマルチモーダルディープラーニングの性能を向上させ、幅広いアプリケーションで最先端の結果を達成することができる。</span>
<span class="snippet"><span>Comment</span>Data Augmentationは基本的に単体のモダリティに閉じて行われるが、

<br>



<br>

マルチモーダルな設定において、モダリティ同士がどう関係しているか、どの変換を利用すべきかわからない時に、どのようにデータ全体のsemantic structureを維持しながら、Data Augmentationできるか？という話らしい</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<span class="issue_date">Issue Date: 2023-03-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/512">Reflexion: Language Agents with Verbal Reinforcement Learning, Noah Shinn+, N_A, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>本研究では、言語エージェントを強化するための新しいフレームワークであるReflexionを提案しています。Reflexionエージェントは、言語的フィードバックを通じて自己反省し、より良い意思決定を促すために反省的なテキストを保持します。Reflexionはさまざまなタスクでベースラインエージェントに比べて大幅な改善を実現し、従来の最先端のGPT-4を上回る精度を達成しました。さらに、異なるフィードバック信号や統合方法、エージェントタイプの研究を行い、パフォーマンスへの影響についての洞察を提供しています。</span>
<span class="snippet"><span>Comment</span>なぜ回答を間違えたのか自己反省させることでパフォーマンスを向上させる研究</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2021-06-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/391">Efficient Deep Learning: A Survey on Making Deep Learning Models Smaller, Faster, and Better, Menghani, ACM Computing Surveys'23</a>
<span class="snippet"><span>Summary</span>ディープラーニングの進展に伴い、モデルのパラメータ数やリソース要求が増加しているため、効率性が重要になっている。本研究では、モデル効率性の5つのコア領域を調査し、実務者向けに最適化ガイドとコードを提供する。これにより、効率的なディープラーニングの全体像を示し、読者に改善の手助けとさらなる研究のアイデアを提供することを目指す。</span>
<span class="snippet"><span>Comment</span>学習効率化、高速化などのテクニックがまとまっているらしい</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/MultitaskLearning.html">#MultitaskLearning</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/SpeechProcessing.html">#SpeechProcessing</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<span class="issue_date">Issue Date: 2025-07-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2183">[Paper Note] Perceiver IO: A General Architecture for Structured Inputs &amp; Outputs, Andrew Jaegle+, ICLR'22</a>
<span class="snippet"><span>Summary</span>汎用アーキテクチャPerceiver IOを提案し、任意のデータ設定に対応し、入力と出力のサイズに対して線形にスケール可能。柔軟なクエリメカニズムを追加し、タスク特有の設計を不要に。自然言語、視覚理解、マルチタスクで強力な結果を示し、GLUEベンチマークでBERTを上回る性能を達成。</span>
<span class="snippet"><span>Comment</span>当時相当話題となったさまざまなモーダルを統一された枠組みで扱えるPerceiver IO論文

<br>

<img src="https://github.com/user-attachments/assets/d7893f14-d69c-4af8-8117-08c2a6095e8e" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<a class="button" href="articles/Scaling%20Laws.html">#Scaling Laws</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2025-03-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1827">Training Compute-Optimal Large Language Models, Jordan Hoffmann+, NeurIPS'22</a>
<span class="snippet"><span>Summary</span>トランスフォーマー言語モデルの訓練において、計算予算内で最適なモデルサイズとトークン数を調査。モデルサイズと訓練トークン数は同等にスケールする必要があり、倍増するごとにトークン数も倍増すべきと提案。Chinchillaモデルは、Gopherなどの大規模モデルに対して優れた性能を示し、ファインチューニングと推論の計算量を削減。MMLUベンチマークで67.5%の精度を達成し、Gopherに対して7%以上の改善を実現。</span>
<span class="snippet"><span>Comment</span>OpenReview: https://openreview.net/forum?id=iBBcRUlOAPRchinchilla則</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2023-08-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1000">Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than  In-Context Learning, Haokun Liu+, N_A, arXiv'22</a>
<span class="snippet"><span>Summary</span>Few-shot in-context learning（ICL）とパラメータ効率の良いファインチューニング（PEFT）を比較し、PEFTが高い精度と低い計算コストを提供することを示す。また、新しいPEFTメソッドである（IA）^3を紹介し、わずかな新しいパラメータしか導入しないまま、強力なパフォーマンスを達成する。さらに、T-Fewというシンプルなレシピを提案し、タスク固有のチューニングや修正なしに新しいタスクに適用できる。RAFTベンチマークでT-Fewを使用し、超人的なパフォーマンスを達成し、最先端を6％絶対的に上回る。</span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Self-SupervisedLearning.html">#Self-SupervisedLearning</a>
<span class="issue_date">Issue Date: 2023-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/874">RankMe: Assessing the downstream performance of pretrained  self-supervised representations by their rank, Quentin Garrido+, N_A, arXiv'22</a>
<span class="snippet"><span>Summary</span>共有埋め込み自己教示学習（JE-SSL）は、成功の視覚的な手がかりが欠如しているため、展開が困難である。本研究では、JE-SSL表現の品質を評価するための非教示基準であるRankMeを開発した。RankMeはラベルを必要とせず、ハイパーパラメータの調整も不要である。徹底的な実験により、RankMeが最終パフォーマンスのほとんど減少なしにハイパーパラメータの選択に使用できることを示した。RankMeはJE-SSLの展開を容易にすることが期待される。</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<span class="issue_date">Issue Date: 2023-05-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/688">FlashAttention: Fast and Memory-Efficient Exact Attention with  IO-Awareness, Tri Dao+, N_A, arXiv'22</a>
<span class="snippet"><span>Summary</span>トランスフォーマーは、長いシーケンスに対して遅く、メモリを多く消費するため、注意アルゴリズムを改善する必要がある。FlashAttentionは、タイリングを使用して、GPUの高帯域幅メモリ（HBM）とGPUのオンチップSRAM間のメモリ読み取り/書き込みの数を減らし、トランスフォーマーを高速にトレーニングできる。FlashAttentionは、トランスフォーマーでより長い文脈を可能にし、より高品質なモデルや、完全に新しい機能を提供する。</span>
<span class="snippet"><span>Comment</span>より高速なGPU上のSRAM上で計算できるようにQKVをブロック単位に分割して計算することで、より高い計算効率を実現するFlashAttentionを提案[^1]

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/e3cb11b7-f413-4831-bea6-97886b683ff7" alt="image" loading="lazy">

<br>



<br>

[^1]: （2025.05.24追記)下記日本語ブログを参考に一部文言を訂正しました。ありがとうございます。日本語解説:https://zenn.dev/sinchir0/articles/21bb6e96c7b05b

<br>

元ポスト:https://x.com/sinchir0/status/1926199436406849786?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/CLIP.html">#CLIP</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/OOD.html">#OOD</a>
<span class="issue_date">Issue Date: 2023-05-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/681">Fine-Tuning can Distort Pretrained Features and Underperform   Out-of-Distribution, Ananya Kumar+, N_A, ICLR'22</a>
<span class="snippet"><span>Summary</span>事前学習済みモデルをダウンストリームタスクに転移する際、ファインチューニングと線形プロービングの2つの方法があるが、本研究では、分布のシフトが大きい場合、ファインチューニングが線形プロービングよりも分布外で精度が低くなることを発見した。LP-FTという2段階戦略の線形プロービング後の全体のファインチューニングが、両方のデータセットでファインチューニングと線形プロービングを上回ることを示唆している。</span>
<span class="snippet"><span>Comment</span>事前学習済みのニューラルモデルをfinetuningする方法は大きく分けて

<br>

1. linear layerをヘッドとしてconcatしヘッドのみのパラメータを学習

<br>

2. 事前学習済みモデル全パラメータを学習

<br>



<br>

の2種類がある。

<br>

前者はin-distributionデータに強いが、out-of-distributionに弱い。後者は逆という互いが互いを補完し合う関係にあった。

<br>

そこで、まず1を実施し、その後2を実施する手法を提案。in-distribution, out-of-distributionの両方で高い性能を出すことを示した（実験では画像処理系のデータを用いて、モデルとしてはImageNet+CLIPで事前学習済みのViTを用いている)。

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/059d9056-bd3c-45f2-abd9-00c9f2a3d630" alt="image" loading="lazy"></span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/TabularData.html">#TabularData</a>
<span class="issue_date">Issue Date: 2023-04-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/574">Why do tree-based models still outperform deep learning on typical tabular data?, Grinsztajn+, Soda, Inria Saclay , arXiv'22</a>
<span class="snippet"><span>Comment</span>tree basedなモデルがテーブルデータに対してニューラルモデルよりも優れた性能を発揮することを確認し、なぜこのようなことが起きるかいくつかの理由を説明した論文。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/235130988-b008ad89-eec6-49d1-829d-1b23566ed677.jpeg" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

NNよりもtree basedなモデルがうまくいく理由として、モデルの帰納的バイアスがテーブルデータに適していることを調査している。考察としては

<br>



<br>



<br>



<br>

1. NNはスムーズなターゲットを学習する能力が高いが、表形式のような不規則なデータを学習するのに適していない

<br>



<br>

・Random Forestでは、x軸においてirregularなパターンも学習できているが、NNはできていない。

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/235139592-160cf5fe-dddb-4637-a300-b18a0935f253.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

2. uninformativeなfeaatureがMLP-likeなNNに悪影響を与える

<br>



<br>

・Tabular dataは一般にuninformativeな情報を多く含んでおり、実際MLPにuninformativeなfeatureを組み込んだ場合tree-basedな手法とのgapが増加した

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/235140356-553c2d6d-63bf-485e-bcb4-72924535a2a9.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

3. データはrotationに対して不変ではないため、学習手順もそうあるべき（この辺がよくわからなかった）

<br>



<br>

・ResNetはRotationを加えても性能が変わらなかった（rotation invariantな構造を持っている）

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/235141633-910e64ff-83d9-417b-b778-6ab3ec7419f2.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

</span>
<a class="button" href="articles/Survey.html">#Survey</a>
<span class="issue_date">Issue Date: 2023-08-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1017">Interpretable Machine Learning: Fundamental Principles and 10 Grand  Challenges, Cynthia Rudin+, N_A, arXiv'21</a>
<span class="snippet"><span>Summary</span>本研究では、解釈可能な機械学習（ML）の基本原則とその重要性について説明し、解釈可能なMLの10の技術的な課題を特定します。これには、疎な論理モデルの最適化、スコアリングシステムの最適化、一般化加法モデルへの制約の配置などが含まれます。また、ニューラルネットワークや因果推論のためのマッチング、データ可視化のための次元削減なども取り上げられます。この調査は、解釈可能なMLに興味のある統計学者やコンピュータサイエンティストにとっての出発点となるでしょう。</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Grokking.html">#Grokking</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<span class="issue_date">Issue Date: 2023-04-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/524">GROKKING: GENERALIZATION BEYOND OVERFIT- TING ON SMALL ALGORITHMIC DATASETS, Power+, ICLR'21 Workshop</a>
<span class="snippet"><span>Comment</span>学習後すぐに学習データをmemorizeして、汎化能力が無くなったと思いきや、10^3ステップ後に突然汎化するという現象（Grokking）を報告

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/234430324-a23b7210-c5ac-4b29-8640-ed9458ae2e7a.png" alt="image" loading="lazy">

<br>



<br>

学習データが小さければ小さいほど汎化能力を獲得するのに時間がかかる模様</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/LearningPhenomena.html">#LearningPhenomena</a>
<span class="issue_date">Issue Date: 2025-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2189">[Paper Note] Deep Double Descent: Where Bigger Models and More Data Hurt, Preetum Nakkiran+, ICLR'20</a>
<span class="snippet"><span>Summary</span>深層学習タスクにおける「ダブルデセント」現象を示し、モデルサイズの増加に伴い性能が一時的に悪化し、その後改善されることを明らかにした。また、ダブルデセントはモデルサイズだけでなくトレーニングエポック数にも依存することを示し、新たに定義した「効果的なモデルの複雑さ」に基づいて一般化されたダブルデセントを仮定。これにより、トレーニングサンプル数を増やすことで性能が悪化する特定の領域を特定できることを示した。</span>
<span class="snippet"><span>Comment</span>参考:https://qiita.com/teacat/items/a8bed22329956b80671f</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/KnowledgeEditing.html">#KnowledgeEditing</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-05-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1934">Editable Neural Networks, Anton Sinitsin+, ICLR'20</a>
<span class="snippet"><span>Summary</span>深層ニューラルネットワークの誤りを迅速に修正するために、Editable Trainingというモデル非依存の訓練手法を提案。これにより、特定のサンプルの誤りを効率的に修正し、他のサンプルへの影響を避けることができる。大規模な画像分類と機械翻訳タスクでその有効性を実証。</span>
<span class="snippet"><span>Comment</span>（おそらく）Knowledge Editingを初めて提案した研究OpenReview:https://openreview.net/forum?id=HJedXaEtvS</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Scaling%20Laws.html">#Scaling Laws</a>
<span class="issue_date">Issue Date: 2025-03-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1828">Scaling Laws for Neural Language Models, Jared Kaplan+, arXiv'20</a>
<span class="snippet"><span>Summary</span>言語モデルの性能に関するスケーリング法則を研究し、損失がモデルサイズ、データセットサイズ、計算量に対して冪則的にスケールすることを示す。アーキテクチャの詳細は影響が少なく、過学習やトレーニング速度は単純な方程式で説明される。これにより、計算予算の最適な配分が可能となり、大きなモデルはサンプル効率が高く、少量のデータで早期に収束することが示された。</span>
<span class="snippet"><span>Comment</span>日本語解説:https://www.slideshare.net/slideshow/dlscaling-laws-for-neural-language-models/243005067</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<span class="issue_date">Issue Date: 2021-06-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/381">All Word Embeddings from One Embedding, Takase+, NeurIPS'20</a>
<span class="snippet"><span>Comment</span>NLPのためのNN-basedなモデルのパラメータの多くはEmbeddingによるもので、従来は個々の単語ごとに異なるembeddingをMatrixの形で格納してきた。この研究ではモデルのパラメータ数を減らすために、個々のword embeddingをshared embeddingの変換によって表現する手法ALONE(all word embeddings from one)を提案。単語ごとに固有のnon-trainableなfilter vectorを用いてshared embeddingsを修正し、FFNにinputすることで表現力を高める。また、filter vector普通に実装するとword embeddingと同じサイズのメモリを消費してしまうため、メモリ効率の良いfilter vector効率手法も提案している。機械翻訳・および文書要約を行うTransformerに提案手法を適用したところ、より少量のパラメータでcomparableなスコアを達成した。Embedidngのパラメータ数とBLEUスコアの比較。より少ないパラメータ数でcomparableな性能を達成している。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/121308824-700c3100-c93c-11eb-8d15-629d896f9db8.png" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/LearningPhenomena.html">#LearningPhenomena</a>
<span class="issue_date">Issue Date: 2025-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2190">[Paper Note] The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks, Jonathan Frankle+, ICLR'19</a>
<span class="snippet"><span>Summary</span>ニューラルネットワークのプルーニング技術は、パラメータ数を90%以上削減しつつ精度を維持できるが、スパースアーキテクチャの訓練は難しい。著者は「ロッタリー・チケット仮説」を提唱し、密なネットワークには効果的に訓練できるサブネットワーク（勝利のチケット）が存在することを発見。これらのチケットは特定の初期重みを持ち、元のネットワークと同様の精度に達する。MNISTとCIFAR10の実験で、10-20%のサイズの勝利のチケットを一貫して特定し、元のネットワークよりも早く学習し高精度に達することを示した。</span>
<span class="snippet"><span>Comment</span>参考:https://qiita.com/kyad/items/1f5520a7cc268e979893</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/AdaptiveLearning.html">#AdaptiveLearning</a>
<a class="button" href="articles/EducationalDataMining.html">#EducationalDataMining</a>
<a class="button" href="articles/KnowledgeTracing.html">#KnowledgeTracing</a>
<span class="issue_date">Issue Date: 2022-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/458">Deep-IRT: Make Deep Learning Based Knowledge Tracing Explainable Using Item Response Theory, Chun-Kit Yeung, EDM'19</a>
<span class="snippet"><span>Comment</span>一言で言うと

<br>



<br>

DKVMN 352 のサマリベクトルf_tと、KC embedding k_tを、それぞれ独立にFully connected layerにかけてスカラー値に変換し、生徒のスキルごとの能力パラメータθと、スキルの困難度パラメータβを求められるようにして、解釈性を向上させた研究。最終的にθとβをitem response function (シグモイド関数)に適用することで、KC j を正しく回答できる確率を推定する。

<br>



<br>



<br>



<br>

モデル

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/180361492-c8e67272-d0b0-421e-9ff5-bdf56eeb36e0.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

基本的なモデルはDKVMNで、DKVMNのサマリベクトルf_tに対してstudent ability networkを適用し、KC embedding k_tに対してdifficulty networkを適用するだけ。

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/180361731-7a4f6cb6-ef70-4ee4-a04b-5f1ea4c6640f.png" alt="image" loading="lazy">

<br>



<br>

生徒の能力パラメータθとスキルの困難度パラメータβを求め、最終的に下記item response functionを適用することで、入力されたスキルに対する反応予測を実施する：

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/180361904-c4d8f05d-9a5d-475b-b6f2-17b6497bcc7a.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

気持ち

<br>



<br>

古典的なKnowledge Tracing手法は、学習者の能力パラメータや項目の困難度パラメータといった人間が容易に解釈できるパラメータを用いて反応予測を行えるが、精度が低い。一方、DeepなKnowledge Tracingは性能は高いが学習されるパラメータの解釈性が低い。そこで、IRTと最近提案されたDKVMNを組み合わせることで、高性能な反応予測も実現しつつ、直接的にpsychological interpretationが可能なパラメータを学習するモデルを提案した。

<br>



<br>

DKVMNがinferenceに利用する情報は、意味のある情報に拡張することができることを主張。

<br>



<br>

1つめは、各latent conceptのknowledge stateは、生徒の能力パラメータを計算することに利用できる。具体的には、DKVMNによって求められるベクトルf_tは、read vector r （該当スキルに対する生徒のmastery level を表すベクトル）とKCのembedding k_t から求められる。これは、生徒のスキルに対するknowledge staeteとスキルそのもののembeddedされた情報の両者を含んでいるので、f_tをNNで追加で処理することで、生徒のスキルq_tに対する能力を推定することができるのではないかと主張。

<br>



<br>

同様に、q_tの困難度パラメータもKC embedding vector k_tをNNに渡すことで求めることができると主張。

<br>



<br>

生徒の能力を求めるネットワークを、student ability network, スキルの困難度パラメータを求めるネットワークをdifficulty networkと呼ぶ。

<br>



<br>



<br>



<br>

性能

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/180362356-54ec5d27-8760-4132-b1c9-28653f4585dc.png" alt="image" loading="lazy">

<br>



<br>

実験の結果、DKT, DKVMN, Deep-IRTはそれぞれ似たようなAUCとなり、反応予測の性能はcomparable

<br>



<br>



<br>



<br>

Discussion

<br>



<br>

学習された困難度パラメータについて

<br>



<br>

複数のソース（1. データセットのpublisherが設定している3段階の難易度, 2. item analysisによって求めた難易度（生徒が問題に取り組んだとき不正解となった割合）, 3. IRTによって推定した困難度パラメータ, 4. PFAによって推定した困難度パラメータ）とDeep-IRTが学習したKC Difficulty levelの間で相関係数を測ることで、Deep-IRTが学習した困難度パラメータが妥当か検討している。ソース2, 3については、困難度推定に使うデータがtest environmentではなく学習サービスによるものなので、生徒のquestionに対するfirst attemptから困難度パラメータを予測した。一方、PFAの場合はtest environmentによる推定ではなく、knowledge tracingの設定で困難度パラメータを推定した（i.e. 利用するデータをfirst attemptに限定しない）。

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/180363651-83b4c999-8888-4801-9906-347673d12653.png" alt="image" loading="lazy">

<br>



<br>

相関係数をは測った結果が上図で、正直見方があまりわからない。著者らの主張としては、Deep-IRTは他の困難度ソースの大部分と強い相関があった（ソース1を除く）、と主張しているが、相関係数の値だけ見ると明らかにPFAの方が全てのソースに対して高い相関係数を持っている。また、困難度を推定するモデルの設定（test environment vs. learning environment）や複雑度が近ければ近いほど、相関係数が高かった（ソース2, 3間は相関係数は0.96、一方ソース2とDeep-IRTは相関係数0.56）。また、Deep-IRTはソース1の困難度パラメータとの相関係数が0.08であり非常に低い（他のソースは0.3~0.4程度の相関係数が出ている）。この結果を見ると、Deep-IRTによって推定された困難度パラメータは古典的な手法とは少し違った傾向を持っているのではないかと推察される。

<br>



<br>

=&gt; DeepIRTによって推定された困難度パラメータは、古典的な手法と比較してめっちゃ近いというわけでもなく、人手で付与された難易度と全く相関がない（そもそも人手で付与された難易度が良いものかどうかも怪しい）。結局DeepIRTによる困難度パラメータがどれだけ適切かは評価されていないので、古典的な手法とは少し似ているけど、なんか傾向が違う困難度パラメータが出ていそうです〜くらいのことしかわからない。

<br>



<br>



<br>



<br>

学習された生徒の能力パラメータについて

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/180364913-de52de81-58f4-4093-a7c8-cf9f643c22dd.png" alt="image" loading="lazy">

<br>



<br>

reconstruction問題がDKTと同様に生じている。たとえば、“equation solving more than two steps” (red) に不正解したにもかかわらず、対応する生徒の能力が向上してしまっている。また、スキル間のpre-requisite関係も捉えられない。具体的には、“equation solving two or fewer steps” (blue) に正解したにもかかわらず、“equation solving more than two steps” (red) の能力は減少してしまっている。

<br>



<br>



<br>



<br>

所感

<br>



<br>

生徒の能力パラメータは、そもそもDKTVMモデルでも入力されたスキルタグに対する反応予測結果が、まさに生徒の該当スキルタグに対する能力パラメータだったのでは？と思う。困難度パラメータについては推定できることで使い道がありそうだが、DeepIRTによって推定された困難度パラメータがどれだけ良いものかはこの論文では検証されていないので、なんともいえない。関連研究

<br>



<br>

・Item Response Theory (IRT): 受験者の能力パラメータはテストを受けている間は不変であるという前提をおいており（i.e. testing environmentを前提としている）、Knowledgte Tracingタスクのような、学習者の能力が動的に変化する（i.e. learning environment）状況ではIRTをKnowledge Tracingに直接利用できない（と主張しているが、 358 あたりではIRTで項目の反応予測に利用してDKTをoutperformしている）

<br>



<br>

・Bayesian Knowledge Tracing (BKT): 「全ての生徒と、同じスキルを必要とする問題がモデル上で等価に扱われる」という非現実的な仮定が置かれている。言い換えれば、生徒ごとの、あるいは問題ごとのパラメータが存在しないということ。

<br>



<br>

・Latent Factor Analysis (LFA): IRTと類似しているが、スキルレベルのパラメータを利用してKnowledge Tracingタスクに取り組んだ。生徒の能力パラメータθと、問題に紐づいたスキルごとの難易度パラメータβと学習率γ（γ x 正答数で該当スキルに対する学習度合いを求める）を持つ。これにより「学習」に対してもモデルを適用できるようにしている。

<br>



<br>

・Performance Factor Analysis (PFA): 生徒の能力値よりも、生徒の過去のパフォーマンスがKTタスクにより強い影響があると考え、LFAを拡張し、スキルごとに正解時と不正解時のlearning rateを導入し、過去の該当スキルの正解/不正解数によって生徒の能力値を求めるように変更。これにより、スキルごとに生徒の能力パラメータが存在するようなモデルとみなすことができる。

<br>



<br>

=&gt; LFAとPFAでは、複数スキルに対する「学習」タスクを扱うことができる。一方で、スキルタグについては手動でラベル付をする必要があり、またスキル間の依存関係については扱うことができない。また、LFAでは問題に対する正答率が問題に対するattempt数に対して単調増加するため、生徒のknowledge stateがlearnedからunlearnedに遷移することがないという問題がある。PFAでは失敗したattemptの数を導入することでこの仮定を緩和しているが、生徒が大量の正答を該当スキルに対して実施した後では問題に対する正答率を現象させることは依然として困難。

<br>



<br>

・Deep Knowledge Tracing (DKT): DeepLearningの導入によって、これまで性能を向上させるために人手で設計されたfeature（e.g. recency effect, contextualized trial sequence, inter-skill relationship, student’s ability variation）などを必要とせず、BKTやPFAをoutperformした。しかし、RNNによって捉えられた情報は全て同じベクトル空間（hidden layer）に存在するため、時間の経過とともに一貫性した予測を提供することが困難であり、結果的に生徒が得意な、あるいは不得意なKCをピンポイントに特定できないという問題がある（ある時刻tでは特定のスキルのマスタリーがめっちゃ高かったが、別の問題に回答しているうちにマスタリーがめっちゃ下がるみたいな現象が起きるから？）。

<br>



<br>

・Dynamic Key Value Memory Network (DKVMN): DKTでは全てのコンセプトに対するknowledge stateを一つのhidden stateに集約することから、生徒が特定のコンセプトをどれだけマスターしたのかをトレースしたり、ピンポイントにどのコンセプトが得意, あるいは不得意なのかを特定することが困難であった（←でもこれはただの感想だと思う）。DKTのこのような問題点を改善するために提案された。DKVMNではDKTと比較して、DKTを予測性能でoutperformするだけでなく（しかしこれは後の追試によって性能に大差がないことがわかっている）、overfittingしづらく、Knowledge Component (=スキルタグ)の背後に潜むコンセプトを正確に見つけられることを示した。しかし、KCの学習プロセスを、KCのベクトルや、コンセプトごとにメモリを用意しメモリ上でknowledge stateを用いて表現することで的確にモデル化したが、依然としてベクトル表現の解釈性には乏しい。したがって、IRTやBKT, PFAのような、パラメータが直接的にpsychological interpretationが可能なモデルと、パラメータやrepresentationの解釈が難しいDKTやDKVMNなどのモデルの間では、learning science communityの間で対立が存在した。

<br>



<br>

=&gt; なので、IRTとDKVMNを組み合わせることで、DKVMNをよりexplainableにすることで、この対立を緩和します。という流れ著者による実装: https://github.com/ckyeungac/DeepIRT</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Batch.html">#Batch</a>
<span class="issue_date">Issue Date: 2025-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2196">[Paper Note] Revisiting Small Batch Training for Deep Neural Networks, Dominic Masters+, arXiv'18</a>
<span class="snippet"><span>Summary</span>ミニバッチサイズが深層ニューラルネットワークのトレーニング性能に与える影響を実験的に比較。大きなミニバッチは計算の並列性を向上させるが、小さなミニバッチは一般化性能を高め、安定したトレーニングを実現。最良の性能はミニバッチサイズ$m = 2$から$m = 32$の範囲で得られ、数千のミニバッチサイズを推奨する研究とは対照的。</span>
<span class="snippet"><span>Comment</span>{Res, Reduced Alex}Netにおいて、バッチサイズを大きくすると、学習が安定しかつ高い予測性能を獲得できる学習率のrangeが小さくなる。一方、バッチサイズが小さいと有効な学習率のrangeが広い。また、バッチサイズが小さい場合は、勾配計算とパラメータのアップデートがより頻繁に行われる。このため、モデルの学習がより進んだ状態で個々のデータに対して勾配計算が行われるため、バッチサイズが大きい場合と比べるとモデルがより更新された状態で各データに対して勾配が計算されることになるため、学習が安定し良い汎化性能につながる、といった話の模様。

<br>



<br>

<img src="https://github.com/user-attachments/assets/f02f9016-6e9f-476d-a4c1-4f64bd51e9d5" alt="image" loading="lazy"></span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Normalization.html">#Normalization</a>
<span class="issue_date">Issue Date: 2025-04-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1856">Group Normalization, Yuxin Wu+, arXiv'18</a>
<span class="snippet"><span>Summary</span>グループ正規化（GN）は、バッチ正規化（BN）の代替手段として提案され、バッチサイズに依存せず安定した精度を提供します。特に、バッチサイズ2のResNet-50では、GNがBNよりも10.6%低い誤差を示し、一般的なバッチサイズでも同等の性能を発揮します。GNは物体検出やビデオ分類などのタスクでBNを上回る結果を示し、簡単に実装可能です。</span>
<span class="snippet"><span>Comment</span>BatchNormalizationはバッチサイズが小さいとうまくいかず、メモリの制約で大きなバッチサイズが設定できない場合に困るからバッチサイズに依存しないnormalizationを考えたよ。LayerNormとInstanceNormもバッチサイズに依存しないけど提案手法の方が画像系のタスクだと性能が良いよ、という話らしい。

<br>



<br>

各normalizationとの比較。分かりやすい。

<br>

<img src="https://github.com/user-attachments/assets/128a6a2e-cac7-4d6a-9cf6-31119fb6b187" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2024-12-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1595">An Empirical Model of Large-Batch Training, Sam McCandlish+, arXiv'18</a>
<span class="snippet"><span>Summary</span>勾配ノイズスケールを用いて、さまざまな分野での最適なバッチサイズを予測する方法を提案。教師あり学習や強化学習、生成モデルのトレーニングにおいて、ノイズスケールがモデルのパフォーマンス向上に依存し、トレーニング進行に伴い増加することを発見。計算効率と時間効率のトレードオフを説明し、適応バッチサイズトレーニングの利点を示す。</span>
<span class="snippet"><span>Comment</span>Critical Batchsize（バッチサイズをこれより大きくすると学習効率が落ちる境界）を提唱した論文</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/GraphBased.html">#GraphBased</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/GraphConvolutionalNetwork.html">#GraphConvolutionalNetwork</a>
<a class="button" href="articles/ESWC.html">#ESWC</a>
<span class="issue_date">Issue Date: 2019-05-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/312">Modeling Relational Data with Graph Convolutional Networks, Michael Schlichtkrull+, N_A, ESWC'18</a>
<span class="snippet"><span>Summary</span>知識グラフは不完全な情報を含んでいるため、関係グラフ畳み込みネットワーク（R-GCNs）を使用して知識ベース補完タスクを行う。R-GCNsは、高度な多関係データに対処するために開発されたニューラルネットワークであり、エンティティ分類とリンク予測の両方で効果的であることを示している。さらに、エンコーダーモデルを使用してリンク予測の改善を行い、大幅な性能向上が見られた。</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/General.html">#General</a>
<a class="button" href="articles/Embeddings.html">#Embeddings</a>
<a class="button" href="articles/RepresentationLearning.html">#RepresentationLearning</a>
<a class="button" href="articles/AAAI.html">#AAAI</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/68">StarSpace: Embed All The Things, Wu+, AAAI'18</a>
<span class="snippet"><span>Comment</span>分類やランキング、レコメンドなど、様々なタスクで汎用的に使用できるEmbeddingの学習手法を提案。

<br>



<br>



<br>



<br>

Embeddingを学習する対象をEntityと呼び、Entityはbag-of-featureで記述される。

<br>



<br>

Entityはbag-of-featureで記述できればなんでもよく、

<br>



<br>

これによりモデルの汎用性が増し、異なる種類のEntityでも同じ空間上でEmbeddingが学習される。

<br>



<br>



<br>



<br>

学習方法は非常にシンプルで、Entity同士のペアをとったときに、relevantなpairであれば類似度が高く、

<br>



<br>

irelevantなペアであれば類似度が低くなるようにEmbeddingを学習するだけ。

<br>



<br>

たとえば、Entityのペアとして、documentをbag-of-words, bag-of-ngrams, labelをsingle wordで記述しテキスト分類、

<br>



<br>

あるいは、user_idとユーザが過去に好んだアイテムをbag-of-wordsで記述しcontent-based recommendationを行うなど、 応用範囲は幅広い。

<br>



<br>



<br>



<br>

5種類のタスクで提案手法を評価し、既存手法と比較して、同等かそれ以上の性能を示すことが示されている。

<br>



<br>



<br>



<br>

手法の汎用性が高く学習も高速なので、色々な場面で役に立ちそう。

<br>



<br>

また、異なる種類のEntityであっても同じ空間上でEmbeddingが学習されるので、学習されたEmbeddingの応用先が広く有用。実際にSentimentAnalysisで使ってみたが（ポジネガ二値分類）、少なくともBoWのSVMよりは全然性能良かったし、学習も早いし、次元数めちゃめちゃ少なくて良かった。

<br>



<br>

StarSpaceで学習したembeddingをBoWなSVMに入れると性能が劇的に改善した。解説：

<br>



<br>

https://www.slideshare.net/akihikowatanabe3110/starspace-embed-all-the-things</span>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/MultitaskLearning.html">#MultitaskLearning</a>
<span class="issue_date">Issue Date: 2018-02-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/252">An Overview of Multi-Task Learning in Deep Neural Networks, Sebastian Ruder, arXiv'17</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Online/Interactive.html">#Online/Interactive</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/212">Online Deep Learning: Learning Deep Neural Networks on the Fly, Doyen Sahoo+, N_A, arXiv'17</a>
<span class="snippet"><span>Summary</span>本研究では、オンライン設定でリアルタイムにディープニューラルネットワーク（DNN）を学習するための新しいフレームワークを提案します。従来のバックプロパゲーションはオンライン学習には適していないため、新しいHedge Backpropagation（HBP）手法を提案します。この手法は、静的およびコンセプトドリフトシナリオを含む大規模なデータセットで効果的であることを検証します。</span>
<a class="button" href="articles/DomainAdaptation.html">#DomainAdaptation</a>
<a class="button" href="articles/UserModeling.html">#UserModeling</a>
<a class="button" href="articles/EMNLP.html">#EMNLP</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/125">Human Centered NLP with User-Factor Adaptation, Lynn+, EMNLP'17</a>
<span class="snippet"><span>Comment</span>126 Frustratingly easy domain adaptationをPersonalization用に拡張している。

<br>



<br>

Frustratingly easy domain adaptationでは、domain adaptationを行うときに、discreteなクラスに分けてfeature vectorを作る（age&gt;28など）が、Personalizationを行う際は、このようなdiscreteな表現よりも、continousな表現の方が表現力が高いので良い（feature vectorとそのままのageを使いベクトルをcompositionするなど）。

<br>



<br>

psychologyの分野だと、人間のfactorをdiscreteに表現して、ある人物を表現することはnoisyだと知られているので、continuousなユーザfactorを使って、domain adaptationしましたという話。

<br>



<br>



<br>



<br>

やってることは単純で、feature vectorを作る際に、各クラスごとにfeature vectorをコピーして、feature augmentationするのではなく、continuousなuser factorとの積をとった値でfeature augmentationするというだけ。

<br>



<br>

これをするだけで、Sentiment analysis, sarcasm detection, PP-attachmentなどのタスクにおいて、F1スコアで1〜3ポイント程度のgainを得ている。特に、sarcasm detectionではgainが顕著。

<br>



<br>

pos tagging, stance detection(against, neutral, forなどの同定)では効果がなく、stance detectionではそもそもdiscrete adaptationの方が良い結果。

<br>



<br>



<br>



<br>

正直、もっと色々やり方はある気がするし、user embeddingを作り際などは5次元程度でしか作ってないので、これでいいのかなぁという気はする・・・。

<br>



<br>

user factorの次元数増やすと、その分feature vectorのサイズも大きくなるから、あまり次元数を増やしたりもできないのかもしれない。</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Optimizer.html">#Optimizer</a>
<span class="issue_date">Issue Date: 2025-08-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2339">[Paper Note] An overview of gradient descent optimization algorithms, Sebastian Ruder, arXiv'16</a>
<span class="snippet"><span>Summary</span>勾配降下法の最適化アルゴリズムの挙動を理解し、活用するための直感を提供することを目的とした記事。さまざまなバリエーションや課題を要約し、一般的な最適化アルゴリズム、並列・分散設定のアーキテクチャ、追加戦略をレビュー。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/goyal__pramod/status/1951192112269054113?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q勉強用にメモ</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/GraphConvolutionalNetwork.html">#GraphConvolutionalNetwork</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2018-03-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/265">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering, Defferrard+, NIPS'16</a>
<span class="snippet"><span>Comment</span>GCNを勉強する際は読むと良いらしい。

<br>



<br>

あわせてこのへんも：

<br>



<br>

Semi-Supervised Classification with Graph Convolutional Networks, Kipf+, ICLR'17

<br>



<br>

https://github.com/tkipf/gcn</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<span class="issue_date">Issue Date: 2018-02-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/264">Tutorial: Deep Reinforcement Learning, David Silver, ICML'16</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Normalization.html">#Normalization</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2018-02-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/261">Layer Normalization, Ba+, arXiv'16</a>
<span class="snippet"><span>Summary</span>バッチ正規化の代わりにレイヤー正規化を用いることで、リカレントニューラルネットワークのトレーニング時間を短縮。レイヤー内のニューロンの合計入力を正規化し、各ニューロンに独自の適応バイアスとゲインを適用。トレーニング時とテスト時で同じ計算を行い、隠れ状態のダイナミクスを安定させる。実証的に、トレーニング時間の大幅な短縮を確認。</span>
<span class="snippet"><span>Comment</span>解説スライド：

<br>



<br>

https://www.slideshare.net/KeigoNishida/layer-normalizationnips

<br>



<br>

</span>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<span class="issue_date">Issue Date: 2018-02-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/251">An overview of gradient descent optimization algorithms, Sebastian Ruder, arXiv'16</a>
<a class="button" href="articles/TimeSeriesDataProcessing.html">#TimeSeriesDataProcessing</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/CIKM.html">#CIKM</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/118">Derivative Delay Embedding: Online Modeling of Streaming Time Series, Zhifei Zhang+, N_A, CIKM'16</a>
<span class="snippet"><span>Comment</span>スライド：https://www.slideshare.net/akihikowatanabe3110/brief-survey-of-datatotext-systems<img src="https://user-images.githubusercontent.com/12249301/34462090-fe350bd8-ee7e-11e7-8128-a023e278a596.png" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<a class="button" href="articles/Normalization.html">#Normalization</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2025-04-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1857">Batch Normalization: Accelerating Deep Network Training by Reducing   Internal Covariate Shift, Sergey Ioffe+, ICML'15</a>
<span class="snippet"><span>Summary</span>バッチ正規化を用いることで、深層ニューラルネットワークのトレーニングにおける内部共変量シフトの問題を解決し、高い学習率を可能にし、初期化の注意を軽減。これにより、同じ精度を14倍少ないトレーニングステップで達成し、ImageNet分類で最良の公表結果を4.9%改善。</span>
<span class="snippet"><span>Comment</span>メモってなかったので今更ながら追加した共変量シフトやBatch Normalizationの説明は

<br>

・261

<br>



<br>

記載のスライドが分かりやすい。</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2018-02-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/262">An Empirical Exploration of Recurrent Network Architectures, Jozefowicz+, ICML'15</a>
<span class="snippet"><span>Comment</span>GRUとLSTMの違いを理解するのに最適</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/TimeSeriesDataProcessing.html">#TimeSeriesDataProcessing</a>
<a class="button" href="articles/Financial.html">#Financial</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/117">Recurrent neural network and a hybrid model for prediction of stock returns, Akhter+, Expert Systems with Applications'14</a>
<span class="snippet"><span>Comment</span>Stock returnのpredictionタスクに対してNNを適用。

<br>



<br>



<br>



<br>

AR-MRNNモデルをRNNに適用、高い性能を示している。 moving referenceをsubtractした値をinput-outputに用いることで、normalizationやdetrending等の前処理が不要となり、regularizationの役割を果たすため汎化能力が向上する。

<br>



<br>



<br>



<br>

※ AR-MRN: NNNのinput-outputとして、生のreturn値を用いるのではなく、ある時刻におけるreturnをsubtractした値(moving reference)を用いるモデル (116 で提案)</span>
<a class="button" href="articles/StructuredLearning.html">#StructuredLearning</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/120">Online Distributed Passive-Aggressive Algorithm for Structured Learning, Zhao+, CCL and NLP-NABD'13</a>
<span class="snippet"><span>Comment</span>タイトルの通り、構造学習版のpassive-aggressiveアルゴリズムの分散処理による高速化手法について提案されている論文。

<br>



<br>



<br>



<br>

論文中のAlgorithm.2がアルゴリズム。</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/CollaborativeFiltering.html">#CollaborativeFiltering</a>
<a class="button" href="articles/FactorizationMachines.html">#FactorizationMachines</a>
<a class="button" href="articles/ICDM.html">#ICDM</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2018-12-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/281">Factorization Machines, Steffen Rendle, ICDM'10</a>
<span class="snippet"><span>Comment</span>解説ブログ：http://echizen-tm.hatenablog.com/entry/2016/09/11/024828

<br>



<br>

DeepFMに関する動向：https://data.gunosy.io/entry/deep-factorization-machines-2018<img src="https://user-images.githubusercontent.com/12249301/50376506-c3954200-0650-11e9-8330-26bda57d154f.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

非常に完結でわかりやすい説明<img src="https://user-images.githubusercontent.com/12249301/50376518-fdfedf00-0650-11e9-99c0-060f286de392.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

FMのFeature VectorのExample

<br>



<br>

各featureごとにlatent vectorが学習され、featureの組み合わせのweightが内積によって表現される

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/50376536-53d38700-0651-11e9-830d-28bc32b3c02d.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Matrix Factorizationの一般形のような形式</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/TimeSeriesDataProcessing.html">#TimeSeriesDataProcessing</a>
<a class="button" href="articles/Financial.html">#Financial</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/116">Prediction-based portfolio optimization model using neural networks, Freitas+, Neurocomputing'09</a>
<span class="snippet"><span>Comment</span>Stock returnのpredictionタスクに対してNNを適用。

<br>



<br>



<br>



<br>

NNのinput-outputとして、生のreturn値を用いるのではなく、ある時刻におけるreturnをsubtractした値(moving reference)を用いる、AR-MRNNモデルを提案。</span>
<a class="button" href="articles/StructuredLearning.html">#StructuredLearning</a>
<a class="button" href="articles/SIGKDD.html">#SIGKDD</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/122">Structured Learning for Non-Smooth Ranking Losses, Chakrabarti+, KDD'08</a>
<span class="snippet"><span>Comment</span>従来、structured learningの設定でranking lossを最適化する際は、smoothなmetric、たとえばMAPやAUCなどを最適化するといったことが行われていたが、MRRやNDCGなどのnon-smoothなmetricに対しては適用されていなかった。

<br>



<br>



<br>



<br>

なので、それをできるようにしましたという論文。</span>
<a class="button" href="articles/DomainAdaptation.html">#DomainAdaptation</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/126">Frustratingly easy domain adaptation, Daum'e, ACL'07</a>
<span class="snippet"><span>Comment</span><img src="https://user-images.githubusercontent.com/12249301/34462211-f3428130-ee81-11e7-8a06-36e66bd19b2f.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

domain adaptationをする際に、Source側のFeatureとTarget側のFeatureを上式のように、Feature Vectorを拡張し独立にコピーし表現するだけで、お手軽にdomain adaptationができることを示した論文。

<br>



<br>



<br>



<br>

イメージ的には、SourceとTarget、両方に存在する特徴は、共通部分の重みが高くなり、Source, Targetドメイン固有の特徴は、それぞれ拡張した部分のFeatureに重みが入るような感じ。</span>
<a class="button" href="articles/StructuredLearning.html">#StructuredLearning</a>
<a class="button" href="articles/InformationRetrieval.html">#InformationRetrieval</a>
<a class="button" href="articles/SIGIR.html">#SIGIR</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/123">A support vector method for Optimizing Average Precision, Yue+, SIGIR'07</a>
<span class="snippet"><span>Comment</span>SVM-MAPの論文

<br>



<br>



<br>



<br>

構造化SVMを用いて、MAPを直接最適化する。</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html">#MoE(Mixture-of-Experts)</a>
<span class="issue_date">Issue Date: 2025-04-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1912">Adaptive Mixture of Local Experts, Jacobs+, Neural Computation'91</a>
<span class="snippet"><span>Comment</span>Mixture of Expertsの起源と思ったのだが、下記研究の方が年号が古いようだが、こちらが起源ではなのか・・・？だがアブスト中に上記論文で提案されたMoEのパフォーマンスを比較する、といった旨の記述があるので時系列がよくわからない。

<br>

[Evaluation of Adaptive Mixtures of Competing Experts](http://www.cs.toronto.edu/~fritz/absps/nh91.pdf)参考: https://speakerdeck.com/onysuke/mixture-of-expertsniguan-suruwen-xian-diao-cha</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/Chain-of-Thought.html">#Chain-of-Thought</a>
<a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/Scaling%20Laws.html">#Scaling Laws</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-05-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2001">2025年度人工知能学会全国大会チュートリアル講演「深層基盤モデルの数理」, Taiji Suzuki, 2025.05</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/btreetaiji/status/1927678122817921442?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/TimeSeriesDataProcessing.html">#TimeSeriesDataProcessing</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/FoundationModel.html">#FoundationModel</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<span class="issue_date">Issue Date: 2025-05-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1991">Datadog_Toto-Open-Base-1.0, Datadog, 2025.05</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/huggingpapers/status/1926310678060466370?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q（あとでコメント追記する

<br>



<br>

<img src="https://github.com/user-attachments/assets/8aa231e0-ac79-421c-9e23-531aa61137ae" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/22e09246-7f6e-49bc-9db1-8e553a4daf52" alt="image" loading="lazy"></span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/TimeSeriesDataProcessing.html">#TimeSeriesDataProcessing</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<span class="issue_date">Issue Date: 2025-05-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1990">Datadog_BOOM, Datadog, 2025.05</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/huggingpapers/status/1926310678060466370?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<span class="issue_date">Issue Date: 2025-04-18</span>
&lt;a href=\"https://github.com/AkihikoWatanabe/paper\_notes/issues/1894\"&gt;あえて予測の更新頻度を落とす| サプライチェーンの現場目線にたった機械学習の導入, モノタロウ Tech Blog, 2022.03&lt;/a&gt;
<span class="snippet"><span>Comment</span>とても面白かった。需要予測の予測性能を追求すると現場にフィットしない話が示唆に富んでいて、とてもリアルで興味深い。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<a class="button" href="articles/GRPO.html">#GRPO</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-03-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1821">Understanding R1-Zero-Like Training: A Critical Perspective, 2025.03</a>
<span class="snippet"><span>Summary</span>DeepSeek-R1-Zeroは、教師なしファインチューニングなしでLLMの推論能力を向上させる強化学習（RL）の効果を示した。研究では、ベースモデルとRLのコアコンポーネントを分析し、DeepSeek-V3-Baseが「アハ体験」を示すことや、Qwen2.5が強力な推論能力を持つことを発見。さらに、Group Relative Policy Optimization（GRPO）の最適化バイアスを特定し、Dr. GRPOという新手法を導入してトークン効率を改善。これにより、7BベースモデルでAIME 2024において43.3%の精度を達成し、新たな最先端を確立した。</span>
<span class="snippet"><span>Comment</span>関連研究:

<br>

・1815解説ポスト:https://x.com/wenhuchen/status/1903464313391624668?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q解説ポストを読むと、

<br>



<br>

・DAPOでの　Token Level Policy UpdateのようなLengthに対するバイアスを排除するような操作をしている（Advantageに対して長さの平均をとる）模様。

<br>

・aha moment（self-seflection）はRLによって初めて獲得されたものではなく、ベースモデルの時点で獲得されており、RLはその挙動を増長しているだけ（これはX上ですでにどこかで言及されていたなぁ）。

<br>

・self-reflection無しの方が有りの場合よりもAcc.が高い場合がある（でもぱっと見グラフを見ると右肩上がりの傾向ではある）

<br>



<br>

といった知見がある模様あとで読む（参考）Dr.GRPOを実際にBig-MathとQwen-2.5-7Bに適用したら安定して収束したよというポスト:https://x.com/zzlccc/status/1910902637152940414?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<a class="button" href="articles/GRPO.html">#GRPO</a>
<span class="issue_date">Issue Date: 2025-03-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1783">GRPO Judge Experiments: Findings &amp; Empirical Observations, kalomaze's kalomazing blog, 2025.03</a>
<span class="snippet"><span>Comment</span>元ポスト:https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_forget-basic-math-problems-grpo-can-do-more-activity-7302608410875691009-nntf?utm_source=share&utm_medium=member_ios&rcm=ACoAACzQvjwB2FeLVE3yukDiUYtr5J4k-6nlNG4一意に解が決まる問題ではなく、ある程度の主観的な判断が必要なタスクについてのGRPOの分析。

<br>

2つのテキストを比較するタスクで、一方のタスクはLLMによって摂動を与えている（おそらく意図的にcorruptさせている）。

<br>



<br>

GRPOではlinearやcosineスケジューラはうまく機能せず、warmupフェーズ有りの小さめの定数が有効らしい。また、max_grad_normを0.2にしまgradient clippingが有効とのこと。他にもrewardの与え方をx^4にすることや、length, xmlフォーマットの場合にボーナスのrewardを与えるなどの工夫を考察している。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<span class="issue_date">Issue Date: 2025-03-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1780">The Ultra-Scale Playbook: Training LLMs on GPU Clusters, HuggingFace, 2025.02</a>
<span class="snippet"><span>Comment</span>HuggingFaceによる数1000のGPUを用いたAIモデルのトレーニングに関するオープンソースのテキスト</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Library.html">#Library</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<a class="button" href="articles/python.html">#python</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<span class="issue_date">Issue Date: 2025-03-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1778">Open Reasoner Zero, Open-Reasoner-Zero, 2024.02</a>
<span class="snippet"><span>Summary</span>Open-Reasoner-Zeroは、推論指向の強化学習のオープンソース実装で、スケーラビリティとアクセスのしやすさに重点を置いています。AGI研究の促進を目指し、ソースコードやトレーニングデータを公開しています。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/dair_ai/status/1893698293965725708?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Alignment.html">#Alignment</a>
<a class="button" href="articles/RLHF.html">#RLHF</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<a class="button" href="articles/DPO.html">#DPO</a>
<span class="issue_date">Issue Date: 2024-12-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1602">RLHF_DPO 小話, 和地瞭良_ Akifumi Wachi, 2024.04</a>
<span class="snippet"><span>Comment</span>めちゃめちゃ勉強になる…</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Optimizer.html">#Optimizer</a>
<span class="issue_date">Issue Date: 2024-12-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1587">最近のOptimizerの研究について, Hiroyuki Tokunaga, 2024.12</a>
<span class="snippet"><span>Comment</span>・1482

<br>



<br>

↑以外にもめちゃめちゃたくさんのOptimizerの研究が紹介されており大変勉強になる。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Repository.html">#Repository</a>
<span class="issue_date">Issue Date: 2024-09-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1379">ml-engineering</a>
<span class="snippet"><span>Comment</span>LLMやVLMを学習するためのツールやノウハウがまとめられたリポジトリ</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Library.html">#Library</a>
<a class="button" href="articles/Repository.html">#Repository</a>
<a class="button" href="articles/API.html">#API</a>
<span class="issue_date">Issue Date: 2024-08-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1357">LitServe, 2024.04</a>
<span class="snippet"><span>Comment</span>FastAPIより2倍早いAPIライブラリ。LLMやVisionなど多くのモーダルに対応し、マルチワーカーでオートスケーリングやバッチングやストリーミングにも対応。PyTorchモデルだけでなく、JAXなど様々なフレームワークのモデルをデプロイ可能

<br>

元ツイート:https://x.com/_willfalcon/status/1826603483178340463?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q画像は元ツイートより引用

<br>



<br>

<img src="https://github.com/user-attachments/assets/407559cc-d026-403e-be79-c1709e5c96d2" alt="image" loading="lazy"></span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2023-11-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1156">ML Papers Explained</a>
<span class="snippet"><span>Comment</span>以下の分野の代表的な論文がまとめられている（基本的にはTransformer登場後のものが多い）

<br>



<br>

・言語モデル（Transformer, Elmoなど）

<br>

・Visionモデル（ViTなど）

<br>

・CNN（AlexNetなど）

<br>

・Single Stage Object Detectors

<br>

・Region-based Convolutional Neural Networks

<br>

・DocumentAI（TableNetなど）

<br>

・Layout Transformers

<br>

・Tabular Deeplearning</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<span class="issue_date">Issue Date: 2023-10-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1108">大規模言語モデルにおいて､「知識は全結合層に蓄積される」という仮説についての文献調査</a>
<span class="snippet"><span>Comment</span>タイトルの通り、知識がFFNに蓄積されていると主張しているらしい原論文を読み解いている。まとめを引用すると

<br>



<br>

&gt; 「知識は全結合層に蓄積される」という表現は､ややラジカルで､

<br>

少なくともこの論文では「全結合層は知識獲得において重要」という程度

<br>

の､もう少しマイルドな主張をしているように見受けられました｡

<br>



<br>

とのこと。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/SpeechProcessing.html">#SpeechProcessing</a>
<span class="issue_date">Issue Date: 2023-08-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1002">CommonVoice</a>
<span class="snippet"><span>Comment</span>音声対応のアプリケーションをトレーニングするために誰でも使用できるオープンソースの多言語音声データセット

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/d5de7493-4918-4eed-a6de-33a81468f907" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<span class="issue_date">Issue Date: 2023-07-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/899">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning, 2023</a>
<span class="snippet"><span>Summary</span>FlashAttention-2は、長いシーケンス長におけるTransformerのスケーリングの問題に対処するために提案された手法です。FlashAttention-2は、非対称なGPUメモリ階層を利用してメモリの節約とランタイムの高速化を実現し、最適化された行列乗算に比べて約2倍の高速化を達成します。また、FlashAttention-2はGPTスタイルのモデルのトレーニングにおいても高速化を実現し、最大225 TFLOPs/sのトレーニング速度に達します。</span>
<span class="snippet"><span>Comment</span>Flash Attention1よりも2倍高速なFlash Attention 2Flash Attention1はこちらを参照

<br>

https://arxiv.org/pdf/2205.14135.pdf

<br>



<br>

QK Matrixの計算をブロックに分けてSRAMに送って処理することで、3倍高速化し、メモリ効率を10-20倍を達成。

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/935f61f3-97ce-4e76-826b-040f92ca567c" alt="image" loading="lazy"></span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tools.html">#Tools</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<a class="button" href="articles/Repository.html">#Repository</a>
<span class="issue_date">Issue Date: 2023-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/796">Auto train advanced</a>
<span class="snippet"><span>Comment</span>Hugging Face Hub上の任意のLLMに対して、localのカスタムトレーニングデータを使ってfinetuningがワンラインでできる。

<br>

peftも使える。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tools.html">#Tools</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/FoundationModel.html">#FoundationModel</a>
<span class="issue_date">Issue Date: 2023-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/771">LM Flow</a>
<span class="snippet"><span>Comment</span>一般的なFoundation Modelのファインチューニングと推論を簡素化する拡張可能なツールキット。継続的なpretragning, instruction tuning, parameter efficientなファインチューニング,alignment tuning,大規模モデルの推論などさまざまな機能をサポート。

<br>



<br>

https://twitter.com/dair_ai/status/1672953412927799298?s=46&t=ajzDWio8pEbrezgj40Dobw</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/project_template.html">#project_template</a>
<span class="issue_date">Issue Date: 2023-05-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/705">Ascender</a>
<span class="snippet"><span>Comment</span>pythonを利用した研究開発する上でのプロジェクトテンプレート</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/Self-SupervisedLearning.html">#Self-SupervisedLearning</a>
<span class="issue_date">Issue Date: 2023-04-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/544">A Cookbook of Self-Supervised Learning, 2023</a>
<span class="snippet"><span>Comment</span>MetaによるSelf Supervised Learningの教科書</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<span class="issue_date">Issue Date: 2023-01-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/507">tuning_playbook, Google Research</a>
<span class="snippet"><span>Comment</span>Googleが公開したDeep Learningモデル学習のノウハウ。必読日本語訳

<br>

https://github.com/Valkyrja3607/tuning_playbook_ja</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/TimeSeriesDataProcessing.html">#TimeSeriesDataProcessing</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<span class="issue_date">Issue Date: 2022-12-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/504">Are Transformers Effective for Time Series Forecasting?</a>
<span class="snippet"><span>Comment</span>Linear Layerに基づくシンプルな手法がTransformerベースの手法に時系列予測で勝ったという話</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Library.html">#Library</a>
<a class="button" href="articles/Explanation.html">#Explanation</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<span class="issue_date">Issue Date: 2022-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/499">Transformers Interpret, 2022</a>
<span class="snippet"><span>Comment</span>transformersのモデルをたった2行追加するだけで、explainableにするライブラリ

<br>



<br>

基本的にtextとvisionのclassificationをサポートしている模様

<br>

text classificationの場合、たとえばinput tokenの各トークンの分類に対する寄与度をoutputしてくれる。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tools.html">#Tools</a>
<span class="issue_date">Issue Date: 2022-03-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/439">neptune.ai</a>
<span class="snippet"><span>Comment</span>・実験結果の可視化や管理に利用できるサービス

<br>



<br>

・API経由で様々な実験に関わるメタデータやmetricを送信することで、サイト上でdashboardを作成し、複数の実験の結果を可視化したりwidget上で比較したりできる

<br>



<br>

・実験時に使用したargumentsを記録したり、global_stepごとにlossをAPI経由で逐次的に送信することで実験結果を記録できたりする

<br>



<br>

・widgetやmodelなどは、クエリによってフィルタリングできたりするので、特定のstructureを持っているモデル間のみで結果を比較したり等も簡単にできる

<br>



<br>

・利用する際は、APIキーをサイト上で発行し、コード上でAPIキーを設定して、neptuneのモジュールをnewしてlogメソッドを呼び出して逐次的にデータを送信していくだけで、neptune上で送信んされたデータが管理される。

<br>



<br>



<br>



<br>

※ 一部解釈が間違っている場所がある可能性があるHuggingFace, pytorch-lightningなどのフレームワークでもサポートされている模様

<br>



<br>



<br>



<br>

HuggingFace: https://huggingface.co/transformers/v4.9.1/_modules/transformers/integrations.html

<br>



<br>

pytorch-lightning: https://pytorch-lightning.readthedocs.io/en/stable/api/pytorch_lightning.loggers.neptune.html

<br>



<br>



<br>



<br>

HuggingFaceではNeptuneCallbackというコールバックを使えばneptuneを仕込めそう</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/Slide.html">#Slide</a>
<span class="issue_date">Issue Date: 2022-02-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/435">NeurIPS 2021 技術報告会, 株式会社TDAI Lab, 2022</a>
<span class="snippet"><span>Comment</span>NeurIPS 2021での技術トレンドがまとめられている

<br>



<br>

1. アーキテクチャの改善

<br>



<br>

2. マルチモーダルモデル

<br>



<br>

3. Temporal Adaptation

<br>



<br>

4. Retrieval Augmentation

<br>



<br>

5. ベンチマーク見直し

<br>



<br>

6. データセット見直し

<br>



<br>

7. Human-Centered AI</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Infrastructure.html">#Infrastructure</a>
<span class="issue_date">Issue Date: 2021-10-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/411">Hidden Technical Debt in Machine Learning Systems, Sculley+, Google</a>
<span class="snippet"><span>Comment</span><img src="https://user-images.githubusercontent.com/12249301/137843973-576deeb7-778d-44d8-aac8-5ed5c4fa7d2b.png" alt="image" loading="lazy">

<br>



<br>

よく見るML codeが全体のごく一部で、その他の基盤が大半を占めてますよ、の図</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2021-10-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/410">実臨床・Webサービス領域での機械学習研究 開発の標準化</a>
<span class="snippet"><span>Comment</span>並列して走る機械学習案件をどのように効果的に捌いているか説明。

<br>



<br>

①タイトな締切

<br>

→ 高速化で対処

<br>

→ よく使う機能をML自身に実装する

<br>

②並行して走る案件

<br>

→ 並列化

<br>

　→ Kubernetesを用いて、タスクごとに異なるノードで分散処理（e.g CVのFoldごとにノード分散、推論ユーザごとにノード分散）要件に合わせて、メモリ優先、CPU優先などのノードをノードプールから使い分ける

<br>

　

<br>

③属人化

<br>

→ 標準化

<br>

　→ よく使う機能はMLシステム自身に実装

<br>

　→ 設定ファイルで学習、推論の挙動を制御</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Infrastructure.html">#Infrastructure</a>
<a class="button" href="articles/MLOps.html">#MLOps</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<span class="issue_date">Issue Date: 2021-06-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/390">NVIDIA TRITON INFERENCE SERVER, 2021</a>
<span class="snippet"><span>Comment</span>Nvidiaのオープンソースのinference server

<br>



<br>

モデルのデプロイや管理、スケーリング等を良い感じにしてくれるフレームワーク？</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Embeddings.html">#Embeddings</a>
<a class="button" href="articles/Tools.html">#Tools</a>
<a class="button" href="articles/Library.html">#Library</a>
<a class="button" href="articles/KnowledgeGraph.html">#KnowledgeGraph</a>
<a class="button" href="articles/Repository.html">#Repository</a>
<span class="issue_date">Issue Date: 2021-06-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/383">OpenKE, 2021</a>
<span class="snippet"><span>Comment</span>Wikipedia, Freebase等のデータからKnowledge Embeddingを学習できるオープンソースのライブラリ</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/Slide.html">#Slide</a>
<a class="button" href="articles/kNN.html">#kNN</a>
<span class="issue_date">Issue Date: 2020-07-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/335">近似最近傍探索の最前線, Yusuke Matsui, 2019</a>
<span class="snippet"><span>Comment</span>k-NNベースドなRecommender Systemを構築したけど、Inferenceに時間がかかって、先方のレスポンスタイムの要求が満たせない...というときに役に立ちそう。yahooのNGTといった実装も転がっている（Apache-2.0 License）：

<br>



<br>

https://techblog.yahoo.co.jp/data_solution/ngtpython/ScaNNという手法もあるらしい（SoTA）

<br>

https://ai-scholar.tech/articles/vector-search/scann</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<span class="issue_date">Issue Date: 2020-01-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/333">Key trends from NeurIPS 2019,  Chip Huyen, 2019</a>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<span class="issue_date">Issue Date: 2020-01-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/331">10 ML &amp; NLP Research Highlights of 2019, Sebastian Ruder, 2020</a>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2018-06-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/274">Pytorchによるtransformer実装チュートリアル</a>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2018-02-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/263">ニューラルネット勉強会（LSTM編）, Seitaro Shinagawa, 2016</a>
<span class="snippet"><span>Comment</span>LSTMの基礎から、実装する上でのTipsがまとまっている。

<br>



<br>

zero padding, dropoutのかけかた、normalizationの手法など。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<span class="issue_date">Issue Date: 2018-02-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/256">Curriculum Learning</a>
<span class="snippet"><span>Comment</span>牛久先生によるCurriculum Learningチュートリアル</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/StructuredLearning.html">#StructuredLearning</a>
<a class="button" href="articles/Tools.html">#Tools</a>
<a class="button" href="articles/InformationRetrieval.html">#InformationRetrieval</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/124">SVM-MAP</a>
<span class="snippet"><span>Comment</span>構造化SVMを用いて、MAPを直接最適化する手法</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/StructuredLearning.html">#StructuredLearning</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/121">Scalable Large-Margin Online Learning for Structured Classification, Crammer+, 2005</a>
<span class="snippet"><span>Comment</span>構造学習ガチ勢のCrammerの論文

<br>



<br>

構造学習やるなら読んだ方が良い</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/OnlineLearning.html">#OnlineLearning</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/119">オンライン学習</a>
<span class="snippet"><span>Comment</span>目次

<br>



<br>

定式化

<br>



<br>

評価法：Regretなど

<br>



<br>

パーセプトロン

<br>



<br>

Passive Aggressive Algorithm

<br>



<br>

(アルゴリズムと損失の限界の評価）

<br>



<br>

Confidence Weighted Algorithm

<br>



<br>

Pegasos

<br>



<br>

Coordinate Descent

<br>



<br>

バッチ、オンライン、ストリームの比較

<br>



<br>

ビッグデータへの対応</span>
<button onclick="hideContent(0)" style="display: none;">hide</button>
</div>


    </div>

</article>
<div class="post-nav">
<a class="previous" href="/paper_notes/articles/MLSys.html" title="MLSysに関する論文・技術記事メモの一覧">MLSysに関する論文・技術記事メモの一覧</a><a class="next" href="/paper_notes/articles/MachineTranslation.html" title="MachineTranslationに関する論文・技術記事メモの一覧">MachineTranslationに関する論文・技術記事メモの一覧</a>
</div>
<div class="post-related">
      <div>Related Articles</div>
      <ul>
        <li class="">
          <a class="post-link" href="/paper_notes/articles/ImageSegmentation.html" title="ImageSegmentationに関する論文・技術記事メモの一覧">
            ImageSegmentationに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/TMLR.html" title="TMLRに関する論文・技術記事メモの一覧">
            TMLRに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/ChatGPT.html" title="ChatGPTに関する論文・技術記事メモの一覧">
            ChatGPTに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/SimulST(SimultaneousSpeechTranslation).html" title="SimulST(SimultaneousSpeechTranslation)に関する論文・技術記事メモの一覧">
            SimulST(SimultaneousSpeechTranslation)に関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
</ul>
    </div>
<div class="post-comments"></div></section>
</div>


  </section>
  <section class="sidebar" style="margin-left: 15px;">
    <!-- Get sidebar items --><style type="text/css" media="screen">
.post-menu ul {
  list-style: none;
  padding: 0;
  margin: 0;
}
</style>

<div class="post-menu">
  <div class="post-menu-title">TOC</div>
  <div class="post-menu-content"></div>
</div>

<script>
  function generateContent() {
    var menu = document.querySelector(".post-menu");
    var menuContent =  menu.querySelector(".post-menu-content");
    var headings = document.querySelector(".post-content").querySelectorAll("h2, h3, h4, h5, h6");

    // Hide menu when no headings
    if (headings.length === 0) {
      return menu.style.display = "none";
    }

    // Generate post menu
    var menuHTML = '';
    for (var i = 0; i < headings.length; i++) {
      var h = headings[i];
      menuHTML += (
        '<li class="h-' + h.tagName.toLowerCase() + '">'
        + '<a href="#h-' + h.getAttribute('id') + '">' + h.textContent + '</a></li>');
    }

    menuContent.innerHTML = '<ul>' + menuHTML + '</ul>';

    // The header element
    var header = document.querySelector('header.site-header');

    function doMenuCollapse(index, over_items) {
      var items = menuContent.firstChild.children;

      if (over_items == undefined) {
        over_items = 20;
      }

      if (items.length < over_items) {
        return;
      }

      var activeItem = items[index];
      var beginItem = activeItem
      var endItem = activeItem
      var beginIndex = index;
      var endIndex = index + 1;
      while (beginIndex >= 0
        && !items[beginIndex].classList.contains('h-h2')) {
        beginIndex -= 1;
      }
      while (endIndex < items.length
        && !items[endIndex].classList.contains('h-h2')) {
        endIndex += 1;
      }
      for (var i = 0; i < beginIndex; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
      for (var i = beginIndex + 1; i < endIndex; i++) {
        item = items[i]
        // if (!item.classList.contains('h-h2')) {
          item.style.display = '';
        // }
      }
      for (var i = endIndex; i < items.length; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
    }

    // Init menu collapsed
    doMenuCollapse(-1);

    // Active the menu item
    window.addEventListener('scroll', function (event) {
      var lastActive = menuContent.querySelector('.active');
      var changed = true;
      var activeIndex = -1;
      for (var i = headings.length - 1; i >= 0; i--) {
        var h = headings[i];
        var headingRect = h.getBoundingClientRect();
        var headerRect = header.getBoundingClientRect();
        var headerTop = Math.floor(headerRect.top);
        var headerHeight = Math.floor(headerRect.height);
        var headerHeight = headerTop + headerHeight + 20;
        if (headingRect.top <= headerHeight) {
          var id = 'h-' + h.getAttribute('id');
          var a = menuContent.querySelector('a[href="#' + id  + '"]');
          var curActive = a.parentNode;
          if (curActive) {
            curActive.classList.add('active');
            activeIndex = i;
          }
          if (lastActive == curActive) {
            changed = false;
          }
          break;
        }
      }
      if (changed) {
        if (lastActive) {
          lastActive.classList.remove('active');
        }
        doMenuCollapse(activeIndex);
      }
      event.preventDefault();
    });
  }
  generateContent();
</script>
</section>
</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/paper_notes/"></data>

  <div class="wrapper">
    <div class="site-footer-inner">
<div>Copyright © 2023-current AkihikoWATANABE. The header images and any thumbnail images for the posts were generated by ChatGPT's DALL-E3.</div>
      <div>Powered by <a title="Jekyll is a simple, blog-aware, static site
      generator." href="https://jekyllrb.com/">Jekyll</a> &amp; <a title="Yat, yet
      another theme." href="https://github.com/jeffreytse/jekyll-theme-yat">Yat Theme</a>.</div>
      <div class="footer-col rss-subscribe">Subscribe <a href="/paper_notes/feed.xml">via RSS</a>
</div>
    </div>
  </div>
</footer>
</body>
</html>
