<!DOCTYPE html>
<html lang="ja">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-translate-customization" content="108d9124921d80c3-80e20d618ff053c8-g4f02ec6f3dba68b7-c">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Selected Papers/Blogsに関する論文・技術記事メモの一覧 | わたしのべんきょうノート</title>
<meta name="generator" content="Jekyll v4.3.2">
<meta property="og:title" content="Selected Papers/Blogsに関する論文・技術記事メモの一覧">
<meta name="author" content="AkihikoWATANABE">
<meta property="og:locale" content="ja">
<meta name="description" content="Selected Papers/Blogs">
<meta property="og:description" content="Selected Papers/Blogs">
<link rel="canonical" href="http://akihikowatanabe.github.io/paper_notes/articles/Selected%20Papers_Blogs.html">
<meta property="og:url" content="http://akihikowatanabe.github.io/paper_notes/articles/Selected%20Papers_Blogs.html">
<meta property="og:site_name" content="わたしのべんきょうノート">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-09-11T00:44:54+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Selected Papers/Blogsに関する論文・技術記事メモの一覧">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"AkihikoWATANABE"},"dateModified":"2025-09-11T00:44:54+00:00","datePublished":"2025-09-11T00:44:54+00:00","description":"Selected Papers/Blogs","headline":"Selected Papers/Blogsに関する論文・技術記事メモの一覧","mainEntityOfPage":{"@type":"WebPage","@id":"http://akihikowatanabe.github.io/paper_notes/articles/Selected%20Papers_Blogs.html"},"url":"http://akihikowatanabe.github.io/paper_notes/articles/Selected%20Papers_Blogs.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="icon" href="">
  <link rel="canonical" href="http://akihikowatanabe.github.io">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/main.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/custom_button.css">
  <script src="/paper_notes/assets/js/main.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script><link type="application/atom+xml" rel="alternate" href="http://akihikowatanabe.github.io/paper_notes/feed.xml" title="わたしのべんきょうノート">
<script>
  function showMore(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "block";
          
      // このボタンの参照を取得して非表示にします
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "none";
      }
          
      // hideボタンの参照を取得して表示します
      const hideButton = contentDiv.querySelector('button[onclick^="hideContent"]');
      if (hideButton) {
        hideButton.style.display = "block";
      }
    }
  }

  function hideContent(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "none";
  
      // moreボタンの参照を取得して表示します
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "block";
      }
  
      // このボタンを隠します
      const hideButtons = document.querySelectorAll('button[onclick^="hideContent"]');
      const hideButton = hideButtons[index];
      if (hideButton) {
        hideButton.style.display = "none";
      }
    }
  }
</script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script charset="UTF-8" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/go.min.js" async></script>



















<script>
// Init highlight js
document.addEventListener('DOMContentLoaded', function(event) {
  var els = document.querySelectorAll('pre code')

  function addLangData(block) {
    var outer = block.parentElement.parentElement.parentElement;
    var lang = block.getAttribute('data-lang');
    for (var i = 0; i < outer.classList.length; i++) {
      var cls = outer.classList[i];
      if (cls.startsWith('language-')) {
        lang = cls;
        break;
      }
    }
    if (!lang) {
      cls = block.getAttribute('class');
      lang = cls ? cls.replace('hljs ', '') : '';
    }
    if (lang.startsWith('language-')) {
      lang = lang.substr(9);
    }
    block.setAttribute('class', 'hljs ' + lang);
    block.parentNode.setAttribute('data-lang', lang);
  }

  function addBadge(block) {
    var enabled = ('true' || 'true').toLowerCase();
    if (enabled == 'true') {
      var pre = block.parentElement;
      pre.classList.add('badge');
    }
  }

  function handle(block) {
    addLangData(block);
    addBadge(block)
    hljs.highlightBlock(block);
  }

  for (var i = 0; i < els.length; i++) {
    var el = els[i];
    handle(el);
  }
});
</script>

<style>
  /* code language badge */
  pre.badge::before {
    content: attr(data-lang);
    color: #fff;
    background-color: #ff4e00;
    padding: 0 .5em;
    border-radius: 0 2px;
    text-transform: uppercase;
    text-align: center;
    min-width: 32px;
    display: inline-block;
    position: absolute;
    right: 0;
  }

  /* fix wrong badge display for firefox browser */
  code > table pre::before {
    display: none;
  }
</style>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe-lightbox.umd.min.js" async></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe.umd.min.js" async></script>
<link href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/photoswipe.min.css" rel="stylesheet">
<style>
  .pswp .pswp__container .pswp__img {
    background-color: white;
  }
</style>

<script>
  function initPhotoSwipe() {
    let customOptions = {};

    try {
      const data = `{"gallery"=>"section.main", "children"=>"a.photo-swipe", "bgOpacity"=>0.8, "padding"=>{"top"=>20, "bottom"=>40, "left"=>100, "right"=>100}}`.replaceAll("=>", ":");
      customOptions = JSON.parse(data);
    } catch (e) {
      console.info("Invalid custom photo previewer options! " + e.message);
    }

    // Define object and gallery options
    const options = Object.assign(
      {
        gallery: "section.main",
        children: "a.photo-swipe",
        photo_scale: 2,
        // dynamic import is not supported in UMD version
        pswpModule: PhotoSwipe,
      },
      customOptions
    );

    const galleryEl = document.querySelector(options.gallery);
    if (!galleryEl) {
      return;
    }

    const imgEls = [];
    const els = galleryEl.querySelectorAll("img:not(.emoji)");
    els.forEach((el) => {
      if (el.src.trim() == "") {
        return;
      }
      if (!imgEls.includes(el)) {
        imgEls.push(el);
      }
    });

    if (imgEls.length === 0) {
      return;
    }

    imgEls.forEach((imgEl) => {
      imgEl.outerHTML = `
      <a class="photo-swipe"
        href="${imgEl.src}"
        data-pswp-width="${
          Math.max(imgEl.naturalWidth, imgEl.width) * options.photo_scale
        }"
        data-pswp-height="${
          Math.max(imgEl.naturalHeight, imgEl.height) * options.photo_scale
        }"
        data-pswp-caption="${imgEl.getAttribute("caption") || imgEl.alt}"
        target="_blank">
        ${imgEl.outerHTML}
      </a>`;
    });

    // Initialize PhotoSwipe 5
    var lightbox = new PhotoSwipeLightbox(options);

    lightbox.init();
  }

  window.addEventListener("load", initPhotoSwipe);
</script>
<meta name="google-site-verification" content="u_DTTPcCZ806iq51zgirHyWq3556HUKGq8AQfH91iFI">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-P70KSB88WH"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-P70KSB88WH');
  </script>
</head>
<body>





































































































































<header class="site-header site-header-transparent" role="banner">

  <div class="wrapper">
    <div class="site-header-inner">
<span class="site-brand"><a class="site-brand-inner" rel="author" href="/paper_notes/">
  <img class="site-favicon" title="わたしのべんきょうノート" src="" onerror="this.style.display='none'">
  わたしのべんきょうノート
</a>
</span><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger">
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewbox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
              </svg>
            </span>
          </label>

          <div class="trigger">









<span class="page-link">



<div id="google_translate_element" style="display: none;">
</div>

<span class="ct-language">
  <ul class="list-unstyled ct-language-dropdown">
    
      <li>
        <a href="#" class="lang-select" data-lang="en">
          
          <img src="https://cdn.countryflags.com/thumbs/united-states-of-america/flag-400.png" title="English">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="fr">
          
          <img src="https://cdn.countryflags.com/thumbs/france/flag-400.png" title="French">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="zh-CN">
          
          <img src="https://cdn.countryflags.com/thumbs/china/flag-400.png" title="Chinese(Simple)">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ja">
          
          <img src="https://cdn.countryflags.com/thumbs/japan/flag-400.png" title="Japanese">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ko">
          
          <img src="https://cdn.countryflags.com/thumbs/south-korea/flag-400.png" title="Korean">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ru">
          
          <img src="https://cdn.countryflags.com/thumbs/russia/flag-400.png" title="Russian">
          
        </a>
      </li>
    
  </ul>
</span>

<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({
    pageLanguage: 'ja',
    autoDisplay: false,
    layout: google.translate.TranslateElement.InlineLayout.VERTICAL
  }, 'google_translate_element');

  // Links to cross-origin destinations are unsafe
  var gll = document.getElementsByClassName('goog-logo-link')[0];
  if (gll) {
    gll.setAttribute('rel', 'noopener');
  }

  function restoreLang() {
    var iframe = document.getElementsByClassName('goog-te-banner-frame')[0];
    if (!iframe) return;

    var innerDoc = iframe.contentDocument || iframe.contentWindow.document;
    var restore_el = innerDoc.getElementsByTagName("button");

    for (var i = 0; i < restore_el.length; i++) {
      if (restore_el[i].id.indexOf("restore") >= 0) {
        restore_el[i].click();
        var close_el = innerDoc.getElementsByClassName("goog-close-link");
        close_el[0].click();
        return;
      }
    }
  }

  function triggerHtmlEvent(element, eventName) {
    var event;
    if (document.createEvent) {
      event = document.createEvent('HTMLEvents');
      event.initEvent(eventName, true, true);
      element.dispatchEvent(event);
    } else {
      event = document.createEventObject();
      event.eventType = eventName;
      element.fireEvent('on' + event.eventType, event);
    }
  }

  var googleCombo = document.querySelector("select.goog-te-combo");
  var langSelect = document.querySelector('.ct-language');
  langSelect.addEventListener('click', function(event) {
    if (!event.target) {
      return;
    }

    var selected = document.querySelector('.ct-language .ct-language-selected');
    if (selected) {
      selected.classList.remove('ct-language-selected');
    }

    var target = event.target;
    while (target && target !== langSelect ) {
      if (target.matches('.lang-select')) {
        break;
      }
      target = target.parentElement;
    }

    if (target && target.matches('.lang-select')) {
      var lang = target.getAttribute('data-lang');
      if (googleCombo.value == lang) {
        restoreLang();
      } else {
        target.parentElement.classList.add('ct-language-selected');
        googleCombo.value = lang;
        triggerHtmlEvent(googleCombo, 'change');
      }
    }

    event.preventDefault();
  });
}
</script>

<script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit" async></script>
</span>
</div>
        </nav>
</div>
  </div>
</header>

<script>
  function initHeader() {
    var lastScrollY = getScrollPos().y;
    var documentElement = document.documentElement;

    function storeScrollData() {
      var y = getScrollPos().y;documentElement.setAttribute("data-header-transparent", "");var scrollStatus = "";

      if (y <= 0) {
        scrollStatus = "top";
      } else if ((window.innerHeight + y) >= document.body.offsetHeight) {
        scrollStatus = "bottom";
      } else {
        var isScrollDown = (y - lastScrollY > 0) ? true : false;
        scrollStatus = isScrollDown ? "down" : "up";
      }

      lastScrollY = y;
      documentElement.setAttribute("data-scroll-status", scrollStatus);
    }

    window.addEventListener('scroll', function(e) {
      storeScrollData();
    });

    storeScrollData();
  }
  document.addEventListener('DOMContentLoaded', initHeader);
</script>


























































































































































<style>
    html .page-banner .page-banner-img > *:first-child {
      opacity: 0.4;
    }

    html[data-theme="dark"] .page-banner .page-banner-img > *:first-child {
      opacity: 0.2872;
    }
  </style>
<section class="page-banner">
    <div class="page-banner-img">
<div style="background-image: url(/paper_notes/assets/images/banner.png)"></div>
        <img class="img-placeholder" src="/paper_notes/assets/images/banner.png">
</div>
    <div class="wrapper">
      <div class="page-banner-inner">
<header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">わたしのべんきょうノート</h1>
  <h2 class="post-subtitle">勉強した論文や技術等の情報をGithubのIssueにメモっているひとのブログ。
それなりにメモの量が蓄積されてきたので、一度整理したいなと思いブログはじめてみました！
自然言語処理(NLP), 推薦システム(RecommenderSystem), Educational Data Mining (EDM), Learning Analytics (LA)などの分野のメモが多いと思います。
最近は特にLLMの勉強が多めです :)</h2>

  <div class="post-meta">
    <time class="dt-published" datetime="2025-09-11T00:44:54+00:00" itemprop="datePublished"><i class="fa fa-calendar"></i> Sep 11, 2025
    </time><span class="post-author left-vsplit"><i class="fa fa-pencil"></i> AkihikoWATANABE</span>
    
































    <span class="post-reading-time left-vsplit"><i class="fa fa-clock-o"></i> About 6 hours 38 mins</span>
  </div></header>
</div>
    </div>
  </section><script>
  function hashLocate(hashValue) {
    hashValue = hashValue.replace(/^.*#h-/, '');
    hashValue = decodeURIComponent(hashValue);
    var element = document.getElementById(hashValue);

    if (!element) {
      return;
    }

    var header = document.querySelector('header.site-header');
    var headerRect = header.getBoundingClientRect();
    var headerTop = Math.floor(headerRect.top);
    var headerHeight = Math.floor(headerRect.height);
    var scrollPos = getScrollPos();
    var offsetY = element.offsetTop - (headerTop + headerHeight + 20);

    if (offsetY == scrollPos.y) {
      return;
    }

    if (headerTop == 0  && offsetY > scrollPos.y) {
      offsetY += headerHeight + 2;
    } else if (headerTop < 0  && offsetY < scrollPos.y) {
      offsetY -= headerHeight - 2;
    }

    smoothScrollTo(offsetY);
  }

  // The first event occurred
  window.addEventListener('load', function(event) {
    if (window.location.hash) {
      hashLocate(window.location.hash);
    }
  });

  // The first event occurred
  window.addEventListener('click', function(event) {
    if (event.target.tagName.toLowerCase() == 'a') {
      hashLocate(event.target.getAttribute('href'));
    }
  });
</script>
<div class="theme-toggle">
  <input type="checkbox" id="theme-switch">
  <label for="theme-switch">
    <div class="toggle"></div>
    <div class="names">
      <p class="light">Light</p>
      <p class="dark">Dark</p>
    </div>
  </label>
</div>




<script>
  (function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var nightModeOption = ('off' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
      storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
      var data = storage.getItem('theme');
      try {
        data = JSON.parse(data ? data : '');
      } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
      }
      return data;
    }

    function handleThemeToggle(nightShift) {
      themeData.nightShift = nightShift;
      saveThemeData(themeData);
      html.dataset.theme = nightShift ? 'dark' : 'light';
      setTimeout(function() {
        sw.checked = nightShift ? true : false;
      }, 50);
    }

    function autoThemeToggle() {
      // Next time point of theme toggle
      var now = new Date();
      var toggleAt = new Date();
      var hours = now.getHours();
      var nightShift = hours >= 19 || hours <=7;

      if (nightShift) {
        if (hours > 7) {
          toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
      } else {
        toggleAt.setHours(19);
      }

      toggleAt.setMinutes(0);
      toggleAt.setSeconds(0);
      toggleAt.setMilliseconds(0)

      var delay = toggleAt.getTime() - now.getTime();

      // auto toggle theme mode
      setTimeout(function() {
        handleThemeToggle(!nightShift);
      }, delay);

      return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
      };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
      handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
      var data = autoThemeToggle();

      // Toggle theme by local setting
      if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
      } else {
        handleThemeToggle(themeData.nightShift);
      }
    } else if (nightModeOption == 'manual') {
      handleThemeToggle(themeData.nightShift);
    } else {
      var nightShift = themeData.nightShift;
      if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
      }
      handleThemeToggle(nightShift);
    }
  })();
</script>
<div id="click-to-top" class="click-to-top">
  <i class="fa fa-arrow-up"></i>
</div>
<script>
  (function () {
    const clickToTop = document.getElementById('click-to-top');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 100) {
        clickToTop.classList.add('show')
      }else {
        clickToTop.classList.remove('show')
      }
    });
    clickToTop.addEventListener('click', () => {
      window.smoothScrollTo(0);
    });
  })();
</script>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="framework">
  <section class="main">

     <div class="post">
  <section>









<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="post-content e-content" itemprop="articleBody">

      <h2 id="selected-papersblogs">Selected Papers/Blogs</h2>

<div class="visible-content">
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/read-later.html">#read-later</a>


<br>


<span class="issue_date">Issue Date: 2025-09-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2726">[Paper Note] SpikingBrain Technical Report: Spiking Brain-inspired Large Models, Yuqi Pan+, arXiv'25</a>
<span class="snippet"><span>Summary</span>SpikingBrainは、長いコンテキストの効率的なトレーニングと推論のために設計された脳にインスパイアされたモデルで、MetaX GPUクラスターを活用。線形およびハイブリッド線形アーキテクチャを採用し、非NVIDIAプラットフォーム上での大規模LLM開発を実現。SpikingBrain-7BとSpikingBrain-76Bを開発し、約150BトークンでオープンソースのTransformerと同等の性能を達成。トレーニング効率を大幅に改善し、低消費電力での運用を可能にすることを示した。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/f14bertolotti/status/1964949822429069761?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QTTFTが4Mコンテキストの時にQwen2.5と比べて100倍高速化…？中国のMetaX社のGPUが利用されている。

<br>



<br>

https://www.metax-tech.com/en/goods/prod.html?cid=3</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/RAG(RetrievalAugmentedGeneration).html">#RAG(RetrievalAugmentedGeneration)</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/SpeculativeDecoding.html">#SpeculativeDecoding</a>


<br>


<span class="issue_date">Issue Date: 2025-09-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2716">[Paper Note] REFRAG: Rethinking RAG based Decoding, Xiaoqiang Lin+, arXiv'25</a>
<span class="snippet"><span>Summary</span>REFRAGは、RAGアプリケーションにおける遅延を改善するための効率的なデコーディングフレームワークであり、スパース構造を利用して初回トークンまでの時間を30.85倍加速します。これにより、LLMsのコンテキストサイズを16まで拡張可能にし、さまざまな長コンテキストタスクで精度を損なうことなくスピードアップを実現しました。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/dr_singularity/status/1964453705430036982?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q興味深い。Speculative Decodingの新手法ともみなせそう。同時期に出た下記研究と比較してどのようなpros/consがあるだろうか？

<br>

・2699</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/LLMAgent.html">#LLMAgent</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/Coding.html">#Coding</a>
<a class="button" href="articles/SoftwareEngineering.html">#SoftwareEngineering</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/Contamination.html">#Contamination</a>


<br>


<span class="issue_date">Issue Date: 2025-09-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2710">[Paper Note] SWE-rebench: An Automated Pipeline for Task Collection and  Decontaminated Evaluation of Software Engineering Agents, Ibragim Badertdinov+, arXiv'25</a>
<span class="snippet"><span>Summary</span>LLMベースのエージェントのSWEタスクにおける課題として、高品質なトレーニングデータの不足と新鮮なインタラクティブタスクの欠如が挙げられる。これに対処するため、21,000以上のインタラクティブなPythonベースのSWEタスクを含む公的データセットSWE-rebenchを自動化されたパイプラインで構築し、エージェントの強化学習に適したベンチマークを提供。これにより、汚染のない評価が可能となり、いくつかのLLMの性能が過大評価されている可能性を示した。</span>
<span class="snippet"><span>Comment</span>pj page:https://swe-rebench.com元ポスト:https://x.com/gneubig/status/1963947072748412990?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qコンタミネーションのない最新のIssueを用いて評価した結果、Sonnet 4が最も高性能</span>
</div>
<p><button onclick="showMore(0)">more</button></p>
<div class="hidden-content">
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/InstructionFollowingCapability.html">#InstructionFollowingCapability</a>
<span class="issue_date">Issue Date: 2025-09-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2702">[Paper Note] Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow  Real Instructions?, Qinyan Zhang+, arXiv'25</a>
<span class="snippet"><span>Summary</span>大規模言語モデル（LLMs）は、標準化されたパターンに従うことに苦労することがある。これを評価するために、Inverse IFEvalというベンチマークを提案し、モデルが対立する指示に従う能力を測定する。8種類の課題を含むデータセットを構築し、既存のLLMに対する実験を行った結果、非従来の文脈での適応性も考慮すべきであることが示された。Inverse IFEvalは、LLMの指示遵守の信頼性向上に寄与することが期待される。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/arankomatsuzaki/status/1963822451550208101?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q興味深い</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Optimizer.html">#Optimizer</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-09-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2674">[Paper Note] Fantastic Pretraining Optimizers and Where to Find Them, Kaiyue Wen+, arXiv'25</a>
<span class="snippet"><span>Summary</span>AdamWは言語モデルの事前学習で広く使用されているオプティマイザですが、代替オプティマイザが1.4倍から2倍のスピードアップを提供するという主張には二つの欠点があると指摘。これらは不均等なハイパーパラメータ調整と誤解を招く評価設定であり、10種類のオプティマイザを系統的に研究することで、公正な比較の重要性を示した。特に、最適なハイパーパラメータはオプティマイザごとに異なり、モデルサイズが大きくなるにつれてスピードアップ効果が減少することが明らかになった。最も高速なオプティマイザは行列ベースの前処理器を使用しているが、その効果はモデルスケールに反比例する。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/iscienceluvr/status/1963168542872014943?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q重要そうに見える関連:

<br>

・2202

<br>

・2675著者ポスト:

<br>

・https://x.com/wen_kaiyue/status/1963633867140526319?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>

・https://x.com/percyliang/status/1963648131394122222?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q考察:https://x.com/rosinality/status/1964098785019060719?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-09-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2669">[Paper Note] AHELM: A Holistic Evaluation of Audio-Language Models, Tony Lee+, arXiv'25</a>
<span class="snippet"><span>Summary</span>音声言語モデル（ALMs）の評価には標準化されたベンチマークが欠如しており、これを解決するためにAHELMを導入。AHELMは、ALMsの多様な能力を包括的に測定するための新しいデータセットを集約し、10の重要な評価側面を特定。プロンプトや評価指標を標準化し、14のALMsをテストした結果、Gemini 2.5 Proが5つの側面でトップにランクされる一方、他のモデルは不公平性を示さなかった。AHELMは今後も新しいデータセットやモデルを追加予定。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/iscienceluvr/status/1962799344001917360?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q関連:

<br>

・786</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/DeepResearch.html">#DeepResearch</a>
<a class="button" href="articles/Science.html">#Science</a>
<span class="issue_date">Issue Date: 2025-08-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2618">[Paper Note] DeepScholar-Bench: A Live Benchmark and Automated Evaluation for  Generative Research Synthesis, Liana Patel+, arXiv'25</a>
<span class="snippet"><span>Summary</span>生成的研究合成の評価のために、DeepScholar-benchというライブベンチマークと自動評価フレームワークを提案。これは、ArXiv論文からクエリを引き出し、関連研究セクションを生成する実際のタスクに焦点を当て、知識合成、検索品質、検証可能性を評価。DeepScholar-baseは強力なベースラインを確立し、他の手法と比較して競争力のあるパフォーマンスを示した。DeepScholar-benchは依然として難易度が高く、生成的研究合成のAIシステムの進歩に重要であることを示す。</span>
<span class="snippet"><span>Comment</span>leaderboard:https://guestrin-lab.github.io/deepscholar-leaderboard/leaderboard/deepscholar_bench_leaderboard.html元ポスト:https://x.com/lianapatel_/status/1961487232331911651?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/EMNLP.html">#EMNLP</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/Contamination.html">#Contamination</a>
<a class="button" href="articles/Game.html">#Game</a>
<span class="issue_date">Issue Date: 2025-08-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2609">[Paper Note] TurnaboutLLM: A Deductive Reasoning Benchmark from Detective Games, Yuan Yuan+, EMNLP'25</a>
<span class="snippet"><span>Summary</span>TurnaboutLLMという新しいフレームワークとデータセットを用いて、探偵ゲームのインタラクティブなプレイを通じてLLMsの演繹的推論能力を評価。証言と証拠の矛盾を特定する課題を設定し、12の最先端LLMを評価した結果、文脈のサイズや推論ステップ数がパフォーマンスに影響を与えることが示された。TurnaboutLLMは、複雑な物語環境におけるLLMsの推論能力に挑戦を提供する。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/hemuyu0327/status/1961275336530039244?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q非常に面白そう。逆転裁判のデータを利用した超long contextな演繹的タスクにおいて、モデルが最終的な回答を間違える際はより多くの正解には貢献しないReasoning Stepを繰り返したり、QwQ-32BとGPT4.1は同等の性能だが、non thinkingモデルであるGPT4.1がより少量のReasoning Step (本研究では回答に至るまでに出力したトークン数と定義)で回答に到達し（＝Test Time Scalingの恩恵がない）、フルコンテキストを与えて性能が向上したのはモデルサイズが大きい場合のみ（＝Test Timeのreasoningよりも、in-contextでのreasoningが重要）だった、といった知見がある模様。じっくり読みたい。</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Regularization.html">#Regularization</a>
<span class="issue_date">Issue Date: 2025-08-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2603">[Paper Note] Drop Dropout on Single-Epoch Language Model Pretraining, Houjun Liu+, arXiv'25</a>
<span class="snippet"><span>Summary</span>ドロップアウトは過学習を防ぐ手法として知られているが、現代の大規模言語モデル（LLM）では過学習が抑えられるため使用されていない。本研究では、BERTやPythiaモデルの単一エポック事前学習においてドロップアウトの影響を調査した結果、ドロップアウトを適用しない方が下流の性能が向上することが判明。また、「早期ドロップアウト」も性能を低下させることが示された。ドロップアウトなしで訓練されたモデルは、モデル編集においてもより成功することがわかり、単一エポックの事前学習中にはドロップアウトを省くことが推奨される。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/jiqizhixin/status/1961589435197505584?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q関連:

<br>

・2604</span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<a class="button" href="articles/Mathematics.html">#Mathematics</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-08-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2568">[Paper Note] Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math  Pretraining Dataset, Rabeeh Karimi Mahabadi+, arXiv'25</a>
<span class="snippet"><span>Summary</span>新しい数学コーパス「Nemotron-CC-Math」を提案し、LLMの推論能力を向上させるために、科学テキスト抽出のためのパイプラインを使用。従来のデータセットよりも高品質で、方程式やコードの構造を保持しつつ、表記を標準化。Nemotron-CC-Math-4+は、以前のデータセットを大幅に上回り、事前学習によりMATHやMBPP+での性能向上を実現。オープンソースとしてコードとデータセットを公開。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/karimirabeeh/status/1960682448867426706?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2553">[Paper Note] InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,  Reasoning, and Efficiency, Weiyun Wang+, arXiv'25</a>
<span class="snippet"><span>Summary</span>InternVL 3.5は、マルチモーダルモデルの新しいオープンソースファミリーで、Cascade Reinforcement Learningを用いて推論能力と効率を向上させる。粗から細へのトレーニング戦略により、MMMやMathVistaなどのタスクで大幅な改善を実現。Visual Resolution Routerを導入し、視覚トークンの解像度を動的に調整。Decoupled Vision-Language Deployment戦略により、計算負荷をバランスさせ、推論性能を最大16.0%向上させ、速度を4.05倍向上。最大モデルは、オープンソースのMLLMで最先端の結果を達成し、商業モデルとの性能ギャップを縮小。全てのモデルとコードは公開。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/gm8xx8/status/1960076908088922147?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Alignment.html">#Alignment</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<a class="button" href="articles/DataMixture.html">#DataMixture</a>
<span class="issue_date">Issue Date: 2025-08-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2537">[Paper Note] Motif 2.6B Technical Report, Junghwan Lim+, arXiv'25</a>
<span class="snippet"><span>Summary</span>Motif-2.6Bは、26億パラメータを持つ基盤LLMで、長文理解の向上や幻覚の減少を目指し、差分注意やポリノルム活性化関数を採用。広範な実験により、同サイズの最先端モデルを上回る性能を示し、効率的でスケーラブルな基盤LLMの発展に寄与する。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/scaling01/status/1959604841577357430?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QHF:https://huggingface.co/Motif-Technologies/Motif-2.6B・アーキテクチャ

<br>

  ・1466

<br>

  ・2538

<br>

・学習手法

<br>

  ・1979

<br>

    ・8B token学習するごとに直近6つのcheckpointのelement-wiseの平均をとりモデルマージ。当該モデルに対して学習を継続、ということを繰り返す。これにより、学習のノイズを低減し、突然パラメータがシフトすることを防ぐ

<br>

  ・1060

<br>

    ・Adaptive Base Frequency (RoPEのbase frequencyを10000から500000にすることでlong contextのattention scoreが小さくなりすぎることを防ぐ)

<br>

  ・2540 

<br>

・事前学習データ

<br>

  ・1943

<br>

  ・2539

<br>

  ・2109 

<br>



<br>

を利用したモデル。同程度のサイズのモデルとの比較ではかなりのgainを得ているように見える。興味深い。

<br>

DatasetのMixtureの比率などについても記述されている。

<br>



<br>

<img width="705" height="441" alt="Image" src="https://github.com/user-attachments/assets/0a26442e-8075-4cbe-8cc1-f1ff471b7356"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Decoding.html">#Decoding</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-08-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2532">[Paper Note] Deep Think with Confidence, Yichao Fu+, arXiv'25</a>
<span class="snippet"><span>Summary</span>「Deep Think with Confidence（DeepConf）」は、LLMの推論タスクにおける精度と計算コストの課題を解決する手法で、モデル内部の信頼性信号を活用して低品質な推論を動的にフィルタリングします。追加の訓練や調整を必要とせず、既存のフレームワークに統合可能です。評価の結果、特に難易度の高いAIME 2025ベンチマークで99.9%の精度を達成し、生成トークンを最大84.7%削減しました。</span>
<span class="snippet"><span>Comment</span>pj page:https://jiaweizzhao.github.io/deepconf

<br>

vLLMでの実装:https://jiaweizzhao.github.io/deepconf/static/htmls/code_example.html元ポスト:https://x.com/jiawzhao/status/1958982524333678877?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qtooluse、追加の訓練なしで、どのようなタスクにも適用でき、85%生成トークン量を減らした上で、OpenModelで初めてAIME2025において99% Acc.を達成した手法とのこと。vLLMを用いて50 line程度で実装できるらしい。reasoning traceのconfidence(i.e., 対数尤度)をgroup sizeを決めてwindow単位で決定し、それらをデコーディングのプロセスで活用することで、品質の低いreasoning traceに基づく結果を排除しつつ、majority votingに活用する方法。直感的にもうまくいきそう。オフラインとオンラインの推論によって活用方法が提案されている。あとでしっかり読んで書く。Confidenceの定義の仕方はグループごとのbottom 10%、tailなどさまざまな定義方法と、それらに基づいたconfidenceによるvotingの重み付けが複数考えられ、オフライン、オンラインによって使い分ける模様。

<br>



<br>

vLLMにPRも出ている模様？</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LLMAgent.html">#LLMAgent</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/x-Use.html">#x-Use</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-08-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2518">[Paper Note] MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents, Shilong Li+, arXiv'25</a>
<span class="snippet"><span>Summary</span>MM-BrowseCompは、AIエージェントのマルチモーダル検索および推論能力を評価する新しいベンチマークで、224の手作りの質問を含む。これにより、画像や動画を含む情報の重要性を考慮し、テキストのみの手法の限界を示す。最先端モデルの評価では、OpenAI o3などのトップモデルでも29.02%の精度にとどまり、マルチモーダル能力の最適化不足が明らかになった。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/gezhang86038849/status/1958381269617955165?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/MachineTranslation.html">#MachineTranslation</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/Japanese.html">#Japanese</a>
<a class="button" href="articles/DPO.html">#DPO</a>
<a class="button" href="articles/ModelMerge.html">#ModelMerge</a>
<span class="issue_date">Issue Date: 2025-08-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2517">PLaMo Translate: 翻訳特化大規模言語モデルの開発,今城+, Jxiv'25</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/imos/status/1958687896321630355?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QSFT-&gt;Iterative DPO-&gt;Model Mergeのパイプライン。SFTでは青空文庫などのオープンなデータから指示追従性能の高いDeepSeek-V3-0324によって元データ→翻訳, 翻訳→再翻訳データを合成し活用。また、翻訳の指示がprompt中に存在せずとも（本モデルを利用するのは翻訳用途であることが自明であるからと推察される）翻訳を適切に実行できるよう、独自のテンプレートを学習。文体指定、常体、敬体の指定、文脈考慮、語彙指定それぞれにういて独自のタグを設けてフォーマットを形成し翻訳に特化したテンプレートを学習。

<br>



<br>

IterativeDPOでは、DeepSeekV3に基づくLLM-as-a-Judgeと、MetricX(2658)に基づいてReward Modelをそれぞれ学習し、1つの入力に対して100個の翻訳を作成しそれぞれのRewardモデルのスコアの合計値に基づいてRejection Samplingを実施することでPreference dataを構築。3段階のDPOを実施し、段階ごとにRewardモデルのスコアに基づいて高品質なPreference Dataに絞ることで性能向上を実現。

<br>



<br>

モデルマージではDPOの各段階のモデルを重み付きでマージすることで各段階での長所を組み合わせたとのこと。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/LLMAgent.html">#LLMAgent</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/x-Use.html">#x-Use</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-08-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2451">[Paper Note] BrowseComp: A Simple Yet Challenging Benchmark for Browsing Agents, Jason Wei+, arXiv'25</a>
<span class="snippet"><span>Summary</span>BrowseCompは、エージェントのウェブブラウジング能力を測定するための1,266の質問からなるベンチマークで、絡み合った情報を探すことを要求します。シンプルで使いやすく、短い回答が求められ、参照回答との照合が容易です。このベンチマークは、ブラウジングエージェントの能力を評価するための重要なツールであり、持続力と創造性を測定します。詳細はGitHubで入手可能です。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/LLMAgent.html">#LLMAgent</a>
<a class="button" href="articles/x-Use.html">#x-Use</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-08-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2436">[Paper Note] OpenCUA: Open Foundations for Computer-Use Agents, Xinyuan Wang+, arXiv'25</a>
<span class="snippet"><span>Summary</span>OpenCUAは、CUAデータと基盤モデルをスケールさせるためのオープンソースフレームワークであり、アノテーションインフラ、AgentNetデータセット、反射的なChain-of-Thought推論を持つスケーラブルなパイプラインを提供。OpenCUA-32Bは、CUAベンチマークで34.8%の成功率を達成し、最先端の性能を示す。研究コミュニティのために、アノテーションツールやデータセットを公開。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/gm8xx8/status/1956157162830418062?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q著者ポスト:https://x.com/xywang626/status/1956400403911962757?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QCUAにおいてProprietaryモデルに近い性能を達成した初めての研究な模様。重要</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-08-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2406">[Paper Note] GLM-4.5: Agentic, Reasoning, and Coding （ARC） Foundation Models, GLM-4. 5 Team+, arXiv'25</a>
<span class="snippet"><span>Summary</span>355Bパラメータを持つオープンソースのMixture-of-ExpertsモデルGLM-4.5を発表。ハイブリッド推論手法を採用し、エージェント的、推論、コーディングタスクで高いパフォーマンスを達成。競合モデルに比べて少ないパラメータ数で上位にランクイン。GLM-4.5とそのコンパクト版GLM-4.5-Airをリリースし、詳細はGitHubで公開。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/grad62304977/status/1954805614011453706?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q・アーキテクチャ

<br>

  ・MoE / sigmoid gates

<br>

    ・1719

<br>

    ・1754

<br>

  ・loss free balanced routing

<br>

    ・2442

<br>

  ・widthを小さく、depthを増やすことでreasoning能力改善

<br>

  ・GQA w/ partial RoPE

<br>

    ・1271

<br>

    ・1310

<br>

  ・Attention Headsの数を2.5倍（何に対して2.5倍なんだ、、？）（96個, 5120次元）にすることで（おそらく）事前学習のlossは改善しなかったがReasoning benchmarkの性能改善

<br>

  ・QK Normを導入しattentionのlogitsの値域を改善

<br>

    ・2443

<br>

  ・Multi Token Prediction

<br>

    ・2444

<br>

    ・1620

<br>



<br>

他モデルとの比較

<br>

<img src="https://github.com/user-attachments/assets/6085b99e-3a76-432c-a759-91dd4feeb219" alt="image" loading="lazy">

<br>



<br>

学習部分は後で追記する・事前学習データ

<br>

  ・web

<br>

    ・英語と中国語のwebページを利用

<br>

    ・1944 と同様にquality scoreyをドキュメントに付与

<br>

    ・最も低いquality scoreの文書群を排除し、quality scoreの高い文書群をup sampling

<br>

    ・最もquality scoreyが大きい文書群は3.2 epoch分利用

<br>

    ・多くのweb pageがテンプレートから自動生成されており高いquality scoreが付与されていたが、MinHashによってdeduplicationできなかったため、 2445 を用いてdocument embeddingに基づいて類似した文書群を排除

<br>

  ・Multilingual

<br>

    ・独自にクロールしたデータとFineWeb-2 2109 から多言語の文書群を抽出し、quality classifierを適用することでeducational utilityを定量化し、高いスコアの文書群をupsamplingして利用

<br>

  ・code

<br>

    ・githubなどのソースコードhosting platformから収集

<br>

    ・ソースコードはルールベースのフィルタリングをかけ、その後言語ごとのquality modelsによって、high,middle, lowの3つに品質を分類

<br>

    ・high qualityなものはupsamplingし、low qualityなものは除外

<br>

    ・2446 で提案されているFill in the Middle objectiveをコードの事前学習では適用

<br>

    ・コードに関連するweb文書も事前学習で収集したテキスト群からルールベースとfasttextによる分類器で抽出し、ソースコードと同様のqualityの分類とサンプリング手法を適用。最終的にフィルタリングされた文書群はre-parseしてフォーマットと内容の品質を向上させた

<br>

  ・math &amp; science

<br>

    ・web page, 本, 論文から、reasoning能力を向上させるために、数学と科学に関する文書を収集

<br>

  ・LLMを用いて文書中のeducational contentの比率に基づいて文書をスコアリングしスコアを予測するsmall-scaleな分類器を学習

<br>

  ・最終的に事前学習コーパスの中の閾値以上のスコアを持つ文書をupsampling

<br>

・事前学習は2 stageに分かれており、最初のステージでは、"大部分は"generalな文書で学習する。次のステージでは、ソースコード、数学、科学、コーディング関連の文書をupsamplingして学習する。

<br>



<br>

上記以上の細かい実装上の情報は記載されていない。

<br>



<br>

mid-training / post trainingについても後ほど追記する以下も参照のこと

<br>

・2318</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-08-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2382">[Paper Note] On the Generalization of SFT: A Reinforcement Learning Perspective with  Reward Rectification, Yongliang Wu+, arXiv'25</a>
<span class="snippet"><span>Summary</span>大規模言語モデル（LLM）の教師ありファインチューニング（SFT）の一般化能力を向上させるため、動的ファインチューニング（DFT）を提案。DFTはトークンの確率に基づいて目的関数を再スケーリングし、勾配更新を安定化させる。これにより、SFTを大幅に上回る性能を示し、オフライン強化学習でも競争力のある結果を得た。理論的洞察と実践的解決策を結びつけ、SFTの性能を向上させる。コードは公開されている。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/theturingpost/status/1953960036126142645?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qこれは大変興味深い。数学以外のドメインでの評価にも期待したい。3節冒頭から3.2節にかけて、SFTとon policy RLのgradientを定式化し、SFT側の数式を整理することで、SFT（のgradient)は以下のようなon policy RLの一つのケースとみなせることを導出している。そしてSFTの汎化性能が低いのは 1/pi_theta によるimportance weightingであると主張し、実験的にそれを証明している。つまり、ポリシーがexpertのgold responseに対して低い尤度を示してしまった場合に、weightか過剰に大きくなり、Rewardの分散が過度に大きくなってしまうことがRLの観点を通してみると問題であり、これを是正することが必要。さらに、分散が大きい報酬の状態で、報酬がsparse(i.e., expertのtrajectoryのexact matchしていないと報酬がzero)であることが、さらに事態を悪化させている。

<br>



<br>

&gt; conventional SFT is precisely an on-policy-gradient with the reward as an indicator function of

<br>

matching the expert trajectory but biased by an importance weighting 1/πθ.

<br>



<br>

まだ斜め読みしかしていないので、後でしっかり読みたい最近は下記で示されている通りSFTでwarm-upをした後にRLによるpost-trainingをすることで性能が向上することが示されており、

<br>

・1746 

<br>



<br>

主要なOpenModelでもSFT wamup -&gt; RLの流れが主流である。この知見が、SFTによるwarm upの有効性とどう紐づくだろうか？

<br>

これを読んだ感じだと、importance weightによって、現在のポリシーが苦手な部分のreasoning capabilityのみを最初に強化し（= warmup）、その上でより広範なサンプルに対するRLが実施されることによって、性能向上と、学習の安定につながっているのではないか？という気がする。日本語解説:https://x.com/hillbig/status/1960108668336390593?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>



<br>

一歩先の視点が考察されており、とても勉強になる。</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Optimizer.html">#Optimizer</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/ModelMerge.html">#ModelMerge</a>
<a class="button" href="articles/Stability.html">#Stability</a>
<span class="issue_date">Issue Date: 2025-08-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2340">[Paper Note] WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM  Pre-training, Changxin Tian+, arXiv'25</a>
<span class="snippet"><span>Summary</span>学習率スケジューリングの新たなアプローチとして、Warmup-Stable and Merge（WSM）を提案。WSMは、学習率の減衰とモデルマージの関係を確立し、さまざまな減衰戦略を統一的に扱う。実験により、マージ期間がモデル性能において重要であることを示し、従来のWSDアプローチを上回る性能向上を達成。特に、MATHで+3.5%、HumanEvalで+2.9%、MMLU-Proで+5.5%の改善を記録。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/stochasticchasm/status/1951427541803106714?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QWeight Decayを無くせるらしいエッセンスの解説:https://x.com/wenhaocha1/status/1951790366900019376?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>



<br>

チェックポイントさえ保存しておいて事後的に活用することだで、細かなハイパラ調整のための試行錯誤する手間と膨大な計算コストがなくなるのであれば相当素晴らしいのでは…？</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<a class="button" href="articles/Test-Time%20Scaling.html">#Test-Time Scaling</a>
<a class="button" href="articles/GRPO.html">#GRPO</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/Non-VerifiableRewards.html">#Non-VerifiableRewards</a>
<a class="button" href="articles/RewardModel.html">#RewardModel</a>
<span class="issue_date">Issue Date: 2025-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2274">[Paper Note] Inference-Time Scaling for Generalist Reward Modeling, Zijun Liu+, arXiv'25</a>
<span class="snippet"><span>Summary</span>強化学習を用いてLLMsの推論能力を向上させるため、報酬モデリング（RM）のスケーラビリティを探求。ポイントワイズ生成報酬モデリング（GRM）を採用し、自己原則批評調整（SPCT）を提案してパフォーマンスを向上。並列サンプリングとメタRMを導入し、スケーリング性能を改善。実験により、SPCTがGRMの質とスケーラビリティを向上させ、既存の手法を上回る結果を示した。DeepSeek-GRMは一部のタスクで課題があるが、今後の取り組みで解決可能と考えられている。モデルはオープンソースとして提供予定。</span>
<span class="snippet"><span>Comment</span>・inputに対する柔軟性と、

<br>

・同じresponseに対して多様なRewardを算出でき (= inference time scalingを活用できる)、 

<br>

・Verifiableな分野に特化していないGeneralなRewardモデルである

<br>



<br>

Inference-Time Scaling for Generalist Reward Modeling (GRM) を提案。

<br>



<br>

<img width="834" height="544" alt="Image" src="https://github.com/user-attachments/assets/18b13e49-745c-4c22-8d29-8b9bbb7fe80c">

<br>



<br>

Figure3に提案手法の学習の流れが図解されておりわかりやすい。</span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Optimizer.html">#Optimizer</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2202">[Paper Note] Muon is Scalable for LLM Training, Jingyuan Liu+, arXiv'25</a>
<span class="snippet"><span>Summary</span>Muonオプティマイザーを大規模モデルにスケールアップするために、ウェイトデケイとパラメータごとの更新スケール調整を導入。これにより、Muonは大規模トレーニングで即座に機能し、計算効率がAdamWの約2倍に向上。新たに提案するMoonlightモデルは、少ないトレーニングFLOPで優れたパフォーマンスを達成し、オープンソースの分散Muon実装や事前トレーニング済みモデルも公開。</span>
<span class="snippet"><span>Comment</span>解説ポスト:https://x.com/hillbig/status/1944902706747072678?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qこちらでも紹介されている:

<br>

・2208</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<a class="button" href="articles/Normalization.html">#Normalization</a>
<span class="issue_date">Issue Date: 2025-07-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2131">[Paper Note] The Curse of Depth in Large Language Models, Wenfang Sun+, arXiv'25</a>
<span class="snippet"><span>Summary</span>本論文では、「深さの呪い」という現象を紹介し、LLMの深い層が期待通りに機能しない理由を分析します。Pre-LNの使用が出力の分散を増加させ、深い層の貢献を低下させることを特定。これを解決するために層正規化スケーリング（LNS）を提案し、出力分散の爆発を抑制します。実験により、LNSがLLMの事前トレーニング性能を向上させることを示し、教師ありファインチューニングにも効果があることを確認しました。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/shiwei_liu66/status/1940377801032446428?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q・1795

<br>



<br>

ではそもそもLayerNormalizationを無くしていた（正確にいうとparametrize tanhに置換)が、どちらが優れているのだろうか？

<br>



<br>

<img src="https://github.com/user-attachments/assets/4bc557a0-ae23-4017-9837-7744de74c12e" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/2eead45c-209d-46e4-87e7-0129a4ec5ec2" alt="image" loading="lazy">・1332

<br>



<br>

では知識ニューロンの存在が示唆されており、これはTransformerの層の深い位置に存在し、かつ異なる知識間で知識ニューロンはシェアされない傾向にあった（ただしこれはPost-LNのBERTの話で本研究はPre-LNの話だが。Post-LNの勾配消失問題を緩和し学習を安定化させる研究も2141 のように存在する)。これはこの研究が明らかにしたこととどういう関係性があるだろうか。

<br>



<br>

また、LayerNormalizationのScalingによって深いTransformerブロックの導関数が単位行列となる（学習に寄与しなくなる）ことが改善された場合、知識ニューロンはどのように変化するだろうか？

<br>



<br>

（下記Geminiの応答を見た上での感想)なんとなーくだけれども、おそらく知識ニューロンの局所化が解消されるのかなーという気がする。

<br>



<br>

となると次の疑問としては、MoEアーキテクチャにはどのような影響があるだろうか？

<br>

そもそも知識ニューロンが局所化しているからMoEアーキテクチャのルータによって関連するExpertsのみをactivateすれば（というより結果的にそうなるように学習される）性能を劣化させずに計算効率を上げられていた、と仮定する。そうすると、知識ニューロンが局所化せずに多くのニューロンでシェアされるようになると、2110 のように、サブネットワーク間の情報を互いにやりとりできます、みたいな仕組みがより効いてきそうな気がする。

<br>



<br>

参考までに、Gemini2.5-Proに考察させてみた結果をメモとして残しておく（あくまで参考程度に...）

<br>

```

<br>

ご質問ありがとうございます。非常に興味深い着眼点ですね。「Knowledge Neurons in Pretrained Transformers」と「The Curse of Depth in Large Language Models」は、一見すると全く異なるテーマを扱っているように見えますが、「Transformerの深い層における振る舞い」という共通点で結びつけて考察すると、非常に示唆に富んだ関係性が見えてきます。

<br>



<br>

以下に、両方の論文の概要を解説し、その関係性について考察します。

<br>



<br>

1. Knowledge Neurons in Pretrained Transformers の概要

<br>

この研究は、事前学習済みTransformerモデル（特にBERTなど）の内部で、特定の事実知識がどのように格納されているかを調査したものです。

<br>



<br>

発見: モデルの中間層、特に全結合層（Feed-Forward Network, FFN）に、特定の知識（例：「ダンテ・アリギエーリはイタリアで生まれた」）に強く反応する「知識ニューロン」が存在することを発見しました。

<br>



<br>

特徴: これらの知識ニューロンは、モデルの深い層（後方の層）に、より多く存在する傾向がありました。

<br>



<br>

意味: これまでブラックボックスとされてきた大規模言語モデルの内部で、知識がどのように表現・局在化しているかについて、具体的な手がかりを与えた画期的な研究です。

<br>



<br>

2. The Curse of Depth in Large Language Models の概要

<br>

この研究は、LLMをより深く（層を多く）することの難しさに焦点を当て、その原因と解決策を提案したものです。

<br>



<br>

問題（深さの呪い）: Transformerの標準的なアーキテクチャ（Pre-LN）では、層が深くなるにつれて、LayerNormalization（LN）への入力の分散が指数関数的に増大してしまいます。

<br>



<br>

結果:

<br>



<br>

出力が大きくなりすぎて学習が不安定になります。

<br>



<br>

さらに深刻なのは、深い層ではモデルの出力に関する導関数（勾配計算に必要）がほぼ単位行列になってしまうことです。これは、その層が入力に対してほとんど変換を行わなくなり、学習に寄与しなくなることを意味します。

<br>



<br>

解決策: この問題を解決するため、各層のLayerNormalizationをその深さ（レイヤー番号 l）に応じてスケーリングするというシンプルな手法を提案しました。これにより、深い層でも勾配が適切に伝播し、学習が安定・改善することが示されました。

<br>



<br>

考察：2つの研究の関係性

<br>

これら2つの研究は、「学習の安定性」と「知識の格納方法」という異なる側面から、Transformerの深い層を分析していますが、両者には以下のような深い関係性があると考えられます。

<br>



<br>

学習の不安定性が「知識ニューロン」形成の背景にある可能性

<br>

「The Curse of Depth」で指摘されているように、標準的なTransformerの深い層は、本質的に学習が不安定で、勾配情報が失われやすい環境にあります。

<br>



<br>

この劣悪な学習環境こそが、「知識ニューロン」という形で知識が局所的に格納される原因の一つになっている可能性があります。つまり、

<br>



<br>

学習の非効率性: 深い層のほとんどのニューロンは、勾配消失問題のために効率的に学習を進めることができません。

<br>



<br>

専門化の発生: そのような不安定な環境下で、たまたま特定の知識パターンをうまく捉えることができた一部のニューロンが、その知識を一身に背負う形で強く活性化するよう特化（専門化）していったのではないか、と考えられます。これは、ネットワーク全体で協調して学習するのが難しい状況で、一部のユニットだけが突出して学習を担う、という現象と解釈できます。

<br>



<br>

学習の安定化が、より効率的な知識獲得につながる

<br>

では、「The Curse of Depth」で提案された手法（LNのスケーリング）によって深い層の学習が安定化すると、知識の格納方法はどのように変わるでしょうか。

<br>



<br>

これは非常に興味深い問いであり、2つの可能性が考えられます。

<br>



<br>

可能性A: より強固な知識ニューロンの形成:

<br>

学習が安定することで、各知識ニューロンはより明確に、そして効率的に特定の知識をエンコードできるようになるかもしれません。ノイズの多い環境で偶然生まれた専門家ではなく、安定した環境で育成された真の専門家へと変化するイメージです。

<br>



<br>

可能性B: 知識の分散化:

<br>

ネットワークの全ニューロンが効率的に学習に寄与できるようになるため、一つの知識を少数のニューロンに集中させる必要がなくなり、より多くのニューロンに分散して知識が格納されるようになる可能性も考えられます。

<br>



<br>

現在のところ、学習の安定化は、知識ニューロンがより効率的に形成・機能するための基盤を提供すると考えるのが最も妥当でしょう。「深さの呪い」という問題を解決することは、モデルが知識をどのように獲得し、整理するかという、より根源的なメカニズム（知識ニューロンの振る舞い）にも直接的な影響を与えるはずです。

<br>



<br>

まとめ

<br>

「Knowledge Neurons」は深い層に存在する知識の「状態」を明らかにし、「The Curse of Depth」は深い層で発生する学習の「問題」とその解決策を提示しました。

<br>



<br>

これらを統合すると、「深い層における学習の不安定性という問題が、知識ニューロンという局所的な知識表現を生み出す一因となっており、この問題を解決することで、より効率的で安定した知識の獲得・格納が可能になるかもしれない」という関係性が見えてきます。

<br>



<br>

両者は、LLMの能力と限界を異なる角度から照らし出しており、組み合わせることでモデルの内部動作の解明をさらに一歩前進させる、非常に重要な研究だと言えます。

<br>

```</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<a class="button" href="articles/MultiModal.html">#MultiModal</a>
<a class="button" href="articles/RLHF.html">#RLHF</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/mid-training.html">#mid-training</a>
<a class="button" href="articles/RewardHacking.html">#RewardHacking</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<a class="button" href="articles/CurriculumLearning.html">#CurriculumLearning</a>
<a class="button" href="articles/RLVR.html">#RLVR</a>
<a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2128">[Paper Note] GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable  Reinforcement Learning, GLM-V Team+, arXiv'25</a>
<span class="snippet"><span>Summary</span>視覚言語モデルGLM-4.1V-Thinkingを発表し、推論中心のトレーニングフレームワークを開発。強力な視覚基盤モデルを構築し、カリキュラムサンプリングを用いた強化学習で多様なタスクの能力を向上。28のベンチマークで最先端のパフォーマンスを達成し、特に難しいタスクで競争力のある結果を示す。モデルはオープンソースとして公開。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/sinclairwang1/status/1940331927724232712?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QQwen2.5-VLよりも性能が良いVLM

<br>

<img src="https://github.com/user-attachments/assets/1215d0cf-3776-4631-a5d5-2c514e7d5a2e" alt="image" loading="lazy">アーキテクチャはこちら。が、pretraining(データのフィルタリング, マルチモーダル→long context継続事前学習)-&gt;SFT(cold startへの対処, reasoning能力の獲得)-&gt;RL(RLVRとRLHFの併用によるパフォーマンス向上とAlignment, RewardHackingへの対処,curriculum sampling)など、全体の学習パイプラインの細かいテクニックの積み重ねで高い性能が獲得されていると考えられる。

<br>

<img src="https://github.com/user-attachments/assets/a692b5de-5f4e-42c6-938e-3718dd2fc0e6" alt="image" loading="lazy"></span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MultiLingual.html">#MultiLingual</a>
<a class="button" href="articles/COLM.html">#COLM</a>
<span class="issue_date">Issue Date: 2025-06-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2109">[Paper Note] FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data  Processing to Every Language, Guilherme Penedo+, COLM'25</a>
<span class="snippet"><span>Summary</span>多言語LLMsの性能向上のために、FineWebに基づく新しい事前学習データセットキュレーションパイプラインを提案。9つの言語に対して設計選択肢を検証し、非英語コーパスが従来のデータセットよりも高性能なモデルを生成できることを示す。データセットの再バランス手法も導入し、1000以上の言語にスケールアップした20テラバイトの多言語データセットFineWeb2を公開。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/gui_penedo/status/1938631842720022572?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qv1

<br>

・1942abstを見る限りFinewebを多言語に拡張した模様openreview:https://openreview.net/forum?id=jnRBe6zatPdiscussion</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<a class="button" href="articles/mid-training.html">#mid-training</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-06-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2107">[Paper Note] OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling, Zengzhi Wang+, arXiv'25</a>
<span class="snippet"><span>Summary</span>異なるベース言語モデル（LlamaやQwen）の強化学習（RL）における挙動を調査し、中間トレーニング戦略がRLのダイナミクスに与える影響を明らかに。高品質の数学コーパスがモデルのパフォーマンスを向上させ、長い連鎖的思考（CoT）がRL結果を改善する一方で、冗長性や不安定性を引き起こす可能性があることを示す。二段階の中間トレーニング戦略「Stable-then-Decay」を導入し、OctoThinkerモデルファミリーを開発。オープンソースのモデルと数学推論コーパスを公開し、RL時代の基盤モデルの研究を支援することを目指す。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/sinclairwang1/status/1938244843857449431?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qmid-trainingの観点から、post trainingにおけるRLがスケーリングする条件をsystematicallyに調査している模様論文中にはmid-training[^1]の定義が記述されている:

<br>



<br>

<img width="808" height="353" alt="Image" src="https://github.com/user-attachments/assets/da206d3d-f811-4d69-8210-a1d0816c827f">

<br>



<br>

[^1]: mid-trainingについてはコミュニティの間で厳密な定義はまだ無くバズワードっぽく使われている、という印象を筆者は抱いており、本稿は文献中でmid-trainingを定義する初めての試みという所感</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<a class="button" href="articles/OpenSource.html">#OpenSource</a>
<a class="button" href="articles/ICCV.html">#ICCV</a>
<a class="button" href="articles/Encoder.html">#Encoder</a>
<a class="button" href="articles/Backbone.html">#Backbone</a>
<span class="issue_date">Issue Date: 2025-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2105">[Paper Note] OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision  Encoders for Multimodal Learning, Xianhang Li+, ICCV'25</a>
<span class="snippet"><span>Summary</span>OpenVisionは、完全にオープンでコスト効果の高いビジョンエンコーダーのファミリーを提案し、CLIPと同等以上の性能を発揮します。既存の研究を基に構築され、マルチモーダルモデルの進展に実用的な利点を示します。5.9Mから632.1Mパラメータのエンコーダーを提供し、容量と効率の柔軟なトレードオフを実現します。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/cihangxie/status/1920575141849030882?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qv2へアップデート:https://x.com/cihangxie/status/1963297223753494832?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>



<br>

事前学習時にtext, image encoderのcontrastive lossで学習していたが、text encoderを無くしimage encoderに入力されたimageからcaptionを生成するcaption lossのみにすることで性能を落とすことなく効率を改善HF:https://huggingface.co/collections/UCSC-VLAA/openvision-681a4c27ee1f66411b4ae919

<br>

pj page: https://ucsc-vlaa.github.io/OpenVision/CLIP, SigLIPとは異なり完全にオープンなVision Encoder

<br>

<img src="https://github.com/user-attachments/assets/b7c8eb07-45df-4ab3-9cd2-6b31af46e761" alt="image" loading="lazy">v2の解説:

<br>

https://x.com/jiqizhixin/status/1963442911108084161?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Alignment.html">#Alignment</a>
<a class="button" href="articles/SyntheticData.html">#SyntheticData</a>
<a class="button" href="articles/SyntheticDataGeneration.html">#SyntheticDataGeneration</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<span class="issue_date">Issue Date: 2025-06-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2094">[Paper Note] Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs   with Nothing, Zhangchen Xu+, ICLR'25</a>
<span class="snippet"><span>Summary</span>高品質な指示データはLLMの整合に不可欠であり、Magpieという自己合成手法を提案。Llama-3-Instructを用いて400万の指示と応答を生成し、30万の高品質なインスタンスを選定。Magpieでファインチューニングしたモデルは、従来のデータセットを用いたモデルと同等の性能を示し、特に整合ベンチマークで優れた結果を得た。</span>
<span class="snippet"><span>Comment</span>OpenReview:https://openreview.net/forum?id=Pnk7vMbznK<img src="https://github.com/user-attachments/assets/9cb451b2-5440-43a4-9867-b5206dd08cca" alt="image" loading="lazy">

<br>



<br>

下記のようなpre-queryテンプレートを与え（i.e., userの発話は何も与えず、ユーザの発話を表す特殊トークンのみを渡す）instructionを生成し、post-queryテンプレートを与える（i.e., pre-queryテンプレート+生成されたinstruction+assistantの発話の開始を表す特殊トークンのみを渡す）ことでresponseを生成することで、prompt engineeringやseed無しでinstruction tuningデータを合成できるという手法。

<br>

<img src="https://github.com/user-attachments/assets/59e9ea58-1088-4f7f-a5e1-05fba7221aca" alt="image" loading="lazy">

<br>



<br>

生成した生のinstruction tuning pair dataは、たとえば下記のようなフィルタリングをすることで品質向上が可能で

<br>

<img src="https://github.com/user-attachments/assets/6dc19e89-2e0d-409d-9d96-eca8d92d27d3" alt="image" loading="lazy">

<br>



<br>

reward modelと組み合わせてLLMからのresponseを生成しrejection samplingすればDPOのためのpreference dataも作成できるし、single turnの発話まで生成させた後もう一度pre/post-queryをconcatして生成すればMulti turnのデータも生成できる。

<br>



<br>

他のも例えば、システムプロンプトに自分が生成したい情報を与えることで、特定のドメインに特化したデータ、あるいは特定の言語に特化したデータも合成できる。

<br>

<img src="https://github.com/user-attachments/assets/f5f06b90-d1cb-4de8-bbaa-622abbcc0b6b" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/RLVR.html">#RLVR</a>
<a class="button" href="articles/DataMixture.html">#DataMixture</a>
<a class="button" href="articles/CrossDomain.html">#CrossDomain</a>
<span class="issue_date">Issue Date: 2025-06-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2070">[Paper Note] Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain  Perspective, Zhoujun Cheng+, arXiv'25</a>
<span class="snippet"><span>Summary</span>Guruを導入し、数学、コード、科学、論理、シミュレーション、表形式の6つの推論ドメインにわたる92KのRL推論コーパスを構築。これにより、LLM推論のためのRLの信頼性と効果を向上させ、ドメイン間の変動を観察。特に、事前学習の露出が限られたドメインでは、ドメイン内トレーニングが必要であることを示唆。Guru-7BとGuru-32Bモデルは、最先端の性能を達成し、複雑なタスクにおいてベースモデルの性能を改善。データとコードは公開。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/chengzhoujun/status/1936113985507803365?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qpost-trainingにおけるRLのcross domain（Math, Code, Science, Logic, Tabular)における影響を調査した研究。非常に興味深い研究。詳細は元論文が著者ポスト参照のこと。Qwenシリーズで実験。以下ポストのまとめ。

<br>



<br>

・mid trainingにおいて重点的に学習されたドメインはRLによるpost trainingで強い転移を発揮する（Code, Math, Science)

<br>

・一方、mid trainingであまり学習データ中に出現しないドメインについては転移による性能向上は最小限に留まり、in-domainの学習データをきちんと与えてpost trainingしないと性能向上は限定的

<br>

・簡単なタスクはcross domainの転移による恩恵をすぐに得やすい（Math500, MBPP),難易度の高いタスクは恩恵を得にくい

<br>

・各ドメインのデータを一様にmixすると、単一ドメインで学習した場合と同等かそれ以上の性能を達成する

<br>

・必ずしもresponse lengthが長くなりながら予測性能が向上するわけではなく、ドメインによって傾向が異なる

<br>

・たとえば、Code, Logic, Tabularの出力は性能が向上するにつれてresponse lengthは縮小していく

<br>

・一方、Science, Mathはresponse lengthが増大していく。また、Simulationは変化しない

<br>

・異なるドメインのデータをmixすることで、最初の数百ステップにおけるrewardの立ち上がりが早く（単一ドメインと比べて急激にrewardが向上していく）転移がうまくいく

<br>

  ・（これは私がグラフを見た感想だが、単一ドメインでlong runで学習した場合の最終的な性能は4/6で同等程度、2/6で向上（Math, Science)

<br>

・非常に難易度の高いmathデータのみにフィルタリングすると、フィルタリング無しの場合と比べて難易度の高いデータに対する予測性能は向上する一方、簡単なOODタスク（HumanEval)の性能が大幅に低下する（特定のものに特化するとOODの性能が低下する）

<br>

・RLはpre(mid)-trainingで学習されたreasoning能力を引き出すだけではなく、新規のタスクに対しては新たなreasoning能力を獲得できる

<br>

・モデルサイズが小さいと、RLでpost-training後のpass@kのkを大きくするとどこかでサチり、baseモデルと交差するが、大きいとサチらず交差しない

<br>

  ・モデルサイズが大きいとより多様なreasoningパスがunlockされている

<br>

・pass@kで観察したところRLには2つのphaseのよつなものが観測され、最初の0-160（1 epoch)ステップではpass@1が改善したが、pass@max_kは急激に性能が劣化した。一方で、160ステップを超えると、双方共に徐々に性能改善が改善していくような変化が見られた本研究で構築されたGuru Dataset:https://huggingface.co/datasets/LLM360/guru-RL-92k

<br>



<br>

math, coding, science, logic, simulation, tabular reasoningに関する高品質、かつverifiableなデータセット。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/Contamination.html">#Contamination</a>
<span class="issue_date">Issue Date: 2025-05-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1982">LiveBench: A Challenging, Contamination-Limited LLM Benchmark, Colin White+, ICLR'25</a>
<span class="snippet"><span>Summary</span>テストセットの汚染を防ぐために、LLM用の新しいベンチマーク「LiveBench」を導入。LiveBenchは、頻繁に更新される質問、自動スコアリング、さまざまな挑戦的タスクを含む。多くのモデルを評価し、正答率は70%未満。質問は毎月更新され、LLMの能力向上を測定可能に。コミュニティの参加を歓迎。</span>
<span class="snippet"><span>Comment</span>テストデータのコンタミネーションに対処できるように設計されたベンチマーク。重要研究</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<span class="issue_date">Issue Date: 2025-05-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1944">Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon   Pretraining Dataset, Dan Su+, ACL'25</a>
<span class="snippet"><span>Summary</span>FineWeb-EduとDCLMは、モデルベースのフィルタリングによりデータの90%を削除し、トレーニングに適さなくなった。著者は、アンサンブル分類器や合成データの言い換えを用いて、精度とデータ量のトレードオフを改善する手法を提案。1Tトークンで8Bパラメータモデルをトレーニングし、DCLMに対してMMLUを5.6ポイント向上させた。新しい6.3Tトークンデータセットは、DCLMと同等の性能を持ちながら、4倍のユニークなトークンを含み、長トークンホライズンでのトレーニングを可能にする。15Tトークンのためにトレーニングされた8Bモデルは、Llama 3.1の8Bモデルを上回る性能を示した。データセットは公開されている。</span>
<a class="button" href="articles/Metrics.html">#Metrics</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/GenerativeAI.html">#GenerativeAI</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<span class="issue_date">Issue Date: 2025-03-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1842">Measuring AI Ability to Complete Long Tasks, Thomas Kwa+, arXiv'25</a>
<span class="snippet"><span>Summary</span>新しい指標「50%-タスク完了時間ホライズン」を提案し、AIモデルの能力を人間の観点から定量化。Claude 3.7 Sonnetは約50分の時間ホライズンを持ち、AIの能力は2019年以降約7か月ごとに倍増。信頼性や論理的推論の向上が要因とされ、5年以内にAIが多くのソフトウェアタスクを自動化できる可能性を示唆。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/hillbig/status/1902854727089656016?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q確かに線形に見える。てかGPT-2と比べるとAIさん進化しすぎである…。

<br>

<img src="https://github.com/user-attachments/assets/266a36aa-a169-492b-b8af-60c0cb152111" alt="image" loading="lazy">利用したデータセットは

<br>

・HCAST: 46のタスクファミリーに基づく97種類のタスクが定義されており、たとえばサイバーセキュリティ、機械学習、ソフトウェアエンジニアリング、一般的な推論タスク（wikipediaから事実情報を探すタスクなど）などがある

<br>

  ・数分で終わるタスク: 上述のwikipedia

<br>

  ・数時間で終わるタスク: Pytorchのちょっとしたバグ修正など

<br>

  ・数文でタスクが記述され、コード、データ、ドキュメント、あるいはwebから入手可能な情報を参照可能

<br>

　・タスクの難易度としては当該ドメインに数年間携わった専門家が解ける問題

<br>

・RE-Bench Suite

<br>

  ・7つのopen endedな専門家が8時間程度を要するMLに関するタスク

<br>

　・e.g., GPT-2をQA用にFinetuningする, Finetuningスクリプトが与えられた時に挙動を変化させずにランタイムを可能な限り短縮する、など

<br>

　・[RE-Bench Technical Report](https://metr.org/AI_R_D_Evaluation_Report.pdf)のTable2等を参照のこと

<br>

・SWAA Suite: 66種類の1つのアクションによって1分以内で終わるソフトウェアエンジニアリングで典型的なタスク

<br>

  ・1分以内で終わるタスクが上記データになかったので著者らが作成

<br>



<br>

であり、画像系やマルチモーダルなタスクは含まれていない。

<br>

<img src="https://github.com/user-attachments/assets/0b3892c9-3c83-4f78-a490-c28fa7470e0e" alt="image" loading="lazy">

<br>



<br>

タスクと人間がタスクに要する時間の対応に関するサンプルは下記

<br>

<img src="https://github.com/user-attachments/assets/5ed472da-e8c9-41be-8fd1-ef6f21713c14" alt="image" loading="lazy">タスク-エージェントペアごとに8回実行した場合の平均の成功率。確かにこのグラフからはN年後には人間で言うとこのくらいの能力の人がこのくらい時間を要するタスクが、このくらいできるようになってます、といったざっくり感覚値はなかなか想像できない。

<br>

<img src="https://github.com/user-attachments/assets/e2bed06e-9234-4607-826a-588106010bcf" alt="image" loading="lazy">成功率とタスクに人間が要する時間に関するグラフ。ロジスティック関数でfittingしており、赤い破線が50% horizon。Claude 3.5 Sonnet （old）からClaude 3.7 Sonnetで50% horizonは18分から59分まで増えている。実際に数字で見るとイメージが湧きやすくおもしろい。

<br>

<img src="https://github.com/user-attachments/assets/efe01e35-6ee6-45a5-8a4c-eccf95284b35" alt="image" loading="lazy">こちらで最新モデルも随時更新される:

<br>

https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<a class="button" href="articles/Chain-of-Thought.html">#Chain-of-Thought</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/RewardHacking.html">#RewardHacking</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-02-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1746">Demystifying Long Chain-of-Thought Reasoning in LLMs, Edward Yeo+, arXiv'25</a>
<span class="snippet"><span>Summary</span>本研究では、大規模言語モデル（LLMs）における長い思考の連鎖（CoTs）推論のメカニズムを調査し、重要な要因を特定。主な発見は、(1) 教師ありファインチューニング（SFT）は必須ではないが効率を向上させる、(2) 推論能力は計算の増加に伴い現れるが、報酬の形状がCoTの長さに影響、(3) 検証可能な報酬信号のスケーリングが重要で、特に分布外タスクに効果的、(4) エラー修正能力は基本モデルに存在するが、RLを通じて効果的に奨励するには多くの計算が必要。これらの洞察は、LLMsの長いCoT推論を強化するためのトレーニング戦略の最適化に役立つ。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/xiangyue96/status/1887332772198371514?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q元ポストのスレッド中に論文の11個の知見が述べられている。どれも非常に興味深い。DeepSeek-R1のテクニカルペーパーと同様、

<br>



<br>

・Long CoTとShort CoTを比較すると前者の方が到達可能な性能のupper bonudが高いことや、

<br>

・SFTを実施してからRLをすると性能が向上することや、

<br>

・RLの際にCoTのLengthに関する報酬を入れることでCoTの長さを抑えつつ性能向上できること、

<br>

・数学だけでなくQAペアなどのノイジーだが検証可能なデータをVerifiableな報酬として加えると一般的なreasoningタスクで数学よりもさらに性能が向上すること、

<br>

・より長いcontext window sizeを活用可能なモデルの訓練にはより多くの学習データが必要なこと、

<br>

・long CoTはRLによって学習データに類似したデータが含まれているためベースモデルの段階でその能力が獲得されていることが示唆されること、

<br>

・aha momentはすでにベースモデル時点で獲得されておりVerifiableな報酬によるRLによって強化されたわけではなさそう、

<br>



<br>

など、興味深い知見が盛りだくさん。非常に興味深い研究。あとで読む。</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-01-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1740">SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model   Post-training, Tianzhe Chu+, ICML'25</a>
<span class="snippet"><span>Summary</span>SFTとRLの一般化能力の違いを研究し、GeneralPointsとV-IRLを用いて評価。RLはルールベースのテキストと視覚変種に対して優れた一般化を示す一方、SFTは訓練データを記憶し分布外シナリオに苦労。RLは視覚認識能力を向上させるが、SFTはRL訓練に不可欠であり、出力形式を安定させることで性能向上を促進。これらの結果は、複雑なマルチモーダルタスクにおけるRLの一般化能力を示す。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/hillbig/status/1884731381517082668?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qopenreview:https://openreview.net/forum?id=dYur3yabMj&referrer=%5Bthe%20profile%20of%20Yi%20Ma%5D(%2Fprofile%3Fid%3D~Yi_Ma4)</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<span class="issue_date">Issue Date: 2025-01-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1730">[Paper Note] Humanity's Last Exam, Long Phan+, arXiv'25</a>
<span class="snippet"><span>Summary</span>「人類の最後の試験（HLE）」を導入し、LLMの能力を測定する新しいマルチモーダルベンチマークを提案。HLEは2,500の質問から成り、数学や自然科学など広範な科目をカバー。専門家によって開発され、自動採点が可能な形式で、インターネット検索では迅速に回答できない。最先端のLLMはHLEに対して低い精度を示し、現在のLLMの能力と専門家の知識との間に大きなギャップがあることを明らかに。HLEは公開され、研究や政策立案に役立てられる。</span>
<span class="snippet"><span>Comment</span>o1, DeepSeekR1の正解率が10%未満の新たなベンチマーク</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/MultiModal.html">#MultiModal</a>
<a class="button" href="articles/FoundationModel.html">#FoundationModel</a>
<a class="button" href="articles/CVPR.html">#CVPR</a>
<a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2529">[Paper Note] InternVL: Scaling up Vision Foundation Models and Aligning for Generic   Visual-Linguistic Tasks, Zhe Chen+, CVPR'24</a>
<span class="snippet"><span>Summary</span>大規模視覚-言語基盤モデル（InternVL）は、60億パラメータで設計され、LLMと整合させるためにウェブ規模の画像-テキストデータを使用。視覚認知タスクやゼロショット分類、検索など32のベンチマークで最先端の性能を達成し、マルチモーダル対話システムの構築に寄与。ViT-22Bの代替として強力な視覚能力を持つ。コードとモデルは公開されている。</span>
<span class="snippet"><span>Comment</span>既存のResNetのようなSupervised pretrainingに基づくモデル、CLIPのようなcontrastive pretrainingに基づくモデルに対して、text encoder部分をLLMに置き換えて、contrastive learningとgenerativeタスクによる学習を組み合わせたパラダイムを提案。

<br>

<img src="https://github.com/user-attachments/assets/eca53a4a-1d3b-46f1-a833-07ef16b8d5f7" alt="image" loading="lazy">

<br>



<br>

InternVLのアーキテクチャは下記で、3 stageの学習で構成される。最初にimage text pairをcontrastive learningし学習し、続いてモデルのパラメータはfreezeしimage text retrievalタスク等でモダリティ間の変換を担う最終的にQlLlama(multilingual性能を高めたllama)をvision-languageモダリティを繋ぐミドルウェアのように捉え、Vicunaをテキストデコーダとして接続してgenerative cossで学習する、みたいなアーキテクチャの模様（斜め読みなので少し違う可能性あり

<br>



<br>

<img src="https://github.com/user-attachments/assets/46a2a0fe-721c-4336-b5ec-657caa5c4771" alt="image" loading="lazy">現在のVLMの主流であるvision encoderとLLMをadapterで接続する方式はここからかなりシンプルになっていることが伺える。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<span class="issue_date">Issue Date: 2025-08-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2338">[Paper Note] YaRN: Efficient Context Window Extension of Large Language Models, Bowen Peng+, ICLR'24</a>
<span class="snippet"><span>Summary</span>YaRN（Yet another RoPE extensioN method）は、トランスフォーマーベースの言語モデルにおける位置情報のエンコードを効率的に行い、コンテキストウィンドウを従来の方法よりも10倍少ないトークンと2.5倍少ない訓練ステップで拡張する手法を提案。LLaMAモデルが長いコンテキストを効果的に利用できることを示し、128kのコンテキスト長まで再現可能なファインチューニングを実現。</span>
<span class="snippet"><span>Comment</span>openreview:https://openreview.net/forum?id=wHBfxhZu1u現在主流なコンテキストウィンドウ拡張手法らしい日本語解説:https://zenn.dev/bilzard/scraps/de7ecd3c380b6e</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/PRM.html">#PRM</a>
<span class="issue_date">Issue Date: 2025-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2103">[Paper Note] Let's Verify Step by Step, Hunter Lightman+, ICLR'24</a>
<span class="snippet"><span>Summary</span>大規模言語モデルの多段階推論能力が向上する中、論理的誤りが依然として問題である。信頼性の高いモデルを訓練するためには、結果監視とプロセス監視の比較が重要である。独自の調査により、プロセス監視がMATHデータセットの問題解決において結果監視を上回ることを発見し、78%の問題を解決した。また、アクティブラーニングがプロセス監視の効果を向上させることも示した。関連研究のために、80万の人間フィードバックラベルからなるデータセットPRM800Kを公開した。</span>
<span class="snippet"><span>Comment</span>OpenReview:https://openreview.net/forum?id=v8L0pN6EOiPRM800K:https://github.com/openai/prm800k/tree/main</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<span class="issue_date">Issue Date: 2025-05-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1942">The FineWeb Datasets: Decanting the Web for the Finest Text Data at   Scale, Guilherme Penedo+, NeurIPS'24</a>
<span class="snippet"><span>Summary</span>本研究では、15兆トークンからなるFineWebデータセットを紹介し、LLMの性能向上に寄与することを示します。FineWebは高品質な事前学習データセットのキュレーション方法を文書化し、重複排除やフィルタリング戦略を詳細に調査しています。また、FineWebから派生した1.3兆トークンのFineWeb-Eduを用いたLLMは、MMLUやARCなどのベンチマークで優れた性能を発揮します。データセット、コードベース、モデルは公開されています。</span>
<span class="snippet"><span>Comment</span>日本語解説:https://zenn.dev/deepkawamura/articles/da9aeca6d6d9f9openreview:https://openreview.net/forum?id=n6SCkn2QaGdiscussion</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/SyntheticData.html">#SyntheticData</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-05-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1929">Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers, Zeyuan Allen-Zhu+, ICML'24 Tutorial</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/hillbig/status/1919878625488449849?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QCanon層の発見著者による解説:https://x.com/zeyuanallenzhu/status/1918684257058197922?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/SyntheticData.html">#SyntheticData</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<span class="issue_date">Issue Date: 2025-05-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1923">Physics of Language Models: Part 3.1, Knowledge Storage and Extraction, Zeyuan Allen-Zhu+, ICML'24</a>
<span class="snippet"><span>Summary</span>大規模言語モデル（LLMs）の知識抽出能力は、訓練データの多様性と強く相関しており、十分な強化がなければ知識は記憶されても抽出可能ではないことが示された。具体的には、エンティティ名の隠れ埋め込みに知識がエンコードされているか、他のトークン埋め込みに分散しているかを調査。LLMのプレトレーニングに関する重要な推奨事項として、補助モデルを用いたデータ再構成と指示微調整データの早期取り入れが提案された。</span>
<span class="snippet"><span>Comment</span>解説:

<br>

・1834SNLP'24での解説スライド:

<br>

https://speakerdeck.com/sosk/physics-of-language-models-part-3-1-knowledge-storage-and-extraction</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/Decoding.html">#Decoding</a>
<a class="button" href="articles/Non-Determinism.html">#Non-Determinism</a>
<span class="issue_date">Issue Date: 2025-04-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1890">Non-Determinism of "Deterministic" LLM Settings, Berk Atil+, arXiv'24</a>
<span class="snippet"><span>Summary</span>本研究では、5つの決定論的LLMにおける非決定性を8つのタスクで調査し、最大15%の精度変動と70%のパフォーマンスギャップを観察。全てのタスクで一貫した精度を提供できないことが明らかになり、非決定性が計算リソースの効率的使用に寄与している可能性が示唆された。出力の合意率を示す新たなメトリクスTARr@NとTARa@Nを導入し、研究結果を定量化。コードとデータは公開されている。</span>
<span class="snippet"><span>Comment</span>・論文中で利用されているベンチマーク:

<br>

  ・785

<br>

  ・901 同じモデルに対して、seedを固定し、temperatureを0に設定し、同じ計算機環境に対して、同じinputを入力したら理論上はLLMの出力はdeterministicになるはずだが、deterministicにならず、ベンチマーク上の性能とそもそものraw response自体も試行ごとに大きく変化する、という話。

<br>

ただし、これはプロプライエタリLLMや、何らかのinferenceの高速化を実施したInferenceEngine（本研究ではTogetherと呼ばれる実装を使っていそう。vLLM/SGLangだとどうなるのかが気になる）を用いてinferenceを実施した場合での実験結果であり、後述の通り計算の高速化のためのさまざまな実装無しで、deterministicな設定でOpenLLMでinferenceすると出力はdeterministicになる、という点には注意。

<br>



<br>

GPTやLlama、Mixtralに対して上記ベンチマークを用いてzero-shot/few-shotの設定で実験している。Reasoningモデルは実験に含まれていない。

<br>

<img width="701" height="325" alt="Image" src="https://github.com/user-attachments/assets/b33f14d8-ed86-4589-a427-18a70b35d61a">

<br>



<br>

LLMのraw_response/multiple choiceのparse結果（i.e., 問題に対する解答部分を抽出した結果）の一致（TARr@N, TARa@N; Nはinferenceの試行回数）も理論上は100%になるはずなのに、ならないことが報告されている。

<br>



<br>

<img width="712" height="432" alt="Image" src="https://github.com/user-attachments/assets/3159ff26-fc92-4fa8-90a6-f8c5e7ccf20e">

<br>



<br>

correlation analysisによって、応答の長さ と TAR{r, a}が強い負の相関を示しており、応答が長くなればなるほど不安定さは増すことが分析されている。このため、ontput tokenの最大値を制限することで出力の安定性が増すことを考察している。また、few-shotにおいて高いAcc.の場合は出力がdeterministicになるわけではないが、性能が安定する傾向とのこと。また、OpenAIプラットフォーム上でGPTのfinetuningを実施し実験したが、安定性に寄与はしたが、こちらもdeterministicになるわけではないとのこと。

<br>



<br>

deterministicにならない原因として、まずmulti gpu環境について検討しているが、multi-gpu環境ではある程度のランダム性が生じることがNvidiaの研究によって報告されているが、これはseedを固定すれば決定論的にできるため問題にならないとのこと。

<br>

続いて、inferenceを高速化するための実装上の工夫（e.g., Chunk Prefilling, Prefix Caching, Continuous Batching）などの実装がdeterministicなハイパーパラメータでもdeterministicにならない原因であると考察しており、実際にlocalマシン上でこれらinferenceを高速化するための最適化を何も実施しない状態でLlama-8Bでinferenceを実施したところ、outputはdeterministicになったとのこと。論文中に記載がなかったため、どのようなInferenceEngineを利用したか公開されているgithubを見ると下記が利用されていた:

<br>



<br>

・Together: https://github.com/togethercomputer/together-python?tab=readme-ov-file

<br>



<br>

Togetherが内部的にどのような処理をしているかまでは追えていないのだが、異なるInferenceEngineを利用した場合に、どの程度outputの不安定さに差が出るのか（あるいは出ないのか）は気になる。たとえば、transformers/vLLM/SGLangを利用した場合などである。

<br>



<br>

論文中でも報告されている通り、昔管理人がtransformersを用いて、deterministicな設定でzephyrを用いてinferenceをしたときは、出力はdeterministicになっていたと記憶している（スループットは絶望的だったが...)。あと個人的には現実的な速度でオフラインでinference engineを利用した時にdeterministicにはせめてなって欲しいなあという気はするので、何が原因なのかを実装レベルで突き詰めてくれるととても嬉しい（KV Cacheが怪しい気がするけど）。

<br>



<br>

たとえば最近SLMだったらKVCacheしてVRAM食うより計算し直した方が効率良いよ、みたいな研究があったような。そういうことをしたらlocal llmでdeterministicにならないのだろうか。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/LLMAgent.html">#LLMAgent</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/SoftwareEngineering.html">#SoftwareEngineering</a>
<span class="issue_date">Issue Date: 2025-04-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1848">SWE-bench: Can Language Models Resolve Real-World GitHub Issues?, Carlos E. Jimenez+, ICLR'24</a>
<span class="snippet"><span>Summary</span>SWE-benchは、12の人気Pythonリポジトリから得られた2,294のソフトウェアエンジニアリング問題を評価するフレームワークで、言語モデルがコードベースを編集して問題を解決する能力を測定します。評価の結果、最先端の商用モデルや微調整されたモデルSWE-Llamaも最も単純な問題しか解決できず、Claude 2はわずか1.96%の問題を解決するにとどまりました。SWE-benchは、より実用的で知的な言語モデルへの進展を示しています。</span>
<span class="snippet"><span>Comment</span>ソフトウェアエージェントの最もpopularなベンチマーク

<br>



<br>

<img width="693" alt="Image" src="https://github.com/user-attachments/assets/ac905221-d3b1-4d16-b447-3bdd4d5e97bb">

<br>



<br>

主にpythonライブラリに関するリポジトリに基づいて構築されている。

<br>

<img width="731" alt="Image" src="https://github.com/user-attachments/assets/14d26dd1-6b4a-4337-a652-4e48e36d633b">SWE-Bench, SWE-Bench Lite, SWE-Bench Verifiedの3種類がありソフトウェアエージェントではSWE-Bench Verifiedを利用して評価することが多いらしい。Verifiedでは、issueの記述に曖昧性がなく、適切なunittestのスコープが適切なもののみが採用されているとのこと（i.e., 人間の専門家によって問題がないと判断されたもの）。

<br>

https://www.swebench.com/Agenticな評価をする際に、一部の評価でエージェントがgit logを参照し本来は存在しないはずのリポジトリのfuture stateを見ることで環境をハッキングしていたとのこと:

<br>

https://x.com/giffmana/status/1963327672827687316?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>



<br>

これまでの評価結果にどの程度の影響があるかは不明。openreview:https://openreview.net/forum?id=VTF8yNQM66</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/Backbone.html">#Backbone</a>
<span class="issue_date">Issue Date: 2025-08-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2564">[Paper Note] Scalable Diffusion Models with Transformers, William Peebles+, ICCV'23</a>
<span class="snippet"><span>Summary</span>新しいトランスフォーマーに基づく拡散モデル（Diffusion Transformers, DiTs）を提案し、U-Netをトランスフォーマーに置き換えた。DiTsは高いGflopsを持ち、低いFIDを維持しながら良好なスケーラビリティを示す。最大のDiT-XL/2モデルは、ImageNetのベンチマークで従来の拡散モデルを上回り、最先端のFID 2.27を達成した。</span>
<span class="snippet"><span>Comment</span>日本語解説:https://qiita.com/sasgawy/items/8546c784bc94d94ef0b2よく見るDiT

<br>



<br>

・2526

<br>



<br>

も同様の呼称だが全く異なる話なので注意</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/VariationalAutoEncoder.html">#VariationalAutoEncoder</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/ColdStart.html">#ColdStart</a>
<a class="button" href="articles/Encoder-Decoder.html">#Encoder-Decoder</a>
<a class="button" href="articles/SemanticID.html">#SemanticID</a>
<span class="issue_date">Issue Date: 2025-07-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2309">[Paper Note] Recommender Systems with Generative Retrieval, Shashank Rajput+, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>新しい生成的検索アプローチを提案し、アイテムのセマンティックIDを用いて次のアイテムを予測するTransformerベースのモデルを訓練。これにより、従来のレコメンダーシステムを大幅に上回る性能を達成し、過去の対話履歴がないアイテムに対しても改善された検索性能を示す。</span>
<span class="snippet"><span>Comment</span>openreview:https://openreview.net/forum?id=BJ0fQUU32wSemantic IDを提案した研究アイテムを意味的な情報を保持したdiscrete tokenのタプル（＝Semantic ID)で表現し、encoder-decoderでNext ItemのSemantic IDを生成するタスクに落としこむことで推薦する。SemanticIDの作成方法は後で読んで理解したい。

<br>



<br>

<img src="https://github.com/user-attachments/assets/38606c7c-011f-46b0-ab14-8d213626be3d" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/a239bac7-c273-4681-a102-65e88c9c65d2" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/7d1822ce-462e-43f6-bb33-af77914919f6" alt="image" loading="lazy"></span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html">#MultiModal</a>
<a class="button" href="articles/ICCV.html">#ICCV</a>
<span class="issue_date">Issue Date: 2025-06-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2111">[Paper Note] Sigmoid Loss for Language Image Pre-Training, Xiaohua Zhai+, ICCV'23</a>
<span class="snippet"><span>Summary</span>シンプルなペアワイズシグモイド損失（SigLIP）を提案し、画像-テキストペアに基づく言語-画像事前学習を改善。シグモイド損失はバッチサイズの拡大を可能にし、小さなバッチサイズでも性能向上を実現。SigLiTモデルは84.5%のImageNetゼロショット精度を達成。バッチサイズの影響を研究し、32kが合理的なサイズであることを確認。モデルは公開され、さらなる研究の促進を期待。</span>
<span class="snippet"><span>Comment</span>SigLIP論文</span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Hallucination.html">#Hallucination</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/ActivationSteering/ITI.html">#ActivationSteering/ITI</a>
<a class="button" href="articles/Probing.html">#Probing</a>
<a class="button" href="articles/Trustfulness.html">#Trustfulness</a>
<span class="issue_date">Issue Date: 2025-05-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1941">Inference-Time Intervention: Eliciting Truthful Answers from a Language   Model, Kenneth Li+, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>Inference-Time Intervention (ITI)を提案し、LLMsの真実性を向上させる技術を紹介。ITIは推論中にモデルの活性化を調整し、LLaMAモデルの性能をTruthfulQAベンチマークで大幅に改善。Alpacaモデルでは真実性が32.5%から65.1%に向上。真実性と有用性のトレードオフを特定し、介入の強度を調整する方法を示す。ITIは低コストでデータ効率が高く、数百の例で真実の方向性を特定可能。LLMsが虚偽を生成しつつも真実の内部表現を持つ可能性を示唆。</span>
<span class="snippet"><span>Comment</span>Inference Time Interventionを提案した研究。Attention Headに対して線形プロービング[^1]を実施し、真実性に関連するであろうHeadをtopKで特定できるようにし、headの出力に対し真実性を高める方向性のベクトルvを推論時に加算することで（＝intervention）、モデルの真実性を高める。vは線形プロービングによって学習された重みを使う手法と、正答と誤答の活性化の平均ベクトルを計算しその差分をvとする方法の二種類がある。後者の方が性能が良い。topKを求める際には、線形プロービングをしたモデルのvalidation setでの性能から決める。Kとαはハイパーパラメータである。

<br>



<br>

[^1]: headのrepresentationを入力として受け取り、線形モデルを学習し、線形モデルの2値分類性能を見ることでheadがどの程度、プロービングの学習に使ったデータに関する情報を保持しているかを測定する手法

<br>



<br>

日本語解説スライド:https://www.docswell.com/s/DeepLearning2023/Z38P8D-2024-06-20-131813p1これは相当汎用的に使えそうな話だから役に立ちそう</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/PositionalEncoding.html">#PositionalEncoding</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<span class="issue_date">Issue Date: 2025-04-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1865">The Impact of Positional Encoding on Length Generalization in   Transformers, Amirhossein Kazemnejad+, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>長さ一般化はTransformerベースの言語モデルにおける重要な課題であり、位置エンコーディング（PE）がその性能に影響を与える。5つの異なるPE手法（APE、T5の相対PE、ALiBi、Rotary、NoPE）を比較した結果、ALiBiやRotaryなどの一般的な手法は長さ一般化に適しておらず、NoPEが他の手法を上回ることが明らかになった。NoPEは追加の計算を必要とせず、絶対PEと相対PEの両方を表現可能である。さらに、スクラッチパッドの形式がモデルの性能に影響を与えることも示された。この研究は、明示的な位置埋め込みが長いシーケンスへの一般化に必須でないことを示唆している。</span>
<span class="snippet"><span>Comment</span>・1863

<br>



<br>

において、Llama4 Scoutが10Mコンテキストウィンドウを実現できる理由の一つとのこと。

<br>



<br>

元ポスト:https://x.com/drjimfan/status/1908615861650547081?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>



<br>

Llama4のブログポストにもその旨記述されている:

<br>

&gt;A key innovation in the Llama 4 architecture is the use of interleaved attention layers without positional embeddings. Additionally, we employ inference time temperature scaling of attention to enhance length generalization.

<br>



<br>

[The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation](https://ai.meta.com/blog/llama-4-multimodal-intelligence/?utm_source=twitter&utm_medium=organic_social&utm_content=image&utm_campaign=llama4)斜め読みだが、length generalizationを評価する上でdownstream taskに焦点を当て、3つの代表的なカテゴリに相当するタスクで評価したところ、この観点においてはT5のrelative positinal encodingとNoPE（位置エンコードディング無し）のパフォーマンスが良く、

<br>



<br>

<img src="https://github.com/user-attachments/assets/dddadfff-ab28-4073-96c3-831eb16845a0" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/c6ec8e0e-7abb-4330-be23-2261486a477c" alt="image" loading="lazy">

<br>



<br>

NoPEは絶対位置エンコーディングと相対位置エンコーディングを理論上実現可能であり[^1]

<br>

<img src="https://github.com/user-attachments/assets/bbcf797a-d394-42d4-b017-08d7dba4261c" alt="image" loading="lazy">

<br>



<br>

実際に学習された異なる2つのモデルに対して同じトークンをそれぞれinputし、同じ深さのLayerの全てのattention distributionの組み合わせからJensen Shannon Divergenceで距離を算出し、最も小さいものを2モデル間の当該layerの距離として可視化すると下記のようになり、NoPEとT5のrelative positional encodingが最も類似していることから、NoPEが学習を通じて（実用上は）相対位置エンコーディングのようなものを学習することが分かった。

<br>

<img src="https://github.com/user-attachments/assets/9619c7e5-0612-45de-8717-1634bee509b7" alt="image" loading="lazy">

<br>



<br>

[^1]:深さ1のLayerのHidden State H^1から絶対位置の復元が可能であり（つまり、当該レイヤーのHが絶対位置に関する情報を保持している）、この前提のもと、後続のLayerがこの情報を上書きしないと仮定した場合に、相対位置エンコーディングを実現できる。また、CoT/Scratchpadはlong sequenceに対する汎化性能を向上させることがsmall scaleではあるが先行研究で示されており、Positional Encodingを変化させた時にCoT/Scratchpadの性能にどのような影響を与えるかを調査。

<br>



<br>

具体的には、CoT/Scratchpadのフォーマットがどのようなものが有効かも明らかではないので、5種類のコンポーネントの組み合わせでフォーマットを構成し、mathematical reasoningタスクで以下のような設定で訓練し

<br>



<br>

・さまざまなコンポーネントの組み合わせで異なるフォーマットを作成し、

<br>

・全ての位置エンコーディングあり/なしモデルを訓練

<br>



<br>

これらを比較した。この結果、CoT/Scratchpadはフォーマットに関係なく、特定のタスクでのみ有効（有効かどうかはタスク依存）であることが分かった。このことから、CoT/Scratcpad（つまり、モデルのinputとoutputの仕方）単体で、long contextに対する汎化性能を向上させることができないので、Positional Encoding（≒モデルのアーキテクチャ）によるlong contextに対する汎化性能の向上が非常に重要であることが浮き彫りになった。

<br>

<img src="https://github.com/user-attachments/assets/e23c4fbf-84de-4344-a01e-1e7e9e66fa7e" alt="image" loading="lazy">

<br>



<br>

また、CoT/Scratchpadが有効だったAdditionに対して各Positional Embeddingモデルを学習し、生成されたトークンのattentionがどの位置のトークンを指しているかを相対距離で可視化したところ（0が当該トークン、つまり現在のScratchpadに着目しており、1が遠いトークン、つまりinputに着目していることを表すように正規化）、NoPEとRelative Positional Encodingがshort/long rangeにそれぞれフォーカスするようなbinomialな分布なのに対し、他のPositional Encodingではよりuniformな分布であることが分かった。このタスクにおいてはNoPEとRelative POの性能が高かったため、binomialな分布の方がより最適であろうことが示唆された。

<br>

<img src="https://github.com/user-attachments/assets/833e6a81-8611-4e79-9d2e-473f7ebee2d0" alt="image" loading="lazy">

<br>

</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Alignment.html">#Alignment</a>
<a class="button" href="articles/DPO.html">#DPO</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2024-09-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1412">Direct Preference Optimization: Your Language Model is Secretly a Reward  Model, Rafael Rafailov+, N_A, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>大規模無監督言語モデル（LM）の制御性を向上させるために、報酬モデルの新しいパラメータ化を導入し、単純な分類損失でRLHF問題を解決する「直接的な好み最適化（DPO）」アルゴリズムを提案。DPOは安定性と性能を持ち、ファインチューニング中のサンプリングやハイパーパラメータ調整を不要にし、既存の方法と同等以上の性能を示す。特に、生成物の感情制御においてPPOベースのRLHFを上回り、応答の質を改善しつつ実装が簡素化される。</span>
<span class="snippet"><span>Comment</span>DPOを提案した研究

<br>



<br>

<img width="838" alt="image" src="https://github.com/user-attachments/assets/2f7edf2c-32fa-4c5c-bc39-fb85112d1837">

<br>



<br>

解説ポスト:

<br>

https://x.com/theturingpost/status/1940194999993585925?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QSNLP'24での解説スライド:https://speakerdeck.com/kazutoshishinoda/lun-wen-shao-jie-direct-preference-optimization-your-language-model-is-secretly-a-reward-model</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/InstructionTuning.html">#InstructionTuning</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/InstructionFollowingCapability.html">#InstructionFollowingCapability</a>
<span class="issue_date">Issue Date: 2023-11-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1137">Instruction-Following Evaluation for Large Language Models, Jeffrey Zhou+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>大規模言語モデル（LLMs）の能力を評価するために、Instruction-Following Eval（IFEval）という評価ベンチマークが導入されました。IFEvalは、検証可能な指示に焦点を当てた直感的で再現性のある評価方法です。具体的には、25種類の検証可能な指示を特定し、それぞれの指示を含む約500のプロンプトを作成しました。この評価ベンチマークの結果は、GitHubで公開されています。</span>
<span class="snippet"><span>Comment</span>LLMがinstructionにどれだけ従うかを評価するために、検証可能なプロンプト（400字以上で書きなさいなど）を考案し評価する枠組みを提案。人間が評価すると時間とお金がかかり、LLMを利用した自動評価だと評価を実施するLLMのバイアスがかかるのだ、それら両方のlimitationを克服できるとのこと。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/0eb3fe10-536d-4674-aa3c-fd76f390f21d" alt="image" loading="lazy"></span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ReversalCurse.html">#ReversalCurse</a>
<span class="issue_date">Issue Date: 2023-10-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1059">[Paper Note] The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A", Lukas Berglund+, arXiv'23</a>
<span class="snippet"><span>Summary</span>自己回帰型大規模言語モデル（LLMs）は、「AはBである」という文から「BはAである」と逆の関係を自動的に一般化できない「逆転の呪い」を示す。例えば、モデルが「ワレンティナ・テレシコワは宇宙に行った最初の女性である」と訓練されても、「宇宙に行った最初の女性は誰か？」に正しく答えられない。実験では、架空の文を用いてGPT-3とLlama-1をファインチューニングし、逆転の呪いの存在を確認。ChatGPT（GPT-3.5およびGPT-4）でも、実在の有名人に関する質問で正答率に大きな差が見られた。</span>
<span class="snippet"><span>Comment</span>A is Bという文でLLMを訓練しても、B is Aという逆方向には汎化されないことを示した。

<br>



<br>

著者ツイート: https://x.com/owainevans_uk/status/1705285631520407821?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/25e20dcc-0313-4cd2-8768-afb0e4e48a68" alt="image" loading="lazy">

<br>

GPT3, LLaMaを A is Bでfinetuneし、B is Aという逆方向のfactを生成するように（質問をして）テストしたところ、0%付近のAcc.だった。

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/d089eb94-6872-40b5-89a1-7532758e1d89" alt="image" loading="lazy">

<br>



<br>

また、Acc.が低いだけでなく、対数尤度もrandomなfactを生成した場合と、すべてのモデルサイズで差がないことがわかった。

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/ba10fff4-cfdc-4e52-8217-c59247209211" alt="image" loading="lazy">

<br>



<br>

このことら、Reversal Curseはモデルサイズでは解決できないことがわかる。関連:

<br>

・1923</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Quantization.html">#Quantization</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<span class="issue_date">Issue Date: 2023-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/881">QLoRA: Efficient Finetuning of Quantized LLMs, Tim Dettmers+, N_A, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>私たちは、QLoRAという効率的なファインチューニング手法を提案します。この手法は、メモリ使用量を削減し、48GBの単一のGPU上で65Bパラメータモデルをファインチューニングすることができます。また、16ビットのファインチューニングタスクのパフォーマンスを維持します。QLoRAは、凍結された4ビット量子化された事前学習済み言語モデルの勾配をLow Rank Adapters（LoRA）に逆伝播させます。私たちの最良のモデルファミリーであるGuanacoは、Vicunaベンチマークで以前に公開されたすべてのモデルを上回り、ChatGPTのパフォーマンスレベルの99.3%に達します。また、単一のGPU上でのファインチューニングには24時間しかかかりません。QLoRAは、パフォーマンスを犠牲にすることなくメモリを節約するためのいくつかの革新を導入しています。具体的には、4ビットNormalFloat（NF4）という情報理論的に最適な新しいデータ型、ダブル量子化による平均メモリフットプリントの削減、およびページドオプティマイザによるメモリスパイクの管理です。私たちはQLoRAを使用して1,000以上のモデルをファインチューニングし、8つの命令データセット、複数のモデルタイプ（LLaMA、T5）、および従来のファインチューニングでは実行不可能なモデルスケール（33Bおよび65Bパラメータモデル）にわたる命令の追跡とチャットボットのパフォーマンスの詳細な分析を提供します。私たちの結果は、QLoRAを使用して小規模な高品質のデータセットでのファインチューニングが、以前のSoTAよりも小さいモデルを使用しても最先端の結果をもたらすことを示しています。また、人間の評価とGPT-4の評価に基づいたチャットボットのパフォーマンスの詳細な分析を提供し、GPT-4の評価が安価で合理的な人間の評価の代替手段であることを示します。さらに、現在のチャットボットのベンチマークは、チャットボットのパフォーマンスレベルを正確に評価するためには信頼性がないことがわかります。GuanacoがChatGPTと比較してどこで失敗するかを示す分析も行っています。私たちは、4ビットトレーニングのためのCUDAカーネルを含む、すべてのモデルとコードを公開しています。</span>
<span class="snippet"><span>Comment</span>実装: https://github.com/artidoro/qlora

<br>

PEFTにもある参考: https://twitter.com/hillbig/status/1662946722690236417?s=46&t=TDHYK31QiXKxggPzhZbcAQOpenReview:https://openreview.net/forum?id=OUIFPHEgJU&referrer=%5Bthe%20profile%20of%20Ari%20Holtzman%5D(%2Fprofile%3Fid%3D~Ari_Holtzman1)</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<span class="issue_date">Issue Date: 2023-07-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/786">Holistic Evaluation of Language Models, Percy Liang+, TMLR'23</a>
<span class="snippet"><span>Summary</span>言語モデルの透明性を向上させるために、Holistic Evaluation of Language Models（HELM）を提案する。HELMでは、潜在的なシナリオとメトリックを分類し、広範なサブセットを選択して評価する。さらに、複数のメトリックを使用し、主要なシナリオごとに評価を行う。30の主要な言語モデルを42のシナリオで評価し、HELM以前に比べて評価のカバレッジを改善した。HELMはコミュニティのためのベンチマークとして利用され、新しいシナリオ、メトリック、モデルが継続的に更新される。</span>
<span class="snippet"><span>Comment</span>OpenReview:https://openreview.net/forum?id=iO4LZibEqWHELMを提案した研究

<br>

当時のLeaderboardは既にdeprecatedであり、現在は下記を参照:

<br>

https://crfm.stanford.edu/helm/</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/LLMAgent.html">#LLMAgent</a>
<span class="issue_date">Issue Date: 2023-04-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/518">REACT : SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS, Yao+, Princeton University and Google brain, ICLR'23</a>
<span class="snippet"><span>Comment</span>概要

<br>



<br>

人間は推論と行動をシナジーさせることで、さまざまな意思決定を行える。近年では言語モデルにより言語による推論を意思決定に組み合わせる可能性が示されてきた。たとえば、タスクをこなすための推論トレースをLLMが導けることが示されてきた（Chain-of-Thought）が、CoTは外部リソースにアクセスできないため知識がアップデートできず、事後的に推論を行うためhallucinationやエラーの伝搬が生じる。一方で、事前学習言語モデルをinteractiveな環境において計画と行動に利用する研究が行われているが、これらの研究では、高レベルの目標について抽象的に推論したり、行動をサポートするための作業記憶を維持したりするために言語モデルを利用していない。推論と行動を一般的な課題解決のためにどのようにシナジーできるか、またそのようなシナジーが単独で推論や行動を実施した場合と比較してどのような利益をもたらすかについて研究されていない。

<br>



<br>

そこで、REACTを提案。REACTは推論と行動をLLMと組み合わせて、多様な推論や意思決定タスクを実現するための一般的な枠組みであり、推論トレースとアクションを交互に生成するため、動的に推論を実行して行動するための大まかな計画を作成、維持、調整できると同時に、wikipediaなどの外部ソースとやりとりして追加情報を収集し、推論プロセスに組み込むことが可能となる。

<br>



<br>



<br>



<br>

・要はいままではGeneralなタスク解決モデルにおいては、推論とアクションの生成は独立にしかやられてこなかったけど、推論とアクションを交互作用させることについて研究したよ

<br>



<br>

・そしたら性能がとってもあがったよ

<br>



<br>

・reasoningを人間が編集すれば、エージェントのコントロールもできるよ　という感じ

<br>



<br>



<br>



<br>

イントロ

<br>



<br>

人間は推論と行動の緊密なシナジーによって、不確実な状況に遭遇しても適切な意思決定が行える。たとえば、任意の2つの特定のアクションの間で、進行状況をトレースするために言語で推論したり（すべて切り終わったからお湯を沸かす必要がある）、例外を処理したり、状況に応じて計画を調整したりする（塩がないから代わりに醤油と胡椒を使おう）。また、推論をサポートし、疑問（いまどんな料理を作ることができるだろうか？）を解消するために、行動（料理本を開いてレシピを読んで、冷蔵庫を開いて材料を確確認したり）をすることもある。

<br>



<br>



<br>



<br>

近年の研究では言語での推論を、インタラクティブな意思決定を組み合わせる可能性についてのヒントが得られてきた。一つは、適切にPromptingされたLLMが推論トレースを実行できることを示している。推論トレースとは、解決策に到達するための一連のステップを経て推論をするためのプロセスのことである。しかしながらChain-of-thoughytは、このアプローチでは、モデルが外界対してgroundingできず、内部表現のみに基づい思考を生成するため限界がある。これによりモデルが事後対応的に推論したり、外部情報に基づいて知識を更新したりできないため、推論プロセス中にhallucinationやエラーの伝搬などの問題が発生する可能性が生じる。

<br>



<br>

一方、近年の研究では事前学習言語モデルをinteractiveな環境において計画と行動に利用する研究が行われている。これらの研究では、通常マルチモーダルな観測結果をテキストに変換し、言語モデルを使用してドメイン固有のアクション、またはプランを生成し、コントローラーを利用してそれらを選択または実行する。ただし、これらのアプローチは高レベルの目標について抽象的に推論したり、行動をサポートするための作業記憶を維持したりするために言語モデルを利用していない。

<br>



<br>

推論と行動を一般的な課題解決のためにどのようにシナジーできるか、またそのようなシナジーが単独で推論や行動を実施した場合と比較してどのような利益をもたらすかについて研究されていない。

<br>



<br>



<br>



<br>

LLMにおける推論と行動を組み合わせて、言語推論と意思決定タスクを解決するREACTと呼ばれる手法を提案。REACTでは、推論と行動の相乗効果を高めることが可能。推論トレースによりアクションプランを誘発、追跡、更新するのに役立ち、アクションでは外部ソースと連携して追加情報を収集できる。

<br>



<br>



<br>



<br>

REACTは推論と行動をLLMと組み合わせて、多様な推論や意思決定タスクを実現するための一般的な枠組みである。REACTのpromptはLLMにverbalな推論トレースとタスクを実行するためのアクションを交互に生成する。これにより、モデルは動的な推論を実行して行動するための大まかな計画を作成、維持、調整できると同時に、wikipediaなどの外部ソースとやりとりして追加情報を収集し、推論プロセスに組み込むことが可能となる。

<br>



<br>



<br>



<br>

手法

<br>



<br>

変数を以下のように定義する：

<br>



<br>

・O_t: Observertion on time t

<br>



<br>

・a_t: Action on time t

<br>



<br>

・c_t: context, i.e. (o_1, a_1, o_2, a_2, ..., a_t-1, o_t)

<br>



<br>

・policy pi\(a\_t | c\_t): Action Spaceからアクションを選択するポリシー

<br>



<br>

・A: Action Space

<br>



<br>

・O: Observation Space

<br>



<br>



<br>



<br>

普通はc_tが与えられたときに、ポリシーに従いAからa_tを選択しアクションを行い、アクションの結果o_tを得て、c_t+1を構成する、といったことを繰り返していく。

<br>



<br>



<br>



<br>

このとき、REACTはAをA ∪ Lに拡張しする。ここで、LはLanguage spaceである。LにはAction a_hatが含まれ、a_hatは環境に対して作用をしない。単純にthought, あるいは reasoning traceを実施し、現在のcontext c_tをアップデートするために有用な情報を構成することを目的とする。Lはunlimitedなので、事前学習された言語モデルを用いる。今回はPaLM-540B（c.f. GPT3は175Bパラメータ）が利用され、few-shotのin-context exampleを与えることで推論を行う。それぞれのin-context exampleは、action, thoughtsそしてobservationのtrajectoryを与える。

<br>



<br>



<br>



<br>

推論が重要なタスクでは、thoughts-action-observationステップから成るtask-solving trajectoryを生成する。一方、多数のアクションを伴う可能性がある意思決定タスクでは、thoughtsのみを行うことをtask-solving trajectory中の任意のタイミングで、自分で判断して行うことができる。

<br>



<br>



<br>



<br>

意思決定と推論能力がLLMによってもたらされているため、REACTは4つのuniqueな特徴を持つ：

<br>



<br>

・直感的で簡単なデザイン

<br>



<br>

  ・REACTのpromptは人間のアノテータがアクションのトップに思考を言語で記述するようなストレートなものであり、ad-hocなフォーマットの選択、思考のデザイン、事例の選定などが必要ない。

<br>



<br>

・一般的で柔軟性が高い

<br>



<br>

  ・柔軟な thought spaceと thought-actionのフォーマットにより、REACTはさまざまなタスクにも柔軟に対応できる

<br>



<br>

・高性能でロバスト

<br>



<br>

  ・REACTは1-6個の事例によって、新たなタスクに対する強力な汎化を示す。そして推論、アクションのみを行うベースラインよりも高い性能を示している。REACTはfinetuningの斧系も得ることができ、promptの選択に対してREACTの性能はrobustである。

<br>



<br>

・人間による調整と操作が可能

<br>



<br>

  ・REACTは、解釈可能な意思決定と推論のsequenceを前提としているため、人間は簡単に推論や事実の正しさを検証できる。加えて、thoughtsを編集することによって、m人間はエージェントの行動を制御、あるいは修正できる。

<br>



<br>



<br>



<br>

KNOWLEDGE INTENSIVE REASONING TASKS</span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<a class="button" href="articles/Deduplication.html">#Deduplication</a>
<span class="issue_date">Issue Date: 2025-09-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2689">[Paper Note] Deduplicating Training Data Makes Language Models Better, Katherine Lee+, ACL'22</a>
<span class="snippet"><span>Summary</span>既存の言語モデルデータセットには重複した例が多く含まれ、訓練されたモデルの出力の1%以上が訓練データからコピーされている。これを解決するために、重複排除ツールを開発し、C4データセットからは60,000回以上繰り返される文を削除。重複を排除することで、モデルの記憶されたテキスト出力を10倍減少させ、精度を維持しつつ訓練ステップを削減。また、訓練とテストの重複を減らし、より正確な評価を実現。研究の再現とコードは公開されている。</span>
<span class="snippet"><span>Comment</span>下記スライドのp.9にまとめが記述されている:

<br>

https://speakerdeck.com/takase/snlp2023-beyond-neural-scaling-laws?slide=9</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Backbone.html">#Backbone</a>
<span class="issue_date">Issue Date: 2025-08-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2597">[Paper Note] A ConvNet for the 2020s, Zhuang Liu+, arXiv'22</a>
<span class="snippet"><span>Summary</span>ConvNetはVision Transformersの登場により地位を失ったが、ハイブリッドアプローチの効果はトランスフォーマーの優位性に依存している。本研究では、ConvNetの限界をテストし、ConvNeXtという新しいモデルを提案。ConvNeXtは標準的なConvNetモジュールのみで構成され、精度とスケーラビリティでトランスフォーマーと競争し、ImageNetで87.8%の精度を達成し、COCO検出およびADE20KセグメンテーションでSwin Transformersを上回る。</span>
<span class="snippet"><span>Comment</span>ConvNeXt</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-05-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1956">LoRA: Low-Rank Adaptation of Large Language Models, Edward J. Hu+, ICLR'22</a>
<span class="snippet"><span>Summary</span>LoRAは、事前学習された大規模モデルの重みを固定し、各層に訓練可能なランク分解行列を追加することで、ファインチューニングに必要なパラメータを大幅に削減する手法です。これにより、訓練可能なパラメータを1万分の1、GPUメモリを3分の1に減少させながら、RoBERTaやGPT-3などで同等以上の性能を実現します。LoRAの実装はGitHubで公開されています。</span>
<span class="snippet"><span>Comment</span>OpenrReview:https://openreview.net/forum?id=nZeVKeeFYf9LoRAもなんやかんやメモってなかったので追加。

<br>



<br>

事前学習済みのLinear Layerをfreezeして、freezeしたLinear Layerと対応する低ランクの行列A,Bを別途定義し、A,BのパラメータのみをチューニングするPEFT手法であるLoRAを提案した研究。オリジナルの出力に対して、A,Bによって入力を写像したベクトルを加算する。

<br>



<br>

チューニングするパラメータ数学はるかに少ないにも関わらずフルパラメータチューニングと（これは諸説あるが）同等の性能でPostTrainingできる上に、事前学習時点でのパラメータがfreezeされているためCatastrophic Forgettingが起きづらく（ただし新しい知識も獲得しづらい）、A,Bの追加されたパラメータのみを保存すれば良いのでストレージに優しいのも嬉しい。</span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<a class="button" href="articles/Scaling%20Laws.html">#Scaling Laws</a>
<span class="issue_date">Issue Date: 2025-03-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1827">Training Compute-Optimal Large Language Models, Jordan Hoffmann+, NeurIPS'22</a>
<span class="snippet"><span>Summary</span>トランスフォーマー言語モデルの訓練において、計算予算内で最適なモデルサイズとトークン数を調査。モデルサイズと訓練トークン数は同等にスケールする必要があり、倍増するごとにトークン数も倍増すべきと提案。Chinchillaモデルは、Gopherなどの大規模モデルに対して優れた性能を示し、ファインチューニングと推論の計算量を削減。MMLUベンチマークで67.5%の精度を達成し、Gopherに対して7%以上の改善を実現。</span>
<span class="snippet"><span>Comment</span>OpenReview: https://openreview.net/forum?id=iBBcRUlOAPRchinchilla則</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html">#MoE(Mixture-of-Experts)</a>
<span class="issue_date">Issue Date: 2025-02-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1754">Switch Transformers: Scaling to Trillion Parameter Models with Simple  and Efficient Sparsity, William Fedus+, JMLR'22</a>
<span class="snippet"><span>Summary</span>Switch Transformerを提案し、Mixture of Experts (MoE)の複雑さや通信コスト、トレーニングの不安定性を改善。これにより、低精度フォーマットでの大規模スパースモデルのトレーニングが可能になり、最大7倍の事前トレーニング速度向上を実現。さらに、1兆パラメータのモデルを事前トレーニングし、T5-XXLモデルに対して4倍の速度向上を達成。</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<a class="button" href="articles/KnowledgeEditing.html">#KnowledgeEditing</a>
<a class="button" href="articles/FactualKnowledge.html">#FactualKnowledge</a>
<a class="button" href="articles/Encoder.html">#Encoder</a>
<span class="issue_date">Issue Date: 2024-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1332">Knowledge Neurons in Pretrained Transformers, Damai Dai+, N_A, ACL'22, 2022.05</a>
<span class="snippet"><span>Summary</span>大規模な事前学習言語モデルにおいて、事実知識の格納方法についての研究を行いました。具体的には、BERTのfill-in-the-blank cloze taskを用いて、関連する事実を表現するニューロンを特定しました。また、知識ニューロンの活性化と対応する事実の表現との正の相関を見つけました。さらに、ファインチューニングを行わずに、知識ニューロンを活用して特定の事実知識を編集しようと試みました。この研究は、事前学習されたTransformers内での知識の格納に関する示唆に富んでおり、コードはhttps://github.com/Hunter-DDM/knowledge-neuronsで利用可能です。</span>
<span class="snippet"><span>Comment</span>1108 日本語解説: https://speakerdeck.com/kogoro/knowledge-neurons-in-pretrained-transformers-for-snlp2022関連:

<br>

・2140上記資料によると、特定の知識を出力する際に活性化する知識ニューロンを特定する手法を提案。MLMを用いたclozeタスクによる実験で[MASK]部分に当該知識を出力する実験をした結果、知識ニューロンの重みをゼロとすると性能が著しく劣化し、値を2倍にすると性能が改善するといった傾向がみられた。　ケーススタディとして、知識の更新と、知識の削除が可能かを検証。どちらとも更新・削除がされる方向性[^1]へモデルが変化した。

<br>



<br>

また、知識ニューロンはTransformerの層の深いところに位置している傾向にあり、異なるrelationを持つような関係知識同士では共有されない傾向にある模様。

<br>



<br>

[^1]: 他の知識に影響を与えず、完璧に更新・削除できたわけではない。知識の更新・削除に伴いExtrinsicな評価によって性能向上、あるいはPerplexityが増大した、といった結果からそういった方向性へモデルが変化した、という話</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/Backbone.html">#Backbone</a>
<span class="issue_date">Issue Date: 2025-08-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2545">[Paper Note] An Image is Worth 16x16 Words: Transformers for Image Recognition at   Scale, Alexey Dosovitskiy+, ICLR'21</a>
<span class="snippet"><span>Summary</span>純粋なトランスフォーマーを画像パッチのシーケンスに直接適用することで、CNNへの依存なしに画像分類タスクで優れた性能を発揮できることを示す。大量のデータで事前学習し、複数の画像認識ベンチマークで最先端のCNNと比較して優れた結果を達成し、計算リソースを大幅に削減。</span>
<span class="snippet"><span>Comment</span>openreview:https://openreview.net/forum?id=YicbFdNTTyViTを提案した研究</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/CodeGeneration.html">#CodeGeneration</a>
<span class="issue_date">Issue Date: 2025-08-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2439">[Paper Note] Program Synthesis with Large Language Models, Jacob Austin+, arXiv'21</a>
<span class="snippet"><span>Summary</span>本論文では、汎用プログラミング言語におけるプログラム合成の限界を大規模言語モデルを用いて評価します。MBPPとMathQA-Pythonの2つのベンチマークで、モデルサイズに対する合成性能のスケールを調査。最も大きなモデルは、少数ショット学習でMBPPの59.6％の問題を解決可能で、ファインチューニングにより約10％の性能向上が見られました。MathQA-Pythonでは、ファインチューニングされたモデルが83.8％の精度を達成。人間のフィードバックを取り入れることでエラー率が半減し、エラー分析を通じてモデルの弱点を明らかにしました。最終的に、プログラム実行結果の予測能力を探るも、最良のモデルでも特定の入力に対する出力予測が困難であることが示されました。</span>
<span class="snippet"><span>Comment</span>代表的なコード生成のベンチマーク。

<br>



<br>

MBPPデータセットは、promptで指示されたコードをモデルに生成させ、テストコード（assertion)を通過するか否かで評価する。974サンプル存在し、pythonの基礎を持つクラウドワーカーによって生成。クラウドワーカーにタスクdescriptionとタスクを実施する一つの関数（関数のみで実行可能でprintは不可）、3つのテストケースを記述するよう依頼。タスクdescriptionは追加なclarificationなしでコードが記述できるよう十分な情報を含むよう記述するように指示。ground truthの関数を生成する際に、webを閲覧することを許可した。

<br>

<img src="https://github.com/user-attachments/assets/e27880f7-4647-462d-b619-e0a7a0959d66" alt="image" loading="lazy">

<br>



<br>

MathQA-Pythonは、MathQAに含まれるQAのうち解答が数値のもののみにフィルタリングしたデータセットで、合計で23914サンプル存在する。pythonコードで与えられた数学に関する問題を解くコードを書き、数値が一致するか否かで評価する、といった感じな模様。斜め読みなので少し読み違えているかもしれない。

<br>



<br>

<img src="https://github.com/user-attachments/assets/d21ee76f-a13d-4ef9-843b-74c233c3c0a6" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/CodeGeneration.html">#CodeGeneration</a>
<span class="issue_date">Issue Date: 2025-08-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2438">[Paper Note] Evaluating Large Language Models Trained on Code, Mark Chen+, arXiv'21</a>
<span class="snippet"><span>Summary</span>CodexはGitHubのコードでファインチューニングされたGPT言語モデルで、Pythonコード生成能力を評価。新しい評価セットHumanEvalでは、Codexが28.8%の問題を解決し、GPT-3は0%、GPT-Jは11.4%だった。繰り返しサンプリングが難しいプロンプトに対しても効果的な戦略を用い、70.2%の問題を解決。モデルの限界として、長い操作の説明や変数へのバインドに苦労する点が明らかに。最後に、コード生成技術の影響について安全性や経済に関する議論を行う。</span>
<span class="snippet"><span>Comment</span>HumanEvalデータセット。Killed by LLMによると、GPT4oによりすでに90%程度の性能が達成され飽和している。

<br>



<br>

164個の人手で記述されたprogrammingの問題で、それぞれはfunction signature, docstring, body, unittestを持つ。unittestは問題当たり約7.7 test存在。handwrittenという点がミソで、コンタミネーションの懸念があるためgithubのような既存ソースからのコピーなどはしていない。pass@k[^1]で評価。

<br>



<br>

[^1]: k個のサンプルを生成させ、k個のサンプルのうち、サンプルがunittestを一つでも通過する確率。ただ、本研究ではよりバイアスをなくすために、kよりも大きいn個のサンプルを生成し、その中からランダムにk個を選択して確率を推定するようなアプローチを実施している。2.1節を参照のこと。

<br>



<br>

<img src="https://github.com/user-attachments/assets/74a74b6f-9d0c-4ce9-ab8b-53b478b4632a" alt="image" loading="lazy"></span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<a class="button" href="articles/ICCV.html">#ICCV</a>
<a class="button" href="articles/Backbone.html">#Backbone</a>
<span class="issue_date">Issue Date: 2025-07-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2258">[Paper Note] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, Ze Liu+, ICCV'21</a>
<span class="snippet"><span>Summary</span>Swin Transformerは、コンピュータビジョンの新しいバックボーンとして機能する階層的トランスフォーマーを提案。シフトウィンドウ方式により、効率的な自己注意計算を実現し、さまざまなスケールでのモデリングが可能。画像分類や物体検出、セマンティックセグメンテーションなどで従来の最先端を上回る性能を示し、トランスフォーマーのビジョンバックボーンとしての可能性を示唆。コードは公開されている。</span>
<span class="snippet"><span>Comment</span>日本語解説:https://qiita.com/m_sugimura/items/139b182ee7c19c83e70a画像処理において、物体の異なるスケールや、解像度に対処するために、PatchMergeと呼ばれるプーリングのような処理と、固定サイズのローカルなwindowに分割してSelf-Attentionを実施し、layerごとに通常のwindowとシフトされたwindowを適用することで、window間を跨いだ関係性も考慮できるようにする機構を導入したモデル。

<br>

<img src="https://github.com/user-attachments/assets/a2d5f78c-27ec-4f18-bd7d-5475085cfa7b" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/92fb10e1-614e-44ef-9e65-3920cd863d46" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/2b8a543a-069e-468a-bc3c-1f288cdcf577" alt="image" loading="lazy"></span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/EMNLP.html">#EMNLP</a>
<a class="button" href="articles/FactualKnowledge.html">#FactualKnowledge</a>
<span class="issue_date">Issue Date: 2025-07-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2140">[Paper Note] Transformer Feed-Forward Layers Are Key-Value Memories, Mor Geva+, EMNLP'21</a>
<span class="snippet"><span>Summary</span>フィードフォワード層はトランスフォーマーモデルの大部分を占めるが、その役割は未探求。研究により、フィードフォワード層がキー・バリュー・メモリとして機能し、トレーニング例のテキストパターンと相関することを示す。実験で、下層は浅いパターン、上層は意味的なパターンを学習し、バリューが出力分布を誘導することが確認された。最終的に、フィードフォワード層の出力はメモリの合成であり、残差接続を通じて洗練される。</span>
<span class="snippet"><span>Comment</span>日本語解説（p.5より）: https://speakerdeck.com/kogoro/knowledge-neurons-in-pretrained-transformers-for-snlp2022?slide=5</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/Mathematics.html">#Mathematics</a>
<a class="button" href="articles/Verification.html">#Verification</a>
<span class="issue_date">Issue Date: 2024-12-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1618">Training Verifiers to Solve Math Word Problems, Karl Cobbe+, arXiv'21</a>
<span class="snippet"><span>Summary</span>GSM8Kデータセットを用いて、多段階の数学的推論における言語モデルの限界を分析。検証器を訓練し、候補解を評価して最適解を選択することで、モデルのパフォーマンスを大幅に向上させることを示した。検証はファインチューニングよりもデータ増加に対して効果的にスケールする。</span>
<span class="snippet"><span>Comment</span>気持ち

<br>



<br>

・当時の最も大きいレベルのモデルでも multi-stepのreasoningが必要な問題は失敗する

<br>



<br>

・モデルをFinetuningをしても致命的なミスが含まれる

<br>



<br>

・特に、数学は個々のミスに対して非常にsensitiveであり、一回ミスをして異なる解法のパスに入ってしまうと、self-correctionするメカニズムがauto-regressiveなモデルではうまくいかない

<br>



<br>

・純粋なテキスト生成の枠組みでそれなりの性能に到達しようとすると、とんでもないパラメータ数が必要になり、より良いscaling lawを示す手法を模索する必要がある

<br>



<br>

Contribution

<br>



<br>

論文の貢献は

<br>



<br>

・GSM8Kを提案し、

<br>



<br>

・verifierを活用しモデルの複数の候補の中から良い候補を選ぶフレームワークによって、モデルのパラメータを30倍にしたのと同等のパフォーマンスを達成し、データを増やすとverifierを導入するとよりよく性能がスケールすることを示した。

<br>



<br>

・また、dropoutが非常に強い正則化作用を促し、finetuningとverificationの双方を大きく改善することを示した。Todo: 続きをまとめる</span>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/Metrics.html">#Metrics</a>
<a class="button" href="articles/Tools.html">#Tools</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<span class="issue_date">Issue Date: 2023-08-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/984">SummEval: Re-evaluating Summarization Evaluation, Fabbri+, TACL'21</a>
<span class="snippet"><span>Comment</span>自動評価指標が人手評価の水準に達しないことが示されており、結局のところROUGEを上回る自動性能指標はほとんどなかった。human judgmentsとのKendall;'s Tauを見ると、chrFがCoherenceとRelevance, METEORがFluencyで上回ったのみだった。また、LEAD-3はやはりベースラインとしてかなり強く、LEAD-3を上回ったのはBARTとPEGASUSだった。</span>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/Metrics.html">#Metrics</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/Reference-free.html">#Reference-free</a>
<a class="button" href="articles/LM-based.html">#LM-based</a>
<span class="issue_date">Issue Date: 2023-08-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/960">BARTSCORE: Evaluating Generated Text as Text Generation, Yuan+ （w_ Neubig氏）, NeurIPS'21</a>
<span class="snippet"><span>Summary</span>本研究では、生成されたテキストの評価方法について検討しました。具体的には、事前学習モデルを使用してテキスト生成の問題をモデル化し、生成されたテキストを参照出力またはソーステキストに変換するために訓練されたモデルを使用しました。提案したメトリックであるBARTSCOREは、情報量、流暢さ、事実性などの異なる視点のテキスト評価に柔軟に適用できます。実験結果では、既存のトップスコアリングメトリックを上回る性能を示しました。BARTScoreの計算に使用するコードは公開されており、インタラクティブなリーダーボードも利用可能です。</span>
<span class="snippet"><span>Comment</span>BARTScore概要

<br>



<br>

ソーステキストが与えられた時に、BARTによって生成テキストを生成する尤度を計算し、それをスコアとする手法。テキスト生成タスクをテキスト生成モデルでスコアリングすることで、pre-trainingされたパラメータをより有効に活用できる（e.g. BERTScoreやMoverScoreなどは、pre-trainingタスクがテキスト生成ではない）。BARTScoreの特徴は

<br>



<br>

1. parameter・and data-efficientである。pre-trainingに利用されたパラメータ以外の追加パラメータは必要なく、unsupervisedなmetricなので、human judgmentのデータなども必要ない。

<br>



<br>

2. 様々な観点から生成テキストを評価できる。conditional text generation problemにすることでinformativeness, coherence, factualityなどの様々な観点に対応可能。

<br>



<br>

3. BARTScoreは、(i) pre-training taskと類似したpromptを与えること、(ii) down stream generation taskでfinetuningすること、でより高い性能を獲得できる

<br>



<br>

BARTScoreを16種類のデータセットの、7つの観点で評価したところ、16/22において、top-scoring metricsよりも高い性能を示した。また、prompting starategyの有効性を示した。たとえば、シンプルに"such as"というフレーズを翻訳テキストに追加するだけで、German-English MTにおいて3%の性能向上が見られた。また、BARTScoreは、high-qualityなテキスト生成システムを扱う際に、よりロバストであることが分析の結果分かった。

<br>



<br>



<br>



<br>

前提

<br>



<br>

Problem Formulation

<br>



<br>

生成されたテキストのqualityを測ることを目的とする。本研究では、conditional text generation (e.g. 機械翻訳)にフォーカスする。すなわち、ゴールは、hypothesis h_bar を source text s_barがgivenな状態で生成することである。一般的には、人間が作成したreference r_barが評価の際は利用される。

<br>



<br>

Gold-standard Human Evaluation

<br>



<br>

評価のgold standardは人手評価であり、人手評価では多くの観点から評価が行われる。以下に代表的な観点を示す：

<br>



<br>

1. Informativeness: ソーステキストのキーアイデアをどれだけ捉えているか

<br>



<br>

2. Relevance: ソーステキストにあ地して、どれだけconsistentか

<br>



<br>

3. Fluency formatting problem, capitarlization errorや非文など、どの程度読むのが困難か

<br>



<br>

4. Coherence: 文間のつながりが、トピックに対してどれだけcoherentか

<br>



<br>

5. Factuality: ソーステキストに含意されるstatementのみを生成できているか

<br>



<br>

6. Semantic Coverage: 参照テキスト中のSemantic Content Unitを生成テキストがどれだけカバーできているか

<br>



<br>

7: Adequacy 入力文に対してアウトプットが同じ意味を出力できているかどうか、あるいは何らかのメッセージが失われる、追加される、歪曲していないかどうか

<br>



<br>



<br>



<br>

多くの性能指標は、これらの観点のうちのsubsetをカバーするようにデザインんされている。たとえば、BLEUは、翻訳におけるAdequacyとFluencyをとらえることを目的としている。一方、ROUGEは、semantic coverageを測るためのメトリックである。

<br>



<br>

BARTScoreは、これらのうち多くの観点を評価することができる。

<br>



<br>



<br>



<br>

Evaluation as Different Tasks

<br>



<br>

ニューラルモデルを異なる方法で自動評価に活用するのが最近のトレンドである。下図がその分類。この分類は、タスクにフォーカスした分類となっている。

<br>



<br>

1. Unsupervised Matching: ROUGE, BLEU, CHRF, BERTScore, MoverScoreのように、hypothesisとreference間での意味的な等価性を測ることが目的である。このために、token-levelのマッチングを用いる。これは、distributedな表現を用いる（BERTScore, MoverScore）場合もあれば、discreteな表現を用いる（ROUGE, BLEU, chrF）場合もある。また、意味的な等価性だけでなく、factual consistencyや、source-hypothesis間の関係性の評価に用いることもできると考えられるが先行研究ではやられていなかったので、本研究で可能なことを示す。

<br>



<br>

2. Supervised Regression: BLEURT, COMET, S^3, VRMのように、regression layer を用いてhuman judgmentをsupervisedに予測する方法である。最近のメトリックｔおしては、BLEURT, COMETがあげられ、古典的なものとしては、S^3, VRMがあげられる。

<br>



<br>

4. Supervised Ranking: COMET, BEERのような、ランキング問題としてとらえる方法もある。これは優れたhypothesisを上位にランキングするようなスコア関数を学習する問題に帰着する。COMETやBEERが例としてあげられ、両者はMTタスクにフォーカスされている。COMETはhunan judgmentsをregressionすることを通じてランキングを作成し、BEERは、多くのシンプルな特徴量を組み合わせて、linear layerでチューニングされる。

<br>



<br>

5. Text Generation: PRISM, BARTScoreが例として挙げられる。BARTScoreでは、生成されたテキストの評価をpre-trained language modelによるテキスト生成タスクとしてとらえる。基本的なアイデアとしては、高品質のhypothesisは、ソース、あるいはreferenceから容易に生成可能であろう、というものである。これはPRISMを除いて、先行研究ではカバーされていない。BARTScoreは、PRISMとはいくつかの点で異なっている。(i) PRISMは評価をparaphrasing taskとしてとらえており、これが2つの意味が同じテキストを比較する前提となってしまっているため、手法を適用可能な範囲を狭めてしまっている。たとえば、文書要約におけるfactual consistencyの評価では、semantic spaceが異なる2つのテキストを比較する必要があるが、このような例には対応できない。(ii) PRISMはparallel dataから学習しなけえｒばならないが、BARTScoreは、pre-trainedなopen-sourceのseq2seq modelを利用できる。(iii) BARTScoreでは、PRISMが検証していない、prompt-basedのlearningもサポートしている。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/4a64ea21-ab9f-4762-bd71-f858663fc195" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

BARTScore

<br>



<br>

Sequence-to-Sequence Pre-trained Models

<br>



<br>

pre-trainingされたモデルは、様々な軸で異なっているが、その一つの軸としては訓練時の目的関数である。基本的には２つの大きな変種があり、1つは、language modeling objectives (e.g. MLM)、2つ目は、seq2seq objectivesである。特に、seq2seqで事前学習されたモデルは、エンコーダーとデコーダーによって構成されているため特に条件付き生成タスクに対して適しており、予測はAutoRegressiveに行われる。本研究ではBARTを用いる。付録には、preliminary experimentsとして、BART with T5, PEGASUSを用いた結果も添付する。

<br>



<br>

BARTScore

<br>



<br>

最も一般的なBARTScoreの定式化は下記である。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/34505fd3-c8bb-49ee-92a8-f5710032b1ea" alt="image" loading="lazy">

<br>



<br>

weighted log probabilityを利用する。このweightsは、異なるトークンに対して、異なる重みを与えることができる。たておば、IDFなどが利用可能であるが、本研究ではすべてのトークンを等価に扱う（uniform weightingだがstopwordを除外、IDFによる重みづけ、事前分布を導入するなど色々試したが、uniform weightingを上回るものがなかった）。

<br>



<br>



<br>



<br>

BARTScoreを用いて、様々な方向に用いて生成を行うことができ、異なる評価のシナリオに対応することができる。

<br>



<br>

・Faithfulness (s -&gt; h):

<br>



<br>

    ・hypothesisがどれだけsource textに基づいて生成されているかを測ることができる。シナリオとしては、FactualityやRelevanceなどが考えられる。また、CoherenceやFluencyのように、target textのみの品質を測るためにも用いることができる。

<br>



<br>

・Precision (r -&gt; h):

<br>



<br>

    ・hypothesisがどれだけgold-referenceに基づいてこう良くされているかを亜評価でき、precision-focusedなシナリオに適している

<br>



<br>

・Recall (h -&gt; r):

<br>



<br>

    ・hypothesisから、gold referenceをどれだけ容易に再現できるかを測ることができる。そして、要約タスクのpyramid-basedな評価（i.e. semantic coverage等）  に適している。pyramid-scoreはSemantic Content Unitsがどれだけカバーされているかによって評価される。

<br>



<br>

・F Score (r &lt;-&gt; h):

<br>



<br>

    ・双方向を考慮し、Precisioon / RecallからF値を算出する。この方法は、referenceと生成テキスト間でのsemantic overlap (informativenss, adequacy)などの評価に広く利用される。

<br>



<br>



<br>



<br>

BARTScore Variants

<br>



<br>

BARTScoreの2つの拡張を提案。(i) xとyをpromptingによって変更する。これにより、評価タスクをpre-training taskと近づける。(ii) パラメータΘを異なるfinetuning taskを考慮して変更する。すなわち、pre-trainingのドメインを、evaluation taskに近づける。

<br>



<br>

Prompt

<br>



<br>

Promptingはinput/outputに対して短いフレーズを追加し、pre-trained modelに対して特定のタスクを遂行させる方法である。BARTにも同様の洞察を簡単に組み込むことができる。この変種をBARTScore-PROMPTと呼ぶ。

<br>



<br>

prompt zが与えられたときに、それを (i) source textに追加し、新たなsource textを用いてBARTScoreを計算する。(ii) target textの先頭に追加し、new target textに対してBARTScoreを計算する。

<br>



<br>

Fine-tuning Task

<br>



<br>

classification-basedなタスクでfine-tuneされるのが一般的なBERT-based metricとは異なり、BARTScoreはgeneration taskでfine-tuneされるため、pre-training domainがevaluation taskと近い。本研究では、2つのdownstream taskを検証する。

<br>



<br>

1つめは、summarizationで、BARTをCNNDM datasetでfinetuningする。2つめは、paraphrasingで、summarizationタスクでfinetuningしたBARTをParaBank2 datasetでさらにfinetuningする。実験

<br>



<br>

baselines and datasets

<br>



<br>

Evaluation Metrics

<br>



<br>

supervised metrics: COMET, BLEURT

<br>



<br>

unsupervised: BLEU, ROUGE-1, ROUGE-2, ROUGE-L, chrF, PRISM, MoverScore, BERTScore

<br>



<br>

と比較

<br>



<br>

Measures for Meta Evaluation

<br>



<br>

Pearson Correlationでlinear correlationを測る。また、Spearman Correlationで2変数間の単調なcorrelationを測定する（線形である必要はない）。Kendall's Tauを用いて、2つの順序関係の関係性を測る。最後に、Accuracyでfactual textsとnon-factual textの間でどれだけ正しいランキングを得られるかを測る。

<br>



<br>



<br>



<br>

Datasets

<br>



<br>

Summarization, MT, DataToTextの3つのデータセットを利用。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/117bf2d4-b096-4a60-a139-4a607ce3ebc6" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Setup

<br>



<br>

Prompt Design

<br>



<br>

seedをparaphrasingすることで、　s-&gt;h方向には70個のpromptを、h&lt;-&gt;rの両方向には、34のpromptを得て実験で用いた。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/dab4f2bc-9b8d-4de6-bbc3-204da39ee2eb" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Settings

<br>



<br>

Summarizationとdata-to-textタスクでは、全てのpromptを用いてデコーダの頭に追加してスコアを計算しスコアを計算した。最終的にすべての生成されたスコアを平均することである事例に対するスコアを求めた（prompt unsembling）。MTについては、事例数が多くcomputational costが多くなってしまうため、WMT18を開発データとし、best prompt "Such as"を選択し、利用した。

<br>



<br>

BARTScoreを使う際は、gold standard human evaluationがrecall-basedなpyrmid methodの場合はBARTScore(h-&gt;r)を用い、humaan judgmentsがlinguistic quality (coherence fluency)そして、factual correctness、あるいは、sourceとtargetが同じモダリティ（e.g. language）の場合は、faitufulness-based BARTScore(s-&gt;h)を用いた。最後に、MTタスクとdata-to-textタスクでは、fair-comparisonのためにBARTScore F-score versionを用いた。

<br>



<br>

実験結果

<br>



<br>

MT

<br>



<br>

・BARTScoreはfinetuning tasksによって性能が向上し、5つのlanguage pairsにおいてその他のunsupervised methodsを統計的に優位にoutperformし、2つのlanguage pairでcomparableであった。

<br>



<br>

-Such asというpromptを追加するだけで、BARTScoreの性能が改善した。特筆すべきは、de-enにおいては、SoTAのsupervised MetricsであるBLEURTとCOMETを上回った。

<br>



<br>

・これは、有望な将来のmetric designとして「human judgment dataで訓練する代わりに、pre-trained language modelに蓄積された知識をより適切に活用できるpromptを探索する」という方向性を提案している。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/ff41b3ec-3cf9-4c9a-90bb-83b46889d759" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Text Summarization

<br>



<br>

・vanilla BARTScoreはBERTScore, MoverScoreをInfo perspective以外でlarge marginでうくぁ回った。

<br>



<br>

・REALSum, SummEval dataseetでの改善は、finetuning taskによってさらに改善した。しかしながら、NeR18では改善しなかった。これは、データに含まれる7つのシステムが容易に区別できる程度のqualityであり、既にvanilla BARTScoreで高いレベルのcorrelationを達成しているからだと考えられる。

<br>



<br>

・prompt combination strategyはinformativenssに対する性能を一貫して改善している。しかし、fluency, factualityでは、一貫した改善は見られなかった。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/cfb33334-e38d-43e0-9b48-a8a8c433bc26" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Factuality datasetsに対する分析を行った。ゴールは、short generated summaryが、元のlong documentsに対してfaithfulか否かを判定するというものである。

<br>



<br>

・BARTScore+CNNは、Rank19データにおいてhuman baselineに近い性能を達成し、ほかのベースラインを上回った。top-performingなfactuality metricsであるFactCCやQAGSに対してもlarge marginで上回った。

<br>



<br>

・paraphraseをfine-tuning taskで利用すると、BARTScoreのパフォーマンスは低下した。これは妥当で、なぜなら二つのテキスト（summary and document）は、paraphrasedの関係性を保持していないからである。

<br>



<br>

・promptを導入しても、性能の改善は見受けられず、パフォーマンスは低下した。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/06dac947-c946-4633-8ff6-a9c3933f6322" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Data-to-Text

<br>



<br>

・CNNDMでfine-tuningすることで、一貫してcorrelationが改善した。

<br>



<br>

・加えて、paraphraseデータセットでfinetuningすることで、さらに性能が改善した。

<br>



<br>

・prompt combination strategyは一貫してcorrelationを改善した。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/acbd6816-e7bc-4ecd-8a53-13137a2bcc94" alt="image" loading="lazy">

<br>



<br>

Analysis

<br>



<br>

Fine-grained Analysis

<br>



<br>

・Top-k Systems: MTタスクにおいて、評価するシステムをtop-kにし、各メトリックごとにcorrelationの変化を見た。その結果、BARTScoreはすべてのunsupervised methodをすべてのkにおいて上回り、supervised metricのBLEURTも上回った。また、kが小さくなるほど、より性能はsmoothになっていき、性能の低下がなくなっていった。これはつまり、high-quality textを生成するシステムに対してロバストであることを示している。

<br>



<br>

・Reference Length: テストセットを4つのバケットにreference lengthに応じてブレイクダウンし、Kendall's Tauの平均のcorrelationを、異なるメトリック、バケットごとに言語をまたいで計算した。unsupervised metricsに対して、全てのlengthに対して、引き分けかあるいは上回った。また、ほかのmetricsと比較して、長さに対して安定感があることが分かった。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/821fdf0a-aabc-448a-81aa-238aae380ea1" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Prompt Analysis

<br>



<br>

(1) semantic overlap (informativeness, pyramid score, relevance), (2) linguistic quality (fluency, coherence), (3) factual correctness (factuality) に評価の観点を分類し、summarizationとdata-to-textをにおけるすべてのpromptを分析することで、promptの効果を分析した。それぞれのグループに対して、性能が改善したpromptの割合を計算した。その結果、semantic overlapはほぼ全てのpromptにて性能が改善し、factualityはいくつかのpromptでしか性能の改善が見られなかった。linguistic qualityに関しては、promptを追加することによる効果はどちらとも言えなかった。

<br>



<br>



<br>



<br>

Bias Analysis

<br>



<br>

BARTScoreが予測不可能な方法でバイアスを導入してしまうかどうかを分析した。バイアスとは、human annotatorが与えたスコアよりも、値が高すぎる、あるいは低すぎるような状況である。このようなバイアスが存在するかを検証するために、human annotatorとBARTScoreによるランクのサを分析した。これを見ると、BARTScoreは、extractive summarizationの品質を区別する能力がabstractive summarizationの品質を区別する能力よりも劣っていることが分かった。しかしながら、近年のトレンドはabstractiveなseq2seqを活用することなので、この弱点は軽減されている。

<br>



<br>



<br>



<br>

Implications and Future Directions

<br>



<br>

prompt-augmented metrics: semantic overlapではpromptingが有効に働いたが、linguistic qualityとfactualityでは有効ではなかった。より良いpromptを模索する研究が今後期待される。

<br>



<br>

Co-evolving evaluation metrics and systems: BARTScoreは、メトリックデザインとシステムデザインの間につながりがあるので、より性能の良いseq2seqシステムが出たら、それをメトリックにも活用することでよりreliableな自動性能指標となることが期待される。

<br>



<br>



<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/08d51f6d-40ad-4b2a-8871-086e12010478" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/Sentence.html">#Sentence</a>
<a class="button" href="articles/Embeddings.html">#Embeddings</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/RepresentationLearning.html">#RepresentationLearning</a>
<a class="button" href="articles/ContrastiveLearning.html">#ContrastiveLearning</a>
<a class="button" href="articles/Catastrophic%20Forgetting.html">#Catastrophic Forgetting</a>
<span class="issue_date">Issue Date: 2023-07-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/907">SimCSE: Simple Contrastive Learning of Sentence Embeddings, Tianyu Gao+, N_A, EMNLP'21</a>
<span class="snippet"><span>Summary</span>この論文では、SimCSEという対比学習フレームワークを提案しています。このフレームワークは、文の埋め込み技術を進化させることができます。教師なしアプローチでは、入力文をノイズとして扱い、自己を対比的に予測します。教師ありアプローチでは、自然言語推論データセットから注釈付きのペアを使用して対比学習を行います。SimCSEは、意味的テキスト類似性タスクで評価され、以前の手法と比較して改善を実現しました。対比学習は、事前学習された埋め込みの空間を均一に正則化し、教師信号が利用可能な場合には正のペアをよりよく整列させることが示されました。</span>
<span class="snippet"><span>Comment</span>462 よりも性能良く、unsupervisedでも学習できる。STSタスクのベースラインにだいたい入ってる手法概要

<br>



<br>

Contrastive Learningを活用して、unsupervised/supervisedに学習を実施する。

<br>



<br>

Unsupervised SimCSEでは、あるsentenceをencoderに2回入力し、それぞれにdropoutを適用させることで、positive pairを作成する。dropoutによって共通のembeddingから異なる要素がマスクされた（noiseが混ざった状態とみなせる）類似したembeddingが作成され、ある種のdata augmentationによって正例を作成しているともいえる。負例はnegative samplingする。（非常にsimpleだが、next sentence predictionで学習するより性能が良くなる）

<br>



<br>

Supervised SimCSEでは、アノテーションされたsentence pairに基づいて、正例・負例を決定する。本研究では、NLIのデータセットにおいて、entailment関係にあるものは正例として扱う。contradictions（矛盾）関係にあるものは負例として扱う。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/ba20a1ca-0078-4227-8bb3-3805ee57a620" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Siamese Networkで用いられるmeans-squared errrorとContrastiveObjectiveの違い

<br>



<br>

どちらもペアワイズで比較するという点では一緒だが、ContrastiveObjectiveは正例と近づいたとき、負例と遠ざかったときにlossが小さくなるような定式化がされている点が異なる。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/d9cad930-e86d-4758-87b5-4237525a154a" alt="image" loading="lazy">

<br>



<br>

（画像はこのブログから引用。ありがとうございます。https://techblog.cccmk.co.jp/entry/2022/08/30/163625）

<br>



<br>



<br>



<br>

Unsupervised SimCSEの実験

<br>



<br>

異なるdata augmentation手法と比較した結果、dropoutを適用する手法の方が性能が高かった。MLMや, deletion, 類義語への置き換え等よりも高い性能を獲得しているのは興味深い。また、Next Sentence Predictionと比較しても、高い性能を達成。Next Sentence Predictionは、word deletion等のほぼ類似したテキストから直接的に類似関係にあるペアから学習するというより、Sentenceの意味内容のつながりに基づいてモデルの言語理解能力を向上させ、そのうえで類似度を測るという間接的な手法だが、word deletionに負けている。一方、dropoutを適用するだけの（直接的に類似ペアから学習する）本手法はより高い性能を示している。

<br>



<br>

[image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/0ea3549e-3363-4857-94e6-a1ef474aa191)

<br>



<br>



<br>



<br>

なぜうまくいくかを分析するために、異なる設定で実験し、alignment（正例との近さ）とuniformity（どれだけembeddingが一様に分布しているか）を、10 stepごとにplotした結果が以下。dropoutを適用しない場合と、常に同じ部分をマスクする方法（つまり、全く同じembeddingから学習する）設定を見ると、学習が進むにつれuniformityは改善するが、alignmentが悪くなっていっている。一方、SimCSEはalignmentを維持しつつ、uniformityもよくなっていっていることがわかる。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/5f488cb2-b15a-4e00-9452-8e48780abe8a" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/5e815cf7-b412-4f1b-8adb-116f0dcd2fee" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Supervised SimCSEの実験

<br>



<br>

アノテーションデータを用いてContrastiveLearningするにあたり、どういったデータを正例としてみなすと良いかを検証するために様々なデータセットで学習し性能を検証した。

<br>



<br>



<br>



<br>

・QQP4: Quora question pairs

<br>



<br>

・Flickr30k (Young et al., 2014): 同じ画像に対して、5つの異なる人間が記述したキャプションが存在

<br>



<br>

・ParaNMT (Wieting and Gimpel, 2018): back-translationによるparaphraseのデータセットa

<br>



<br>

・NLI datasets: SNLIとMNLI

<br>



<br>



<br>



<br>

実験の結果、NLI datasetsが最も高い性能を示した。この理由としては、NLIデータセットは、crowd sourcingタスクで人手で作成された高品質なデータセットであることと、lexical overlapが小さくなるようにsentenceのペアが作成されていることが起因している。実際、NLI datsetのlexical overlapは39%だったのに対し、ほかのデータセットでは60%であった。

<br>



<br>



<br>



<br>

また、condunctionsとなるペアを明示的に負例として与えることで、より性能が向上した（普通はnegative samplingする、というかバッチ内の正例以外のものを強制的に負例とする。こうすると、意味が同じでも負例になってしまう事例が出てくることになる）。より難しいNLIタスクを含むANLIデータセットを追加した場合は、性能が改善しなかった。この理由については考察されていない。性能向上しそうな気がするのに。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/ae05711b-5ad4-4a53-837b-c57e9a39da62" alt="image" loading="lazy">

<br>



<br>

他手法との比較結果

<br>



<br>

SimCSEがよい。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/510744ff-01bb-47be-9e30-2efa49e0f923" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Ablation Studies

<br>



<br>

異なるpooling方法で、どのようにsentence embeddingを作成するかで性能の違いを見た。originalのBERTの実装では、CLS token のembeddingの上にMLP layerがのっかっている。これの有無などと比較。

<br>



<br>

Unsupervised SimCSEでは、training時だけMLP layerをのっけて、test時はMLPを除いた方が良かった。一方、Supervised SimCSEでは、 MLP layerをのっけたまんまで良かったとのこと。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/73116c6a-d48f-42bc-aa5e-8342bb068052" alt="image" loading="lazy">

<br>



<br>

また、SimCSEで学習したsentence embeddingを別タスクにtransferして活用する際には、SimCSEのobjectiveにMLMを入れた方が、catastrophic forgettingを防げて性能が高かったとのこと。

<br>



<br>



<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/cc6d20c3-5a0c-4b5e-aa6d-63447c55363f" alt="image" loading="lazy">ablation studiesのhard negativesのところと、どのようにミニバッチを構成するか、それぞれのtransferしたタスクがどのようなものがしっかり読めていない。あとでよむ。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<span class="issue_date">Issue Date: 2023-07-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/901">Measuring Massive Multitask Language Understanding, Dan Hendrycks+, N_A, ICLR'21</a>
<span class="snippet"><span>Summary</span>私たちは、マルチタスクのテキストモデルの正確性を測定するための新しいテストを提案しています。このテストは57のタスクをカバーし、広範な世界知識と問題解決能力が必要です。現在のモデルはまだ専門家レベルの正確性に達しておらず、性能に偏りがあります。私たちのテストは、モデルの弱点を特定するために使用できます。</span>
<span class="snippet"><span>Comment</span>OpenReview:https://openreview.net/forum?id=d7KBjmI3GmQMMLU論文・2736

<br>



<br>

において、多くのエラーが含まれることが指摘され、再アノテーションが実施されている。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-07-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2287">[Paper Note] Exploring Simple Siamese Representation Learning, Xinlei Chen+, arXiv'20</a>
<span class="snippet"><span>Summary</span>Siameseネットワークを用いた教師なし視覚表現学習に関する研究で、ネガティブサンプルペア、大きなバッチ、モーメンタムエンコーダーを使用せずに意味のある表現を学習できることを示した。ストップグラディエント操作が崩壊解を防ぐ重要な役割を果たすことを確認し、SimSiamメソッドがImageNetおよび下流タスクで競争力のある結果を達成した。これにより、Siameseアーキテクチャの役割を再考するきっかけとなることを期待している。</span>
<span class="snippet"><span>Comment</span>日本語解説:

<br>



<br>

https://qiita.com/saliton/items/2f7b1bfb451df75a286f

<br>

https://qiita.com/koshian2/items/a31b85121c99af0eb050</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-07-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2286">[Paper Note] Bootstrap your own latent: A new approach to self-supervised Learning, Jean-Bastien Grill+, arXiv'20</a>
<span class="snippet"><span>Summary</span>BYOL（Bootstrap Your Own Latent）は、自己教師あり画像表現学習の新しい手法で、オンラインネットワークとターゲットネットワークの2つのニューラルネットワークを用いて学習を行う。BYOLは、ネガティブペアに依存せずに最先端の性能を達成し、ResNet-50でImageNetにおいて74.3%の分類精度を達成、より大きなResNetでは79.6%に達する。転送学習や半教師ありベンチマークでも優れた性能を示し、実装と事前学習済みモデルはGitHubで公開されている。</span>
<span class="snippet"><span>Comment</span>日本語解説:

<br>

https://sn-neural-compute.netlify.app/202006250/</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/DataAugmentation.html">#DataAugmentation</a>
<a class="button" href="articles/ContrastiveLearning.html">#ContrastiveLearning</a>
<a class="button" href="articles/Self-SupervisedLearning.html">#Self-SupervisedLearning</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<span class="issue_date">Issue Date: 2025-05-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1975">A Simple Framework for Contrastive Learning of Visual Representations, Ting Chen+, ICML'20</a>
<span class="snippet"><span>Summary</span>本論文では、視覚表現の対比学習のためのシンプルなフレームワークSimCLRを提案し、特別なアーキテクチャやメモリバンクなしで対比自己教師あり学習を簡素化します。データ拡張の重要性、学習可能な非線形変換の導入による表現の質向上、対比学習が大きなバッチサイズと多くのトレーニングステップから利益を得ることを示し、ImageNetで従来の手法を上回る結果を達成しました。SimCLRによる自己教師あり表現を用いた線形分類器は76.5%のトップ1精度を達成し、教師ありResNet-50に匹敵します。ラベルの1%でファインチューニングした場合、85.8%のトップ5精度を達成しました。</span>
<span class="snippet"><span>Comment</span>日本語解説:https://techblog.cccmkhd.co.jp/entry/2022/08/30/163625</span>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<span class="issue_date">Issue Date: 2025-05-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1960">PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive   Summarization, Jingqing Zhang+, ICML'20</a>
<span class="snippet"><span>Summary</span>大規模なテキストコーパスに対して新しい自己教師ありの目的でトランスフォーマーを事前学習し、抽象的なテキスト要約に特化したモデルPEGASUSを提案。重要な文を削除またはマスクし、残りの文から要約を生成。12の下流要約タスクで最先端のROUGEスコアを達成し、限られたリソースでも優れたパフォーマンスを示す。人間評価でも複数のデータセットで人間のパフォーマンスに達したことを確認。</span>
<span class="snippet"><span>Comment</span>PEGASUSもなかったので追加。BARTと共に文書要約のBackboneとして今でも研究で利用される模様。関連:

<br>

・984</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/TransferLearning.html">#TransferLearning</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-05-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1955">Exploring the Limits of Transfer Learning with a Unified Text-to-Text  Transformer, Colin Raffel+, JMLR'20</a>
<span class="snippet"><span>Summary</span>転移学習はNLPにおいて強力な技術であり、本論文ではテキストをテキストに変換する統一フレームワークを提案。事前学習の目的やアーキテクチャを比較し、最先端の結果を達成。データセットやモデル、コードを公開し、今後の研究を促進する。</span>
<span class="snippet"><span>Comment</span>T5もメモっていなかったので今更ながら追加。全てのNLPタスクをテキスト系列からテキスト系列へ変換するタスクとみなし、Encoder-DecoderのTransformerを大規模コーパスを用いて事前学習をし、downstreamタスクにfinetuningを通じて転移する。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/Decoding.html">#Decoding</a>
<span class="issue_date">Issue Date: 2025-04-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1889">The Curious Case of Neural Text Degeneration, Ari Holtzman+, ICLR'20</a>
<span class="snippet"><span>Summary</span>深層ニューラル言語モデルは高品質なテキスト生成において課題が残る。尤度の使用がモデルの性能に影響を与え、人間のテキストと機械のテキストの間に分布の違いがあることを示す。デコーディング戦略が生成テキストの質に大きな影響を与えることが明らかになり、ニュークリアスsamplingを提案。これにより、多様性を保ちながら信頼性の低い部分を排除し、人間のテキストに近い質を実現する。</span>
<span class="snippet"><span>Comment</span>現在のLLMで主流なNucleus (top-p) Samplingを提案した研究</span>
<a class="button" href="articles/MachineTranslation.html">#MachineTranslation</a>
<a class="button" href="articles/Metrics.html">#Metrics</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/EMNLP.html">#EMNLP</a>
<span class="issue_date">Issue Date: 2024-05-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1312">COMET: A Neural Framework for MT Evaluation, Ricardo Rei+, N_A, EMNLP'20</a>
<span class="snippet"><span>Summary</span>COMETは、多言語機械翻訳評価モデルを訓練するためのニューラルフレームワークであり、人間の判断との新しい最先端の相関レベルを達成します。クロスリンガル事前学習言語モデリングの進展を活用し、高度に多言語対応かつ適応可能なMT評価モデルを実現します。WMT 2019 Metrics shared taskで新たな最先端のパフォーマンスを達成し、高性能システムに対する堅牢性を示しています。</span>
<span class="snippet"><span>Comment</span>Better/Worseなhypothesisを利用してpair-wiseにランキング関数を学習する

<br>

![Image](https://github.com/user-attachments/assets/a1fd6f36-48e8-44fc-8fcb-0900a51759b3)

<br>



<br>

![Image](https://github.com/user-attachments/assets/19ad7a57-7de3-4255-afde-4a1fde41587d)

<br>



<br>

Inference時は単一のhypothesisしかinputされないので、sourceとreferenceに対してそれぞれhypothesisの距離をはかり、その調和平均でスコアリングする

<br>



<br>

![Image](https://github.com/user-attachments/assets/21642c70-a7fd-4c0e-8678-6125fdbfefce)ACL2024, EMNLP2024あたりのMT研究のmetricをざーっと見る限り、BLEU/COMETの双方で評価する研究が多そう</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/ActivationFunction.html">#ActivationFunction</a>
<span class="issue_date">Issue Date: 2024-05-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1311">GLU Variants Improve Transformer, Noam Shazeer, N_A, arXiv'20</a>
<span class="snippet"><span>Summary</span>GLUのバリエーションをTransformerのフィードフォワード・サブレイヤーでテストし、通常の活性化関数よりもいくつかのバリエーションが品質向上をもたらすことを発見した。</span>
<span class="snippet"><span>Comment</span>一般的なFFNでは、linear layerをかけた後に、何らかの活性化関数をかませる方法が主流である。

<br>



<br>

このような構造の一つとしてGLUがあるが、linear layerと活性化関数には改良の余地があり、様々なvariantが考えられるため、色々試しました、というはなし。

<br>



<br>



<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/72b1d0bb-64ac-4155-9a3b-5624cd06ccc9" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/b38321c6-d414-4764-9147-10a5fa83fbe6" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

オリジナルのGLUと比較して、T5と同じ事前学習タスクを実施したところ、perplexityが改善

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/9e67a054-2148-41ed-aae1-5a752c21a242" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

また、finetuningをした場合の性能も、多くの場合オリジナルのGLUよりも高い性能を示した。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/77ccab88-e5cc-48fc-b9e0-f2dad24e53e8" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/8f60ca8c-50eb-4869-bab4-f02ec6d8e085" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/8124fc25-aa7e-4e10-8cd2-9d24c818f410" alt="image" loading="lazy">

<br>



<br>



<br>



<br>



<br>



<br>

</span>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/Metrics.html">#Metrics</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/Reference-based.html">#Reference-based</a>
<span class="issue_date">Issue Date: 2023-05-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/668">BERTScore: Evaluating Text Generation with BERT, Tianyi Zhang+, N_A, ICLR'20</a>
<span class="snippet"><span>Summary</span>BERTScoreは、文脈埋め込みを使用してトークンの類似度を計算するテキスト生成の自動評価メトリックであり、363の機械翻訳および画像キャプションシステムの出力を使用して評価されました。BERTScoreは、既存のメトリックよりも人間の判断との相関が高く、より強力なモデル選択性能を提供し、敵対的な言い換え検出タスクにおいてもより堅牢であることが示されました。</span>
<span class="snippet"><span>Comment</span>概要

<br>

既存のテキスト生成の評価手法（BLEUやMETEOR）はsurface levelのマッチングしかしておらず、意味をとらえられた評価になっていなかったので、pretrained BERTのembeddingを用いてsimilarityを測るような指標を提案しましたよ、という話。

<br>



<br>

prior metrics

<br>

n-gram matching approaches

<br>

n-gramがreferenceとcandidateでどれだけ重複しているかでPrecisionとrecallを測定

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/a620d564-72e3-4078-97e2-1ff62b333324" alt="image" loading="lazy">

<br>



<br>

BLEU

<br>

MTで最も利用される。n-gramのPrecision（典型的にはn=1,2,3,4）と短すぎる候補訳にはペナルティを与える（brevity penalty）ことで実現される指標。SENT-BLEUといった亜種もある。BLEUと比較して、BERTScoreは、n-gramの長さの制約を受けず、潜在的には長さの制限がないdependencyをcontextualized embeddingsでとらえることができる。

<br>



<br>

METEOR

<br>

669 METEOR 1.5では、内容語と機能語に異なるweightを割り当て、マッチングタイプによってもweightを変更する。METEOR++2.0では、学習済みの外部のparaphrase resourceを活用する。METEORは外部のリソースを必要とするため、たった5つの言語でしかfull feature setではサポートされていない。11の言語では、恥部のfeatureがサポートされている。METEORと同様に、BERTScoreでも、マッチに緩和を入れていることに相当するが、BERTの事前学習済みのembeddingは104の言語で取得可能である。BERTScoreはまた、重要度によるweightingをサポートしている（コーパスの統計量で推定）。

<br>



<br>

Other Related Metrics

<br>

・NIST: BLEUとは異なるn-gramの重みづけと、brevity penaltyを利用する

<br>

・ΔBLEU: multi-reference BLEUを、人手でアノテーションされたnegative reference sentenceで変更する

<br>

・CHRF: 文字n-gramを比較する

<br>

・CHRF++: CHRFをword-bigram matchingに拡張したもの

<br>

・ROUGE: 文書要約で利用される指標。ROUGE-N, ROUGE^Lといった様々な変種がある。

<br>

・CIDEr: image captioningのmetricであり、n-gramのtf-idfで重みづけされたベクトルのcosine similrityを測定する

<br>



<br>

Edit-distance based Metrics

<br>

・Word Error Rate (WER): candidateからreferenceを再現するまでに必要なedit operationの数をカウントする手法

<br>

・Translation Edit Rate (TER): referenceの単語数によってcandidateからreferenceまでのedit distanceを正規化する手法

<br>

・ITER: 語幹のマッチと、より良い正規化に基づく手法

<br>

・PER: positionとは独立したError Rateを算出

<br>

・CDER: edit operationにおけるblock reorderingをモデル化

<br>

・CHARACTER / EED: character levelで評価

<br>



<br>

Embedding-based Metrics

<br>

・MEANT 2.0: lexical, structuralの類似度を測るために、word embeddingとshallow semantic parsesを利用

<br>

・YISI-1: MEANT 2.0と同様だが、semantic parseの利用がoptionalとなっている

<br>

これらはBERTScoreと同様の、similarityをシンプルに測るアプローチで、BERTScoreもこれにinspireされている。が、BERTScoreはContextualized Embeddingを利用する点が異なる。また、linguistic structureを生成するような外部ツールは利用しない。これにより、BERTScoreをシンプルで、新たなlanguageに対しても使いやすくしている。greedy matchingの代わりに、WMD, WMDo, SMSはearth mover's distanceに基づく最適なマッチングを利用することを提案している。greedy matchingとoptimal matchingのtradeoffについては研究されている。sentence-levelのsimilarityを計算する手法も提案されている。これらと比較して、BERTScoreのtoken-levelの計算は、重要度に応じて、tokenに対して異なる重みづけをすることができる。

<br>



<br>

Learned Metrics

<br>

様々なmetricが、human judgmentsとのcorrelationに最適化するために訓練されてきた。

<br>

・BEER: character-ngram, word bigramに基づいたregresison modelを利用

<br>

・BLEND: 29の既存のmetricを利用してregressionを実施

<br>

・RUSE: 3種類のpre-trained sentence embedding modelを利用する手法

<br>

これらすべての手法は、コストのかかるhuman judgmentsによるsupervisionが必要となる。そして、新たなドメインにおける汎化能力の低さのリスクがある。input textが人間が生成したものか否か予測するneural modelを訓練する手法もある。このアプローチは特定のデータに対して最適化されているため、新たなデータに対して汎化されないリスクを持っている。これらと比較して、BERTScoreは特定のevaluation taskに最適化されているモデルではない。

<br>



<br>

BERTScore

<br>

referenceとcandidateのトークン間のsimilarityの最大値をとり、それらを集約することで、Precision, Recallを定義し、PrecisionとRecallを利用してF値も計算する。Recallは、reference中のすべてのトークンに対して、candidate中のトークンとのcosine similarityの最大値を測る。一方、Precisionは、candidate中のすべてのトークンに対して、reference中のトークンとのcosine similarityの最大値を測る。ここで、類似度の式が単なる内積になっているが、これはpre-normalized vectorを利用する前提であり、正規化が必要ないからである。

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/9ed88ea6-8ecf-465c-81d5-bc85592ad7ff" alt="image" loading="lazy">

<br>



<br>

また、IDFによるトークン単位でのweightingを実施する。IDFはテストセットの値を利用する。TFを使わない理由は、BERTScoreはsentence同士を比較する指標であるため、TFは基本的に1となりやすい傾向にあるためである。IDFを計算する際は出現数を+1することによるスムージングを実施。

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/d4b132fb-7830-4a00-b845-11f38b909bba" alt="image" loading="lazy">

<br>



<br>

さらに、これはBERTScoreのランキング能力には影響を与えないが、BERTScoreの値はコサイン類似度に基づいているため、[-1, 1]となるが、実際は学習したcontextual embeddingのgeometryに値域が依存するため、もっと小さなレンジでの値をとることになってしまう。そうすると、人間による解釈が難しくなる（たとえば、極端な話、スコアの0.1程度の変化がめちゃめちゃ大きな変化になってしまうなど）ため、rescalingを実施。rescalingする際は、monolingualコーパスから、ランダムにsentenceのペアを作成し（BETRScoreが非常に小さくなるケース）、これらのBERTScoreを平均することでbを算出し、bを利用してrescalingした。典型的には、rescaling後は典型的には[0, 1]の範囲でBERTScoreは値をとる（ただし数式を見てわかる通り[0, 1]となることが保証されているわけではない点に注意）。これはhuman judgmentsとのcorrelationとランキング性能に影響を与えない（スケールを変えているだけなので）。

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/9049ed99-d192-465d-b4fe-d628bc673927" alt="image" loading="lazy">

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/bb78074e-2fa4-4bb3-a920-df543aeb98b8" alt="image" loading="lazy">

<br>

実験

<br>



<br>

Contextual Embedding Models

<br>



<br>

12種類のモデルで検証。BERT, RoBERTa, XLNet, XLMなど。

<br>



<br>



<br>



<br>

Machine Translation

<br>



<br>

WMT18のmetric evaluation datasetを利用。149種類のMTシステムの14 languageに対する翻訳結果, gold referencesと2種類のhuman judgment scoreが付与されている。segment-level human judgmentsは、それぞれのreference-candiate pairに対して付与されており、system-level human judgmentsは、それぞれのシステムに対して、test set全体のデータに基づいて、単一のスコアが付与されている。pearson correlationの絶対値と、kendall rank correration τをmetricsの品質の評価に利用。そしてpeason correlationについてはWilliams test、kendall τについては、bootstrap re-samplingによって有意差を検定した。システムレベルのスコアをBERTScoreをすべてのreference-candidate pairに対するスコアをaveragingすることによって求めた。また、ハイブリッドシステムについても実験をした。具体的には、それぞれのreference sentenceについて、システムの中からランダムにcandidate sentenceをサンプリングした。これにより、system-level experimentをより多くのシステムで実現することができる。ハイブリッドシステムのシステムレ4ベルのhuman judgmentsは、WMT18のsegment-level human judgmentsを平均することによって作成した。BERTScoreを既存のメトリックと比較した。

<br>



<br>



<br>



<br>

通常の評価に加えて、モデル選択についても実験した。10kのハイブリッドシステムを利用し、10kのうち100をランダムに選択、そして自動性能指標でそれらをランキングした。このプロセスを100K回繰り返し、human rankingとmetricのランキングがどれだけagreementがあるかをHits@1で評価した（best systemの一致で評価）。モデル選択の指標として新たにtop metric-rated systemとhuman rankingの間でのMRR, 人手評価でtop-rated systemとなったシステムとのスコアの差を算出した。WMT17, 16のデータセットでも同様の評価を実施した。

<br>



<br>



<br>



<br>

Image Captioning

<br>



<br>

COCO 2015 captioning challengeにおける12種類のシステムのsubmissionデータを利用。COCO validationセットに対して、それぞれのシステムはimageに対するcaptionを生成し、それぞれのimageはおよそ5個のreferenceを持っている。先行研究にならい、Person Correlationを2種類のシステムレベルmetricで測定した。

<br>



<br>

・M1: 人間によるcaptionと同等、あるいはそれ以上と評価されたcaptionの割合

<br>



<br>

・M2: 人間によるcaptionと区別がつかないcaptionの割合

<br>



<br>

BERTScoreをmultiple referenceに対して計算し、最も高いスコアを採用した。比較対象のmetricはtask-agnostic metricを採用し、BLEU, METEOR, CIDEr, BEER, EED, CHRF++, CHARACTERと比較した。そして、2種類のtask-specific metricsとも比較した：SPICE, LEIC

<br>



<br>



<br>



<br>

実験結果

<br>



<br>

Machine Translation

<br>



<br>

system-levelのhuman judgmentsとのcorrelationの比較、hybrid systemとのcorrelationの比較、model selection performance

<br>



<br>

to-Englishの結果では、BERTScoreが最も一貫して性能が良かった。RUSEがcompetitiveな性能を示したが、RUSEはsupervised methodである。from-Englishの実験では、RUSEは追加のデータと訓練をしないと適用できない。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/e3b0482e-a30b-46be-b8df-72a1c4fe510d" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/b769ac8f-1a43-48d6-9316-cb78cffc3b88" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/3b204434-9f9a-4672-be5a-6e463d3289f4" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

以下は、segment-levelのcorrelationを示したものである。BERTScoreが一貫して高い性能を示している。BLEUから大幅な性能アップを示しており、特定のexampleについての良さを検証するためには、BERTScoreが最適であることが分かる。BERTScoreは、RUSEをsignificantlyに上回っている。idfによる重要度のweightingによって、全体としては、small benefitがある場合があるが全体としてはあんまり効果がなかった。importance weightingは今後の課題であり、テキストやドメインに依存すると考えられる。FBERTが異なる設定でも良く機能することが分かる。異なるcontextual embedding model間での比較などは、appendixに示す。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/1684fa38-0663-4649-849f-1885cd97286e" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Image Captioning

<br>



<br>

task-agnostic metricの間では、BETRScoreはlarge marginで勝っている。image captioningはchallengingな評価なので、n-gramマッチに基づくBLEU, ROUGEはまったく機能していない。また、idf weightingがこのタスクでは非常に高い性能を示した。これは人間がcontent wordsに対して、より高い重要度を置いていることがわかる。最後に、LEICはtrained metricであり、COCO dataに最適化されている。この手法は、ほかのすべてのmetricを上回った。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/5842611a-38bd-441f-a467-8bb3714dc33a" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Speed

<br>



<br>

pre-trained modelを利用しているにもかかわらず、BERTScoreは比較的高速に動作する。192.5 candidate-reference pairs/secondくらい出る（GTX-1080Ti GPUで）。WMT18データでは、15.6秒で処理が終わり、SacreBLEUでは5.4秒である。計算コストそんなにないので、BERTScoreはstoppingのvalidationとかにも使える。Robustness analysis

<br>



<br>

BERTScoreのロバスト性をadversarial paraphrase classificationでテスト。Quora Question Pair corpus (QQP) を利用し、Word Scrambling dataset (PAWS) からParaphrase Adversariesを取得。どちらのデータも、各sentenceペアに対して、それらがparaphraseかどうかラベル付けされている。QQPの正例は、実際のduplicate questionからきており、負例は関連するが、異なる質問からきている。PAWSのsentence pairsは単語の入れ替えに基づいているものである。たとえば、"Flights from New York to Florida" は "Flights from Florida to New York" のように変換され、良いclassifierはこれらがparaphraseではないと認識できなければならない。PAWSはPAWS_QQPとPAWS_WIKIによって構成さえｒており、PAWS_QQPをdevelpoment setとした。automatic metricsでは、paraphrase detection training dataは利用しないようにした。自動性能指標で高いスコアを獲得するものは、paraphraseであることを想定している。

<br>



<br>



<br>



<br>

下図はAUCのROC curveを表しており、PAWS_QQPにおいて、QQPで訓練されたclassifierはrandom guessよりも性能が低くなることが分かった。つまりこれらモデルはadversaial exampleをparaphraseだと予測してしまっていることになる。adversarial examplesがtrainingデータで与えられた場合は、supervisedなモデルも分類ができるようになる。が、QQPと比べると性能は落ちる。多くのmetricsでは、QQP ではまともなパフォーマンスを示すが、PAWS_QQP では大幅なパフォーマンスの低下を示し、ほぼrandomと同等のパフォーマンスとなる。これは、これらの指標がより困難なadversarial exampleを区別できないことを示唆している。一方、BERTSCORE のパフォーマンスはわずかに低下するだけであり、他の指標よりもロバスト性が高いことがわかる。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/7a3b3c3b-ff4e-4f65-a6b3-c71b8f100c8a" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Discussion

<br>



<br>

・BERTScoreの単一の設定が、ほかのすべての指標を明確に上回るということはない

<br>



<br>

・ドメインや言語を考慮して、指標や設定を選択すべき

<br>



<br>

・一般的に、機械翻訳の評価にはFBERTを利用することを推奨

<br>



<br>

・英語のテキスト生成の評価には、24層のRoBERTa largeモデルを使用して、BERTScoreを計算したほうが良い

<br>



<br>

・非英語言語については、多言語のBERT_multiが良い選択肢だが、このモデルで計算されたBERTScoreは、low resource languageにおいて、パフォーマンスが安定しているとは言えない</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Zero/Few/ManyShotPrompting.html">#Zero/Few/ManyShotPrompting</a>
<a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<span class="issue_date">Issue Date: 2023-04-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/552">Language Models are Few-Shot Learners, Tom B. Brown+, NeurIPS'20</a>
<span class="snippet"><span>Summary</span>GPT-3は1750億パラメータを持つ自己回帰型言語モデルで、少数ショット設定においてファインチューニングなしで多くのNLPタスクで強力な性能を示す。翻訳や質問応答などで優れた結果を出し、即時推論やドメイン適応が必要なタスクでも良好な性能を発揮する一方、依然として苦手なデータセットや訓練に関する問題も存在する。また、GPT-3は人間が書いた記事と区別が難しいニュース記事を生成できることが確認され、社会的影響についても議論される。</span>
<span class="snippet"><span>Comment</span>In-Context Learningを提案した論文論文に記載されているIn-Context Learningの定義は、しっかり押さえておいた方が良い。

<br>



<br>

下図はmeta-learningの観点から見たときの、in-contextの位置付け。事前学習時にSGDでパラメータをupdateするのをouter loopとし、そこで広いスキルとパターン認識の能力を身につける。一方で、in-context learningは、Inference時に事前学習時に得たそれらのスキルを用いて、求めるタスクを認識、あるいは適応するInner loopのことを指す。

<br>

<img src="https://github.com/user-attachments/assets/679129f3-93e3-445f-b9e8-5d909261737b" alt="image" loading="lazy">

<br>



<br>

この上で、論文中では In-Context Learningについて:

<br>

&gt; Recent work [RWC+19] attempts to do this via what we call “in-context learning”, using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.

<br>



<br>

と定義している。</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<a class="button" href="articles/Scaling%20Laws.html">#Scaling Laws</a>
<a class="button" href="articles/Backbone.html">#Backbone</a>
<span class="issue_date">Issue Date: 2025-05-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1957">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, Mingxing Tan+, ICML'19</a>
<span class="snippet"><span>Summary</span>本論文では、ConvNetsのスケーリングを深さ、幅、解像度のバランスを考慮して体系的に研究し、新しいスケーリング手法を提案。これにより、MobileNetsやResNetのスケールアップを実証し、EfficientNetsという新しいモデルファミリーを設計。特にEfficientNet-B7は、ImageNetで84.3%のトップ1精度を達成し、従来のConvNetsよりも小型かつ高速である。CIFAR-100やFlowersなどのデータセットでも最先端の精度を記録。ソースコードは公開されている。</span>
<span class="snippet"><span>Comment</span>元論文をメモってなかったので追加。

<br>

・346

<br>



<br>

も参照のこと。</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/SequentialRecommendation.html">#SequentialRecommendation</a>
<a class="button" href="articles/ICDM.html">#ICDM</a>
<span class="issue_date">Issue Date: 2025-07-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2137">[Paper Note] Self-Attentive Sequential Recommendation, Wang-Cheng Kang+, ICDM'18</a>
<span class="snippet"><span>Summary</span>自己注意に基づく逐次モデル（SASRec）を提案し、マルコフ連鎖と再帰型ニューラルネットワークの利点を統合。SASRecは、少数のアクションから次のアイテムを予測し、スパースおよび密なデータセットで最先端のモデルを上回る性能を示す。モデルの効率性と注意重みの視覚化により、データセットの密度に応じた適応的な処理が可能であることが確認された。</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/General.html">#General</a>
<a class="button" href="articles/Embeddings.html">#Embeddings</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/RepresentationLearning.html">#RepresentationLearning</a>
<a class="button" href="articles/AAAI.html">#AAAI</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/68">StarSpace: Embed All The Things, Wu+, AAAI'18</a>
<span class="snippet"><span>Comment</span>分類やランキング、レコメンドなど、様々なタスクで汎用的に使用できるEmbeddingの学習手法を提案。

<br>



<br>



<br>



<br>

Embeddingを学習する対象をEntityと呼び、Entityはbag-of-featureで記述される。

<br>



<br>

Entityはbag-of-featureで記述できればなんでもよく、

<br>



<br>

これによりモデルの汎用性が増し、異なる種類のEntityでも同じ空間上でEmbeddingが学習される。

<br>



<br>



<br>



<br>

学習方法は非常にシンプルで、Entity同士のペアをとったときに、relevantなpairであれば類似度が高く、

<br>



<br>

irelevantなペアであれば類似度が低くなるようにEmbeddingを学習するだけ。

<br>



<br>

たとえば、Entityのペアとして、documentをbag-of-words, bag-of-ngrams, labelをsingle wordで記述しテキスト分類、

<br>



<br>

あるいは、user_idとユーザが過去に好んだアイテムをbag-of-wordsで記述しcontent-based recommendationを行うなど、 応用範囲は幅広い。

<br>



<br>



<br>



<br>

5種類のタスクで提案手法を評価し、既存手法と比較して、同等かそれ以上の性能を示すことが示されている。

<br>



<br>



<br>



<br>

手法の汎用性が高く学習も高速なので、色々な場面で役に立ちそう。

<br>



<br>

また、異なる種類のEntityであっても同じ空間上でEmbeddingが学習されるので、学習されたEmbeddingの応用先が広く有用。実際にSentimentAnalysisで使ってみたが（ポジネガ二値分類）、少なくともBoWのSVMよりは全然性能良かったし、学習も早いし、次元数めちゃめちゃ少なくて良かった。

<br>



<br>

StarSpaceで学習したembeddingをBoWなSVMに入れると性能が劇的に改善した。解説：

<br>



<br>

https://www.slideshare.net/akihikowatanabe3110/starspace-embed-all-the-things</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/CollaborativeFiltering.html">#CollaborativeFiltering</a>
<a class="button" href="articles/MatrixFactorization.html">#MatrixFactorization</a>
<a class="button" href="articles/WWW.html">#WWW</a>
<span class="issue_date">Issue Date: 2018-02-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/260">Neural Collaborative Filtering, He+, WWW'17</a>
<span class="snippet"><span>Comment</span>Collaborative FilteringをMLPで一般化したNeural Collaborative Filtering、およびMatrix Factorizationはuser, item-embeddingのelement-wise product + linear transofmration + activation で一般化できること（GMF; Generalized Matrix Factorization）を示し、両者を組み合わせたNeural Matrix Factorizationを提案している。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/121464723-5c6dd280-c9ef-11eb-9c56-7382f2403dc1.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

学習する際は、Implicit Dataの場合は負例をNegative Samplingし、LogLoss（Binary Cross-Entropy Loss）で学習する。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/121464911-bb334c00-c9ef-11eb-88a6-697fab50e60d.png" alt="image" loading="lazy">

<br>



<br>

Neural Matrix Factorizationが、ItemKNNやBPRといったベースラインをoutperform

<br>



<br>



<br>



<br>

Negative Samplingでサンプリングする負例の数は、3~4程度で良さそう

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/121464991-e9189080-c9ef-11eb-96ce-4e743f84a183.png" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/MachineTranslation.html">#MachineTranslation</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/PositionalEncoding.html">#PositionalEncoding</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<span class="issue_date">Issue Date: 2018-01-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/245">Attention is all you need, Vaswani+, NIPS'17</a>
<span class="snippet"><span>Comment</span>Transformer (self-attentionを利用) 論文

<br>



<br>

解説スライド：https://www.slideshare.net/DeepLearningJP2016/dlattention-is-all-you-need

<br>



<br>

解説記事：https://qiita.com/nishiba/items/1c99bc7ddcb2d62667c6

<br>



<br>



<br>



<br>

 新しい翻訳モデル(Transformer)を提案。既存のモデルよりも並列化に対応しており、短時間の訓練で（既存モデルの1/4以下のコスト）高いBLEUスコアを達成した。

<br>



<br>

 TransformerはRNNやCNNを使わず、attentionメカニズムに基づいている。

<br>



<br>



<br>



<br>

（解説より）分かりやすい:

<br>

https://qiita.com/halhorn/items/c91497522be27bde17ceTransformerの各コンポーネントでのoutputのshapeや、attention_maskの形状、実装について記述されており有用:

<br>

https://qiita.com/FuwaraMiyasaki/items/239f3528053889847825集合知</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/DataToTextGeneration.html">#DataToTextGeneration</a>
<a class="button" href="articles/EMNLP.html">#EMNLP</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/207">Challenges in Data-to-Document Generation, Wiseman+ （with Rush）, EMNLP'17</a>
<span class="snippet"><span>Comment</span>・RotoWire（NBAのテーブルデータ + サマリ）データを収集し公開

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/119625430-23f1c480-be45-11eb-8ff8-5e9223d41481.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

・Rotowireデータの統計量

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/119625488-323fe080-be45-11eb-952e-d2d21d6e5847.png" alt="image" loading="lazy">【モデルの概要】

<br>



<br>

・attention-based encoder-decoder model

<br>



<br>



<br>



<br>

・BaseModel

<br>



<br>

　・レコードデータ r の各要素（r.e: チーム名等のENTITY r.t: POINTS等のデータタイプ, r.m: データのvalue）からembeddingをlookupし、1-layer MLPを適用し、レコードの各要素のrepresentation（source data records）を取得

<br>



<br>

　・Luongらのattentionを利用したLSTM Decoderを用意し、source data recordsとt-1ステップ目での出力によって条件付けてテキストを生成していく

<br>



<br>

　・negative log likelihoodがminimizeされるように学習する

<br>



<br>



<br>



<br>

・Copying

<br>



<br>

　・コピーメカニズムを導入し、生成時の確率分布に生成テキストを入力からコピーされるか否かを含めた分布からテキストを生成。コピーの対象は、入力レコードのvalueがコピーされるようにする。

<br>



<br>

　・コピーメカニズムには下記式で表現される Conditional Copy Modelを利用し、p\(zt|y1:t-1, s)はMLPで表現する。

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/119628147-cc088d00-be47-11eb-84de-6a1d158d78e5.png" alt="image" loading="lazy">

<br>



<br>

　・またpcopyは、生成している文中にあるレコードのエンティティとタイプが出現する場合に、対応するvalueをコピーし生成されるように、下記式で表現する

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/119628389-07a35700-be48-11eb-9c69-27b70fcbcdef.png" alt="image" loading="lazy">

<br>



<br>

　・ここで r(yt) =

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/119628615-39b4b900-be48-11eb-9305-509a6eed8182.png" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/Single.html">#Single</a>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Document.html">#Document</a>
<a class="button" href="articles/Supervised.html">#Supervised</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Abstractive.html">#Abstractive</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/135">Get To The Point: Summarization with Pointer-Generator Networks, See+, ACL'17</a>
<span class="snippet"><span>Comment</span>解説スライド：https://www.slideshare.net/akihikowatanabe3110/get-to-the-point-summarization-with-pointergenerator-networks/1単語の生成と単語のコピーの両方を行えるハイブリッドなニューラル文書要約モデルを提案。

<br>



<br>

同じ単語の繰り返し現象(repetition)をなくすために、Coverage Mechanismも導入した。

<br>



<br>



<br>



<br>

136 などと比較するとシンプルなモデル。一般的に、PointerGeneratorと呼ばれる。

<br>



<br>

OpenNMTなどにも実装されている: https://opennmt.net/OpenNMT-py/_modules/onmt/modules/copy_generator.html（参考）Pointer Generator Networksで要約してみる：

<br>



<br>

https://qiita.com/knok/items/9a74430b279e522d5b93</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Sentence.html">#Sentence</a>
<a class="button" href="articles/Embeddings.html">#Embeddings</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/RepresentationLearning.html">#RepresentationLearning</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/69">A structured self-attentive sentence embedding, Li+ （Bengio group）, ICLR'17</a>
<span class="snippet"><span>Comment</span>OpenReview:https://openreview.net/forum?id=BJC_jUqxe</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/SpeechProcessing.html">#SpeechProcessing</a>
<span class="issue_date">Issue Date: 2025-06-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2038">[Paper Note] WaveNet: A Generative Model for Raw Audio, Aaron van den Oord+, arXiv'16</a>
<span class="snippet"><span>Summary</span>本論文では、音声波形を生成する深層ニューラルネットワークWaveNetを提案。自己回帰的なモデルでありながら、効率的に音声データを訓練可能。テキストから音声への変換で最先端の性能を示し、人間のリスナーに自然な音と評価される。話者の特性を忠実に捉え、アイデンティティに基づく切り替えが可能。音楽生成にも応用でき、リアルな音楽の断片を生成。また、音素認識のための有望な識別モデルとしての利用も示唆。</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/SessionBased.html">#SessionBased</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/SequentialRecommendation.html">#SequentialRecommendation</a>
<span class="issue_date">Issue Date: 2019-08-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/315">SESSION-BASED RECOMMENDATIONS WITH RECURRENT NEURAL NETWORKS, Hidasi+, ICLR'16</a>
<span class="snippet"><span>Comment</span>RNNを利用したsequential recommendation (session-based recommendation)の先駆け的論文。日本語解説: https://qiita.com/tatamiya/items/46e278a808a51893deac</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/RecSys.html">#RecSys</a>
<span class="issue_date">Issue Date: 2018-12-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/298">Deep Neural Networks for YouTube Recommendations, Covington+, RecSys'16</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/GraphConvolutionalNetwork.html">#GraphConvolutionalNetwork</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<span class="issue_date">Issue Date: 2018-03-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/265">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering, Defferrard+, NIPS'16</a>
<span class="snippet"><span>Comment</span>GCNを勉強する際は読むと良いらしい。

<br>



<br>

あわせてこのへんも：

<br>



<br>

Semi-Supervised Classification with Graph Convolutional Networks, Kipf+, ICLR'17

<br>



<br>

https://github.com/tkipf/gcn</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Normalization.html">#Normalization</a>
<span class="issue_date">Issue Date: 2018-02-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/261">Layer Normalization, Ba+, arXiv'16</a>
<span class="snippet"><span>Summary</span>バッチ正規化の代わりにレイヤー正規化を用いることで、リカレントニューラルネットワークのトレーニング時間を短縮。レイヤー内のニューロンの合計入力を正規化し、各ニューロンに独自の適応バイアスとゲインを適用。トレーニング時とテスト時で同じ計算を行い、隠れ状態のダイナミクスを安定させる。実証的に、トレーニング時間の大幅な短縮を確認。</span>
<span class="snippet"><span>Comment</span>解説スライド：

<br>



<br>

https://www.slideshare.net/KeigoNishida/layer-normalizationnips

<br>



<br>

</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/CoNLL.html">#CoNLL</a>
<span class="issue_date">Issue Date: 2018-02-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/258">Generating Sentences from a Continuous Space, Bowman+, CoNLL'16</a>
<span class="snippet"><span>Comment</span>VAEを利用して文生成【Variational Autoencoder徹底解説】

<br>



<br>

https://qiita.com/kenmatsu4/items/b029d697e9995d93aa24</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/CollaborativeFiltering.html">#CollaborativeFiltering</a>
<a class="button" href="articles/WSDM.html">#WSDM</a>
<span class="issue_date">Issue Date: 2018-01-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/216">Collaborative Denoising Auto-Encoders for Top-N Recommender Systems, Wu+, WSDM'16</a>
<span class="snippet"><span>Comment</span>Denoising Auto-Encoders を用いたtop-N推薦手法、Collaborative Denoising Auto-Encoder (CDAE)を提案。

<br>



<br>

モデルベースなCollaborative Filtering手法に相当する。corruptedなinputを復元するようなDenoising Auto Encoderのみで推薦を行うような手法は、この研究が初めてだと主張。

<br>



<br>



<br>



<br>

学習する際は、userのitemsetのsubsetをモデルに与え（noiseがあることに相当）、全体のitem setを復元できるように、学習する（すなわちDenoising Auto-Encoder）。

<br>



<br>

推薦する際は、ユーザのその時点でのpreference setをinputし、new itemを推薦する。

<br>



<br>



<br>



<br>

221 もStacked Denoising Auto EncoderとCollaborative Topic Regression 226 を利用しているが、221 ではarticle recommendationというspecificな問題を解いているのに対して、提案手法はgeneralなtop-N推薦に利用できることを主張。</span>
<a class="button" href="articles/Single.html">#Single</a>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Document.html">#Document</a>
<a class="button" href="articles/Supervised.html">#Supervised</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Abstractive.html">#Abstractive</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/136">Incorporating Copying Mechanism in Sequence-to-Sequence Learning, Gu+, ACL'16</a>
<span class="snippet"><span>Comment</span>解説スライド：https://www.slideshare.net/akihikowatanabe3110/incorporating-copying-mechanism-in-sequene-to-sequence-learning単語のコピーと生成、両方を行えるネットワークを提案。

<br>



<br>

location based addressingなどによって、生成された単語がsourceに含まれていた場合などに、copy-mode, generate-modeを切り替えるような仕組みになっている。

<br>



<br>



<br>



<br>

65 と同じタイミングで発表</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/MachineTranslation.html">#MachineTranslation</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/65">Pointing the unknown words, Gulcehre+, ACL'16</a>
<span class="snippet"><span>Comment</span>テキストを生成する際に、source textからのコピーを行える機構を導入することで未知語問題に対処した話CopyNetと同じタイミングで（というか同じconferenceで）発表</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/MachineTranslation.html">#MachineTranslation</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<span class="issue_date">Issue Date: 2025-05-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1954">Neural Machine Translation by Jointly Learning to Align and Translate, Dzmitry Bahdanau+, ICLR'15</a>
<span class="snippet"><span>Summary</span>ニューラル機械翻訳は、エンコーダー-デコーダーアーキテクチャを用いて翻訳性能を向上させる新しいアプローチである。本論文では、固定長のベクトルの使用が性能向上のボトルネックであるとし、モデルが関連するソース文の部分を自動的に検索できるように拡張することを提案。これにより、英語からフランス語への翻訳タスクで最先端のフレーズベースシステムと同等の性能を達成し、モデルのアライメントが直感と一致することを示した。</span>
<span class="snippet"><span>Comment</span>(Cross-)Attentionを初めて提案した研究。メモってなかったので今更ながら追加。Attentionはここからはじまった（と認識している）</span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<a class="button" href="articles/Normalization.html">#Normalization</a>
<span class="issue_date">Issue Date: 2025-04-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1857">Batch Normalization: Accelerating Deep Network Training by Reducing   Internal Covariate Shift, Sergey Ioffe+, ICML'15</a>
<span class="snippet"><span>Summary</span>バッチ正規化を用いることで、深層ニューラルネットワークのトレーニングにおける内部共変量シフトの問題を解決し、高い学習率を可能にし、初期化の注意を軽減。これにより、同じ精度を14倍少ないトレーニングステップで達成し、ImageNet分類で最良の公表結果を4.9%改善。</span>
<span class="snippet"><span>Comment</span>メモってなかったので今更ながら追加した共変量シフトやBatch Normalizationの説明は

<br>

・261

<br>



<br>

記載のスライドが分かりやすい。</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/MachineTranslation.html">#MachineTranslation</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/EMNLP.html">#EMNLP</a>
<span class="issue_date">Issue Date: 2021-06-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/369">Effective Approaches to Attention-based Neural Machine Translation, Luong+, EMNLP'15</a>
<span class="snippet"><span>Comment</span>Luong論文。attentionの話しはじめると、だいたいBahdanau+か、Luong+論文が引用される。

<br>



<br>



<br>



<br>

Global Attentionと、Local Attentionについて記述されている。Global Attentionがよく利用される。

<br>



<br>



<br>



<br>

Global Attention

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/120452200-008ec280-c3cd-11eb-8ced-47dc9e67f487.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Local Attention

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/120452397-2025eb00-c3cd-11eb-9d3b-0f7802a40712.png" alt="image" loading="lazy">

<br>



<br>

やはり菊池さんの解説スライドが鉄板。

<br>



<br>

https://www.slideshare.net/yutakikuchi927/deep-learning-nlp-attention参考までに、LuongらのGlobal Attentionの計算の流れは下記となっている：

<br>



<br>

・h_t -&gt; a_t -&gt; c_t -&gt; h^~_t

<br>



<br>



<br>



<br>

BahdanauらのAttentionは下記

<br>



<br>

・h_t-1 -&gt; a_t -&gt; c_t -&gt; h_t

<br>



<br>



<br>



<br>

t-1のhidden stateを使うのか、input feeding後の現在のhidden stateをattention weightの計算に使うのかが異なっている。また、過去のalignmentの情報を考慮した上でデコーディングしていくために、input-feeding approachも提案

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/120877145-cfdaa300-c5ef-11eb-8a8b-a57d03d864b4.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

input-feeding appproachでは、t-1ステップ目のoutputの算出に使ったh^~_t（hidden_stateとcontext vectorをconcatし、tanhのactivationを噛ませた線形変換を行なったベクトル）を、時刻tのinput embeddingにconcatして、RNNに入力する。</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<a class="button" href="articles/IJCNLP.html">#IJCNLP</a>
<span class="issue_date">Issue Date: 2018-03-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/266">Unsupervised prediction of acceptability judgements, Lau+, ACL-IJCNLP'15</a>
<span class="snippet"><span>Comment</span>文のacceptability（容認度）論文。

<br>



<br>

文のacceptabilityとは、native speakerがある文を読んだときに、その文を正しい文として容認できる度合いのこと。

<br>



<br>

acceptabilityスコアが低いと、Readabilityが低いと判断できる。

<br>



<br>

言語モデルをトレーニングし、トレーニングした言語モデルに様々な正規化を施すことで、acceptabilityスコアを算出する。</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<span class="issue_date">Issue Date: 2018-02-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/262">An Empirical Exploration of Recurrent Network Architectures, Jozefowicz+, ICML'15</a>
<span class="snippet"><span>Comment</span>GRUとLSTMの違いを理解するのに最適</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<span class="issue_date">Issue Date: 2018-02-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/257">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks, Tai+, ACL'15</a>
<span class="snippet"><span>Comment</span>Tree-LSTM論文</span>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Sentence.html">#Sentence</a>
<a class="button" href="articles/Supervised.html">#Supervised</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Abstractive.html">#Abstractive</a>
<a class="button" href="articles/EMNLP.html">#EMNLP</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/137">A Neural Attention Model for Sentence Summarization, Rush+, EMNLP'15</a>
<span class="snippet"><span>Comment</span>解説スライド：https://www.slideshare.net/akihikowatanabe3110/a-neural-attention-model-for-sentence-summarization-65612331</span>
<a class="button" href="articles/Single.html">#Single</a>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Sentence.html">#Sentence</a>
<a class="button" href="articles/Document.html">#Document</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/Abstractive.html">#Abstractive</a>
<a class="button" href="articles/EMNLP.html">#EMNLP</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/75">LCSTS: A large scale chinese short text summarizatino dataset, Hu+, EMNLP'15</a>
<span class="snippet"><span>Comment</span>Large Chinese Short Text Summarization (LCSTS) datasetを作成

<br>



<br>



<br>



<br>

データセットを作成する際は、Weibo上の特定のorganizationの投稿の特徴を利用。

<br>



<br>

Weiboにニュースを投稿する際に、投稿の冒頭にニュースのvery short summaryがまず記載され、その後ニュース本文（短め）が記載される特徴があるので、この対をsource-reference対として収集した。

<br>



<br>

収集する際には、約１００個のルールに基づくフィルタリングやclearning, 抽出等を行なっている。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34411045-95f7baf2-ec17-11e7-94fb-faf2559d6994.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

データセットのpropertyとしては、下記のPartI, II, IIIに分かれている。

<br>



<br>



<br>



<br>

PartI: 2.4Mのshort text ・summary pair

<br>



<br>

PartII: PartIからランダムにサンプリングされた10kのpairに対して、5 scaleで要約のrelevanceをratingしたデータ。ただし、各pairにラベルづけをしたevaluatorは1名のみ。

<br>



<br>

PartIII: 2kのpairに対して（PartI, PartIIとは独立）、3名のevaluatorが5-scaleでrating。evaluatorのratingが一致した1kのpairを抽出したデータ。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34411199-8db4df90-ec18-11e7-8703-fd8f9512a903.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

RNN-GRUを用いたSummarizerも提案している。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34411224-b5543eba-ec18-11e7-8556-a3b42bfcf334.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

CopyNetなどはLCSTSを使って評価している。他にも使ってる論文あったはず。ACL'17のPointer Generator Networkでした。</span>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Sentence.html">#Sentence</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/EMNLP.html">#EMNLP</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/59">Sentence Compression by Deletion with LSTMs, Fillipova+, EMNLP'15</a>
<span class="snippet"><span>Comment</span>slide:https://www.slideshare.net/akihikowatanabe3110/sentence-compression-by-deletion-with-lstms</span>
<a class="button" href="articles/Multi.html">#Multi</a>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Extractive.html">#Extractive</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/58">Hierarchical Summarization: Scaling Up Multi-Document Summarization, Christensen+, ACL'14</a>
<span class="snippet"><span>Comment</span>概要

<br>



<br>

だいぶ前に読んだ。好きな研究。

<br>



<br>

テキストのsentenceを階層的にクラスタリングすることで、抽象度が高い情報から、関連する具体度の高いsentenceにdrill downしていけるInteractiveな要約を提案している。

<br>



<br>



<br>



<br>

手法

<br>



<br>

通常のMDSでのデータセットの規模よりも、実際にMDSを使う際にはさらに大きな規模のデータを扱わなければならないことを指摘し（たとえばNew York Timesで特定のワードでイベントを検索すると数千、数万件の記事がヒットしたりする）そのために必要な事項を検討。

<br>



<br>

これを実現するために、階層的なクラスタリングベースのアプローチを提案。

<br>



<br>

提案手法では、テキストのsentenceを階層的にクラスタリングし、下位の層に行くほどより具体的な情報になるようにsentenceを表現。さらに、上位、下位のsentence間にはエッジが張られており、下位に紐付けられたsentence

<br>



<br>

は上位に紐付けられたsentenceの情報をより具体的に述べたものとなっている。

<br>



<br>

これを活用することで、drill down型のInteractiveな要約を実現。</span>
<a class="button" href="articles/Multi.html">#Multi</a>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/QueryBiased.html">#QueryBiased</a>
<a class="button" href="articles/Extractive.html">#Extractive</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/57">Query-Chain Focused Summarization, Baumel+, ACL'14</a>
<span class="snippet"><span>Comment</span>（管理人が作成した過去の紹介資料）

<br>

[Query-Chain Focused Summarization.pdf](https://github.com/AkihikoWatanabe/paper_notes/files/1590916/Query-Chain.Focused.Summarization.pdf)

<br>



<br>

上記スライドは私が当時作成した論文紹介スライドです。スライド中のスクショは説明のために論文中のものを引用しています。</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/MatrixFactorization.html">#MatrixFactorization</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<span class="issue_date">Issue Date: 2018-01-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/224">Deep content-based music recommendation, Oord+, NIPS'13</a>
<span class="snippet"><span>Comment</span>Contents-Basedな音楽推薦手法(cold-start problemに強い)。

<br>



<br>

Weighted Matrix Factorization (WMF) (Implicit Feedbackによるデータに特化したMatrix Factorization手法) 225 に、Convolutional Neural Networkによるmusic audioのlatent vectorの情報が組み込まれ、item vectorが学習されるような仕組みになっている。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34815522-01679f0e-f6f5-11e7-8534-22e5b5edd7a6.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

CNNでmusic audioのrepresentationを生成する際には、audioのtime-frequencyの情報をinputとする。学習を高速化するために、window幅を3秒に設定しmusic clipをサンプルしinputする。music clip全体のrepresentationを求める際には、consecutive windowからpredictionしたrepresentationを平均したものを使用する。</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<a class="button" href="articles/ImageClassification.html">#ImageClassification</a>
<a class="button" href="articles/Backbone.html">#Backbone</a>
<span class="issue_date">Issue Date: 2025-05-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1958">ImageNet Classification with Deep Convolutional Neural Networks, Krizhevsky+, NIPS'12</a>
<span class="snippet"><span>Comment</span>ILSVRC 2012において圧倒的な性能示したことで現代のDeepLearningの火付け役となった研究AlexNet。メモってなかったので今更ながら追加した。AlexNet以前の画像認識技術については牛久先生がまとめてくださっている（当時の課題とそれに対する解決法、しかしまだ課題が…と次々と課題に直面し解決していく様子が描かれており非常に興味深かった)。現在でも残っている技術も紹介されている。:

<br>

https://speakerdeck.com/yushiku/pre_alexnet

<br>



<br>

&gt; 過去の技術だからといって聞き流していると時代背景の変化によってなし得たイノベーションを逃すかも

<br>



<br>

これは肝に銘じたい。</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/CollaborativeFiltering.html">#CollaborativeFiltering</a>
<a class="button" href="articles/MatrixFactorization.html">#MatrixFactorization</a>
<a class="button" href="articles/SIGKDD.html">#SIGKDD</a>
<span class="issue_date">Issue Date: 2018-01-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/226">Collaborative topic modeling for recommending scientific articles, Wang+, KDD'11</a>
<span class="snippet"><span>Comment</span>Probabilistic Matrix Factorization (PMF) 227 に、Latent Dirichllet Allocation (LDA) を組み込んだCollaborative Topic Regression (CTR)を提案。

<br>



<br>

LDAによりitemのlatent vectorを求め、このitem vectorと、user vectorの内積を（平均値として持つ正規表現からのサンプリング）用いてratingを生成する。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34816213-9e21a07c-f6f7-11e7-8310-3cb55c45c71d.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34816387-1a4b1a02-f6f8-11e7-8000-f74af099bc6f.png" alt="image" loading="lazy">

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34816394-1f981122-f6f8-11e7-80c8-4941f2a6c191.png" alt="image" loading="lazy">

<br>



<br>

CFとContents-basedな手法が双方向にinterationするような手法解説ブログ：http://d.hatena.ne.jp/repose/20150531/1433004688</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/159">Collaborative Filtering Recommender Systems, Ekstrand+ （with Joseph A. Konstan）, Foundations and TrendsR in Human–Computer Interaction'11</a>
<a class="button" href="articles/Multi.html">#Multi</a>
<a class="button" href="articles/PersonalizedDocumentSummarization.html">#PersonalizedDocumentSummarization</a>
<a class="button" href="articles/InteractivePersonalizedSummarization.html">#InteractivePersonalizedSummarization</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/EMNLP.html">#EMNLP</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1">Summarize What You Are Interested In: An Optimization Framework for Interactive Personalized Summarization, Yan+, EMNLP'11, 2011.07</a>
<span class="snippet"><span>Comment</span><img src="https://user-images.githubusercontent.com/12249301/34400733-97c86614-ebd7-11e7-9fe9-a6b36c726a21.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

ユーザとシステムがインタラクションしながら個人向けの要約を生成するタスク、InteractivePersonalizedSummarizationを提案。

<br>



<br>



<br>



<br>

ユーザはテキスト中のsentenceをクリックすることで、システムに知りたい情報のフィードバックを送ることができる。このとき、ユーザがsentenceをクリックする量はたかがしれているので、click smoothingと呼ばれる手法を提案し、sparseにならないようにしている。click smoothingは、ユーザがクリックしたsentenceに含まれる単語？等を含む別のsentence等も擬似的にclickされたとみなす手法。

<br>



<br>



<br>



<br>

4つのイベント（Influenza A, BP Oil Spill, Haiti Earthquake, Jackson Death）に関する、数千記事のニュースストーリーを収集し（10k〜100k程度のsentence）、評価に活用。収集したニュースサイト（BBC, Fox News, Xinhua, MSNBC, CNN, Guardian, ABC, NEwYorkTimes, Reuters, Washington Post）には、各イベントに対する人手で作成されたReference Summaryがあるのでそれを活用。

<br>



<br>

objectiveな評価としてROUGE、subjectiveな評価として3人のevaluatorに5scaleで要約の良さを評価してもらった。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34400727-8c8ab022-ebd7-11e7-85df-c238fd2255de.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

結論としては、ROUGEはGenericなMDSモデルに勝てないが、subjectiveな評価においてベースラインを上回る結果に。ReferenceはGenericに生成されているため、この結果を受けてPersonalizationの必要性を説いている。

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34400721-82d1bb8e-ebd7-11e7-83d2-697ac61eb38a.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

また、提案手法のモデルにおいて、Genericなモデルの影響を強くする（Personalizedなハイパーパラメータを小さくする）と、ユーザはシステムとあまりインタラクションせずに終わってしまうのに対し、Personalizedな要素を強くすると、よりたくさんクリックをし、結果的にシステムがより多く要約を生成しなおすという結果も示している。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34400718-7b9a4912-ebd7-11e7-83cf-aba826a41d34.png" alt="image" loading="lazy"></span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/CollaborativeFiltering.html">#CollaborativeFiltering</a>
<a class="button" href="articles/FactorizationMachines.html">#FactorizationMachines</a>
<a class="button" href="articles/ICDM.html">#ICDM</a>
<span class="issue_date">Issue Date: 2018-12-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/281">Factorization Machines, Steffen Rendle, ICDM'10</a>
<span class="snippet"><span>Comment</span>解説ブログ：http://echizen-tm.hatenablog.com/entry/2016/09/11/024828

<br>



<br>

DeepFMに関する動向：https://data.gunosy.io/entry/deep-factorization-machines-2018<img src="https://user-images.githubusercontent.com/12249301/50376506-c3954200-0650-11e9-8330-26bda57d154f.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

非常に完結でわかりやすい説明<img src="https://user-images.githubusercontent.com/12249301/50376518-fdfedf00-0650-11e9-99c0-060f286de392.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

FMのFeature VectorのExample

<br>



<br>

各featureごとにlatent vectorが学習され、featureの組み合わせのweightが内積によって表現される

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/50376536-53d38700-0651-11e9-830d-28bc32b3c02d.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Matrix Factorizationの一般形のような形式</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/171">Content-based Recommender Systems: State of the Art and Trends, Lops+, Recommender Systems Handbook'10</a>
<span class="snippet"><span>Comment</span>RecSysの内容ベースフィルタリングシステムのユーザプロファイルについて知りたければこれ</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/ImageClassification.html">#ImageClassification</a>
<a class="button" href="articles/ObjectRecognition.html">#ObjectRecognition</a>
<a class="button" href="articles/ObjectLocalization.html">#ObjectLocalization</a>
<span class="issue_date">Issue Date: 2025-05-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1959">ImageNet: A Large-Scale Hierarchical Image Database, Deng+, CVPR'09</a>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/LearningToRank.html">#LearningToRank</a>
<a class="button" href="articles/ImplicitFeedback.html">#ImplicitFeedback</a>
<a class="button" href="articles/UAI.html">#UAI</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/28">BPR: Bayesian Personalized Ranking from Implicit Feedback, Rendle+, UAI'09, 2009.06</a>
<span class="snippet"><span>Comment</span>重要論文

<br>



<br>

ユーザのアイテムに対するExplicit/Implicit Ratingを利用したlearning2rank。

<br>



<br>

AUCを最適化するようなイメージ。

<br>



<br>

負例はNegative Sampling。

<br>



<br>

計算量が軽く、拡張がしやすい。

<br>



<br>



<br>



<br>

Implicitデータを使ったTop-N Recsysを構築する際には検討しても良い。

<br>



<br>

また、MFのみならず、Item-Based KNNに活用することなども可能。

<br>



<br>



<br>



<br>

http://tech.vasily.jp/entry/2016/07/01/134825参考: https://techblog.zozo.com/entry/2016/07/01/134825pytorchでのBPR実装: https://github.com/guoyang9/BPR-pytorch</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/MultitaskLearning.html">#MultitaskLearning</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<span class="issue_date">Issue Date: 2018-02-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/250">A unified architecture for natural language processing: Deep neural networks with multitask learning, Collobert+, ICML'2008.</a>
<span class="snippet"><span>Comment</span>Deep Neural Netを用いてmultitask learningを行いNLPタスク（POS tagging, Semantic Role Labeling, Chunking etc.）を解いた論文。

<br>



<br>

被引用数2000を超える。

<br>



<br>



<br>



<br>

multitask learningの学習プロセスなどが引用されながら他論文で言及されていたりする。</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/MatrixFactorization.html">#MatrixFactorization</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<span class="issue_date">Issue Date: 2018-01-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/227">Probabilistic Matrix Factorization, Salakhutdinov+, NIPS'08</a>
<span class="snippet"><span>Comment</span>Matrix Factorizationを確率モデルとして表した論文。

<br>



<br>

解説：http://yamaguchiyuto.hatenablog.com/entry/2017/07/13/080000

<br>



<br>

既存のMFは大規模なデータに対してスケールしなかったが、PMFではobservationの数に対して線形にスケールし、さらには、large, sparse, imbalancedなNetflix datasetで良い性能が出た（Netflixデータセットは、rating件数が少ないユーザとかも含んでいる。MovieLensとかは含まれていないのでより現実的なデータセット）。

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34817061-30757582-f6fa-11e7-90fb-ad5e5fc65781.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

また、Constrained PMF（同じようなsetの映画にrateしているユーザは似ているといった仮定に基づいたモデル ※1）を用いると、少ないratingしかないユーザに対しても良い性能が出た。

<br>



<br>



<br>



<br>

※1　ratingの少ないユーザの潜在ベクトルは平均から動きにくい、つまりなんの特徴もない平均的なユーザベクトルになってしまうので、同じ映画をratingした人は似た事前分布を持つように制約を導入したモデル

<br>



<br>



<br>



<br>

（解説ブログ、解説スライドより）</span>
<a class="button" href="articles/MachineTranslation.html">#MachineTranslation</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<span class="issue_date">Issue Date: 2024-12-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1609">Large Language Models in Machine Translation, Brants+, EMNLP-CoNLL'07</a>
<span class="snippet"><span>Summary</span>本論文では、機械翻訳における大規模な統計的言語モデルの利点を報告し、最大2兆トークンでトレーニングした3000億n-gramのモデルを提案。新しいスムージング手法「Stupid Backoff」を導入し、大規模データセットでのトレーニングが安価で、Kneser-Neyスムージングに近づくことを示す。</span>
<span class="snippet"><span>Comment</span>N-gram言語モデル+スムージングの手法において、学習データを増やして扱えるngramのタイプ数（今で言うところのvocab数に近い）を増やしていったら、perplexityは改善するし、MTにおけるBLEUスコアも改善するよ（BLEUはサチってるかも？）という考察がされている

<br>



<br>

<img src="https://github.com/user-attachments/assets/035f28db-12c6-4b69-b39f-7eb41581d00c" alt="image" loading="lazy">元ポスト:https://x.com/odashi_t/status/1871024428739604777?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QLarge Language Modelsという用語が利用されたのはこの研究が初めてなのかも…？</span>
<a class="button" href="articles/Multi.html">#Multi</a>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/Document.html">#Document</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/IntegerLinearProgramming%20(ILP).html">#IntegerLinearProgramming (ILP)</a>
<a class="button" href="articles/Extractive.html">#Extractive</a>
<a class="button" href="articles/ECIR.html">#ECIR</a>
<span class="issue_date">Issue Date: 2018-01-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/241">A study of global inference algorithms in multi-document summarization, Ryan McDonald, ECIR'07</a>
<span class="snippet"><span>Comment</span>文書要約をナップサック問題として定式化し、厳密解（動的計画法、ILP Formulation）、近似解(Greedy)を求める手法を提案。</span>
<a class="button" href="articles/InformationRetrieval.html">#InformationRetrieval</a>
<a class="button" href="articles/LearningToRank.html">#LearningToRank</a>
<a class="button" href="articles/ListWise.html">#ListWise</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/193">Learning to Rank: From Pairwise Approach to Listwise Approach （ListNet）, Cao+, ICML'2007</a>
<span class="snippet"><span>Comment</span>解説スライド：http://www.nactem.ac.uk/tsujii/T-FaNT2/T-FaNT.files/Slides/liu.pdf

<br>



<br>

解説ブログ：https://qiita.com/koreyou/items/a69750696fd0b9d88608従来行われてきたLearning to Rankはpairwiseな手法が主流であったが、pairwiseな手法は2つのインスタンス間の順序が正しく識別されるように学習されているだけであった。

<br>



<br>

pairwiseなアプローチには以下の問題点があった：

<br>



<br>



<br>



<br>

 インスタンスのペアのclassification errorを最小化しているだけで、インスタンスのランキングのerrorを最小化しているわけではない。

<br>



<br>

 インスタンスペアが i.i.d な分布から生成されるという制約は強すぎる制約

<br>



<br>

 queryごとに生成されるインスタンスペアは大きく異なるので、インスタンスペアよりもクエリに対してバイアスのかかった学習のされ方がされてしまう

<br>



<br>



<br>



<br>

これらを解決するために、listwiseなアプローチを提案。

<br>



<br>

listwiseなアプローチを用いると、インスタンスのペアの順序を最適化するのではなく、ランキング全体を最適化できる。

<br>



<br>

listwiseなアプローチを用いるために、Permutation Probabilityに基づくloss functionを提案。loss functionは、2つのインスタンスのスコアのリストが与えられたとき、Permutation Probability Distributionを計算し、これらを用いてcross-entropy lossを計算するようなもの。

<br>



<br>

また、Permutation Probabilityを計算するのは計算量が多すぎるので、top-k probabilityを提案。

<br>



<br>

top-k probabilityはPermutation Probabilityの計算を行う際のインスタンスをtop-kに限定するもの。

<br>



<br>

論文中ではk=1を採用しており、k=1はsoftmaxと一致する。

<br>



<br>

パラメータを学習する際は、Gradient Descentを用いる。k=1の設定で計算するのが普通なようなので、普通にoutputがsoftmaxでlossがsoftmax cross-entropyなモデルとほぼ等価なのでは。</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/Explanation.html">#Explanation</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/162">A Survey of Explanations in Recommender Systems, Tintarev+, ICDEW'07</a>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/CollaborativeFiltering.html">#CollaborativeFiltering</a>
<a class="button" href="articles/MatrixFactorization.html">#MatrixFactorization</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/161">Matrix Factorization Techniques for Recommender Systems, Koren+, Computer'07</a>
<span class="snippet"><span>Comment</span>Matrix Factorizationについてよくまとまっている</span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/DomainAdaptation.html">#DomainAdaptation</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/126">Frustratingly easy domain adaptation, Daum'e, ACL'07</a>
<span class="snippet"><span>Comment</span><img src="https://user-images.githubusercontent.com/12249301/34462211-f3428130-ee81-11e7-8a06-36e66bd19b2f.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

domain adaptationをする際に、Source側のFeatureとTarget側のFeatureを上式のように、Feature Vectorを拡張し独立にコピーし表現するだけで、お手軽にdomain adaptationができることを示した論文。

<br>



<br>



<br>



<br>

イメージ的には、SourceとTarget、両方に存在する特徴は、共通部分の重みが高くなり、Source, Targetドメイン固有の特徴は、それぞれ拡張した部分のFeatureに重みが入るような感じ。</span>
<a class="button" href="articles/InformationRetrieval.html">#InformationRetrieval</a>
<a class="button" href="articles/LearningToRank.html">#LearningToRank</a>
<a class="button" href="articles/PairWise.html">#PairWise</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/192">Learning to Rank using Gradient Descent （RankNet）, Burges+, ICML'2005</a>
<span class="snippet"><span>Comment</span>pair-wiseのlearning2rankで代表的なRankNet論文

<br>



<br>

解説ブログ：https://qiita.com/sz_dr/items/0e50120318527a928407

<br>



<br>



<br>



<br>

lossは2個のインスタンスのpair、A, Bが与えられたとき、AがBよりも高くランクされる場合は確率1, AがBよりも低くランクされる場合は確率0、そうでない場合は1/2に近くなるように、スコア関数を学習すれば良い。</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/157">Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions, Adomavicius+, IEEE Transactions on Knowledge and Data Engineering'05</a>
<span class="snippet"><span>Comment</span>有名なやつ</span>
<a class="button" href="articles/Single.html">#Single</a>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/Document.html">#Document</a>
<a class="button" href="articles/GraphBased.html">#GraphBased</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Extractive.html">#Extractive</a>
<a class="button" href="articles/EMNLP.html">#EMNLP</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/214">TextRank: Bringing Order into Texts, Mihalcea+, EMNLP'04</a>
<span class="snippet"><span>Comment</span>PageRankベースの手法で、キーワード抽出/文書要約 を行う手法。

<br>



<br>

キーワード抽出/文書要約 を行う際には、ノードをそれぞれ 単語/文 で表現する。

<br>



<br>

ノードで表現されている 単語/文 のsimilarityを測り、ノード間のedgeの重みとすることでAffinity Graphを構築。

<br>



<br>

あとは構築したAffinity Graphに対してPageRankを適用して、ノードの重要度を求める。

<br>



<br>

ノードの重要度に従いGreedyに 単語/文 を抽出すれば、キーワード抽出/文書要約 を行うことができる。単一文書要約のベースラインとして使える。gensimに実装がある。

<br>



<br>

個人的にも実装している：https://github.com/AkihikoWatanabe/textrank</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/158">Evaluating Collaborative Filtering Recommener Systems, Herlocker+, TOIS'04</a>
<span class="snippet"><span>Comment</span>GroupLensのSurvey</span>
<a class="button" href="articles/InformationRetrieval.html">#InformationRetrieval</a>
<a class="button" href="articles/LearningToRank.html">#LearningToRank</a>
<a class="button" href="articles/PointWise.html">#PointWise</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/190">PRanking with Ranking, Crammer+, NIPS'01</a>
<span class="snippet"><span>Comment</span>Point-WiseなLearning2Rankの有名手法</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/CollaborativeFiltering.html">#CollaborativeFiltering</a>
<a class="button" href="articles/ItemBased.html">#ItemBased</a>
<a class="button" href="articles/WWW.html">#WWW</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/179">Item-based collaborative filtering recommendation algorithms, Sarwar+（with Konstan）, WWW'01</a>
<span class="snippet"><span>Comment</span>アイテムベースな協調フィルタリングを提案した論文（GroupLens）</span>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/Document.html">#Document</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/NAACL.html">#NAACL</a>
<span class="issue_date">Issue Date: 2018-01-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/246">Cut and paste based text summarization, Jing+, NAACL'00</a>
<span class="snippet"><span>Comment</span>AbstractiveなSummarizationの先駆け的研究。

<br>



<br>

AbstractiveなSummarizationを研究するなら、押さえておいたほうが良い。</span>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/InformationRetrieval.html">#InformationRetrieval</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Search.html">#Search</a>
<a class="button" href="articles/SIGIR.html">#SIGIR</a>
<span class="issue_date">Issue Date: 2018-01-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/243">The Use of MMR, Diversity-Based Reranking for Reordering Documents and Producing Summaries, Carbonell+, SIGIR'98</a>
<span class="snippet"><span>Comment</span>Maximal Marginal Relevance (MMR) 論文。

<br>



<br>

検索エンジンや文書要約において、文書/文のランキングを生成する際に、既に選んだ文書と類似度が低く、かつqueryとrelevantな文書をgreedyに選択していく手法を提案。

<br>



<br>

ILPによる定式化が提案される以前のMulti Document Summarization (MDS) 研究において、冗長性の排除を行う際には典型的な手法。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<a class="button" href="articles/OpenSource.html">#OpenSource</a>
<a class="button" href="articles/GRPO.html">#GRPO</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/RLVR.html">#RLVR</a>
<span class="issue_date">Issue Date: 2025-09-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2755">[Paper Note] K2-Think: A Parameter-Efficient Reasoning System, Institute of Foundation Models, Mohamed bin Zayed University of Artificial Intelligence, 2025.09</a>
<span class="snippet"><span>Comment</span>HF:https://huggingface.co/LLM360/K2-Think

<br>

code:

<br>

・https://github.com/MBZUAI-IFM/K2-Think-SFT

<br>

・https://github.com/MBZUAI-IFM/K2-Think-Inference

<br>



<br>

RLはverl+GRPOで実施したとテクニカルペーパーに記述されているが、当該部分のコードの公開はされるのだろうか？

<br>

RLで利用されたデータはこちら:

<br>

・2070元ポスト:https://x.com/gm8xx8/status/1965713404418805806?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/Japanese.html">#Japanese</a>
<span class="issue_date">Issue Date: 2025-09-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2733">『JamC-QA』: 日本の文化や風習に特化した質問応答ベンチマークの構築・公開（前編）, SB Intuitions, 2025.09</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/sbintuitions/status/1965243405812011263?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q後編も参照のこと:https://www.sbintuitions.co.jp/blog/entry/2025/09/09/113132日本の文化、風習、風土、地理、日本史、行政、法律、医療に関する既存のベンチマークによりも難易度が高いQAを人手によってスクラッチから作成した評価データ。人手で作成されたQAに対して、8種類の弱いLLM（パラメータ数の小さい日本語LLMを含む）の半数以上が正しく回答できたものを除外、その後さらに人手で確認といったフィルタリングプロセスを踏んでいる。記事中は事例が非常に豊富で興味深い。

<br>



<br>

後編では実際の評価結果が記載されており、フルスクラッチの日本語LLMが高い性能を獲得しており、Llama-Swallowなどの継続事前学習をベースとしたモデルも高いスコアを獲得している。評価時は4-shotでドメインごとにExamplarは固定し、greedy decodingで評価したとのこと。NLP'25:https://www.anlp.jp/proceedings/annual_meeting/2025/pdf_dir/Q2-18.pdf・1890

<br>



<br>

のような話もあるので、greedy decodingだけでなくnucleus/temperature samplingを複数trial実施した場合の性能の平均で何か変化があるだろうか、という点が気になったが、下記研究でMMLUのような出力空間が制約されているような設定の場合はほとんど影響がないことが実験的に示されている模様:

<br>

・2735

<br>



<br>

これはnucleus/temperature samplingが提案された背景（＝出力の自然さを保ったまま多様性を増やしたい）とも一致する。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Repository.html">#Repository</a>
<span class="issue_date">Issue Date: 2025-09-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2721">FinePDFs, HuggingFaceFW, 2025.09</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/aratako_lm/status/1964596642067402987?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QThomas Wolf氏のポスト:

<br>

https://x.com/thom_wolf/status/1964653264986656922?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QODC-By 1.0 license</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<a class="button" href="articles/Composition.html">#Composition</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-09-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2712">From f（x） and g（x） to f（g（x））: LLMs Learn New Skills in RL by Composing Old Ones, Yuan+, 2025.09</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/rosinality/status/1964235195613143127?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qコントロールされた実験において、深さ2のnestedなcompostition g(f(x))のデータでRLした場合は、テスト時に深さ6までのcompostitionを実行できるようになったが（＝メタスキルとしてcompostitionを獲得した）、深さ1のnon-nestedなデータでRLした場合は複雑なcompostitionが必要なタスクを解けなかった。また、一般的にベースモデルがある程度解ける問題に対してRLを適用したモデルのpass@1000はあまり向上しないことから、RLは新しいスキルを何も教えていないのではないか、といった解釈がされることがあるが、より高次のcompostitionが必要なタスクで評価すると明確に性能が良くなるので、実はより高次のcompostitionが必要なタスクに対する汎化性能を伸ばしている。compostitionでの能力を発揮するにはまず幅広いatomicなスキルが必要なので、しっかりそれを事前学習で身につけさせ、その後post-trainingによって解決したいタスクのためのatomic skillのcompostitionの方法を学習させると効果的なのではないか、といった話な模様。この辺のICLの話と似ている

<br>

・1362</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Hallucination.html">#Hallucination</a>
<span class="issue_date">Issue Date: 2025-09-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2709">Why Language Models Hallucinate, Kalai+, 2025.09</a>
<span class="snippet"><span>Comment</span>著者ポスト:https://x.com/adamfungi/status/1964040819196752312?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q解説:https://x.com/hillbig/status/1964837910278189271?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q所見:https://x.com/ziv_ravid/status/1964384106567127465?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-09-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2695">FineVision: Open Data Is All You Need, Wiedmann+, Hugging Face, 2025.09</a>
<span class="snippet"><span>Comment</span>HF:https://huggingface.co/datasets/HuggingFaceM4/FineVision元ポスト:https://x.com/andimarafioti/status/1963610135328104945?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LLMAgent.html">#LLMAgent</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/Repository.html">#Repository</a>
<a class="button" href="articles/Coding.html">#Coding</a>
<a class="button" href="articles/SoftwareEngineering.html">#SoftwareEngineering</a>
<span class="issue_date">Issue Date: 2025-09-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2682">OpenHands PR Arena, neulab, 2025.09</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/gneubig/status/1963267468853477809?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q実際に存在するIssueにタグ付けすることで、リアルタイムに複数LLMによってPRを作成（API callはOpenHandswが負担する）し、ユーザは複数LLMの中で良いものを選択する、といったことができる模様？リーダーボードも将来的に公開するとのことなので、実際にユーザがどのモデルのoutputを選んだかによって勝敗がつくので、それに基づいてランキング付けをするのだろうと推測。興味深い。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<a class="button" href="articles/LLMServing.html">#LLMServing</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-09-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2668">Inside vLLM: Anatomy of a High-Throughput LLM Inference System, Aleksa Gordić blog, 2025.08</a>
<span class="snippet"><span>Comment</span>めっちゃ良さそう</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-09-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2652">August 2025 - China Open Source  Highlights, 2025.09</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/adinayakup/status/1962508234549329969?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/MachineTranslation.html">#MachineTranslation</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<span class="issue_date">Issue Date: 2025-09-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2634">Hunyuan-MT-7B, Tencent, 2025.09</a>
<span class="snippet"><span>Comment</span>テクニカルレポート:https://github.com/Tencent-Hunyuan/Hunyuan-MT/blob/main/Hunyuan_MT_Technical_Report.pdf元ポスト:https://x.com/tencenthunyuan/status/1962466712378577300?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QBase Modelに対してまず一般的な事前学習を実施し、その後MTに特化した継続事前学習（モノリンガル/パラレルコーパスの利用）、事後学習（SFT, GRPO)を実施している模様。

<br>

継続事前学習では、最適なDataMixの比率を見つけるために、RegMixと呼ばれる手法を利用。Catastrophic Forgettingを防ぐために、事前学習データの20%を含めるといった施策を実施。

<br>



<br>

SFTでは2つのステージで構成されている。ステージ1は基礎的な翻訳力の強化と翻訳に関する指示追従能力の向上のために、Flores-200の開発データ(33言語の双方向の翻訳をカバー)、前年度のWMTのテストセット(English to XXをカバー）、Mandarin to Minority, Minority to Mandarinのcuratedな人手でのアノテーションデータ、DeepSeek-V3-0324での合成パラレルコーパス、general purpose/MT orientedな指示チューニングデータセットのうち20%を構成するデータで翻訳のinstructinoに関するモデルの凡化性能を高めるためキュレーションされたデータ、で学習している模様。パラレルコーパスはReference-freeな手法を用いてスコアを算出し閾値以下の低品質な翻訳対は除外している。ステージ2では、詳細が書かれていないが、少量でよりfidelityの高い約270kの翻訳対を利用した模様。また、先行研究に基づいて、many-shotのin-context learningを用いて、訓練データをさらに洗練させたとのこと（先行研究が引用されているのみで詳細な記述は無し）。また、複数の評価ラウンドでスコアの一貫性が無いサンプルは手動でアノテーション、あるいはverificationをして品質を担保している模様。

<br>



<br>

RLではGRPOを採用し、rewardとしてsemantic(2635), terminology(2649; ドメイン特有のterminologyを捉える), repetitionに基づいたrewardを採用している。最終的にSFT-&gt;RLで学習されたHuayuan-MT-7Bに対して、下記プロンプトを用いて複数のoutputを統合してより高品質な翻訳を出力するキメラモデルを同様のrewardを用いて学習する、といったpipelineになっている。

<br>



<br>

<img width="884" height="462" alt="Image" src="https://github.com/user-attachments/assets/dbb7a799-6304-4cfa-b75c-74b44fe39a2e">

<br>



<br>

<img width="921" height="279" alt="Image" src="https://github.com/user-attachments/assets/33b49ef7-b93b-4094-b83e-5931d2b411e5">関連:

<br>

・1220 

<br>

・2635 

<br>

・2636 

<br>

・2637

<br>

・2638

<br>

・2639

<br>

・2649関連: PLaMo翻訳

<br>

・2517

<br>



<br>

こちらはSFT-&gt;Iterative DPO-&gt;Model Mergeを実施し、翻訳に特化した継続事前学習はやっていないように見える。一方、SFT時点で独自のテンプレートを作成し、語彙の指定やスタイル、日本語特有の常体、敬体の指定などを実施できるように翻訳に特化したテンプレートを学習している点が異なるように見える。Hunyuanは多様な翻訳の指示に対応できるように学習しているが、PLaMo翻訳はユースケースを絞り込み、ユースケースに対する性能を高めるような特化型のアプローチをとるといった思想の違いが伺える。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/Coding.html">#Coding</a>
<a class="button" href="articles/Mathematics.html">#Mathematics</a>
<span class="issue_date">Issue Date: 2025-09-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2631">Nemotron-CC-v2, Nvidia, 2025.08</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/zeyuanallenzhu/status/1962119316427706828?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QCCだけでなく、数学やコーディングの事前学習データ、SFT styleの合成データセットも含まれている。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-08-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2620">LongCat-Flash-Chat, meituan-longcat, 2025.08</a>
<span class="snippet"><span>Comment</span>テクニカルレポート:https://github.com/meituan-longcat/LongCat-Flash-Chat/blob/main/tech_report.pdf元ポスト:https://x.com/rosinality/status/1961955926136832381?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QAgent周りのベンチで高性能なnon thinkingモデル。毎秒100+トークンの生成速度で、MITライセンス。Dynamic Activation...?関連:

<br>

・2621Dynamic Activation (activation paramが入力に応じて変化(全てのトークンをMoEにおいて均一に扱わない）することで効率化）は、下記を利用することで実現している模様

<br>



<br>

・2622しかし中国は本当に次々に色々な企業から基盤モデルが出てくるなぁ…すごい・2623 解説:https://x.com/nrehiew_/status/1962186876099739767?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q解説:https://x.com/gm8xx8/status/1962980770550628841?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2025-08-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2389">Diffusion Language Models are Super Data Learners, Jinjie Ni and the team, 2025.08</a>
<span class="snippet"><span>Comment</span>dLLMは学習データの繰り返しに強く、データ制約下においては十分な計算量を投入してepochを重ねると、性能向上がサチらずにARモデルを上回る。

<br>



<br>

<img src="https://github.com/user-attachments/assets/ff668aac-cbcd-48ed-a5d6-50d0fa381f5a" alt="image" loading="lazy">・2268

<br>

・追記: 上記研究の著者による本ポストで取り上げられたissueに対するclarification

<br>

　　・https://x.com/mihirp98/status/1954240474891653369?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>



<br>

でも同様の知見が得られている。

<br>

が、スレッド中で両者の違いが下記のように（x rollrng reviewなるものを用いて）ポストされており、興味がある場合は読むといいかも。（ところで、x rolling reviewとは、、？もしやLLMによる自動的な査読システム？）

<br>



<br>

<img src="https://github.com/user-attachments/assets/295dcd4b-2b81-4439-b117-94dcf6cce5a7" alt="image" loading="lazy">・1829

<br>



<br>

において、ARモデルではrepetitionは4回までがコスパ良いという話と比べると、dLLMにとんでもない伸び代があるような話に見える。個人的にはアーキテクチャのさらなる進化は興味深いが、ユーザが不完全な質問をLLMに投げた時に、LLMがユーザの意図が「不明な部分のcontextを質問を返すことによって補う」という挙動があると嬉しい気がするのだが、そういった研究はないのだろうか。

<br>



<br>

ただ、事前学習時点でそういったデータが含まれて知識として吸収され、かつmid/post-trainingでそういった能力を引き出すと言う両軸で取り組まないと、最悪膨大な計算資源を投じたものの「わからない！どういうこと！？」と返し続けるLLMが完成し全く役に立たない、ということになりそうで怖い。

<br>



<br>

gpt5が出た時に、「3.9と3.11はどちらが大きいですか？」というクエリを投げた際にいまだに「3.11」と回答してくる、みたいなポストが印象的であり、これはLLMが悪いと言うより、ユーザ側が算数としての文脈できいているのか、ソフトウェアのバージョンの文脈できいているのか、を指定していないことが原因であり、上記の回答はソフトウェアのバージョニングという文脈では正答となる。LLMが省エネになって、ユーザのデータを蓄積しまくって、一人一人に対してあなただけのLLM〜みたいな時代がくれば少しは変わるのだろうが、それでもユーザがプロファイルとして蓄積した意図とは異なる意図で質問しなければならないという状況になると、上記のような意図の取り違えが生じるように思う。

<br>

なのでやはりりLLM側が情報が足りん〜と思ったら適切なturn数で、最大限の情報をユーザから引き出せるような逆質問を返すみたいな挙動、あるいは足りない情報があったときに、いくつかの候補を提示してユーザ側に提示させる（e.g., 算数の話？それともソフトウェアの話？みたいな）、といった挙動があると嬉しいなぁ、感。

<br>



<br>

んでそこの部分の性能は、もしやるな、promptingでもある程度は実現でき、それでも全然性能足りないよね？となった後に、事前学習、事後学習でより性能向上します、みたいな流れになるのかなぁ、と想像するなどした。

<br>



<br>

しかしこういう話をあまり見ないのはなぜだろう？私の観測範囲が狭すぎる or 私のアイデアがポンコツなのか、ベンチマーク競争になっていて、そこを向上させることに業界全体が注力してしまっているからなのか、はたまた裏ではやられているけど使い物にならないのか、全然わからん。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/AttentionSinks.html">#AttentionSinks</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2358">gpt-oss-120b, OpenAI, 2025.08</a>
<span class="snippet"><span>Comment</span>blog:https://openai.com/index/introducing-gpt-oss/

<br>



<br>

HF:

<br>

https://huggingface.co/datasets/choosealicense/licenses/blob/main/markdown/apache-2.0.mdアーキテクチャで使われている技術まとめ:

<br>

・https://x.com/gneubig/status/1952799735900979219?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>

・https://x.com/yampeleg/status/1952875217367245195?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>

・https://x.com/adamzweiger/status/1952799642636148917?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>

・https://x.com/cwolferesearch/status/1956132685102887059?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>

  ・こちらにも詳細に論文がまとめられている上記ポスト中のアーキテクチャの論文メモリンク（管理人が追加したものも含む）

<br>

・Sliding Window Attention

<br>

  ・2388 

<br>

  ・2359

<br>

・MoE

<br>

  ・1754

<br>

・RoPE w/ YaRN

<br>

  ・1310

<br>

  ・2338

<br>

・Attention Sinks

<br>

  ・1861

<br>

    ・Attention Sinksの定義とその気持ちについてはこのメモを参照のこと。

<br>

  ・1860

<br>

    ・Attention Sinksが実際にどのように効果的に作用しているか？についてはこちらのメモを参照。

<br>

  ・1862

<br>

    ・https://x.com/gu_xiangming/status/1952811057673642227?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>

  ・Attention Sinkの導入により、decodei-onlyモデルの深い層でのrepresentationのover mixingを改善し、汎化性能を高め、promptに対するsensitivityを抑えていると考えられる。

<br>

・GQA

<br>

  ・1271

<br>

・SwiGLU

<br>

  ・1311-

<br>

・(Attentionの計算に利用する) SoftmaxへのLearned bias の導入 （によるスケーリング）

<br>

  ・1863 

<br>

  ・1866

<br>

  ・Softmaxはlong contextになると、attentionの分布が均一になり、重要な情報にattendする能力が下がるためスケーリングが必要で、そのために分母にlearnedなbiasを導入していると考えられる。Llamaや上記研究では分子に係数としてlearnableなパラメータを導入しているが、少し形式が違う。もしかしたら解釈が違うかもしれない。・group size 8でGQAを利用

<br>

・Context Windowは128k

<br>

・学習データの大部分は英語のテキストのみのデータセット

<br>

  ・STEM, Coding, general knowledgeにフォーカス

<br>

  ・https://openai.com/index/gpt-oss-model-card/

<br>



<br>

あとで追記する他Open Weight Modelとのベンチマークスコア比較:

<br>

・https://x.com/gneubig/status/1952795149584482665?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>

・https://x.com/artificialanlys/status/1952887733803991070?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>

・https://x.com/terryyuezhuo/status/1952829578130670053?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>

・https://x.com/artificialanlys/status/1952823565642023044?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>

  ・long context

<br>

・https://x.com/thienhn97/status/1953152808334852124?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>

  ・Multihop QA解説:

<br>

https://x.com/gm8xx8/status/1952915080229863761?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qlearned attention sinks, MXFP4の解説:

<br>

https://x.com/carrigmat/status/1952779877569978797?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QSink Valueの分析:

<br>

https://x.com/wenhaocha1/status/1952851897414762512?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qgpt-oss の使い方:

<br>

https://note.com/npaka/n/nf39f327c3bde?sub_rt=share_sb9fd064b2-338a-4f8d-953c-67e458658e39Qwen3との深さと広さの比較:

<br>

・2364Phi4と同じtokenizerを使っている？:

<br>

https://x.com/bgdidenko/status/1952829980389343387?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qpost-training / pre-trainingの詳細はモデルカード中に言及なし:

<br>

・https://x.com/teortaxestex/status/1952806676492689652?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>

・https://x.com/okoge_kaz/status/1952787196253265955?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qattention headsのsoftmaxの分母にlearnableなパラメータが導入されている:

<br>

https://x.com/okoge_kaz/status/1952785895352041784?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>



<br>

・1866

<br>



<br>

で得られている知見と同様に、long contextになった場合にsoftmaxの値が平坦になる問題に対して、learnableなパラメータを導入してスケーリングすることで対処しているのだと考えられる。使ってみた所見:

<br>

・https://x.com/imai_eruel/status/1952825403263046073?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>

・https://x.com/wenhuchen/status/1953100554793828406?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>

・https://x.com/jasondeanlee/status/1953031988635451556?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qライセンスに関して:

<br>



<br>

&gt; Apache 2.0 ライセンスおよび当社の gpt-oss 利用規約に基づくことで利用可能です。

<br>



<br>

引用元: https://openai.com/ja-JP/index/gpt-oss-model-card/

<br>



<br>

gpt-oss利用規約: https://github.com/openai/gpt-oss/blob/main/USAGE_POLICYcookbook全体:https://cookbook.openai.com/topic/gpt-ossgpt-oss-120bをpythonとvLLMで触りながら理解する:https://tech-blog.abeja.asia/entry/gpt-oss-vllm指示追従能力（IFEVal)が低いという指摘:

<br>

https://x.com/webbigdata/status/1962332061437706589?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<span class="issue_date">Issue Date: 2025-07-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2318">GLM-4.5: Reasoning, Coding, and Agentic Abililties, Zhipu AI Inc., 2025.07</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/scaling01/status/1949825490488795275?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QHF:https://huggingface.co/collections/zai-org/glm-45-687c621d34bda8c9e4bf503b詳細なまとめ:https://x.com/gm8xx8/status/1949879437547241752?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q関連:

<br>

・2128こちらでもMuon Optimizerが使われており、アーキテクチャ的にはGQAやMulti Token Prediction, QK Normalization, MoE, 広さよりも深さを重視の構造、みたいな感じな模様？

<br>

・2202</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/LLMServing.html">#LLMServing</a>
<a class="button" href="articles/SoftwareEngineering.html">#SoftwareEngineering</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2273">LLM Servingを支える技術, Kotoba Technologies, 2025.07</a>
<span class="snippet"><span>Comment</span>こちらも参照のこと:

<br>

・2263</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/Metrics.html">#Metrics</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/LLMServing.html">#LLMServing</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/SoftwareEngineering.html">#SoftwareEngineering</a>
<a class="button" href="articles/Parallelism.html">#Parallelism</a>
<a class="button" href="articles/Inference.html">#Inference</a>
<a class="button" href="articles/Batch.html">#Batch</a>
<span class="issue_date">Issue Date: 2025-07-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2263">LLM推論に関する技術メモ, iwashi.co, 2025.07</a>
<span class="snippet"><span>Comment</span>```

<br>

メモリ (GB) = P × (Q ÷ 8) × (1 + オーバーヘッド)

<br>



<br>

・P：パラメータ数（単位は10億）

<br>

・Q：ビット精度（例：16、32）、8で割ることでビットをバイトに変換

<br>

・オーバーヘッド（％）：推論中の追加メモリまたは一時的な使用量（例：KVキャッシュ、アクティベーションバッファ、オプティマイザの状態）

<br>

```

<br>



<br>

↑これ、忘れがちなのでメモ…関連（量子化関連研究）:

<br>

・2264

<br>

・1570

<br>

・1043すごいメモだ…勉強になります</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/Slide.html">#Slide</a>
<a class="button" href="articles/Japanese.html">#Japanese</a>
<a class="button" href="articles/SoftwareEngineering.html">#SoftwareEngineering</a>
<span class="issue_date">Issue Date: 2025-07-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2228">論文では語られないLLM開発において重要なこと Swallow Projectを通して, Kazuki Fujii, NLPコロキウム, 2025.07</a>
<span class="snippet"><span>Comment</span>独自LLM開発の私の想像など遥かに超える非常に困難な側面が記述されており、これをできるのはあまりにもすごいという感想を抱いた（小並感だけど本当にすごいと思う。すごいとしか言いようがない）</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Optimizer.html">#Optimizer</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/Stability.html">#Stability</a>
<span class="issue_date">Issue Date: 2025-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2195">Kimi K2: Open Agentic Intelligence, moonshotai, 2025.07</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/kimi_moonshot/status/1943687594560332025?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q1T-A32Bのモデル。さすがに高性能。

<br>



<br>

<img src="https://github.com/user-attachments/assets/39b524d3-6e22-456d-8d61-fcd22519d58d" alt="image" loading="lazy">

<br>



<br>

（追記） Reasoningモデルではないのにこの性能のようである。1T-A32Bのモデルを15.5Tトークン訓練するのに一度もtraining instabilityがなかったらしい

<br>

元ポスト:https://x.com/eliebakouch/status/1943689105721667885?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q関連:

<br>

・2188量子化したモデルが出た模様:

<br>

https://x.com/ivanfioravanti/status/1944069021709615119?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>



<br>

仕事早すぎるDeepSeek V3/R1とのアーキテクチャの違い:

<br>

https://x.com/rasbt/status/1944056316424577525?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>



<br>

MLAのヘッドの数が減り、エキスパートの数を増加させている解説ポスト:https://x.com/hillbig/status/1944902706747072678?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q利用されているOptimizer:

<br>

・22022つほどバグがあり修正された模様:

<br>

https://x.com/kimi_moonshot/status/1945050874067476962?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qchatbot arenaでOpenLLMの中でトップのスコア

<br>

元ポスト:https://x.com/lmarena_ai/status/1945866381880373490?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qテクニカルペーパーが公開:https://github.com/MoonshotAI/Kimi-K2/blob/main/tech_report.pdf

<br>



<br>

元ポスト:https://x.com/iscienceluvr/status/1947384629314396302?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qテクニカルレポートまとめ:https://x.com/scaling01/status/1947400424622866793?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q以下のような技術が使われている模様

<br>

・1937

<br>

・MLA 1621

<br>

・MuonCip

<br>

・MuonOptimizer 2202 

<br>

・QK-Clip

<br>

  ・参考（こちらはLayerNormを使っているが）: 1202

<br>

・RLVR

<br>

  ・1719 

<br>

・Self-Critique

<br>

  ・関連: 2274

<br>

  ・2017 

<br>

・Temperature Decay  

<br>

  ・最初はTemperatureを高めにした探索多めに、後半はTemperatureを低めにして効用多めになるようにスケジューリング

<br>

・Tool useのためのSynthetic Data

<br>



<br>

<img width="1058" height="336" alt="Image" src="https://github.com/user-attachments/assets/74eacdb2-8f64-4d53-b2d0-66df770f2e8b">Reward Hackingに対処するため、RLVRではなくpairwise comparisonに基づくself judging w/ critique を利用きており、これが非常に効果的な可能性があるのでは、という意見がある:

<br>

https://x.com/grad62304977/status/1953408751521632401?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Zero/FewShotLearning.html">#Zero/FewShotLearning</a>
<span class="issue_date">Issue Date: 2025-06-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2042">[Paper Note] Language Models are Unsupervised Multitask Learners, Radford+, OpenAI, 2019</a>
<span class="snippet"><span>Comment</span>今更ながら、GPT-2論文をメモってなかったので追加。

<br>



<br>

従来のモデルは特定のタスクを解くためにタスクごとに個別のモデルをFinetuningする必要があったが、大規模なWebTextデータ（Redditにおいて最低3つのupvoteを得たポストの外部リンクを収集）によって言語モデルを訓練し、モデルサイズをスケーリングさせることで、様々なタスクで高い性能を獲得でき、Zero-Shot task transfer, p\(output | input, task) , が実現できるよ、という話。

<br>



<br>

今ざっくり見返すと、Next Token Predictionという用語は論文中に出てきておらず、かつ "Language Modeling" という用語のみで具体的なlossは記述されておらず（当時はRNN言語モデルで広く学習方法が知られていたからだろうか？）、かつソースコードも学習のコードは提供されておらず、lossの定義も含まれていないように見える。

<br>



<br>

ソースコードのモデル定義:

<br>

https://github.com/openai/gpt-2/blob/master/src/model.pyL169</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/InstructionTuning.html">#InstructionTuning</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-05-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1953">Stanford Alpaca: An Instruction-following LLaMA Model, Taori +, 2023.03</a>
<span class="snippet"><span>Comment</span>今更ながらメモに追加。アカデミアにおけるOpenLLMに対するInstruction Tuningの先駆け的研究。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<span class="issue_date">Issue Date: 2025-03-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1834">言語モデルの物理学, 佐藤竜馬, 2025.03</a>
<span class="snippet"><span>Comment</span>必読</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/CVPR.html">#CVPR</a>
<a class="button" href="articles/Backbone.html">#Backbone</a>
<span class="issue_date">Issue Date: 2021-11-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/430">Deep Residual Learning for Image Recognition, He+, Microsoft Research, CVPR’16</a>
<span class="snippet"><span>Comment</span>ResNet論文

<br>



<br>

ResNetでは、レイヤーの計算する関数を、残差F(x)と恒等関数xの和として定義する。これにより、レイヤーが入力との差分だけを学習すれば良くなり、モデルを深くしても最適化がしやすくなる効果ぎある。数レイヤーごとにResidual Connectionを導入し、恒等関数によるショートカットができるようにしている。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/140301726-1d2e89e1-1d69-43d9-8d2b-0adb272e577a.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

ResNetが提案される以前、モデルを深くすれば表現力が上がるはずなのに、実際には精度が下がってしまうことから、理論上レイヤーが恒等関数となるように初期化すれば、深いモデルでも浅いモデルと同等の表現が獲得できる、と言う考え方を発展させた。

<br>



<br>



<br>



<br>

（ステートオブAIガイドに基づく）同じパラメータ数でより層を深くできる（Plainな構造と比べると層が1つ増える）Bottleneckアーキテクチャも提案している。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/140302452-649b0ea7-cce4-44c1-9e7d-b509ef8bca52.png" alt="image" loading="lazy">

<br>



<br>

今や当たり前のように使われているResidual Connectionは、層の深いネットワークを学習するために必須の技術なのだと再認識。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Slide.html">#Slide</a>
<span class="issue_date">Issue Date: 2019-11-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/329">事前学習言語モデルの動向 _ Survey of Pretrained Language Models, Kyosuke Nishida, 2019</a>
<span class="snippet"><span>Comment</span>[2019/06まで]

<br>



<br>

・ELMo（双方向2層LSTM言語モデル）

<br>



<br>

・GPT（left-to-rightの12層Transformer自己回帰言語モデル）

<br>



<br>

・BERT（24層のTransformer双方向言語モデル）

<br>



<br>

・MT-DNN（BERTの上にマルチタスク層を追加した研究）

<br>



<br>

・XLM（パラレル翻訳コーパスを用いてクロスリンガルに穴埋めを学習）

<br>



<br>

・TransformerXL（系列長いに制限のあった既存モデルにセグメントレベルの再帰を導入し長い系列を扱えるように）

<br>



<br>

・GPT-2（48層Transformerの自己回帰言語モデル）

<br>



<br>

・ERNIE 1.0（Baidu, エンティティとフレーズの外部知識を使ってマスクに利用）

<br>



<br>

・ERNIE（Tsinghua, 知識グラフの情報をfusionしたLM）

<br>



<br>

・Glover（ドメイン、日付、著者などを条件とした生成を可能としたGPT）

<br>



<br>

・MASS（Encoder-Decoder型の生成モデルのための事前学習）

<br>



<br>

・UniLM（Sequence-to-Sequenceを可能にした言語モデル）

<br>



<br>

・XLNet（自己回帰（単方向）モデルと双方向モデルの両方の利点を得ることを目指す）

<br>



<br>



<br>



<br>

[2019/07~]

<br>



<br>

・SpanBERT（i.i.dではなく範囲でマスクし、同時に範囲の境界も予測する）

<br>



<br>

・ERNIE 2.0（Baidu, マルチタスク事前学習; 単語レベル・構造レベル・意味レベル）

<br>



<br>

・RoBERTa（BERTと同じ構造で工夫を加えることで性能向上）

<br>



<br>

　・より大きなバッチサイズを使う（256から8192）

<br>



<br>

　・より多くのデータを使う（16GBから160GB）

<br>



<br>

　・より長いステップ数の学習をする（BERT換算で16倍）

<br>



<br>

　・次文予測（NSP）は不要

<br>



<br>

　→ GLUEでBERT, XLNetをoutperform

<br>



<br>

・StructBERT (ALICE, NSPに代わる学習の目的関数を工夫)

<br>



<br>

　・マスクした上で単語の順番をシャッフルし元に戻す

<br>



<br>

　・ランダム・正順・逆順の3種類を分類

<br>



<br>

　→ BERTと同サイズ、同データでBERT, RoBERTa超え

<br>



<br>

・DistilBERT（蒸留により、12層BERTを6層に小型化（40%減））

<br>



<br>

　・BERTの出力を教師として、生徒が同じ出力を出すように学習

<br>



<br>

　・幅（隠れ層）サイズを減らすと、層数を経あｒスよりも悪化

<br>



<br>

　→ 推論は60%高速化、精度は95%程度を保持

<br>



<br>

・Q8BERT（精度を落とさずにfine-tuning時にBERTを8bit整数に量子化）

<br>



<br>

　・Embedding, FCは8bit化、softmax, LNorm, GELUは32bitをキープ

<br>



<br>

　→ モデルサイズ1/4, 速度3.7倍

<br>



<br>

・CTRL（条件付き言語モデル）

<br>



<br>

　・条件となる制御テキストを本文の前に与えて学習

<br>



<br>

　・48層/1280次元Transformer（パラメータ数1.6B）

<br>



<br>

・MegatronLM（72層、隠れ状態サイズ3072、長さ1024; BERTの24倍サイズ）

<br>



<br>

・ALBERT（BERTの層のパラメータをすべて共有することで学習を高速化; 2020年あたりのデファクト）

<br>



<br>

　・Largeを超えたモデルは学習が難しいため、表現は落ちるが学習しやすくした

<br>



<br>

　・単語埋め込みを低次元にすることでパラメータ数削減

<br>



<br>

　・次文予測を、文の順序入れ替え判定に変更

<br>



<br>

　→ GLUE, RACE, SQuADでSoTAを更新

<br>



<br>

・T5（NLPタスクをすべてtext-to-textとして扱い、Enc-Dec Transformerを745GBコーパスで事前学習して転移する）

<br>



<br>

　・モデルはEncoder-DecoderのTransformer

<br>



<br>

　・学習タスクをエンコーダ・デコーダに合わせて変更

<br>



<br>

　・エンコーダ側で範囲を欠落させて、デコーダ側で予測

<br>



<br>

　→ GLUE, SuperGLUE, SQuAD1.1, CNN/DMでSoTA更新

<br>



<br>

・BART（Seq2Seqの事前学習として、トークンマスク・削除、範囲マスク、文の入れ替え、文書の回転の複数タスクで学習）

<br>



<br>

　→ CNN/DMでT5超え、WMT'16 RO-ENで逆翻訳を超えてSoTAELMo, GPT, BERT, GPT-2, XLNet, RoBERTa, DistilBERT, ALBERT, T5あたりは良く見るような感各データセットでの各モデルの性能も後半に記載されており興味深い。

<br>



<br>



<br>



<br>

ちなみに、CNN/DailyMail Datasetでは、T5, BARTあたりがSoTA。

<br>



<br>

R2で比較すると

<br>



<br>

　・Pointer-Generator + Coverage Vectorが17,28

<br>



<br>

　・LEAD-3が17.62

<br>



<br>

　・BARTが21.28

<br>



<br>

　・T5が21.55

<br>



<br>

となっている</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/Library.html">#Library</a>
<span class="issue_date">Issue Date: 2019-09-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/324">Implicit</a>
<span class="snippet"><span>Comment</span>Implicitデータに対するCollaborative Filtering手法がまとまっているライブラリ

<br>



<br>

Bayesian Personalized Ranking, Logistic Matrix Factorizationなどが実装。Implicitの使い方はこの記事がわかりやすい：

<br>



<br>

https://towardsdatascience.com/building-a-collaborative-filtering-recommender-system-with-clickstream-data-dffc86c8c65ALSの元論文の日本語解説

<br>



<br>

https://cympfh.cc/paper/WRMF</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<span class="issue_date">Issue Date: 2019-04-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/308">Recommender System Datasets, Julian McAuley</a>
<span class="snippet"><span>Comment</span>Recommender Systems研究に利用できる各種データセットを、Julian McAuley氏がまとめている。

<br>



<br>

氏が独自にクロールしたデータ等も含まれている。

<br>



<br>

非常に有用。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/Explanation.html">#Explanation</a>
<span class="issue_date">Issue Date: 2019-01-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/299">Designing and Evaluating Explanations for Recommender Systems, Tintarev+, Recommender Systems Handbook, 2011</a>
<span class="snippet"><span>Comment</span>Recommender Systems HandbookのChapter。162 のSurveyと同じ著者による執筆。

<br>



<br>

推薦のExplanationといえばこの人というイメージ。D論：http://navatintarev.com/papers/Nava%20Tintarev_PhD_Thesis_(2010).pdf</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/AdaptiveLearning.html">#AdaptiveLearning</a>
<a class="button" href="articles/StudentPerformancePrediction.html">#StudentPerformancePrediction</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<span class="issue_date">Issue Date: 2018-12-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/297">Deep Knowledge Tracing, Piech+, NIPS, 2015</a>
<span class="snippet"><span>Comment</span>Knowledge Tracingタスクとは：

<br>



<br>

　特定のlearning taskにおいて、生徒によってとられたインタラクションの系列x0, ..., xtが与えられたとき、次のインタラクションxt+1を予測するタスク

<br>



<br>

　典型的な表現としては、xt={qt, at}, where qt=knowledge component (KC) ID (あるいは問題ID)、at=正解したか否か

<br>



<br>

　モデルが予測するときは、qtがgivenな時に、atを予測することになる

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/50377468-2989c580-0661-11e9-97c9-328056fbd692.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Contribution:

<br>



<br>

　1. A novel way to encode student interactions as input to a recurrent neural network.

<br>



<br>

　2. A 25% gain in AUC over the best previous result on a knowledge tracing benchmark.

<br>



<br>

　3. Demonstration that our knowledge tracing model does not need expert annotations.

<br>



<br>

　4. Discovery of exercise influence and generation of improved exercise curricula.

<br>



<br>



<br>



<br>

モデル：

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/50377473-432b0d00-0661-11e9-97e1-a60a68a6ef32.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Inputは、ExerciseがM個あったときに、M個のExerciseがcorrectか否かを表すベクトル（長さ2Mベクトルのone-hot）。separateなrepresentationにするとパフォーマンスが下がるらしい。

<br>



<br>

Output ytの長さは問題数Mと等しく、各要素は、生徒が対応する問題を正答する確率。

<br>



<br>



<br>



<br>

InputとしてExerciseを用いるか、ExerciseのKCを用いるかはアプリケーション次第っぽいが、典型的には各スキルの潜在的なmasteryを測ることがモチベーションなのでKCを使う。

<br>



<br>



<br>



<br>

（もし問題数が膨大にあるような設定の場合は、各問題-正/誤答tupleに対して、random vectorを正規分布からサンプリングして、one-hot high-dimensional vectorで表現する。）

<br>



<br>



<br>



<br>

hidden sizeは200, mini-batch sizeは100としている。

<br>



<br>



<br>



<br>

[Educational Applicationsへの応用]

<br>



<br>



<br>



<br>

生徒へ最適なパスの学習アイテムを選んで提示することができること

<br>



<br>

　生徒のknowledge stateを予測し、その後特定のアイテムを生徒にassignすることができる。たとえば、生徒が50個のExerciseに回答した場合、生徒へ次に提示するアイテムを計算するだけでなく、その結果期待される生徒のknowledge stateも推測することができる

<br>



<br>



<br>



<br>

Exercises間の関係性を見出すことができる

<br>



<br>

　y\( j | i )を考える。y\( j | i )は、はじめにexercise iを正答した後に、second time stepでjを正答する確率。これによって、pre-requisiteを明らかにすることができる。

<br>



<br>



<br>



<br>

[評価]

<br>



<br>

3種類のデータセットを用いる。

<br>



<br>

　1. simulated Data

<br>



<br>

　　2000人のvirtual studentを作り、1〜5つのコンセプトから生成された、50問を、同じ順番で解かせた。このとき、IRTモデルを用いて、シミュレーションは実施した。このとき、hidden stateのラベルには何も使わないで、inputは問題のIDと正誤データだけを与えた。さらに、2000人のvirtual studentをテストデータとして作り、それぞれのコンセプト（コンセプト数を1〜5に変動させる）に対して、20回ランダムに生成したデータでaccuracyの平均とstandard errorを測った。

<br>



<br>

　2. Khan Academy Data

<br>



<br>

　　1.4MのExerciseと、69の異なるExercise Typeがあり、47495人の生徒がExerciseを行なっている。

<br>



<br>

　　PersonalなInformationは含んでいない。

<br>



<br>

　3. Assistsments bemchmark Dataset

<br>



<br>

　　2009-2011のskill builder public benchmark datasetを用いた。Assistmentsは、online tutorが、数学を教えて、教えるのと同時に生徒を評価するような枠組みである。

<br>



<br>



<br>



<br>

それぞれのデータセットに対して、AUCを計算。

<br>



<br>

ベースラインは、BKTと生徒がある問題を正答した場合の周辺確率？

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/50377495-b0d73900-0661-11e9-9ca2-1cb97393d698.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/50377501-b92f7400-0661-11e9-87ce-9f836c860209.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

simulated dataの場合、問題番号5がコンセプト1から生成され、問題番号22までの問題は別のコンセプトから生成されていたにもかかわらず、きちんと二つの問題の関係をとらえられていることがわかる。

<br>



<br>

Khan Datasetについても同様の解析をした。これは、この結果は専門家が見たら驚くべきものではないかもしれないが、モデルが一貫したものを学習したと言える。

<br>



<br>



<br>



<br>

[Discussion]

<br>



<br>

提案モデルの特徴として、下記の２つがある：

<br>



<br>



<br>



<br>

専門家のアノテーションを必要としない（concept patternを勝手に学習してくれる）

<br>



<br>

ベクトル化された生徒のinputであれば、なんでもoperateすることができる

<br>



<br>

drawbackとしては、大量のデータが必要だということ。small classroom environmentではなく、online education environmentに向いている。

<br>



<br>

今後の方向性としては、

<br>



<br>

・incorporate other feature as inputs (such as time taken)

<br>



<br>

・explore other educational impacts (hint generation, dropout prediction)

<br>



<br>

・validate hypotheses posed in education literature (such as spaced repetition, modeling how students forget)

<br>



<br>

・open-ended programmingとかへの応用とか（proramのvectorizationの方法とかが最近提案されているので）

<br>



<br>

などがある。knewtonのグループが、DKTを既存手法であるIRTの変種やBKTの変種などでoutperformすることができることを示す：

<br>



<br>

https://arxiv.org/pdf/1604.02336.pdf

<br>



<br>



<br>



<br>

vanillaなDKTはかなりナイーブなモデルであり、今後の伸びが結構期待できると思うので、単純にoutperformしても、今後の発展性を考えるとやはりDKTには注目せざるを得ない感DKT元論文では、BKTを大幅にoutperformしており、割と衝撃的な結果だったようだが、

<br>



<br>

後に論文中で利用されているAssistmentsデータセット中にdupilcate entryがあり、

<br>



<br>

それが原因で性能が不当に上がっていることが判明。

<br>



<br>



<br>



<br>

結局DKTの性能的には、BKTとどっこいみたいなことをRyan Baker氏がedXで言っていた気がする。Deep Knowledge TracingなどのKnowledge Tracingタスクにおいては、

<br>



<br>

基本的に問題ごとにKnowledge Component(あるいは知識タグ, その問題を解くのに必要なスキルセット）が付与されていることが前提となっている。

<br>



<br>

ただし、このような知識タグを付与するには専門家によるアノテーションが必要であり、

<br>



<br>

適用したいデータセットに対して必ずしも付与されているとは限らない。

<br>



<br>



<br>



<br>

このような場合は、DKTは単なる”問題”の正答率予測モデルとして機能させることしかできないが、

<br>



<br>

知識タグそのものもNeural Networkに学習させてしまおうという試みが行われている：

<br>



<br>

https://www.jstage.jst.go.jp/article/tjsai/33/3/33_C-H83/_article/-char/jaDKTに関する詳細な説明が書かれているブログポスト：

<br>



<br>

expectimaxアルゴリズムの説明や、最終的なoutput vector y_i の図解など、説明が省略されガチなところが詳細に書いてあって有用。（英語に翻訳して読むと良い）

<br>



<br>

https://hcnoh.github.io/2019-06-14-deep-knowledge-tracingこちらのリポジトリではexpectimaxアルゴリズムによってvirtualtutorを実装している模様。

<br>



<br>

詳細なレポートもアップロードされている。

<br>



<br>

https://github.com/alessandroscoppio/VirtualIntelligentTutorDKTのinputの次元数が 2  num_skills, outputの次元数がnum_skillsだと明記されているスライド。

<br>



<br>

元論文だとこの辺が言及されていなくてわかりづらい・・・

<br>



<br>

http://gdac.uqam.ca/Workshop@EDM20/slides/LSTM_tutorial_Application.pdf

<br>



<br>

http://gdac.uqam.ca/Workshop@EDM20/slides/LSTM_Tutorial.pdf

<br>



<br>



<br>



<br>

こちらのページが上記チュートリアルのページ

<br>



<br>

http://gdac.uqam.ca/Workshop@EDM20/</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/AdaptiveLearning.html">#AdaptiveLearning</a>
<a class="button" href="articles/EducationalDataMining.html">#EducationalDataMining</a>
<a class="button" href="articles/LearningAnalytics.html">#LearningAnalytics</a>
<span class="issue_date">Issue Date: 2018-12-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/294">Educational Data Mining and Learning Analytics, Baker+, 2014</a>
<span class="snippet"><span>Comment</span>Ryan BakerらによるEDM Survey</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/Classic.html">#Classic</a>
<a class="button" href="articles/ContextAware.html">#ContextAware</a>
<span class="issue_date">Issue Date: 2018-12-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/287">Context-Aware Recommender Systems, Adomavicius+, Recommender Systems Handbook, 2011</a>
<span class="snippet"><span>Comment</span>Context-aware Recsysのパイオニア的研究通常のuser/item paradigmを拡張して、いかにコンテキストの情報を考慮するかを研究。

<br>



<br>



<br>



<br>

コンテキスト情報は、

<br>



<br>

Explicit: ユーザのマニュアルインプットから取得

<br>



<br>

Implicit: 自動的に取得

<br>



<br>

inferred: ユーザとツールやリソースのインタラクションから推測（たとえば現在のユーザのタスクとか）

<br>



<br>



<br>



<br>

いくつかの異なるパラダイムが提案された：

<br>



<br>



<br>



<br>

1. recommendation via context-driven  querying and search approach

<br>



<br>

　コンテキストの情報を、特定のリポジトリのリソース（レストラン）に対して、クエリや検索に用いる。そして、best matchingなリソースを(たとえば、現在開いているもっとも近いレストランとか)をユーザに推薦。

<br>



<br>



<br>



<br>

2. Contextual preference elicitation and estimation approach

<br>



<br>

　こっちは2012年くらいの主流。contextual user preferencesをモデル化し学習する。データレコードをしばしば、&lt;user, item, context, rating&gt;の形式で表現する。これによって、特定のアイテムが特定のコンテキストでどれだけ好まれたか、が評価できるようになる。

<br>



<br>



<br>



<br>

3. contextual prefiltering approach

<br>



<br>

　contextualな情報を（学習したcontextualなpreferenceなどを）、tradittionalなrecommendation algorithmを適用する前にデータのフィルタリングに用いる。

<br>



<br>



<br>



<br>

4. contextual postfiltering approach

<br>



<br>

　entire setから推薦を作り、あとでcontextの情報を使ってsetを整える。

<br>



<br>



<br>



<br>

5. Contextual modeling

<br>



<br>

　contextualな情報を、そのままrecommendationの関数にぶちこんでしまい、アイテムのratingのexplicitなpredictorとして使う。

<br>



<br>



<br>



<br>

3, 4はtraditionalな推薦アルゴリズムが適用できる。

<br>



<br>

1,2,5はmulti-dimensionalな推薦アルゴリズムになる。heuristic-based, model-based approachesが述べられているらしい。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Classic.html">#Classic</a>
<a class="button" href="articles/AdaptiveLearning.html">#AdaptiveLearning</a>
<a class="button" href="articles/LearningStyle.html">#LearningStyle</a>
<span class="issue_date">Issue Date: 2018-12-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/284">LEARNING AND TEACHING STYLES IN ENGINEERING EDUCATION, Felder, Engr. Education, 78（7）, 674–681 （1988）</a>
<span class="snippet"><span>Comment</span>LearningStyleに関して研究している古典的な研究。

<br>



<br>

context-aware recsysの研究初期の頃は、だいたいはこのFelder-Silverman Theoryというのをベースに研究されていたらしい。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Slide.html">#Slide</a>
<span class="issue_date">Issue Date: 2018-01-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/235">自然言語処理のためのDeep Learning, Yuta Kikuchi</a>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/CollaborativeFiltering.html">#CollaborativeFiltering</a>
<a class="button" href="articles/MatrixFactorization.html">#MatrixFactorization</a>
<span class="issue_date">Issue Date: 2018-01-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/225">Collaborative filtering for implicit feedback datasets, Hu+, International Conference on Data Mining, 2008</a>
<span class="snippet"><span>Comment</span>Implicit Feedbackなデータに特化したMatrix Factorization (MF)、Weighted Matrix Factorization (WMF)を提案。

<br>



<br>

ユーザのExplicitなFeedback（ratingやlike, dislikeなど）がなくても、MFが適用可能。

<br>



<br>



<br>



<br>

目的関数は下のようになっている。

<br>



<br>

通常のMFでは、ダイレクトにrating r_{ui}を予測したりするが、WMFでは r_{ui}をratingではなく、たとえばユーザuがアイテムiを消費した回数などに置き換え、binarizeした数値p_{ui}を目的関数に用いる。

<br>



<br>

このとき、itemを消費した回数が多いほど、そのユーザはそのitemを好んでいると仮定し、そのような事例については重みが高くなるようにc_{ui}を計算し、目的関数に導入している。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34815894-79edc89e-f6f6-11e7-9be5-0beacd724c23.png" alt="image" loading="lazy">

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34815905-80e9f10e-f6f6-11e7-95bb-4ff0506134ad.png" alt="image" loading="lazy">

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34815909-85c803dc-f6f6-11e7-87a8-8b7007f73524.png" alt="image" loading="lazy">

<br>



<br>

日本語での解説: https://cympfh.cc/paper/WRMFImplicit 324 でのAlternating Least Square (ALS)という手法が、この手法の実装に該当する。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/CollaborativeFiltering.html">#CollaborativeFiltering</a>
<a class="button" href="articles/MatrixFactorization.html">#MatrixFactorization</a>
<a class="button" href="articles/SIGKDD.html">#SIGKDD</a>
<span class="issue_date">Issue Date: 2018-01-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/221">Collaborative Deep Learning for Recommender Systems Wang+, KDD’15</a>
<span class="snippet"><span>Comment</span>Rating Matrixからuserとitemのlatent vectorを学習する際に、Stacked Denoising Auto Encoder（SDAE）によるitemのembeddingを活用する話。

<br>



<br>

Collaborative FilteringとContents-based Filteringのハイブリッド手法。

<br>



<br>

Collaborative FilteringにおいてDeepなモデルを活用する初期の研究。

<br>



<br>



<br>



<br>

通常はuser vectorとitem vectorの内積の値が対応するratingを再現できるように目的関数が設計されるが、そこにitem vectorとSDAEによるitemのEmbeddingが近くなるような項（3項目）、SDAEのエラー（4項目）を追加する。

<br>



<br>



<br>



<br>

（3項目の意義について、解説ブログより）アイテム i に関する潜在表現 vi は学習データに登場するものについては推定できるけれど，未知のものについては推定できない．そこでSDAEの中間層の結果を「推定したvi」として「真の」 vi にできる限り近づける，というのがこの項の気持ち

<br>



<br>



<br>



<br>

cite-ulikeデータによる論文推薦、Netflixデータによる映画推薦で評価した結果、ベースライン（Collective Matrix Factorization 222 , SVDFeature 223 , DeepMusic 224 , Collaborative Topic Regresison 226 ）をoutperform。

<br>



<br>



<br>

（下記は管理人が過去に作成した論文メモスライドのスクショ）

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34813194-58142a60-f6ec-11e7-938e-34b7d0cfb930.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34813227-786b9640-f6ec-11e7-8713-940433dc9e8f.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34813243-87832d28-f6ec-11e7-8371-fa60a54a1ba6.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34813251-91d5896a-f6ec-11e7-94ec-3b2c225ddf9a.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34813259-9b18b5e2-f6ec-11e7-98ae-1b5323b3e8b3.png" alt="image" loading="lazy">

<br>



<br>

解説ブログ：http://d.hatena.ne.jp/repose/20150531/1433004688</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Alignment.html">#Alignment</a>
<a class="button" href="articles/SIGIR.html">#SIGIR</a>
<span class="issue_date">Issue Date: 2018-01-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/220">The Decomposition of Human-Written Summary Sentences. Hongyan Jing et al. SIGIR’99.</a>
<span class="snippet"><span>Comment</span>参照要約 ・原文書対が与えられた時に、参照要約中の単語と原文書中の単語のアライメントをとるHMMベースな手法を提案。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34812500-2d1d7d32-f6e9-11e7-8d9d-723804236081.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

outputはこんな感じ。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Multi.html">#Multi</a>
<a class="button" href="articles/Single.html">#Single</a>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/Document.html">#Document</a>
<a class="button" href="articles/Unsupervised.html">#Unsupervised</a>
<a class="button" href="articles/GraphBased.html">#GraphBased</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Extractive.html">#Extractive</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/215">LexRank: Graph-based Lexical Centrality as Salience in Text Summarization, Erkan+, Journal of Artificial Intelligence Research, 2004</a>
<span class="snippet"><span>Comment</span>代表的なグラフベースな(Multi) Document Summarization手法。

<br>



<br>

ほぼ 214 と同じ手法。

<br>



<br>



<br>



<br>

2種類の手法が提案されている：

<br>



<br>



<br>



<br>

 [LexRank] tf-idfスコアでsentenceのbag-of-wordsベクトルを作り、cosine similarityを計算し閾値以上となったsentenceの間にのみedgeを張る（重みは確率的に正規化）。その後べき乗法でPageRank。

<br>



<br>

 [ContinousLexRank] tf-idfスコアでsentenceのbag-of-wordsベクトルを作り、cosine similarityを用いてAffinity Graphを計算し、PageRankを適用（べき乗法）。

<br>



<br>



<br>



<br>

DUC2003, 2004（MDS）で評価。

<br>



<br>

Centroidベースドな手法をROUGE-1の観点でoutperform。

<br>



<br>

document clusterの17%をNoisyなデータにした場合も実験しており、Noisyなデータを追加した場合も性能劣化が少ないことも示している。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/174">推薦システムのアルゴリズム, 神嶌, 2016</a>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/CollaborativeFiltering.html">#CollaborativeFiltering</a>
<a class="button" href="articles/Novelty.html">#Novelty</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/46">Discovery-oriented Collaborative Filtering for Improving User Satisfaction, Hijikata+, IUI’09</a>
<span class="snippet"><span>Comment</span>・従来のCFはaccuracyをあげることを目的に研究されてきたが，ユーザがすでに知っているitemを推薦してしまう問題がある．おまけに（推薦リスト内のアイテムの観点からみた）diversityも低い．このような推薦はdiscoveryがなく，user satisfactionを損ねるので，ユーザがすでに何を知っているかの情報を使ってよりdiscoveryのある推薦をCFでやりましょうという話．

<br>



<br>

・特徴としてユーザのitemへのratingに加え，そのitemをユーザが知っていたかどうかexplicit feedbackしてもらう必要がある．

<br>



<br>

・手法は単純で，User-based，あるいはItem-based CFを用いてpreferenceとあるitemをユーザが知っていそうかどうかの確率を求め，それらを組み合わせる，あるいはrating-matrixにユーザがあるitemを知っていたか否かの数値を組み合わせて新たなmatrixを作り，そのmatrix上でCFするといったもの．

<br>



<br>

・offline評価の結果，通常のCF，topic diversification手法と比べてprecisionは低いものの，discovery ratioとprecision(novelty)は圧倒的に高い．

<br>



<br>

・ユーザがitemを知っていたかどうかというbinary ratingはユーザに負荷がかかるし，音楽推薦の場合previewがなければそもそも提供されていないからratingできないなど，必ずしも多く集められるデータではない．そこで，データセットのratingの情報を25%, 50%, 75%に削ってratingの数にbiasをかけた上で実験をしている．その結果，事前にratingをcombineし新たなmatrixを作る手法はratingが少ないとあまりうまくいかなかった．

<br>



<br>

・さらにonlineでuser satisfaction（3つの目的のもとsatisfactionをratingしてもらう　1. purchase 2. on-demand-listening 3. discovery）を評価した. 結果，purchaseとdiscoveryにおいては，ベースラインを上回った．ただし，これは推薦リスト中の満足したitemの数の問題で，推薦リスト全体がどうだった

<br>



<br>

　かと問われた場合は，ベースラインと同等程度だった．重要論文</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/PersonalizedDocumentSummarization.html">#PersonalizedDocumentSummarization</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/NAACL.html">#NAACL</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/17">A Study for Documents Summarization based on Personal Annotation, HLT-NAACL-DUC’03, [Zhang+, 2003], 2003.05</a>
<span class="snippet"><span>Comment</span>（過去に管理人が作成したスライドでの論文メモのスクショ）

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34402434-d521f19e-ebe4-11e7-82cf-2f3452fa4014.png" alt="image" loading="lazy">

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34402437-dbd6db9e-ebe4-11e7-8954-3a0754929ad3.png" alt="image" loading="lazy">

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34402439-e13bff9c-ebe4-11e7-97b6-dfeb97f7e6af.png" alt="image" loading="lazy">

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34402446-e8578e2c-ebe4-11e7-970a-f9db5ff0c548.png" alt="image" loading="lazy">

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34402454-f0c8867e-ebe4-11e7-9c4a-64a727388402.png" alt="image" loading="lazy">

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34402465-fa26e788-ebe4-11e7-82cd-80df4eb5e2b5.png" alt="image" loading="lazy">

<br>



<br>

重要論文だと思われる。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/PersonalizedDocumentSummarization.html">#PersonalizedDocumentSummarization</a>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/8">User-model based personalized summarization, Diaz+, Information Processing and Management 2007.11</a>
<span class="snippet"><span>Comment</span>PDSの先駆けとなった重要論文。必ずreferすべき。</span>
<button onclick="hideContent(0)" style="display: none;">hide</button>
</div>


    </div>

</article>
<div class="post-nav">
<a class="previous" href="/paper_notes/articles/Search.html" title="Searchに関する論文・技術記事メモの一覧">Searchに関する論文・技術記事メモの一覧</a><a class="next" href="/paper_notes/articles/Self-SupervisedLearning.html" title="Self-SupervisedLearningに関する論文・技術記事メモの一覧">Self-SupervisedLearningに関する論文・技術記事メモの一覧</a>
</div>
<div class="post-related">
      <div>Related Articles</div>
      <ul>
        <li class="">
          <a class="post-link" href="/paper_notes/articles/VerifiableRewards.html" title="VerifiableRewardsに関する論文・技術記事メモの一覧">
            VerifiableRewardsに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/Regularization.html" title="Regularizationに関する論文・技術記事メモの一覧">
            Regularizationに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/Personality.html" title="Personalityに関する論文・技術記事メモの一覧">
            Personalityに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/StructuredData.html" title="StructuredDataに関する論文・技術記事メモの一覧">
            StructuredDataに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
</ul>
    </div>
<div class="post-comments"></div></section>
</div>


  </section>
  <section class="sidebar" style="margin-left: 15px;">
    <!-- Get sidebar items --><style type="text/css" media="screen">
.post-menu ul {
  list-style: none;
  padding: 0;
  margin: 0;
}
</style>

<div class="post-menu">
  <div class="post-menu-title">TOC</div>
  <div class="post-menu-content"></div>
</div>

<script>
  function generateContent() {
    var menu = document.querySelector(".post-menu");
    var menuContent =  menu.querySelector(".post-menu-content");
    var headings = document.querySelector(".post-content").querySelectorAll("h2, h3, h4, h5, h6");

    // Hide menu when no headings
    if (headings.length === 0) {
      return menu.style.display = "none";
    }

    // Generate post menu
    var menuHTML = '';
    for (var i = 0; i < headings.length; i++) {
      var h = headings[i];
      menuHTML += (
        '<li class="h-' + h.tagName.toLowerCase() + '">'
        + '<a href="#h-' + h.getAttribute('id') + '">' + h.textContent + '</a></li>');
    }

    menuContent.innerHTML = '<ul>' + menuHTML + '</ul>';

    // The header element
    var header = document.querySelector('header.site-header');

    function doMenuCollapse(index, over_items) {
      var items = menuContent.firstChild.children;

      if (over_items == undefined) {
        over_items = 20;
      }

      if (items.length < over_items) {
        return;
      }

      var activeItem = items[index];
      var beginItem = activeItem
      var endItem = activeItem
      var beginIndex = index;
      var endIndex = index + 1;
      while (beginIndex >= 0
        && !items[beginIndex].classList.contains('h-h2')) {
        beginIndex -= 1;
      }
      while (endIndex < items.length
        && !items[endIndex].classList.contains('h-h2')) {
        endIndex += 1;
      }
      for (var i = 0; i < beginIndex; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
      for (var i = beginIndex + 1; i < endIndex; i++) {
        item = items[i]
        // if (!item.classList.contains('h-h2')) {
          item.style.display = '';
        // }
      }
      for (var i = endIndex; i < items.length; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
    }

    // Init menu collapsed
    doMenuCollapse(-1);

    // Active the menu item
    window.addEventListener('scroll', function (event) {
      var lastActive = menuContent.querySelector('.active');
      var changed = true;
      var activeIndex = -1;
      for (var i = headings.length - 1; i >= 0; i--) {
        var h = headings[i];
        var headingRect = h.getBoundingClientRect();
        var headerRect = header.getBoundingClientRect();
        var headerTop = Math.floor(headerRect.top);
        var headerHeight = Math.floor(headerRect.height);
        var headerHeight = headerTop + headerHeight + 20;
        if (headingRect.top <= headerHeight) {
          var id = 'h-' + h.getAttribute('id');
          var a = menuContent.querySelector('a[href="#' + id  + '"]');
          var curActive = a.parentNode;
          if (curActive) {
            curActive.classList.add('active');
            activeIndex = i;
          }
          if (lastActive == curActive) {
            changed = false;
          }
          break;
        }
      }
      if (changed) {
        if (lastActive) {
          lastActive.classList.remove('active');
        }
        doMenuCollapse(activeIndex);
      }
      event.preventDefault();
    });
  }
  generateContent();
</script>
</section>
</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/paper_notes/"></data>

  <div class="wrapper">
    <div class="site-footer-inner">
<div>Copyright © 2023-current AkihikoWATANABE. The header images and any thumbnail images for the posts were generated by ChatGPT's DALL-E3.</div>
      <div>Powered by <a title="Jekyll is a simple, blog-aware, static site
      generator." href="https://jekyllrb.com/">Jekyll</a> &amp; <a title="Yat, yet
      another theme." href="https://github.com/jeffreytse/jekyll-theme-yat">Yat Theme</a>.</div>
      <div class="footer-col rss-subscribe">Subscribe <a href="/paper_notes/feed.xml">via RSS</a>
</div>
    </div>
  </div>
</footer>
</body>
</html>
