<!DOCTYPE html>
<html lang="ja"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-translate-customization" content="108d9124921d80c3-80e20d618ff053c8-g4f02ec6f3dba68b7-c"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Pretrainingに関する論文・技術記事メモの一覧 | わたしのべんきょうノート</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Pretrainingに関する論文・技術記事メモの一覧" />
<meta name="author" content="AkihikoWATANABE" />
<meta property="og:locale" content="ja" />
<meta name="description" content="Pretraining #Pocket#NLP#LanguageModel#DiffusionModel#Scaling Laws#read-laterIssue Date: 2025-07-22 Paper Note Diffusion Beats Autoregressive in Data-Constrained Settings, Mihir Prabhudesai+, arXiv25 Summaryマスク付き拡散モデルは、データ制約のある設定で自己回帰（AR）モデルを大幅に上回ることを発見。拡散モデルはデータを効果的に活用し、検証損失を低下させ、下流のパフォーマンスを向上させる。新しいスケーリング法則を見つけ、拡散がARを上回る臨界計算閾値を導出。データがボトルネックの場合、拡散モデルはARの魅力的な代替手段となる。 Comment元ポスト:https://x.com/iscienceluvr/status/1947567159045197924?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-QいつかdLLMの時代きそうだなあ #Pocket#NLP#LanguageModel#MulltiModal#Scaling Laws#DataMixture#VisionLanguageModelIssue Date: 2025-07-18 Paper Note Scaling Laws for Optimal Data Mixtures, Mustafa Shukor+, arXiv25 Summary本研究では、スケーリング法則を用いて任意のターゲットドメインに対する最適なデータ混合比率を決定する方法を提案。特定のドメイン重みベクトルを持つモデルの損失を正確に予測し、LLM、NMM、LVMの事前訓練における予測力を示す。少数の小規模な訓練実行でパラメータを推定し、高価な試行錯誤法に代わる原則的な選択肢を提供。 #ComputerVision#Pocket#Transformer#PEFT(Adaptor/LoRA)#ICML#FinetuningIssue Date: 2025-07-14 Paper Note ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision Transformers under Domain Shifts, Samar Khanna+, ICML25 SummaryPEFT技術を用いたExPLoRAは、事前学習済みビジョントランスフォーマー（ViT）を新しいドメインに適応させる手法で、教師なし事前学習を通じて効率的にファインチューニングを行う。実験では、衛星画像において最先端の結果を達成し、従来のアプローチよりも少ないパラメータで精度を最大8%向上させた。 Comment元ポスト:https://x.com/samar_a_khanna/status/1944781066591748336?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Qこれまでドメイン適応する場合にラベル付きデータ+LoRAでFinetuningしていたのを、ラベル無しデータ+継続事前学習の枠組みでやりましょう、という話のようである。 ![image](https://github.com/user-attachments/assets/dcae10cf-6b5d-4b29-8d9a-a94227f29a11)" />
<meta property="og:description" content="Pretraining #Pocket#NLP#LanguageModel#DiffusionModel#Scaling Laws#read-laterIssue Date: 2025-07-22 Paper Note Diffusion Beats Autoregressive in Data-Constrained Settings, Mihir Prabhudesai+, arXiv25 Summaryマスク付き拡散モデルは、データ制約のある設定で自己回帰（AR）モデルを大幅に上回ることを発見。拡散モデルはデータを効果的に活用し、検証損失を低下させ、下流のパフォーマンスを向上させる。新しいスケーリング法則を見つけ、拡散がARを上回る臨界計算閾値を導出。データがボトルネックの場合、拡散モデルはARの魅力的な代替手段となる。 Comment元ポスト:https://x.com/iscienceluvr/status/1947567159045197924?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-QいつかdLLMの時代きそうだなあ #Pocket#NLP#LanguageModel#MulltiModal#Scaling Laws#DataMixture#VisionLanguageModelIssue Date: 2025-07-18 Paper Note Scaling Laws for Optimal Data Mixtures, Mustafa Shukor+, arXiv25 Summary本研究では、スケーリング法則を用いて任意のターゲットドメインに対する最適なデータ混合比率を決定する方法を提案。特定のドメイン重みベクトルを持つモデルの損失を正確に予測し、LLM、NMM、LVMの事前訓練における予測力を示す。少数の小規模な訓練実行でパラメータを推定し、高価な試行錯誤法に代わる原則的な選択肢を提供。 #ComputerVision#Pocket#Transformer#PEFT(Adaptor/LoRA)#ICML#FinetuningIssue Date: 2025-07-14 Paper Note ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision Transformers under Domain Shifts, Samar Khanna+, ICML25 SummaryPEFT技術を用いたExPLoRAは、事前学習済みビジョントランスフォーマー（ViT）を新しいドメインに適応させる手法で、教師なし事前学習を通じて効率的にファインチューニングを行う。実験では、衛星画像において最先端の結果を達成し、従来のアプローチよりも少ないパラメータで精度を最大8%向上させた。 Comment元ポスト:https://x.com/samar_a_khanna/status/1944781066591748336?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Qこれまでドメイン適応する場合にラベル付きデータ+LoRAでFinetuningしていたのを、ラベル無しデータ+継続事前学習の枠組みでやりましょう、という話のようである。 ![image](https://github.com/user-attachments/assets/dcae10cf-6b5d-4b29-8d9a-a94227f29a11)" />
<link rel="canonical" href="http://akihikowatanabe.github.io/paper_notes/articles/Pretraining.html" />
<meta property="og:url" content="http://akihikowatanabe.github.io/paper_notes/articles/Pretraining.html" />
<meta property="og:site_name" content="わたしのべんきょうノート" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-07-22T16:47:57+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Pretrainingに関する論文・技術記事メモの一覧" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"AkihikoWATANABE"},"dateModified":"2025-07-22T16:47:57+00:00","datePublished":"2025-07-22T16:47:57+00:00","description":"Pretraining #Pocket#NLP#LanguageModel#DiffusionModel#Scaling Laws#read-laterIssue Date: 2025-07-22 Paper Note Diffusion Beats Autoregressive in Data-Constrained Settings, Mihir Prabhudesai+, arXiv25 Summaryマスク付き拡散モデルは、データ制約のある設定で自己回帰（AR）モデルを大幅に上回ることを発見。拡散モデルはデータを効果的に活用し、検証損失を低下させ、下流のパフォーマンスを向上させる。新しいスケーリング法則を見つけ、拡散がARを上回る臨界計算閾値を導出。データがボトルネックの場合、拡散モデルはARの魅力的な代替手段となる。 Comment元ポスト:https://x.com/iscienceluvr/status/1947567159045197924?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-QいつかdLLMの時代きそうだなあ #Pocket#NLP#LanguageModel#MulltiModal#Scaling Laws#DataMixture#VisionLanguageModelIssue Date: 2025-07-18 Paper Note Scaling Laws for Optimal Data Mixtures, Mustafa Shukor+, arXiv25 Summary本研究では、スケーリング法則を用いて任意のターゲットドメインに対する最適なデータ混合比率を決定する方法を提案。特定のドメイン重みベクトルを持つモデルの損失を正確に予測し、LLM、NMM、LVMの事前訓練における予測力を示す。少数の小規模な訓練実行でパラメータを推定し、高価な試行錯誤法に代わる原則的な選択肢を提供。 #ComputerVision#Pocket#Transformer#PEFT(Adaptor/LoRA)#ICML#FinetuningIssue Date: 2025-07-14 Paper Note ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision Transformers under Domain Shifts, Samar Khanna+, ICML25 SummaryPEFT技術を用いたExPLoRAは、事前学習済みビジョントランスフォーマー（ViT）を新しいドメインに適応させる手法で、教師なし事前学習を通じて効率的にファインチューニングを行う。実験では、衛星画像において最先端の結果を達成し、従来のアプローチよりも少ないパラメータで精度を最大8%向上させた。 Comment元ポスト:https://x.com/samar_a_khanna/status/1944781066591748336?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Qこれまでドメイン適応する場合にラベル付きデータ+LoRAでFinetuningしていたのを、ラベル無しデータ+継続事前学習の枠組みでやりましょう、という話のようである。 ![image](https://github.com/user-attachments/assets/dcae10cf-6b5d-4b29-8d9a-a94227f29a11)","headline":"Pretrainingに関する論文・技術記事メモの一覧","mainEntityOfPage":{"@type":"WebPage","@id":"http://akihikowatanabe.github.io/paper_notes/articles/Pretraining.html"},"url":"http://akihikowatanabe.github.io/paper_notes/articles/Pretraining.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="icon" href="">
  <link rel="canonical" href="http://akihikowatanabe.github.io">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/main.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/custom_button.css">
  <script src="/paper_notes/assets/js/main.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script><link type="application/atom+xml" rel="alternate" href="http://akihikowatanabe.github.io/paper_notes/feed.xml" title="わたしのべんきょうノート" /><script>
  function showMore(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "block";
          
      // このボタンの参照を取得して非表示にします
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "none";
      }
          
      // hideボタンの参照を取得して表示します
      const hideButton = contentDiv.querySelector('button[onclick^="hideContent"]');
      if (hideButton) {
        hideButton.style.display = "block";
      }
    }
  }

  function hideContent(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "none";
  
      // moreボタンの参照を取得して表示します
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "block";
      }
  
      // このボタンを隠します
      const hideButtons = document.querySelectorAll('button[onclick^="hideContent"]');
      const hideButton = hideButtons[index];
      if (hideButton) {
        hideButton.style.display = "none";
      }
    }
  }
</script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script charset="UTF-8"
        src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/go.min.js"
        async></script>



















<script>
// Init highlight js
document.addEventListener('DOMContentLoaded', function(event) {
  var els = document.querySelectorAll('pre code')

  function addLangData(block) {
    var outer = block.parentElement.parentElement.parentElement;
    var lang = block.getAttribute('data-lang');
    for (var i = 0; i < outer.classList.length; i++) {
      var cls = outer.classList[i];
      if (cls.startsWith('language-')) {
        lang = cls;
        break;
      }
    }
    if (!lang) {
      cls = block.getAttribute('class');
      lang = cls ? cls.replace('hljs ', '') : '';
    }
    if (lang.startsWith('language-')) {
      lang = lang.substr(9);
    }
    block.setAttribute('class', 'hljs ' + lang);
    block.parentNode.setAttribute('data-lang', lang);
  }

  function addBadge(block) {
    var enabled = ('true' || 'true').toLowerCase();
    if (enabled == 'true') {
      var pre = block.parentElement;
      pre.classList.add('badge');
    }
  }

  function handle(block) {
    addLangData(block);
    addBadge(block)
    hljs.highlightBlock(block);
  }

  for (var i = 0; i < els.length; i++) {
    var el = els[i];
    handle(el);
  }
});
</script>

<style>
  /* code language badge */
  pre.badge::before {
    content: attr(data-lang);
    color: #fff;
    background-color: #ff4e00;
    padding: 0 .5em;
    border-radius: 0 2px;
    text-transform: uppercase;
    text-align: center;
    min-width: 32px;
    display: inline-block;
    position: absolute;
    right: 0;
  }

  /* fix wrong badge display for firefox browser */
  code > table pre::before {
    display: none;
  }
</style>
<script
  src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe-lightbox.umd.min.js" async></script>
<script
  src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe.umd.min.js" async></script>
<link
  href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/photoswipe.min.css"
  rel="stylesheet"
/>
<style>
  .pswp .pswp__container .pswp__img {
    background-color: white;
  }
</style>

<script>
  function initPhotoSwipe() {
    let customOptions = {};

    try {
      const data = `{"gallery"=>"section.main", "children"=>"a.photo-swipe", "bgOpacity"=>0.8, "padding"=>{"top"=>20, "bottom"=>40, "left"=>100, "right"=>100}}`.replaceAll("=>", ":");
      customOptions = JSON.parse(data);
    } catch (e) {
      console.info("Invalid custom photo previewer options! " + e.message);
    }

    // Define object and gallery options
    const options = Object.assign(
      {
        gallery: "section.main",
        children: "a.photo-swipe",
        photo_scale: 2,
        // dynamic import is not supported in UMD version
        pswpModule: PhotoSwipe,
      },
      customOptions
    );

    const galleryEl = document.querySelector(options.gallery);
    if (!galleryEl) {
      return;
    }

    const imgEls = [];
    const els = galleryEl.querySelectorAll("img:not(.emoji)");
    els.forEach((el) => {
      if (el.src.trim() == "") {
        return;
      }
      if (!imgEls.includes(el)) {
        imgEls.push(el);
      }
    });

    if (imgEls.length === 0) {
      return;
    }

    imgEls.forEach((imgEl) => {
      imgEl.outerHTML = `
      <a class="photo-swipe"
        href="${imgEl.src}"
        data-pswp-width="${
          Math.max(imgEl.naturalWidth, imgEl.width) * options.photo_scale
        }"
        data-pswp-height="${
          Math.max(imgEl.naturalHeight, imgEl.height) * options.photo_scale
        }"
        data-pswp-caption="${imgEl.getAttribute("caption") || imgEl.alt}"
        target="_blank">
        ${imgEl.outerHTML}
      </a>`;
    });

    // Initialize PhotoSwipe 5
    var lightbox = new PhotoSwipeLightbox(options);

    lightbox.init();
  }

  window.addEventListener("load", initPhotoSwipe);
</script>
<meta name="google-site-verification" content="u_DTTPcCZ806iq51zgirHyWq3556HUKGq8AQfH91iFI" />
</head><body>





































































































































<header class="site-header site-header-transparent" role="banner">

  <div class="wrapper">
    <div class="site-header-inner"><span class="site-brand"><a class="site-brand-inner" rel="author" href="/paper_notes/">
  <img class="site-favicon" title="わたしのべんきょうノート" src="" onerror="this.style.display='none'">
  わたしのべんきょうノート
</a>
</span><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>

          <div class="trigger">









<span class="page-link">



<div id="google_translate_element" style="display: none;">
</div>

<span class="ct-language">
  <ul class="list-unstyled ct-language-dropdown">
    
      <li>
        <a href="#" class="lang-select" data-lang="en">
          
          <img src="https://cdn.countryflags.com/thumbs/united-states-of-america/flag-400.png" title="English">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="fr">
          
          <img src="https://cdn.countryflags.com/thumbs/france/flag-400.png" title="French">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="zh-CN">
          
          <img src="https://cdn.countryflags.com/thumbs/china/flag-400.png" title="Chinese(Simple)">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ja">
          
          <img src="https://cdn.countryflags.com/thumbs/japan/flag-400.png" title="Japanese">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ko">
          
          <img src="https://cdn.countryflags.com/thumbs/south-korea/flag-400.png" title="Korean">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ru">
          
          <img src="https://cdn.countryflags.com/thumbs/russia/flag-400.png" title="Russian">
          
        </a>
      </li>
    
  </ul>
</span>

<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({
    pageLanguage: 'ja',
    autoDisplay: false,
    layout: google.translate.TranslateElement.InlineLayout.VERTICAL
  }, 'google_translate_element');

  // Links to cross-origin destinations are unsafe
  var gll = document.getElementsByClassName('goog-logo-link')[0];
  if (gll) {
    gll.setAttribute('rel', 'noopener');
  }

  function restoreLang() {
    var iframe = document.getElementsByClassName('goog-te-banner-frame')[0];
    if (!iframe) return;

    var innerDoc = iframe.contentDocument || iframe.contentWindow.document;
    var restore_el = innerDoc.getElementsByTagName("button");

    for (var i = 0; i < restore_el.length; i++) {
      if (restore_el[i].id.indexOf("restore") >= 0) {
        restore_el[i].click();
        var close_el = innerDoc.getElementsByClassName("goog-close-link");
        close_el[0].click();
        return;
      }
    }
  }

  function triggerHtmlEvent(element, eventName) {
    var event;
    if (document.createEvent) {
      event = document.createEvent('HTMLEvents');
      event.initEvent(eventName, true, true);
      element.dispatchEvent(event);
    } else {
      event = document.createEventObject();
      event.eventType = eventName;
      element.fireEvent('on' + event.eventType, event);
    }
  }

  var googleCombo = document.querySelector("select.goog-te-combo");
  var langSelect = document.querySelector('.ct-language');
  langSelect.addEventListener('click', function(event) {
    if (!event.target) {
      return;
    }

    var selected = document.querySelector('.ct-language .ct-language-selected');
    if (selected) {
      selected.classList.remove('ct-language-selected');
    }

    var target = event.target;
    while (target && target !== langSelect ) {
      if (target.matches('.lang-select')) {
        break;
      }
      target = target.parentElement;
    }

    if (target && target.matches('.lang-select')) {
      var lang = target.getAttribute('data-lang');
      if (googleCombo.value == lang) {
        restoreLang();
      } else {
        target.parentElement.classList.add('ct-language-selected');
        googleCombo.value = lang;
        triggerHtmlEvent(googleCombo, 'change');
      }
    }

    event.preventDefault();
  });
}
</script>

<script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit" async></script>
</span></div>
        </nav></div>
  </div>
</header>

<script>
  function initHeader() {
    var lastScrollY = getScrollPos().y;
    var documentElement = document.documentElement;

    function storeScrollData() {
      var y = getScrollPos().y;documentElement.setAttribute("data-header-transparent", "");var scrollStatus = "";

      if (y <= 0) {
        scrollStatus = "top";
      } else if ((window.innerHeight + y) >= document.body.offsetHeight) {
        scrollStatus = "bottom";
      } else {
        var isScrollDown = (y - lastScrollY > 0) ? true : false;
        scrollStatus = isScrollDown ? "down" : "up";
      }

      lastScrollY = y;
      documentElement.setAttribute("data-scroll-status", scrollStatus);
    }

    window.addEventListener('scroll', function(e) {
      storeScrollData();
    });

    storeScrollData();
  }
  document.addEventListener('DOMContentLoaded', initHeader);
</script>


























































































































































<style>
    html .page-banner .page-banner-img > *:first-child {
      opacity: 0.4;
    }

    html[data-theme="dark"] .page-banner .page-banner-img > *:first-child {
      opacity: 0.2872;
    }
  </style><section class="page-banner">
    <div class="page-banner-img"><div style="background-image: url(/paper_notes/assets/images/banner.png)"></div>
        <img class="img-placeholder" src="/paper_notes/assets/images/banner.png"></div>
    <div class="wrapper">
      <div class="page-banner-inner"><header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">わたしのべんきょうノート</h1>
  <h2 class="post-subtitle">勉強した論文や技術等の情報をGithubのIssueにメモっているひとのブログ。
それなりにメモの量が蓄積されてきたので、一度整理したいなと思いブログはじめてみました！
自然言語処理(NLP), 推薦システム(RecommenderSystem), Educational Data Mining (EDM), Learning Analytics (LA)などの分野のメモが多いと思います。
最近は特にLLMの勉強が多めです :)</h2>

  <div class="post-meta">
    <time class="dt-published" datetime="2025-07-22T16:47:57+00:00" itemprop="datePublished"><i class="fa fa-calendar"></i> Jul 22, 2025
    </time><span class="post-author left-vsplit"><i class="fa fa-pencil"></i> AkihikoWATANABE</span>
    
































    <span class="post-reading-time left-vsplit"><i class="fa fa-clock-o"></i> About 2 hours 14 mins</span>
  </div></header>
</div>
    </div>
  </section><script>
  function hashLocate(hashValue) {
    hashValue = hashValue.replace(/^.*#h-/, '');
    hashValue = decodeURIComponent(hashValue);
    var element = document.getElementById(hashValue);

    if (!element) {
      return;
    }

    var header = document.querySelector('header.site-header');
    var headerRect = header.getBoundingClientRect();
    var headerTop = Math.floor(headerRect.top);
    var headerHeight = Math.floor(headerRect.height);
    var scrollPos = getScrollPos();
    var offsetY = element.offsetTop - (headerTop + headerHeight + 20);

    if (offsetY == scrollPos.y) {
      return;
    }

    if (headerTop == 0  && offsetY > scrollPos.y) {
      offsetY += headerHeight + 2;
    } else if (headerTop < 0  && offsetY < scrollPos.y) {
      offsetY -= headerHeight - 2;
    }

    smoothScrollTo(offsetY);
  }

  // The first event occurred
  window.addEventListener('load', function(event) {
    if (window.location.hash) {
      hashLocate(window.location.hash);
    }
  });

  // The first event occurred
  window.addEventListener('click', function(event) {
    if (event.target.tagName.toLowerCase() == 'a') {
      hashLocate(event.target.getAttribute('href'));
    }
  });
</script>
<div class="theme-toggle">
  <input type="checkbox" id="theme-switch">
  <label for="theme-switch">
    <div class="toggle"></div>
    <div class="names">
      <p class="light">Light</p>
      <p class="dark">Dark</p>
    </div>
  </label>
</div>




<script>
  (function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var nightModeOption = ('off' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
      storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
      var data = storage.getItem('theme');
      try {
        data = JSON.parse(data ? data : '');
      } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
      }
      return data;
    }

    function handleThemeToggle(nightShift) {
      themeData.nightShift = nightShift;
      saveThemeData(themeData);
      html.dataset.theme = nightShift ? 'dark' : 'light';
      setTimeout(function() {
        sw.checked = nightShift ? true : false;
      }, 50);
    }

    function autoThemeToggle() {
      // Next time point of theme toggle
      var now = new Date();
      var toggleAt = new Date();
      var hours = now.getHours();
      var nightShift = hours >= 19 || hours <=7;

      if (nightShift) {
        if (hours > 7) {
          toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
      } else {
        toggleAt.setHours(19);
      }

      toggleAt.setMinutes(0);
      toggleAt.setSeconds(0);
      toggleAt.setMilliseconds(0)

      var delay = toggleAt.getTime() - now.getTime();

      // auto toggle theme mode
      setTimeout(function() {
        handleThemeToggle(!nightShift);
      }, delay);

      return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
      };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
      handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
      var data = autoThemeToggle();

      // Toggle theme by local setting
      if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
      } else {
        handleThemeToggle(themeData.nightShift);
      }
    } else if (nightModeOption == 'manual') {
      handleThemeToggle(themeData.nightShift);
    } else {
      var nightShift = themeData.nightShift;
      if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
      }
      handleThemeToggle(nightShift);
    }
  })();
</script>
<div id="click-to-top" class="click-to-top">
  <i class="fa fa-arrow-up"></i>
</div>
<script>
  (function () {
    const clickToTop = document.getElementById('click-to-top');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 100) {
        clickToTop.classList.add('show')
      }else {
        clickToTop.classList.remove('show')
      }
    });
    clickToTop.addEventListener('click', () => {
      window.smoothScrollTo(0);
    });
  })();
</script>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="framework">
  <section class="main">

     <div class="post">
  <section>









<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="post-content e-content" itemprop="articleBody">

      <h2 id="pretraining">Pretraining</h2>
<div class="visible-content">
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a><a class="button" href="articles/Scaling Laws.html">#Scaling Laws</a><a class="button" href="articles/read-later.html">#read-later</a><br /><span class="issue_date">Issue Date: 2025-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2268">Paper Note Diffusion Beats Autoregressive in Data-Constrained Settings, Mihir Prabhudesai+, arXiv25</a>
<span class="snippet"><span>Summary</span>マスク付き拡散モデルは、データ制約のある設定で自己回帰（AR）モデルを大幅に上回ることを発見。拡散モデルはデータを効果的に活用し、検証損失を低下させ、下流のパフォーマンスを向上させる。新しいスケーリング法則を見つけ、拡散がARを上回る臨界計算閾値を導出。データがボトルネックの場合、拡散モデルはARの魅力的な代替手段となる。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/iscienceluvr/status/1947567159045197924?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-QいつかdLLMの時代きそうだなあ</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/MulltiModal.html">#MulltiModal</a><a class="button" href="articles/Scaling Laws.html">#Scaling Laws</a><a class="button" href="articles/DataMixture.html">#DataMixture</a><a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a><br /><span class="issue_date">Issue Date: 2025-07-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2250">Paper Note Scaling Laws for Optimal Data Mixtures, Mustafa Shukor+, arXiv25</a>
<span class="snippet"><span>Summary</span>本研究では、スケーリング法則を用いて任意のターゲットドメインに対する最適なデータ混合比率を決定する方法を提案。特定のドメイン重みベクトルを持つモデルの損失を正確に予測し、LLM、NMM、LVMの事前訓練における予測力を示す。少数の小規模な訓練実行でパラメータを推定し、高価な試行錯誤法に代わる原則的な選択肢を提供。</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/Transformer.html">#Transformer</a><a class="button" href="articles/PEFT(Adaptor_LoRA).html">#PEFT(Adaptor/LoRA)</a><a class="button" href="articles/ICML.html">#ICML</a><a class="button" href="articles/Finetuning.html">#Finetuning</a><br /><span class="issue_date">Issue Date: 2025-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2206">Paper Note ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision   Transformers under Domain Shifts, Samar Khanna+, ICML25</a>
<span class="snippet"><span>Summary</span>PEFT技術を用いたExPLoRAは、事前学習済みビジョントランスフォーマー（ViT）を新しいドメインに適応させる手法で、教師なし事前学習を通じて効率的にファインチューニングを行う。実験では、衛星画像において最先端の結果を達成し、従来のアプローチよりも少ないパラメータで精度を最大8%向上させた。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/samar_a_khanna/status/1944781066591748336?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Qこれまでドメイン適応する場合にラベル付きデータ+LoRAでFinetuningしていたのを、ラベル無しデータ+継続事前学習の枠組みでやりましょう、という話のようである。
![image](https://github.com/user-attachments/assets/dcae10cf-6b5d-4b29-8d9a-a94227f29a11)

手法は下記で、事前学習済みのモデルに対してLoRAを適用し継続事前学習する。ただし、最後尾のLayer、あるいは最初と最後尾のLayerの両方をunfreezeして、trainableにする。また、LoRAはfreezeしたLayerのQ,Vに適用し、それらのLayerのnormalization layerもunfreezeする。最終的に、継続事前学習したモデルにヘッドをconcatしてfinetuningすることで目的のタスクを実行できるようにする。

![image](https://github.com/user-attachments/assets/6b7ef497-2253-46c9-bbe7-ffdd50765fa3)
![image](https://github.com/user-attachments/assets/84596039-0a10-4556-896b-1fee164a153b)

同じモデルで単にLoRAを適用しただけの手法や、既存手法をoutperform

<img width="679" height="364" alt="Image" src="https://github.com/user-attachments/assets/14935879-75a4-4e4a-a176-1b1eabc4b8fd" />画像+ViT系のモデルだけで実験されているように見えるが、LLMとかにも応用可能だと思われる。
</span>
</div>
<p><button onclick="showMore(0)">more</button></p>

<div class="hidden-content">
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Batch.html">#Batch</a><br /><span class="issue_date">Issue Date: 2025-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2192">Paper Note Small Batch Size Training for Language Models: When Vanilla SGD Works,  and Why Gradient Accumulation Is Wasteful, Martin Marek+, arXiv25</a>
<span class="snippet"><span>Summary</span>小さなバッチサイズに対するAdamのハイパーパラメータをスケーリングする新しいルールを提案。これにより、小さなバッチサイズでも安定したトレーニングが可能で、大きなバッチサイズと同等以上のパフォーマンスを達成。勾配蓄積は推奨せず、実用的なハイパーパラメータ設定のガイドラインを提供。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/giffmana/status/1943384733418950815?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q

論文中のFigure1において、AdamWにおいてbatchsizeが1の方が512の場合と比べてlearning_rateの変化に対してロバストである旨が記述されている。

<img width="977" height="642" alt="Image" src="https://github.com/user-attachments/assets/0c1efb5d-6eeb-4fd7-ba06-e4296e988a6c" />似たような話でMTでバッチサイズ小さいほうが性能良いです、みたいな話が昔あったような

（追記）
気になって思い出そうとしていたが、MTではなく画像認識の話だったかもしれない（だいぶうろ覚え）
- #2196 参考:https://x.com/odashi_t/status/1944034128707342815?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q関連:
- #1541</span>
<a class="button" href="articles/Analysis.html">#Analysis</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/COLM.html">#COLM</a><a class="button" href="articles/Stability.html">#Stability</a><br /><span class="issue_date">Issue Date: 2025-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2188">Paper Note Spike No More: Stabilizing the Pre-training of Large Language Models, Sho Takase+, COLM25</a>
<span class="snippet"><span>Summary</span>大規模言語モデルの事前学習中に発生する損失のスパイクは性能を低下させるため、避けるべきである。勾配ノルムの急激な増加が原因とされ、サブレイヤーのヤコビ行列の分析を通じて、勾配ノルムを小さく保つための条件として小さなサブレイヤーと大きなショートカットが必要であることを示した。実験により、これらの条件を満たす手法が損失スパイクを効果的に防ぐことが確認された。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/shot4410/status/1943301371010388175?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Qsmall sub-layers, large shortcutsの説明はこちらに書かれている。前者については、現在主流なLLMの初期化手法は満たしているが、後者はオリジナルのTransformerの実装では実装されている[^1]が、最近の実装では失われてしまっているとのこと。
![image](https://github.com/user-attachments/assets/55cf847c-fc6a-4e76-88c9-1507464e96a0)

下図が実験結果で、条件の双方を満たしているのはEmbedLN[^2]とScaled Embed[^3]のみであり、実際にスパイクが生じていないことがわかる。
![image](https://github.com/user-attachments/assets/79494662-3d58-4d8e-ae9d-8ed9241e0f65)

[^1]:オリジナル論文 #245 の3.4節末尾、embedding layersに対してsqrt(d_model)を乗じるということがサラッと書いてある。これが実はめちゃめちゃ重要だったという…
[^2]: positional embeddingを加算する前にLayer Normalizationをかける方法
[^3]: EmbeddingにEmbeddingの次元数d（i.e., 各レイヤーのinputの次元数)の平方根を乗じる方法前にScaled dot-product attentionのsqrt(d_k)がめっちゃ重要ということを実験的に示した、という話もあったような…
（まあそもそも元論文になぜスケーリングさせるかの説明は書いてあるけども）</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/SyntheticData.html">#SyntheticData</a><a class="button" href="articles/Programming.html">#Programming</a><a class="button" href="articles/Mathematics.html">#Mathematics</a><a class="button" href="articles/mid-training.html">#mid-training</a><a class="button" href="articles/COLM.html">#COLM</a><br /><span class="issue_date">Issue Date: 2025-07-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2180">Paper Note MegaMath: Pushing the Limits of Open Math Corpora, Fan Zhou+, COLM25</a>
<span class="snippet"><span>Summary</span>MegaMathは、数学に特化したオープンデータセットで、LLMの数学的推論能力を向上させるために作成された。ウェブデータの再抽出、数学関連コードの特定、合成データの生成を通じて、371Bトークンの高品質なデータを提供し、既存のデータセットを上回る量と品質を実現した。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/fazhou_998/status/1942610771915202590?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q非常に大規模な数学の事前学習/mid-training向けのデータセット

CommonCrawlのHTMLから、さまざまなフィルタリング処理（reformatting, 2 stageのHTML parserの活用（片方はnoisyだが高速、もう一方は高性能だが遅い）, fasttextベースの分類器による抽出, deduplication等）を実施しMegaMath-Webを作成、また、MegaMathWebをさらに分類器で低品質なものをフィルタリングし、LLMによってノイズ除去、テキストのreorganizingを実施し（≠ピュアな合成データ）継続事前学習、mid-training向けの高品質なMegaMath-Web-Proを作成。

MegaMathCodeはThe Stack V2 (#2199) をベースにしており、mathematical reasoning, logic puzzles, scientific computationに関するコードを収集。まずこれらのコードと関連が深い11のプログラミング言語を選定し、そのコードスニペットのみを対象とする。次にstrong LLMを用いて、数学に関するrelevanceスコアと、コードの品質を0--6のdiscrete scoreでスコアリングし学習データを作成。作成した学習データでSLMを学習し大規模なフィルタリングを実施することでMegaMath-Codeを作成。

最後にMegaMath-{Web, code}を用いて、Q&amp;A, code data, text&amp;code block dataの3種類を合成。Q&amp;Aデータの合成では、MegaMath-WebからQAペアを抽出し、多様性とデータ量を担保するためQwen2.5-72B-Instruct, Llama3.3-70B-Instructの両方を用いて、QAのsolutionを洗練させる（reasoning stepの改善, あるいはゼロから生成する[^1])ことで生成。また、code dataでは、pythonを対象にMegaMath-Codeのデータに含まれるpython以外のコードを、Qwen2.5-Coder-32B-Instructと、Llamd3.1-70B-Instructによってpythonに翻訳することでデータ量を増やした。text&amp;code blockデータでは、MegaMath-Webのドキュメントを与えて、ブロックを生成（タイトル、数式、結果、コードなど[^1]）し、ブロックのverificationを行い（コードが正しく実行できるか、実行結果とanswerが一致するか等）、verifiedなブロックを残すことで生成。

![image](https://github.com/user-attachments/assets/8975019b-5ab4-437c-bd4e-f3b761439c9c)

![image](https://github.com/user-attachments/assets/995ea6ce-69eb-4f88-8a98-9e55de3e7814)

![image](https://github.com/user-attachments/assets/c6d1ec61-49f4-459f-92b2-fa0a2625178e)

[^1]: この辺は論文の記述を咀嚼して記述しており実サンプルを見ていないので少し正しい認識か不安</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a><a class="button" href="articles/MulltiModal.html">#MulltiModal</a><a class="button" href="articles/RLHF.html">#RLHF</a><a class="button" href="articles/Reasoning.html">#Reasoning</a><a class="button" href="articles/LongSequence.html">#LongSequence</a><a class="button" href="articles/mid-training.html">#mid-training</a><a class="button" href="articles/RewardHacking.html">#RewardHacking</a><a class="button" href="articles/PostTraining.html">#PostTraining</a><a class="button" href="articles/CurriculumLearning.html">#CurriculumLearning</a><a class="button" href="articles/RLVR.html">#RLVR</a><a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a><br /><span class="issue_date">Issue Date: 2025-07-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2128">Paper Note GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable  Reinforcement Learning, GLM-V Team+, arXiv25</a>
<span class="snippet"><span>Summary</span>視覚言語モデルGLM-4.1V-Thinkingを発表し、推論中心のトレーニングフレームワークを開発。強力な視覚基盤モデルを構築し、カリキュラムサンプリングを用いた強化学習で多様なタスクの能力を向上。28のベンチマークで最先端のパフォーマンスを達成し、特に難しいタスクで競争力のある結果を示す。モデルはオープンソースとして公開。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/sinclairwang1/status/1940331927724232712?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-QQwen2.5-VLよりも性能が良いVLM
![image](https://github.com/user-attachments/assets/1215d0cf-3776-4631-a5d5-2c514e7d5a2e)アーキテクチャはこちら。が、pretraining(データのフィルタリング, マルチモーダル→long context継続事前学習)-&gt;SFT(cold startへの対処, reasoning能力の獲得)-&gt;RL(RLVRとRLHFの併用によるパフォーマンス向上とAlignment, RewardHackingへの対処,curriculum sampling)など、全体の学習パイプラインの細かいテクニックの積み重ねで高い性能が獲得されていると考えられる。
![image](https://github.com/user-attachments/assets/a692b5de-5f4e-42c6-938e-3718dd2fc0e6)</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/MultiLingual.html">#MultiLingual</a><br /><span class="issue_date">Issue Date: 2025-06-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2109">Paper Note FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data  Processing to Every Language, Guilherme Penedo+, arXiv25</a>
<span class="snippet"><span>Summary</span>多言語LLMsの性能向上のために、FineWebに基づく新しい事前学習データセットキュレーションパイプラインを提案。9つの言語に対して設計選択肢を検証し、非英語コーパスが従来のデータセットよりも高性能なモデルを生成できることを示す。データセットの再バランス手法も導入し、1000以上の言語にスケールアップした20テラバイトの多言語データセットFineWeb2を公開。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/gui_penedo/status/1938631842720022572?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Qv1
- #1942abstを見る限りFinewebを多言語に拡張した模様</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a><a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/MulltiModal.html">#MulltiModal</a><br /><span class="issue_date">Issue Date: 2025-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2105">Paper Note OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision  Encoders for Multimodal Learning, Xianhang Li+, arXiv25</a>
<span class="snippet"><span>Summary</span>OpenVisionは、完全にオープンでコスト効果の高いビジョンエンコーダーのファミリーを提案し、CLIPと同等以上の性能を発揮します。既存の研究を基に構築され、マルチモーダルモデルの進展に実用的な利点を示します。5.9Mから632.1Mパラメータのエンコーダーを提供し、容量と効率の柔軟なトレードオフを実現します。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/cihangxie/status/1920575141849030882?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/MoE(Mixture-of-Experts).html">#MoE(Mixture-of-Experts)</a><a class="button" href="articles/ICLR.html">#ICLR</a><br /><span class="issue_date">Issue Date: 2025-06-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2088">Paper Note Drop-Upcycling: Training Sparse Mixture of Experts with Partial   Re-initialization, Taishi Nakamura+, ICLR25</a>
<span class="snippet"><span>Summary</span>Drop-Upcycling手法を提案し、MoEモデルのトレーニング効率を向上。事前にトレーニングされた密なモデルの知識を活用しつつ、一部の重みを再初期化することで専門家の専門化を促進。大規模実験により、5.9BパラメータのMoEモデルが13B密なモデルと同等の性能を達成し、トレーニングコストを約1/4に削減。すべての実験リソースを公開。</span>
<span class="snippet"><span>Comment</span>OpenReview:https://openreview.net/forum?id=gx1wHnf5Vp関連:
- #1546提案手法の全体像とDiversity re-initializationの概要。元のUpcyclingでは全てidenticalな重みでreplicateされていたため、これが個々のexpertがlong termでの学習で特化することの妨げになり、最終的に最大限のcapabilityを発揮できず、収束が遅い要因となっていた。これを、Upcyclingした重みのうち、一部のindexのみを再初期化することで、replicate元の知識を保持しつつ、expertsの多様性を高めることで解決する。
![image](https://github.com/user-attachments/assets/46ec75a2-30b1-4f48-9f21-cf5f6e30df95)
![image](https://github.com/user-attachments/assets/ef3c66b2-32a5-46ab-bb31-828fb4570b53)

提案手法は任意のactivation function適用可能。今回はFFN Layerのactivation functionとして一般的なSwiGLUを採用した場合で説明している。

Drop-Upcyclingの手法としては、通常のUpcyclingと同様、FFN Layerの重みをn個のexpertsの数だけreplicateする。その後、re-initializationを実施する比率rに基づいて、[1, intermediate size d_f]の範囲からr*d_f個のindexをサンプリングする。最終的にSwiGLU、およびFFNにおける3つのWeight W_{gate, up, down}において、サンプリングされたindexと対応するrow/columnと対応する重みをre-initializeする。

re-initializeする際には、各W_{gate, up, down}中のサンプリングされたindexと対応するベクトルの平均と分散をそれぞれ独立して求め、それらの平均と分散を持つ正規分布からサンプリングする。

学習の初期から高い性能を発揮し、long termでの性能も向上している。また、learning curveの形状もscratchから学習した場合と同様の形状となっており、知識の転移とexpertsのspecializationがうまく進んだことが示唆される。
![image](https://github.com/user-attachments/assets/945e5ae5-05cd-4117-80e8-078b47f0e53c)解説:https://llm-jp.nii.ac.jp/news/post-566/</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/SyntheticData.html">#SyntheticData</a><br /><span class="issue_date">Issue Date: 2025-06-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2084">Paper Note Recycling the Web: A Method to Enhance Pre-training Data Quality and  Quantity for Language Models, Thao Nguyen+, arXiv25</a>
<span class="snippet"><span>Summary</span>スケーリング法則に基づき、低品質なウェブデータを再利用する手法「REWIRE」を提案。これにより、事前学習データの合成表現を増やし、フィルタリングされたデータのみでのトレーニングと比較して、22のタスクで性能を向上。生データと合成データの混合が効果的であることを示し、ウェブテキストのリサイクルが事前学習データのスケーリングに有効であることを示唆。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/thao_nguyen26/status/1937210428876292457?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q学習データの枯渇に対する対処として別の方向性としては下記のような研究もある:
- #1829</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Tokenizer.html">#Tokenizer</a><br /><span class="issue_date">Issue Date: 2025-06-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2073">Paper Note From Bytes to Ideas: Language Modeling with Autoregressive U-Nets, Mathurin Videau+, arXiv25</a>
<span class="snippet"><span>Summary</span>自己回帰型U-Netを用いてトークン化の柔軟性を向上させ、モデルが生のバイトから単語や単語のペアを生成することでマルチスケールの視点を提供。深い段階では広範な意味パターンに注目し、浅い段階はBPEベースラインに匹敵する性能を発揮。これにより、文字レベルのタスクやリソースの少ない言語間での知識移転が可能となる。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/dair_ai/status/1936825784473096335?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a><br /><span class="issue_date">Issue Date: 2025-06-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2031">Paper Note Reinforcement Pre-Training, Qingxiu Dong+, arXiv25</a>
<span class="snippet"><span>Summary</span>本研究では、強化学習と大規模言語モデルの新しいスケーリング手法「強化事前学習（RPT）」を提案。次のトークン予測を強化学習の推論タスクとして再定義し、一般的なRLを活用することで、ドメイン特有の注釈に依存せずにスケーラブルな方法を提供。RPTは次のトークン予測の精度を向上させ、強化ファインチューニングの基盤を形成。トレーニング計算量の増加が精度を改善することを示し、RPTが言語モデルの事前学習において有望な手法であることを示した。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/hillbig/status/1932922314578145640?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Analysis.html">#Analysis</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Transformer.html">#Transformer</a><a class="button" href="articles/PostTraining.html">#PostTraining</a><a class="button" href="articles/COLT.html">#COLT</a><br /><span class="issue_date">Issue Date: 2025-06-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2007">Paper Note Learning Compositional Functions with Transformers from Easy-to-Hard   Data, Zixuan Wang+, COLT25</a>
<span class="snippet"><span>Summary</span>本研究では、Transformerベースの言語モデルの学習可能性を探求し、$k$-fold compositionタスクに焦点を当てる。$O(\log k)$層のトランスフォーマーでこのタスクを表現できる一方、SQオラクルに対するクエリの下限を示し、サンプルサイズが指数的である必要があることを証明。さらに、カリキュラム学習戦略を用いて、簡単な例と難しい例を含むデータ分布がトランスフォーマーの効率的な学習に必要であることを明らかにした。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/zzzixuanwang/status/1928465115478708604?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Qこちらはまず元ポストのスレッドを読むのが良いと思われる。要点をわかりやすく説明してくださっている。元ポストとalphaxivでざっくり理解したところ、

Transformerがcontextとして与えられた情報(σ)とparametric knowledge(π)をk回の知識マッピングが必要なタスク(k-fold composition task)を学習するにはO(log k)のlayer数が必要で、直接的にk回の知識マッピングが必要なタスクを学習するためにはkの指数オーダーのデータ量が最低限必要となることが示された。これはkが大きくなると（すなわち、複雑なreasoning stepが必要なタスク）になると非現実的なものとなるため、何らかの方法で緩和したい。学習データを簡単なものから難しいものをmixingすること（カリキュラム学習）ことで、この条件が緩和され、指数オーダーから多項式オーダーのデータ量で学習できることが示された

といった感じだと思われる。じゃあ最新の32Bモデルよりも、よりパラメータ数が大きくてlayer数が多い古いモデルの方が複雑なreasoningが必要なタスクを実は解けるってこと！？直感に反する！と一瞬思ったが、おそらく最近のモデルでは昔のモデルと比べてparametric knowledgeがより高密度に適切に圧縮されるようになっていると思われるので、昔のモデルではk回の知識マッピングをしないと解けないタスクが、最新のモデルではk-n回のマッピングで解けるようになっていると推察され、パラメータサイズが小さくても問題なく解けます、みたいなことが起こっているのだろう、という感想を抱くなどした</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Scaling Laws.html">#Scaling Laws</a><br /><span class="issue_date">Issue Date: 2025-05-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1981">Parallel Scaling Law for Language Models, Mouxiang Chen+, arXiv25</a>
<span class="snippet"><span>Summary</span>本研究では、言語モデルのスケーリングにおいて、並列計算を増加させる新しい手法「ParScale」を提案。これにより、モデルの前方パスを並列に実行し、出力を動的に集約することで、推論効率を向上させる。ParScaleは、少ないメモリ増加とレイテンシで同等の性能向上を実現し、既存のモデルを再利用することでトレーニングコストも削減可能。新しいスケーリング法則は、リソースが限られた状況での強力なモデル展開を促進する。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/hillbig/status/1924959706331939099?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q- #405

と考え方が似ている</span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/ModelMerge.html">#ModelMerge</a><br /><span class="issue_date">Issue Date: 2025-05-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1979">Model Merging in Pre-training of Large Language Models, Yunshui Li+, arXiv25</a>
<span class="snippet"><span>Summary</span>モデルマージングは大規模言語モデルの強化に有望な技術であり、本論文ではその事前学習プロセスにおける包括的な調査を行う。実験により、一定の学習率で訓練されたチェックポイントをマージすることで性能向上とアニーリング挙動の予測が可能になることを示し、効率的なモデル開発と低コストのトレーニングに寄与する。マージ戦略やハイパーパラメータに関するアブレーション研究を通じて新たな洞察を提供し、実用的な事前学習ガイドラインをオープンソースコミュニティに提示する。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/iscienceluvr/status/1924804324812873990?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q解説ポスト:https://x.com/giffmana/status/1924849877634449878?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/Safety.html">#Safety</a><a class="button" href="articles/DPO.html">#DPO</a><a class="button" href="articles/Toxicity.html">#Toxicity</a><a class="button" href="articles/ITI (Inference Time Intervention).html">#ITI (Inference Time Intervention)</a><br /><span class="issue_date">Issue Date: 2025-05-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1940">When Bad Data Leads to Good Models, Kenneth Li+, arXiv25</a>
<span class="snippet"><span>Summary</span>本論文では、LLMの事前学習におけるデータの質の再検討を行い、有害データが事後学習における制御を向上させる可能性を探ります。トイ実験を通じて、有害データの割合が増加することで有害性の概念が線形表現に影響を与えることを発見し、有害データが生成的有害性を増加させつつも除去しやすくなることを示しました。評価結果は、有害データで訓練されたモデルが生成的有害性を低下させつつ一般的な能力を保持する良好なトレードオフを達成することを示唆しています。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/ke_li_2021/status/1920646069613957606?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Qこれは面白そうWebコーパスなどを事前学習で利用する際は、質の高いデータを残して学習した方が良いとされているが、4chanのようなtoxicなデータを混ぜて事前学習して、後からdetox（Inference Time Intervention #1941 , SFT, DPO)することで、最終的なモデルのtoxicなoutputが減るという話らしい。これはそもそも事前学習時点でtoxicなデータのsignalが除外されることで、モデルがtoxicな内容のrepresentationを学習できず、最終的にtoxicか否かをコントロールできなくなるため、と考察している（っぽい）
![image](https://github.com/user-attachments/assets/7f6efd4b-0679-4143-9a7d-1bf3ea5b6f3a)
![image](https://github.com/user-attachments/assets/0acec11b-d851-4137-b0aa-1ed7172388e1)有害な出力を減らせそうなことは分かったが、Activation Steeringによってどの程度モデルの性能に影響を与えるのかが気になる、と思ったがAppendixに記載があった。細かく書かれていないので推測を含むが、各データに対してToxicデータセットでProbingすることでTopKのheadを決めて、Kの値を調整することでinterventionの強さを調整し、Toxicデータの割合を変化させて評価してみたところ、モデルの性能に大きな影響はなかったということだと思われる（ただし1Bモデルでの実験しかない）

![image](https://github.com/user-attachments/assets/4c79ca22-6916-438d-ad31-07596c82bfd1)
おそらく2,3節あたりが一番おもしろいポイントなのだと思われるがまだ読めていない。</span>
<a class="button" href="articles/Analysis.html">#Analysis</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/ICLR.html">#ICLR</a><a class="button" href="articles/read-later.html">#read-later</a><br /><span class="issue_date">Issue Date: 2025-03-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1837">Overtrained Language Models Are Harder to Fine-Tune, Jacob Mitchell Springer+, ICLR25</a>
<span class="snippet"><span>Summary</span>大規模言語モデルの事前学習において、トークン予算の増加がファインチューニングを難しくし、パフォーマンス低下を引き起こす「壊滅的な過学習」を提唱。3Tトークンで事前学習されたOLMo-1Bモデルは、2.3Tトークンのモデルに比べて2%以上の性能低下を示す。実験と理論分析により、事前学習パラメータの感度の増加が原因であることを示し、事前学習設計の再評価を促す。</span>
<span class="snippet"><span>Comment</span>著者によるポスト:https://x.com/jacspringer/status/1904960783341023521?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q事前学習のトークン数を増やすとモデルのsensitivityが増し、post-trainingでのパフォーマンスの劣化が起こることを報告している。事前学習で学習するトークン数を増やせば、必ずしもpost-training後のモデルの性能がよくなるわけではないらしい。
![image](https://github.com/user-attachments/assets/ba60ae24-f3e5-4956-b29f-37b4fe01a9d1)ICLR'25のOutstanding Paperに選ばれた模様:
https://x.com/jacspringer/status/1917174452531724718?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q

きちんと読んだ方が良さげ。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Scaling Laws.html">#Scaling Laws</a><br /><span class="issue_date">Issue Date: 2025-03-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1825">Compute Optimal Scaling of Skills: Knowledge vs Reasoning, Nicholas Roberts+, arXiv25</a>
<span class="snippet"><span>Summary</span>スケーリング法則はLLM開発において重要であり、特に計算最適化によるトレードオフが注目されている。本研究では、スケーリング法則が知識や推論に基づくスキルに依存することを示し、異なるデータミックスがスケーリング挙動に与える影響を調査した。結果、知識とコード生成のスキルは根本的に異なるスケーリング挙動を示し、誤指定された検証セットが計算最適なパラメータ数に約50%の影響を与える可能性があることが明らかになった。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/dair_ai/status/1903843682509312218?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q知識を問うQAのようなタスクはモデルのパラメータ量が必要であり、コーディングのようなReasoningに基づくタスクはデータ量が必要であり、異なる要素に依存してスケールすることを示している研究のようである。

![image](https://github.com/user-attachments/assets/5d2bb3c6-437a-4184-9848-3232745d0de1)</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br /><span class="issue_date">Issue Date: 2025-02-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1763">LLM Pretraining with Continuous Concepts, Jihoon Tack+, arXiv25</a>
<span class="snippet"><span>Summary</span>次トークン予測に代わる新しい事前学習フレームワークCoCoMixを提案。これは、スパースオートエンコーダから学習した連続的な概念をトークンの隠れ表現と交互に混ぜることで、モデルの性能を向上させる。実験により、CoCoMixは従来の手法を上回り、解釈可能性と操作性も向上させることが示された。</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a><a class="button" href="articles/MachineLearning.html">#MachineLearning</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/ICLR.html">#ICLR</a><a class="button" href="articles/Batch.html">#Batch</a><br /><span class="issue_date">Issue Date: 2024-11-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1541">How Does Critical Batch Size Scale in Pre-training?, Hanlin Zhang+, ICLR25</a>
<span class="snippet"><span>Summary</span>大規模モデルの訓練には、クリティカルバッチサイズ（CBS）を考慮した並列化戦略が重要である。CBSの測定法を提案し、C4データセットで自己回帰型言語モデルを訓練。バッチサイズや学習率などの要因を調整し、CBSがデータサイズに比例してスケールすることを示した。この結果は、ニューラルネットワークの理論的分析によって支持され、ハイパーパラメータ選択の重要性も強調されている。</span>
<span class="snippet"><span>Comment</span>Critical Batch Sizeはモデルサイズにはあまり依存せず、データサイズに応じてスケールする
![image](https://github.com/user-attachments/assets/4a1a720f-37a1-485d-9b02-bb2e8a5c2da4)
![image](https://github.com/user-attachments/assets/8bc5f621-caac-438a-afd1-de1d689ee210)</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Programming.html">#Programming</a><br /><span class="issue_date">Issue Date: 2025-07-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2199">Paper Note StarCoder 2 and The Stack v2: The Next Generation, Anton Lozhkov+, arXiv24</a>
<span class="snippet"><span>Summary</span>BigCodeプロジェクトは、責任あるCode LLMsの開発に焦点を当て、StarCoder2を発表。Software Heritageと提携し、The Stack v2を構築し、619のプログラミング言語を含む大規模なトレーニングセットを作成。StarCoder2モデルは3B、7B、15Bのパラメータを持ち、徹底的なベンチマーク評価で優れた性能を示す。特にStarCoder2-15Bは、同等の他モデルを大幅に上回り、数学やコード推論でも高い性能を発揮。モデルの重みはOpenRAILライセンスで公開され、トレーニングデータの透明性も確保。</span>
<span class="snippet"><span>Comment</span>関連:
- #661</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/InstructionTuning.html">#InstructionTuning</a><a class="button" href="articles/EMNLP.html">#EMNLP</a><br /><span class="issue_date">Issue Date: 2025-06-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2089">Paper Note Instruction Pre-Training: Language Models are Supervised Multitask   Learners, Daixuan Cheng+, EMNLP24</a>
<span class="snippet"><span>Summary</span>無監督のマルチタスク事前学習に加え、監視されたマルチタスク学習の可能性を探るために、Instruction Pre-Trainingフレームワークを提案。指示応答ペアを生成し、2億のペアを合成して実験を行い、事前学習モデルの性能を向上させることを確認。Instruction Pre-TrainingはLlama3-8BをLlama3-70Bと同等以上の性能に引き上げる。モデルやデータは公開されている。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br /><span class="issue_date">Issue Date: 2025-05-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1944">Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon  Pretraining Dataset, Dan Su+, arXiv24</a>
<span class="snippet"><span>Summary</span>FineWeb-EduとDCLMは、モデルベースのフィルタリングによりデータの90%を削除し、トレーニングに適さなくなった。著者は、アンサンブル分類器や合成データの言い換えを用いて、精度とデータ量のトレードオフを改善する手法を提案。1Tトークンで8Bパラメータモデルをトレーニングし、DCLMに対してMMLUを5.6ポイント向上させた。新しい6.3Tトークンデータセットは、DCLMと同等の性能を持ちながら、4倍のユニークなトークンを含み、長トークンホライズンでのトレーニングを可能にする。15Tトークンのためにトレーニングされた8Bモデルは、Llama 3.1の8Bモデルを上回る性能を示した。データセットは公開されている。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br /><span class="issue_date">Issue Date: 2025-05-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1943">DataComp-LM: In search of the next generation of training sets for  language models, Jeffrey Li+, arXiv24</a>
<span class="snippet"><span>Summary</span>DataComp for Language Models（DCLM）を紹介し、240Tトークンのコーパスと53の評価スイートを提供。DCLMでは、モデルスケール412Mから7Bパラメータのデータキュレーション戦略を実験可能。DCLM-Baselineは2.6Tトークンでトレーニングし、MMLUで64%の精度を達成し、従来のMAP-Neoより6.6ポイント改善。計算リソースも40%削減。結果はデータセット設計の重要性を示し、今後の研究の基盤を提供。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br /><span class="issue_date">Issue Date: 2025-05-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1942">The FineWeb Datasets: Decanting the Web for the Finest Text Data at  Scale, Guilherme Penedo+, arXiv24</a>
<span class="snippet"><span>Summary</span>本研究では、15兆トークンからなるFineWebデータセットを紹介し、LLMの性能向上に寄与することを示します。FineWebは高品質な事前学習データセットのキュレーション方法を文書化し、重複排除やフィルタリング戦略を詳細に調査しています。また、FineWebから派生した1.3兆トークンのFineWeb-Eduを用いたLLMは、MMLUやARCなどのベンチマークで優れた性能を発揮します。データセット、コードベース、モデルは公開されています。</span>
<span class="snippet"><span>Comment</span>日本語解説:https://zenn.dev/deepkawamura/articles/da9aeca6d6d9f9</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/InstructionTuning.html">#InstructionTuning</a><a class="button" href="articles/ACL.html">#ACL</a><a class="button" href="articles/PerplexityCurse.html">#PerplexityCurse</a><br /><span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1708">Instruction-tuned Language Models are Better Knowledge Learners, Zhengbao Jiang+, ACL24</a>
<span class="snippet"><span>Summary</span>新しい文書からの知識更新には、事前指示調整（PIT）を提案。これは、文書の訓練前に質問に基づいて指示調整を行う手法で、LLMが新しい情報を効果的に吸収する能力を向上させ、標準的な指示調整を17.8%上回る結果を示した。</span>
<span class="snippet"><span>Comment</span>興味深い</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Catastrophic Forgetting.html">#Catastrophic Forgetting</a><br /><span class="issue_date">Issue Date: 2025-01-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1638">Examining Forgetting in Continual Pre-training of Aligned Large Language  Models, Chen-An Li+, arXiv24</a>
<span class="snippet"><span>Summary</span>LLMの継続的な事前学習がファインチューニングされたモデルに与える影響を調査し、壊滅的な忘却の現象を評価。出力形式や知識、信頼性の次元での実験結果が、特に繰り返しの問題における忘却の課題を明らかにする。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/gyakuse/status/1874357127248306200?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/Transformer.html">#Transformer</a><a class="button" href="articles/NeurIPS.html">#NeurIPS</a><br /><span class="issue_date">Issue Date: 2024-12-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1584">Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale   Prediction, Keyu Tian+, NeurIPS24</a>
<span class="snippet"><span>Summary</span>Visual AutoRegressive modeling (VAR)を提案し、画像生成において自己回帰学習を次のスケール予測として再定義。VARは、GPTのようなARモデルが拡散トランスフォーマーを上回ることを実現し、ImageNet 256x256ベンチマークでFIDを18.65から1.73、ISを80.4から350.2に改善。推論速度は約20倍向上し、画像品質やデータ効率でも優れた性能を示す。VARはゼロショット一般化能力を持ち、スケーリング法則を示す。全モデルとコードを公開し、視覚生成の研究を促進。</span>
<span class="snippet"><span>Comment</span>NeurIPS2024のベストペーパー第一著者がByteDance社から訴訟を起こされている模様…？
https://var-integrity-report.github.ioOpenReview:https://openreview.net/forum?id=gojL67CfS8Next Token Prediction, Next Image Token Generation (従来手法）, Next Scale (resolution) prediction (提案手法)の違いの図解。非常に分かりやすい。next token predictionでは次トークンのみを予測するがVARでは、次の解像度画像の全体のトークンマップを予測する。

![image](https://github.com/user-attachments/assets/668d7523-f262-45c1-a1d0-2dd479c0a708)

学習方法の概要。2-Stageで学習される。最初のステージでK種類の解像度の画像（＝K種類のマルチスケールのtoken maps r_k）を得るためにAutoEncoderを学習し、次のステージでblock-wiseのcausal attention maskを用いて、K_&lt;k個目の解像度の画像からK個目の解像度の画像を予測する（図を見るとイメージを掴みやすい）。inference時はKV Cacheを利用し、maskは不要となる。
各r_kをデコードする際にr_&lt;kのみに依存する設計にすることでcoase-to-fineに画像を生成することに相当し、これは人間の粗く捉えてから詳細を見る認知プロセスと合致する。また、flatten操作が存在せず、それぞれのr_&lt;k内のトークンがr_k生成時に全て考慮されるため空間的局所性も担保される。また、r_k内のトークンは並列に生成可能なので計算量のオーダーが大幅に削減される（O(n^4)。
![image](https://github.com/user-attachments/assets/e1a85712-e66a-4c9a-9cf1-6556f2b8e687)

従来手法と比べより小さいパラメータで高い性能を実現し、inference timeも非常に早い。
![image](https://github.com/user-attachments/assets/90a6a7de-995d-49e6-94a2-cd709e68777f)

ScalingLawsも成立する。
![image](https://github.com/user-attachments/assets/351c2a7b-85aa-4cc7-8ba2-a5e9528cabd4)</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/MulltiModal.html">#MulltiModal</a><br /><span class="issue_date">Issue Date: 2024-11-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1542">Multimodal Autoregressive Pre-training of Large Vision Encoders, Enrico Fini+, arXiv24</a>
<span class="snippet"><span>Summary</span>新しい手法AIMV2を用いて、大規模なビジョンエンコーダの事前学習を行う。これは画像とテキストを組み合わせたマルチモーダル設定に拡張され、シンプルな事前学習プロセスと優れた性能を特徴とする。AIMV2-3BエンコーダはImageNet-1kで89.5%の精度を達成し、マルチモーダル画像理解において最先端のコントラストモデルを上回る。</span>
<a class="button" href="articles/Analysis.html">#Analysis</a><a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/Japanese.html">#Japanese</a><a class="button" href="articles/read-later.html">#read-later</a><br /><span class="issue_date">Issue Date: 2024-11-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1524">Balancing Speed and Stability: The Trade-offs of FP8 vs. BF16 Training  in LLMs, Kazuki Fujii+, arXiv24</a>
<span class="snippet"><span>Summary</span>大規模言語モデル（LLMs）は、その言語理解能力と適用可能性から注目を集めており、特にLlama 3シリーズは4050億パラメータを持つ。トレーニングの効率化が求められる中、NVIDIAのH100 GPUはFP8フォーマットを導入し、トレーニング時間を短縮する可能性がある。初期研究ではFP8が性能を損なわずに効率を向上させることが示唆されているが、トレーニングの安定性や下流タスクへの影響はまだ不明である。本研究は、LLMsのトレーニングにおけるBF16とFP8のトレードオフを探る。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/okoge_kaz/status/1857639065421754525?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-QFP8で継続的事前学習をするとスループットは向上するが、lossのスパイクを生じたり、downstreamタスクの性能がBF16よりも低下したりする（日本語と英語の両方）との報告のようである。現状アブストと付録しか記載がないが、内容はこれから更新されるのだろうか。

![image](https://github.com/user-attachments/assets/8d60d59b-de00-483a-bff0-04a4145715c1)</span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Subword.html">#Subword</a><a class="button" href="articles/Tokenizer.html">#Tokenizer</a><br /><span class="issue_date">Issue Date: 2024-11-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1507">LBPE: Long-token-first Tokenization to Improve Large Language Models, Haoran Lian+, arXiv24</a>
<span class="snippet"><span>Summary</span>LBPEは、長いトークンを優先する新しいエンコーディング手法で、トークン化データセットにおける学習の不均衡を軽減します。実験により、LBPEは従来のBPEを一貫して上回る性能を示しました。</span>
<span class="snippet"><span>Comment</span>BPEとは異なりトークンの長さを優先してマージを実施することで、最終的なトークンを決定する手法で、
![image](https://github.com/user-attachments/assets/99b91472-88d8-4792-bf04-acc67956e4f5)

![image](https://github.com/user-attachments/assets/99103316-bd1c-448d-b52a-5db815298e7e)

BPEよりも高い性能を獲得し、
![image](https://github.com/user-attachments/assets/c7dccf00-b9c2-4739-82f3-4f8eeacd4fc7)

トークンの長さがBPEと比較して長くなり、かつ5Bトークン程度を既存のBPEで事前学習されたモデルに対して継続的事前学習するだけで性能を上回るようにでき、
![image](https://github.com/user-attachments/assets/10f4ff2e-1d49-4c8a-87ec-67466bdce2f0)

同じVocabサイズでBPEよりも高い性能を獲得できる手法
![image](https://github.com/user-attachments/assets/5e19fc11-10f6-467a-ae06-8fb62b5f0a65)

らしい</span>
<a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Alignment.html">#Alignment</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/SyntheticData.html">#SyntheticData</a><a class="button" href="articles/PostTraining.html">#PostTraining</a><br /><span class="issue_date">Issue Date: 2024-10-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1464">Self-Taught Evaluators, Tianlu Wang+, N_A, arXiv24</a>
<span class="snippet"><span>Summary</span>本研究では、人間の注釈なしで評価者を改善するアプローチを提案。合成トレーニングデータを用い、自己改善スキームによりLLMを評価者としてトレーニング。これにより、RewardBenchでのLLMのパフォーマンスを75.4から88.3に向上させ、GPT-4を超える結果を達成。</span>
<span class="snippet"><span>Comment</span>LLMのアラインメント等をSFTする際に、preferenceのラベル付きデータが必要になるが、このようなデータを作るのはコストがかかって大変なので自動生成して、より良いreward modelを作りたいよね、という話。
具体的には、LLMを用いて good responseと、instructionを変化させてbad sesponseを生成し、JudgeモデルM_tにpairwiseでどちらが良いかをjudgeさせることで学習データを作成。新たに作成されたデータを用いてJudgeモデルを再学習し、同様のプロセスを繰り返すことで、人手の介在なく強力なJudgeモデルが完成する。
![image](https://github.com/user-attachments/assets/837c4567-6993-4e4c-81c8-650b7777c49b)
![image](https://github.com/user-attachments/assets/10a4fb62-160d-4bcf-b3a2-a960a7c9bc46)</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><br /><span class="issue_date">Issue Date: 2024-10-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1459">Addition is All You Need for Energy-efficient Language Models, Hongyin Luo+, N_A, arXiv24</a>
<span class="snippet"><span>Summary</span>本研究では、浮動小数点乗算を高精度で整数加算器によって近似するL-Mulアルゴリズムを提案。これにより、8ビット浮動小数点乗算に比べて計算リソースを大幅に削減しつつ、より高い精度を実現。L-Mulをテンソル処理ハードウェアに適用することで、エネルギーコストを95％（要素ごとの乗算）および80％（ドット積）削減可能。実験結果は理論的誤差推定と一致し、L-Mulは従来の浮動小数点乗算と同等またはそれ以上の精度を達成。トランスフォーマーモデル内の浮動小数点乗算をL-Mulに置き換えることで、ファインチューニングと推論において高い精度を維持できることを示した。</span>
<a class="button" href="articles/Tools.html">#Tools</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/LLMAgent.html">#LLMAgent</a><br /><span class="issue_date">Issue Date: 2024-10-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1458">ToolGen: Unified Tool Retrieval and Calling via Generation, Renxi Wang+, N_A, arXiv24</a>
<span class="snippet"><span>Summary</span>ToolGenは、外部ツールとの直接対話を可能にする新しいフレームワークで、各ツールをユニークなトークンとして表現し、LLMのパラメータに統合します。これにより、LLMはツール呼び出しや引数を自然言語生成の一部としてシームレスに生成でき、情報取得ステップなしで多くのツールにアクセス可能になります。実験結果は、ToolGenが自律的なタスク完了と情報取得で優れた性能を示し、より効率的で自律的なAIシステムの基盤を築くことを示しています。</span>
<span class="snippet"><span>Comment</span>昔からよくある特殊トークンを埋め込んで、特殊トークンを生成したらそれに応じた処理をする系の研究。今回はツールに対応するトークンを仕込む模様。斜め読みだが、3つのstepでFoundation Modelを訓練する。まずはツールのdescriptionからツールトークンを生成する。これにより、モデルにツールの情報を覚えさせる（memorization）。斜め読みなので読めていないが、ツールトークンをvocabに追加してるのでここは継続的事前学習をしているかもしれない。続いて、（おそらく）人手でアノテーションされたクエリ-必要なツールのペアデータから、クエリに対して必要なツールを生成するタスクを学習させる。最後に、（おそらく人手で作成された）クエリ-タスクを解くためのtrajectoryペアのデータで学習させる。
![image](https://github.com/user-attachments/assets/eebe4260-2e4f-4be7-9b59-a0b84913e667)
![image](https://github.com/user-attachments/assets/d03ed971-e5c9-49f3-8385-cfb00505907c)学習データのサンプル。Appendix中に記載されているものだが、本文のデータセット節とAppendixの双方に、データの作り方の詳細は記述されていなかった。どこかに書いてあるのだろうか。
![image](https://github.com/user-attachments/assets/41975d34-dc9d-405d-aaca-062a3ee1a4b0)![image](https://github.com/user-attachments/assets/41e80988-5770-420e-bc80-a4cc0a724994)最終的な性能
![image](https://github.com/user-attachments/assets/a247cc99-10eb-4346-9f0d-b406a022c3b4)特殊トークンを追加のvocabとして登録し、そのトークンを生成できるようなデータで学習し、vocabに応じて何らかの操作を実行するという枠組み、その学習手法は色々なタスクで役立ちそう。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/SyntheticData.html">#SyntheticData</a><br /><span class="issue_date">Issue Date: 2024-09-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1427">Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal  Sampling, Hritik Bansal+, N_A, arXiv24</a>
<span class="snippet"><span>Summary</span>高品質な合成データを生成するために、強力なSEモデルと安価なWCモデルのトレードオフを再検討。WCモデルからのデータはカバレッジと多様性が高いが偽陽性率も高い。ファインチューニングの結果、WC生成データでトレーニングされたモデルがSE生成データのモデルを上回ることが示され、WCが計算最適なアプローチである可能性を示唆。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/rohanpaul_ai/status/1840172683528425718?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Analysis.html">#Analysis</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><br /><span class="issue_date">Issue Date: 2024-08-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1352">Amuro &amp; Char: Analyzing the Relationship between Pre-Training and  Fine-Tuning of Large Language Models, Kaiser Sun+, N_A, arXiv24</a>
<span class="snippet"><span>Summary</span>大規模なテキストコーパスで事前学習された複数の中間事前学習モデルのチェックポイントを微調整することによって、事前学習と微調整の関係を調査した。18のデータセットでの結果から、i）継続的な事前学習は、微調整後にモデルを改善する潜在的な方法を示唆している。ii）追加の微調整により、モデルが事前学習段階でうまく機能しないデータセットの改善が、うまく機能するデータセットよりも大きいことを示している。iii）監督された微調整を通じてモデルは恩恵を受けるが、以前のドメイン知識や微調整中に見られないタスクを忘れることがある。iv）監督された微調整後、モデルは評価プロンプトに対して高い感度を示すが、これはより多くの事前学習によって緩和できる。</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Transformer.html">#Transformer</a><a class="button" href="articles/InstructionTuning.html">#InstructionTuning</a><a class="button" href="articles/MulltiModal.html">#MulltiModal</a><a class="button" href="articles/SpeechProcessing.html">#SpeechProcessing</a><a class="button" href="articles/CVPR.html">#CVPR</a><a class="button" href="articles/Encoder-Decoder.html">#Encoder-Decoder</a><a class="button" href="articles/Robotics.html">#Robotics</a><br /><span class="issue_date">Issue Date: 2023-12-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1202">Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision,   Language, Audio, and Action, Jiasen Lu+, N_A, CVPR24</a>
<span class="snippet"><span>Summary</span>Unified-IO 2は、最初の自己回帰型のマルチモーダルモデルであり、画像、テキスト、音声、アクションを理解し生成することができます。異なるモダリティを統一するために、共有の意味空間に入力と出力を配置し、単一のエンコーダ・デコーダトランスフォーマーモデルで処理します。さまざまなアーキテクチャの改善を提案し、大規模なマルチモーダルな事前トレーニングコーパスを使用してモデルをトレーニングします。Unified-IO 2は、GRITベンチマークを含む35以上のベンチマークで最先端のパフォーマンスを発揮します。</span>
<span class="snippet"><span>Comment</span>画像、テキスト、音声、アクションを理解できる初めてのautoregressive model。AllenAI![image](https://github.com/user-attachments/assets/fa54f7bf-6689-4346-a1ca-031e4e5516ea)

![image](https://github.com/user-attachments/assets/4282ffb0-18f1-40c9-b6d7-f004d03b8382)マルチモーダルに拡張したことで、訓練が非常に不安定になったため、アーキテクチャ上でいくつかの工夫を加えている:

- 2D Rotary Embedding
  - Positional EncodingとしてRoPEを採用
  - 画像のような2次元データのモダリティの場合はRoPEを2次元に拡張する。具体的には、位置(i, j)のトークンについては、Q, Kのembeddingを半分に分割して、それぞれに対して独立にi, jのRoPE Embeddingを適用することでi, j双方の情報を組み込む。
- QK Normalization
  - image, audioのモダリティを組み込むことでMHAのlogitsが非常に大きくなりatteetion weightが0/1の極端な値をとるようになり訓練の不安定さにつながった。このため、dot product attentionを適用する前にLayerNormを組み込んだ。
- Scaled Cosine Attention
  - Image Historyモダリティにおいて固定長のEmbeddingを得るためにPerceiver Resamplerを扱ったているが、こちらも上記と同様にAttentionのlogitsが極端に大きくなったため、cosine類似度をベースとしたScaled Cosine Attention #2259 を利用することで、大幅に訓練の安定性が改善された。
- その他
  - attention logitsにはfp32を適用
  - 事前学習されたViTとASTを同時に更新すると不安定につながったため、事前学習の段階ではfreezeし、instruction tuningの最後にfinetuningを実施

![image](https://github.com/user-attachments/assets/74c8fa3a-8fb5-4785-8dd3-6a8cf3c7cfeb)目的関数としては、Mixture of Denoisers (#1424)に着想を得て、Multimodal Mixture of Denoisersを提案。MoDでは、
- \[R\]: 通常のspan corruption (1--5 token程度のspanをmaskする)
- \[S\]: causal language modeling (inputを2つのサブシーケンスに分割し、前方から後方を予測する。前方部分はBi-directionalでも可)
- \[X\]: extreme span corruption (12&gt;=token程度のspanをmaskする)

の3種類が提案されており、モダリティごとにこれらを使い分ける:
- text modality: UL2 (#1424)を踏襲
- image, audioがtargetの場合: 2つの類似したパラダイムを定義し利用
  - \[R\]: patchをランダムにx%マスクしre-constructする
  - \[S\]: inputのtargetとは異なるモダリティのみの情報から、targetモダリティを生成する

訓練時には prefixとしてmodality token \[Text\], \[Image\], \[Audio\] とparadigm token \[R\], \[S\], \[X\] をタスクを指示するトークンとして利用している。また、image, audioのマスク部分のdenoisingをautoregressive modelで実施する際には普通にやるとdecoder側でリークが発生する(a)。これを防ぐには、Encoder側でマスクされているトークンを、Decoder側でteacher-forcingする際にの全てマスクする方法(b)があるが、この場合、生成タスクとdenoisingタスクが相互に干渉してしまいうまく学習できなくなってしまう（生成タスクでは通常Decoderのinputとして[mask]が入力され次トークンを生成する、といったことは起きえないが、愚直に(b)をやるとそうなってしまう）。ので、(c)に示したように、マスクされているトークンをinputとして生成しなければならない時だけ、マスクを解除してdecoder側にinputする、という方法 (Dynamic Masking) でこの問題に対処している。
<img width="597" height="394" alt="Image" src="https://github.com/user-attachments/assets/0dba8d5d-0c93-4c56-852b-fce9869428e7" /></span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br /><span class="issue_date">Issue Date: 2023-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1072">Think before you speak: Training Language Models With Pause Tokens, Sachin Goyal+, N_A, ICLR24</a>
<span class="snippet"><span>Summary</span>言語モデルのトレーニングと推論において、遅延を導入することでモデルの性能を向上させる手法を提案しました。具体的には、入力に特定のトークンを追加し、そのトークンが現れるまでモデルの出力を遅らせることで、追加の計算を行うことができます。実験結果では、この手法が推論タスクにおいて有益であり、特にQAタスクでの性能向上が見られました。今後は、この遅延予測の手法をさらに研究していく必要があります。</span>
<span class="snippet"><span>Comment</span>この研究は興味深いが、事前学習時に入れないと効果が出にくいというのは直感的にわかるので、実用的には活用しづらい。
また、promptでこの研究をimitateする方法については、ZeroShot CoTにおいて、思考プロセスを明示的に指定するようなpromptingと同様のことを行っており、これは実際に効果があると思う。</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/Transformer.html">#Transformer</a><a class="button" href="articles/ImageSegmentation.html">#ImageSegmentation</a><a class="button" href="articles/FoundationModel.html">#FoundationModel</a><br /><span class="issue_date">Issue Date: 2023-04-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/600">Segment Anything in Medical Images, Jun Ma+, N_A, Nature Communications24</a>
<span class="snippet"><span>Summary</span>本研究では、自然画像セグメンテーションに革新的な手法であるSegment anything model (SAM)を医療画像に拡張するためのMedSAMを提案し、様々な医療ターゲットのセグメンテーションのための汎用ツールを作成することを目的としています。MedSAMは、大規模な医療画像データセットを用いて開発され、SAMを一般的な医療画像セグメンテーションに適応するためのシンプルなファインチューニング手法を開発しました。21の3Dセグメンテーションタスクと9の2Dセグメンテーションタスクに対する包括的な実験により、MedSAMは、平均Dice類似係数（DSC）がそれぞれ22.5％と17.6％で、デフォルトのSAMモデルを上回ることが示されました。コードとトレーニング済みモデルは、\url{https://github.com/bowang-lab/MedSAM}で公開されています。</span>
<span class="snippet"><span>Comment</span>SAMの性能は医療画像に対しては限定的だったため、11の異なるモダリティに対して200kのマスクをした医療画像を用意しfinetuningしたMedSAMによって、医療画像のセグメンテーションの性能を大幅に向上。
コードとモデルはpublicly available![image](https://github.com/user-attachments/assets/ea394adc-b1da-4764-bf29-534323bfc443)</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/MulltiModal.html">#MulltiModal</a><a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a><a class="button" href="articles/ICCV.html">#ICCV</a><br /><span class="issue_date">Issue Date: 2025-06-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2111">Paper Note Sigmoid Loss for Language Image Pre-Training, Xiaohua Zhai+, ICCV23</a>
<span class="snippet"><span>Summary</span>シンプルなペアワイズシグモイド損失（SigLIP）を提案し、画像-テキストペアに基づく言語-画像事前学習を改善。シグモイド損失はバッチサイズの拡大を可能にし、小さなバッチサイズでも性能向上を実現。SigLiTモデルは84.5%のImageNetゼロショット精度を達成。バッチサイズの影響を研究し、32kが合理的なサイズであることを確認。モデルは公開され、さらなる研究の促進を期待。</span>
<span class="snippet"><span>Comment</span>SigLIP論文</span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/MoE(Mixture-of-Experts).html">#MoE(Mixture-of-Experts)</a><a class="button" href="articles/PostTraining.html">#PostTraining</a><br /><span class="issue_date">Issue Date: 2024-11-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1546">Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints, Aran Komatsuzaki+, ICLR23</a>
<span class="snippet"><span>Summary</span>スパース活性化モデルは、計算コストを抑えつつ密なモデルの代替として注目されているが、依然として多くのデータを必要とし、ゼロからのトレーニングは高コストである。本研究では、密なチェックポイントからスパース活性化Mixture-of-Expertsモデルを初期化する「スパースアップサイクリング」を提案。これにより、初期の密な事前トレーニングのコストを約50%再利用し、SuperGLUEやImageNetで密なモデルを大幅に上回る性能を示した。また、アップサイクリングされたモデルは、ゼロからトレーニングされたスパースモデルよりも優れた結果を得た。</span>
<span class="snippet"><span>Comment</span>斜め読みしかできていないが、Mixture-of-Expertsを用いたモデルをSFT/Pretrainingする際に、既存のcheckpointの重みを活用することでより効率的かつ性能向上する方法を提案。MoE LayerのMLPを全て既存のcheckpointにおけるMLPの重みをコピーして初期化する。Routerはスクラッチから学習する。
![image](https://github.com/user-attachments/assets/d51a0746-d2cc-4343-a462-20034ef373d9)

継続事前学習においては、同じ学習時間の中でDense Layerを用いるベースラインと比較してでより高い性能を獲得。
![image](https://github.com/user-attachments/assets/d7a67c99-15d7-4803-82e4-63187bb3d4ec)
Figure2で継続事前学習したモデルに対して、フルパラメータのFinetuningをした場合でもUpcyclingは効果がある（Figure3）。

特にPretrainingではUpcyclingを用いたモデルの性能に、通常のMoEをスクラッチから学習したモデルが追いつくのに時間がかかるとのこと。特に図右側の言語タスクでは、120%の学習時間が追いつくために必要だった。
![image](https://github.com/user-attachments/assets/f0ca37ac-65a7-43ff-afef-ffc309b17040)

Sparse Upcycingと、Dense tilingによる手法（warm start; 元のモデルに既存の層を複製して新しい層を追加する方法）、元のモデルをそれぞれ継続事前学習すると、最も高い性能を獲得している。
![image](https://github.com/user-attachments/assets/b357a08a-d202-47d3-977f-f02b192723d1)

（すごい斜め読みなのでちょっも自信なし、、、）</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/MulltiModal.html">#MulltiModal</a><a class="button" href="articles/ICLR.html">#ICLR</a><br /><span class="issue_date">Issue Date: 2024-09-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1424">UL2: Unifying Language Learning Paradigms, Yi Tay+, N_A, ICLR23</a>
<span class="snippet"><span>Summary</span>本論文では、事前学習モデルの普遍的なフレームワークを提案し、事前学習の目的とアーキテクチャを分離。Mixture-of-Denoisers（MoD）を導入し、複数の事前学習目的の効果を示す。20Bパラメータのモデルは、50のNLPタスクでSOTAを達成し、ゼロショットやワンショット学習でも優れた結果を示す。UL2 20Bモデルは、FLAN指示チューニングにより高いパフォーマンスを発揮し、関連するチェックポイントを公開。</span>
<span class="snippet"><span>Comment</span>OpenReview:https://openreview.net/forum?id=6ruVLB727MC[R] standard span corruption, [S] causal language modeling, [X] extreme span corruption の3種類のパラダイムを持つMoD (Mixture of Denoisers)を提案

<img width="1187" height="1203" alt="Image" src="https://github.com/user-attachments/assets/a07372c6-854c-4bd1-8f59-f8c4dbdc5d23" /></span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Chain-of-Thought.html">#Chain-of-Thought</a><br /><span class="issue_date">Issue Date: 2023-11-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1147">Implicit Chain of Thought Reasoning via Knowledge Distillation, Yuntian Deng+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>本研究では、言語モデルの内部の隠れ状態を使用して暗黙的な推論を行う手法を提案します。明示的なチェーン・オブ・ソートの推論ステップを生成する代わりに、教師モデルから抽出した暗黙的な推論ステップを使用します。実験により、この手法が以前は解決できなかったタスクを解決できることが示されました。</span>
<span class="snippet"><span>Comment</span>これは非常に興味深い話</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/FoundationModel.html">#FoundationModel</a><a class="button" href="articles/Mathematics.html">#Mathematics</a><br /><span class="issue_date">Issue Date: 2023-10-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1104">Llemma: An Open Language Model For Mathematics, Zhangir Azerbayev+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>私たちは、数学のための大規模な言語モデルであるLlemmaを提案します。Llemmaは、Proof-Pile-2と呼ばれるデータセットを用いて事前学習され、MATHベンチマークで他のモデルを上回る性能を示しました。さらに、Llemmaは追加のfine-tuningなしでツールの使用や形式的な定理証明が可能です。アーティファクトも公開されています。</span>
<span class="snippet"><span>Comment</span>CodeLLaMAを200B tokenの数学テキスト（proof-pile-2データ;論文、数学を含むウェブテキスト、数学のコードが含まれるデータ）で継続的に事前学習することでfoundation modelを構築
![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/87f9bbe1-3377-4e80-a7d4-904345ebb7d9)

約半分のパラメータ数で数学に関する性能でGoogleのMinervaと同等の性能を達成
![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/5d209059-2275-415a-8b8d-f73f46712ba6)元ツイート: https://twitter.com/zhangir_azerbay/status/1714098823080063181まだ4-shotしてもAcc.50%くらいなのか。</span>
<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/87f9bbe1-3377-4e80-a7d4-904345ebb7d9" alt="image" loading="lazy" /><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/DataGeneration.html">#DataGeneration</a><br /><span class="issue_date">Issue Date: 2023-10-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1099">Zephyr: Direct Distillation of LM Alignment, Lewis Tunstall+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>私たちは、小さな言語モデルを作成するために、教師モデルからの優先データを使用する手法を提案しています。この手法により、自然なプロンプトに対するモデルの応答が改善されます。提案手法を用いて学習されたZephyr-7Bモデルは、チャットベンチマークで最先端の性能を発揮し、人間の注釈を必要としません。詳細はGitHubで利用可能です。</span>
<span class="snippet"><span>Comment</span>7BパラメータでLlaMa70Bと同等の性能を達成したZephyrの論文。

![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/1348b3c1-f70a-49b6-97c9-4a27bf7805fa)

- dSFT:既存データからpromptをサンプリングし、user,assistantのmulti turnの対話をLLMでシミュレーションしてデータ生成しSFT
- AIF:既存データからpromstをサンプリングし、異なる4つのLLMのレスポンスをGPT4でランクづけしたデータの活用
- dDPO: 既存データからpromptをサンプリングし、ベストなレスポンスとランダムにサンプリングしたレスポンスの活用

人手を一切介していない。
![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/f2cd7b48-4036-49eb-bfb7-0ce3cc8a09b8)Blog: https://huggingface.co/blog/Isamu136/understanding-zephyr</span>
<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/1348b3c1-f70a-49b6-97c9-4a27bf7805fa" alt="image" loading="lazy" /><a class="button" href="articles/MachineLearning.html">#MachineLearning</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a><br /><span class="issue_date">Issue Date: 2023-07-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/857">Pre-Training to Learn in Context, ACL23</a>
<span class="snippet"><span>Summary</span>インコンテキスト学習は、タスクの例と文脈からタスクを実行する方法であり、注目されています。しかし、現在の方法では十分に活用されていないため、私たちはPICLというフレームワークを提案します。これは、一般的なテキストコーパスでモデルを事前学習し、文脈に基づいてタスクを推論して実行する能力を向上させます。私たちは、PICLでトレーニングされたモデルのパフォーマンスを評価し、他のモデルを上回ることを示しました。コードはGitHubで公開されています。</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Transformer.html">#Transformer</a><a class="button" href="articles/MulltiModal.html">#MulltiModal</a><br /><span class="issue_date">Issue Date: 2023-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/806">Generative Pretraining in Multimodality, Quan Sun+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>Emuは、マルチモーダルなコンテキストで画像とテキストを生成するためのTransformerベースのモデルです。このモデルは、単一モダリティまたはマルチモーダルなデータ入力を受け入れることができます。Emuは、マルチモーダルなシーケンスでトレーニングされ、画像からテキストへのタスクやテキストから画像へのタスクなど、さまざまなタスクで優れたパフォーマンスを示します。また、マルチモーダルアシスタントなどの拡張機能もサポートしています。</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/MulltiModal.html">#MulltiModal</a><br /><span class="issue_date">Issue Date: 2023-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/805">EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the  Backbone, Shraman Pramanick+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>エゴセントリックビデオ言語の事前学習の第2世代（EgoVLPv2）は、ビデオと言語のバックボーンにクロスモーダルの融合を直接組み込むことができる。EgoVLPv2は強力なビデオテキスト表現を学習し、柔軟かつ効率的な方法でさまざまなダウンストリームタスクをサポートする。さらに、提案されたバックボーン戦略は軽量で計算効率が高い。EgoVLPv2は幅広いVLタスクで最先端のパフォーマンスを達成している。詳細はhttps://shramanpramanick.github.io/EgoVLPv2/を参照。</span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/KnowledgeGraph.html">#KnowledgeGraph</a><br /><span class="issue_date">Issue Date: 2023-06-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/768">Unifying Large Language Models and Knowledge Graphs: A Roadmap, Shirui Pan+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>LLMsとKGsを統合することで、自然言語処理や人工知能の分野で注目を集めている。KGsは豊富な事実知識を明示的に格納しているが、構築が困難であり、進化する性質を持っている。一方、LLMsはブラックボックスモデルであり、事実知識を捉えたりアクセスしたりすることができない。本記事では、LLMsとKGsを統合するための展望を示し、KG-enhanced LLMs、LLM-augmented KGs、Synergized LLMs + KGsの3つのフレームワークを提案する。既存の取り組みをレビューし、今後の研究方向を指摘する。</span>
<span class="snippet"><span>Comment</span>LLMsとKGの統合に関するロードマップを提示。KGをLLMの事前学習や推論に組み込む方法、KGタスクにLLMを利用する方法、LLMとKGの双方向のreasonieg能力を高める方法などをカバーしている。
![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/c008d409-e5db-4140-a82c-a658a4847780)</span>
<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/c008d409-e5db-4140-a82c-a658a4847780" alt="image" loading="lazy" /><a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a><a class="button" href="articles/MachineLearning.html">#MachineLearning</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br /><span class="issue_date">Issue Date: 2023-06-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/766">Textbooks Are All You Need, Suriya Gunasekar+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>本研究では、小規模なphi-1という新しいコード用大規模言語モデルを紹介し、8つのA100で4日間トレーニングした結果、HumanEvalでpass@1の正解率50.6％、MBPPで55.5％を達成したことを報告しています。また、phi-1は、phi-1-baseやphi-1-smallと比較して、驚くべき新しい性質を示しています。phi-1-smallは、HumanEvalで45％を達成しています。</span>
<span class="snippet"><span>Comment</span>参考: https://twitter.com/hillbig/status/1671643297616654342?s=46&amp;t=JYDYid2m0v7vYaL7jhZYjQ![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/9f0b945a-f965-42ae-b5d8-ac464359af35)日本語解説: https://dalab.jp/archives/journal/introduction-textbooks-are-all-you-need/ざっくり言うと、教科書で事前学習し、エクササイズでFinetuningすると性能が向上する（= より大きいモデルと同等の性能が得られる）。</span>
<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/9f0b945a-f965-42ae-b5d8-ac464359af35" alt="image" loading="lazy" /><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/DataDistillation.html">#DataDistillation</a><br /><span class="issue_date">Issue Date: 2023-05-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/698">DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining, Sang Michael Xie+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>本論文では、言語モデルの性能に影響を与える事前学習データのドメインの混合比について、DoReMiという手法を提案する。DoReMiは、小さなプロキシモデルを使用してドメインの重みを生成し、再サンプリングして大きなモデルをトレーニングすることで、効率的にドメインの重みを見つけることができる。実験では、DoReMiはThe PileやGLaMデータセットで高い精度を発揮し、few-shot下流精度を6.5％改善することができる。</span>
<span class="snippet"><span>Comment</span>事前学習する際の各ドメインのデータをどのような比率でmixtureするかの話。各ドメインごとに小さなproxy modelを訓練し、downstream taskの知識無しでドメインごとの重みを生成。データセットを生成されたドメインごとの重みに従いリサンプリングすることで、（1/30のプロキシモデルを用いた場合）オリジナルのデータより2.6倍高速で、6.5%oneshotのaccuracyを向上させることに成功
![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/2c0b125a-5ecc-4ee3-8c3b-022c03606c60)</span>
<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/2c0b125a-5ecc-4ee3-8c3b-022c03606c60" alt="image" loading="lazy" /><a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Transformer.html">#Transformer</a><a class="button" href="articles/Architecture.html">#Architecture</a><a class="button" href="articles/MoE(Mixture-of-Experts).html">#MoE(Mixture-of-Experts)</a><br /><span class="issue_date">Issue Date: 2025-02-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1754">Switch Transformers: Scaling to Trillion Parameter Models with Simple  and Efficient Sparsity, William Fedus+, JMLR22</a>
<span class="snippet"><span>Summary</span>Switch Transformerを提案し、Mixture of Experts (MoE)の複雑さや通信コスト、トレーニングの不安定性を改善。これにより、低精度フォーマットでの大規模スパースモデルのトレーニングが可能になり、最大7倍の事前トレーニング速度向上を実現。さらに、1兆パラメータのモデルを事前トレーニングし、T5-XXLモデルに対して4倍の速度向上を達成。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/ICLR.html">#ICLR</a><br /><span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1717">Towards Continual Knowledge Learning of Language Models, Joel Jang+, ICLR22</a>
<span class="snippet"><span>Summary</span>大規模言語モデル（LMs）の知識が陳腐化する問題に対処するため、「継続的知識学習（CKL）」という新しい継続的学習問題を定式化。CKLでは、時間不変の知識の保持、陳腐化した知識の更新、新しい知識の獲得を定量化するためのベンチマークとメトリックを構築。実験により、CKLが独自の課題を示し、知識を信頼性高く保持し学習するためにはパラメータの拡張が必要であることが明らかに。ベンチマークデータセットやコードは公開されている。</span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/Self-SupervisedLearning.html">#Self-SupervisedLearning</a><br /><span class="issue_date">Issue Date: 2023-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/874">RankMe: Assessing the downstream performance of pretrained  self-supervised representations by their rank, Quentin Garrido+, N_A, arXiv22</a>
<span class="snippet"><span>Summary</span>共有埋め込み自己教示学習（JE-SSL）は、成功の視覚的な手がかりが欠如しているため、展開が困難である。本研究では、JE-SSL表現の品質を評価するための非教示基準であるRankMeを開発した。RankMeはラベルを必要とせず、ハイパーパラメータの調整も不要である。徹底的な実験により、RankMeが最終パフォーマンスのほとんど減少なしにハイパーパラメータの選択に使用できることを示した。RankMeはJE-SSLの展開を容易にすることが期待される。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><br /><span class="issue_date">Issue Date: 2022-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/500">Revisiting Pretraining Objectives for Tabular Deep Learning, Rubachev+, Yandex+, arXiv22</a>
<span class="snippet"><span>Summary</span>表形式データに対する深層学習モデルはGBDTと競争しており、事前学習がパフォーマンス向上に寄与することが示された。異なるデータセットやアーキテクチャに適用可能な事前学習のベストプラクティスを特定し、オブジェクトターゲットラベルの使用が有益であることを発見。適切な事前学習により、深層学習モデルはGBDTを上回る性能を発揮することが確認された。</span>
<span class="snippet"><span>Comment</span>Tabular Dataを利用した場合にKaggleなどでDeepなモデルがGBDT等に勝てないことが知られているが、GBDT等とcomparable になる性能になるようなpre-trainingを提案したよ、的な内容っぽいICLR 2023 OpenReview: https://openreview.net/forum?id=kjPLodRa0n</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/Transformer.html">#Transformer</a><a class="button" href="articles/Architecture.html">#Architecture</a><br /><span class="issue_date">Issue Date: 2025-07-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2259">Paper Note Swin Transformer V2: Scaling Up Capacity and Resolution, Ze Liu+, arXiv21</a>
<span class="snippet"><span>Summary</span>本論文では、大規模ビジョンモデルのトレーニングと応用における課題に対処するための3つの技術を提案。具体的には、トレーニングの安定性向上のための残差後正規化法、低解像度から高解像度への転送を可能にする位置バイアス法、ラベル付きデータの必要性を減少させる自己教師あり学習法を用いる。これにより、30億パラメータのSwin Transformer V2モデルをトレーニングし、複数のビジョンタスクで新記録を樹立。トレーニング効率も向上し、ラベル付きデータと時間を大幅に削減。</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a><a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Transformer.html">#Transformer</a><a class="button" href="articles/MulltiModal.html">#MulltiModal</a><br /><span class="issue_date">Issue Date: 2023-08-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1009">ViLT: Vision-and-Language Transformer Without Convolution or Region   Supervision, Wonjae Kim+, N_A, ICML21</a>
<span class="snippet"><span>Summary</span>VLP（Vision-and-Language Pre-training）のアプローチは、ビジョンと言語のタスクでのパフォーマンスを向上させているが、現在の方法は効率性と表現力の面で問題がある。そこで、本研究では畳み込みフリーのビジョンと言語のトランスフォーマ（ViLT）モデルを提案する。ViLTは高速でありながら競争力のあるパフォーマンスを示し、コードと事前学習済みの重みはGitHubで利用可能である。</span>
<span class="snippet"><span>Comment</span>日本語解説:https://tech.fusic.co.jp/posts/2021-12-29-vilt/</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/TransferLearning.html">#TransferLearning</a><a class="button" href="articles/PostTraining.html">#PostTraining</a><a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a><br /><span class="issue_date">Issue Date: 2025-05-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1955">Exploring the Limits of Transfer Learning with a Unified Text-to-Text  Transformer, Colin Raffel+, JMLR20</a>
<span class="snippet"><span>Summary</span>転移学習はNLPにおいて強力な技術であり、本論文ではテキストをテキストに変換する統一フレームワークを提案。事前学習の目的やアーキテクチャを比較し、最先端の結果を達成。データセットやモデル、コードを公開し、今後の研究を促進する。</span>
<span class="snippet"><span>Comment</span>T5もメモっていなかったので今更ながら追加。全てのNLPタスクをテキスト系列からテキスト系列へ変換するタスクとみなし、Encoder-DecoderのTransformerを大規模コーパスを用いて事前学習をし、downstreamタスクにfinetuningを通じて転移する。</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a><a class="button" href="articles/Unsupervised.html">#Unsupervised</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/EMNLP.html">#EMNLP</a><br /><span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/83">Unsupervised Pretraining for Sequence to Sequence Learning, Ramachandran+, EMNLP17</a>
<span class="snippet"><span>Comment</span>seq2seqにおいてweightのpretrainingを行う手法を提案
seq2seqでは訓練データが小さいとoverfittingしやすいという弱点があるので、大規模なデータでunsupervisedにpretrainingし、その後目的のデータでfinetuneすることで精度を向上させましょう、というお話。
WMTの翻訳タスクにおいて、1.3ポイント BLEUスコアが改善、abstractive summarizationでも実験したが、精度は向上せず。しかしながら要約ではpretrainingによってrepetitionが減少したと主張。

encoder, decoderそれぞれを切り離して考えると、それぞれ言語モデルとみなすことができるため(encoderにはoutput-layerを追加)、それぞれの言語モデルを独立に大規模なラベルなしデータでpretrainingする。
fine-tuneする際は、targetデータだけでなく、pretrainingする際のデータも同時に学習を続ける（LM Objective）
LM Objectiveは、target側のobjective functionにpretraining側のobjective functionの項を重み付きで追加したもの。

Abltion studyによると、MTにおいてはsoftmax-layerをpretrainingすることが重要。softmax-layerのpretrainingをablationするとBLEUスコアが1.6ポイント減少。
LM objectiveをなくすと、pretrainingの効果がほとんどなくなる(BLEUスコア-2.0ポイント)。
sumarizationにおいては、embeddingのpretrainingが大幅なROUGEスコアの改善を見せた。また、MTと異なり、encoder側のpretrainingがスコア向上に寄与。

LM Objectiveは結構使えそうな印象</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Transformer.html">#Transformer</a><a class="button" href="articles/Repository.html">#Repository</a><a class="button" href="articles/Optimizer.html">#Optimizer</a><a class="button" href="articles/Decoder.html">#Decoder</a><br /><span class="issue_date">Issue Date: 2025-07-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2223">Modded-NanoGPT, KellerJordan, 2024.05</a>
<span class="snippet"><span>Comment</span>NanoGPT speedrun関連:
- #2118
- #2208</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Blog.html">#Blog</a><a class="button" href="articles/Optimizer.html">#Optimizer</a><br /><span class="issue_date">Issue Date: 2025-07-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2208">きみはNanoGPT speedrunを知っているか？, PredNext, 2025.07</a>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Tutorial.html">#Tutorial</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Evaluation.html">#Evaluation</a><a class="button" href="articles/Blog.html">#Blog</a><a class="button" href="articles/OpenWeight.html">#OpenWeight</a><a class="button" href="articles/Japanese.html">#Japanese</a><a class="button" href="articles/PostTraining.html">#PostTraining</a><br /><span class="issue_date">Issue Date: 2025-06-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2092">LLM-jp-3.1 シリーズ instruct4 の公開, LLM-jp, 2025.05</a>
<span class="snippet"><span>Comment</span>関連
- #2089
- #2090
- #2091</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Tutorial.html">#Tutorial</a><a class="button" href="articles/MachineLearning.html">#MachineLearning</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Transformer.html">#Transformer</a><a class="button" href="articles/Chain-of-Thought.html">#Chain-of-Thought</a><a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a><a class="button" href="articles/Attention.html">#Attention</a><a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a><a class="button" href="articles/SSM (StateSpaceModel).html">#SSM (StateSpaceModel)</a><a class="button" href="articles/Scaling Laws.html">#Scaling Laws</a><a class="button" href="articles/PostTraining.html">#PostTraining</a><br /><span class="issue_date">Issue Date: 2025-05-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2001">2025年度人工知能学会全国大会チュートリアル講演「深層基盤モデルの数理」, Taiji Suzuki, 2025.05</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/btreetaiji/status/1927678122817921442?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/ComputerVision.html">#ComputerVision</a><a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Transformer.html">#Transformer</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/MulltiModal.html">#MulltiModal</a><a class="button" href="articles/Blog.html">#Blog</a><a class="button" href="articles/SSM (StateSpaceModel).html">#SSM (StateSpaceModel)</a><br /><span class="issue_date">Issue Date: 2025-03-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1830">Nemotron-H: A Family of Accurate, Efficient Hybrid Mamba-Transformer Models, Nvidia, 2025.03</a>
<span class="snippet"><span>Comment</span>関連:
- #1820TransformerのSelf-attention LayerをMamba2 Layerに置換することで、様々なベンチマークで同等の性能、あるいは上回る性能で3倍程度のInference timeの高速化をしている（65536 input, 1024 output）。

56B程度のmediumサイズのモデルと、8B程度の軽量なモデルについて述べられている。特に、8BモデルでMambaとTransformerのハイブリッドモデルと、通常のTransformerモデルを比較している。学習データに15 Trillion Tokenを利用しており、このデータ量でのApple to Appleのアーキテクチャ間の比較は、現状では最も大規模なものとのこと。性能は多くのベンチマークでハイブリッドにしても同等、Commonsense Understandingでは上回っている。

また、学習したNemotron-Hをバックボーンモデルとして持つVLMについてもモデルのアーキテクチャが述べられている。</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/MachineLearning.html">#MachineLearning</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><br /><span class="issue_date">Issue Date: 2025-03-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1780">The Ultra-Scale Playbook: Training LLMs on GPU Clusters, HuggingFace, 2025.02</a>
<span class="snippet"><span>Comment</span>HuggingFaceによる数1000のGPUを用いたAIモデルのトレーニングに関するオープンソースのテキスト</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Slide.html">#Slide</a><br /><span class="issue_date">Issue Date: 2025-02-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1762">LLMの事前学習のためのテキストデータの収集と構築, Shun Kiyono, 2015.02</a>
<span class="snippet"><span>Comment</span>詳細は著書に記載とのこと。興味深い。</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Tutorial.html">#Tutorial</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/Video.html">#Video</a><br /><span class="issue_date">Issue Date: 2024-12-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1614">Stanford CS229 I Machine Learning I Building Large Language Models （LLMs）, StanfordUnivercity, 2024.09</a>
<span class="snippet"><span>Comment</span>スタンフォード大学によるLLM構築に関する講義。事前学習と事後学習両方ともカバーしているらしい。</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/AES(AutomatedEssayScoring).html">#AES(AutomatedEssayScoring)</a><br /><span class="issue_date">Issue Date: 2024-11-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1555">Cross-prompt Pre-finetuning of Language Models for Short Answer Scoring, Funayama+, 2024.09</a>
<span class="snippet"><span>Summary</span>自動短答スコアリング（SAS）では、異なるルーブリックと参照回答に基づいてスコアを付けるが、新しいプロンプトごとにモデルを再訓練する必要がありコストがかかる。本研究では、既存のルーブリックと回答を用いて新しいプロンプトでファインチューニングする二段階アプローチを提案。重要なフレーズを学習することで、特に訓練データが限られている場合にスコアリング精度を向上させることを実験で示した。</span>
<span class="snippet"><span>Comment</span>SASでは回答データが限られているので、限られたデータからより効果的に学習をするために、事前に他のデータでモデルをpre-finetuningしておき、対象データが来たらpre-finetuningされたモデルをさらにfinetuningするアプローチを提案。ここで、prompt中にkeyphraseを含めることが有用であると考え、実験的に有効性を示している。
![image](https://github.com/user-attachments/assets/9ab4eb22-b72e-4573-8fbb-1c376047c2b0)
![image](https://github.com/user-attachments/assets/b671a564-c5a8-4344-aaec-06875f654f8b)

BERTでfinetuningをした場合は、key-phraseを含めた方が性能が高く、特にfinetuningのサンプル数が小さい場合にその差が顕著であった。
![image](https://github.com/user-attachments/assets/cdced65b-060b-43ae-a2b4-fcfc5750a6ed)

次に、LLM（swallow-8B, 70B）をpre-finetuningし、pre-finetuningを実施しない場合と比較することで、pre-finetuningがLLMのzero-shot、およびICL能力にどの程度影響を与えるかを検証した。検証の結果、pre-finetuningなしでは、そもそも10-shotにしてもQWKが非常に低かったのに対し、pre-finetuningによってzero-shotの能力が大幅に性能が向上した。一方、few-shotについては3-shotで性能が頭打ちになっているようにみえる。ここで、Table1のLLMでは、ターゲットとする問題のpromptでは一切finetuningされていないことに注意する（Unseenな問題）。
<img width="639" alt="image" src="https://github.com/user-attachments/assets/7c9f141d-dc55-4388-8dc4-6a56f81d6cad" />

続いて、LLMをfinetuningした場合も検証。提案手法が高い性能を示し、200サンプル程度ある場合にHuman Scoreを上回っている（しかもBERTは200サンプルでサチったが、LLMはまだサチっていないように見える）。また、サンプル数がより小さい場合に、提案手法がより高いgainを得ていることがわかる。
<img width="775" alt="image" src="https://github.com/user-attachments/assets/898b2bea-e9df-4c5c-b172-0507a3a83c3c" />

また、個々の問題ごとにLLMをfinetuningするのは現実的に困難なので、個々の問題ごとにfinetuningした場合と、全ての問題をまとめてfinetuningした場合の性能差を比較したところ、まとめて学習しても性能は低下しない、どころか21問中18問で性能が向上した（LLMのマルチタスク学習の能力のおかげ）。
<img width="762" alt="image" src="https://github.com/user-attachments/assets/a8ec62fb-2984-4e7c-8eeb-1b3b6333e9ac" />
[Perplexity(hallucinationに注意)](https://www.perplexity.ai/search/tian-fu-sitalun-wen-wodu-mi-ne-3_TrRyxTQJ.2Bm2fJLqvTQ#0)</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/OpenWeight.html">#OpenWeight</a><a class="button" href="articles/Japanese.html">#Japanese</a><br /><span class="issue_date">Issue Date: 2024-11-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1545">Sarashina2-8x70Bの公開, SB Intuitions, 2024.11</a>
<span class="snippet"><span>Comment</span>MoE Layerの説明、Sparse Upcyclingの説明、MoEモデルを学習する際に、学習時の学習率の設定が大きすぎると初期に損失が増大し、小さすぎると損失の増大は防げるがlong runで学習した際の性能向上が小さかったこと、元のモデルのパラメータを毀損しないように、Upcyclingをした元モデルの最終的な学習率を踏襲して学習をし、学習率をさらに減衰させていったこと、などが記載されている。

また、性能評価として同等のactivation parameter数を持つモデルと日本語のQAタスクで比較した結果も載っている。

- #1546MoE Layerについては
- #1204

も参照のこと</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><br /><span class="issue_date">Issue Date: 2024-11-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1487">ZeRO: DeepSpeedの紹介, レトリバ, 2021.07 </a>
<span class="snippet"><span>Comment</span>ZeROの説明がわかりやすいこちらの記事もわかりやすい
https://zenn.dev/turing_motors/articles/d00c46a79dc976DeepSpeedのコンフィグの一覧
https://www.deepspeed.ai/docs/config-json/ZeRO Stage3を使う場合、ページ後方にしれっととんでもなく重要なことが書いてあるので気をつけましょう。。。。
https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/deepspeed#constructing-massive-models

![image](https://github.com/user-attachments/assets/677b6656-1302-4b1b-8be6-ca954c7edda6)
ZeROはparameterとoptimizerのmemory footprintの最適化を頑張っていて、activation memory footprint（バッチをforward passに流す時に消費されるメモリ）の削減は、tiling, activation/gradient checkpointingとかで頑張ってねという

という話が本家issueの4047に記載されている。結論: つまづいたらDeepSpeedのIssueをエラーメッセージで検索かけるのが一番効果的</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Tutorial.html">#Tutorial</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Blog.html">#Blog</a><br /><span class="issue_date">Issue Date: 2024-07-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1327">GENIAC: 172B 事前学習知見, 2024</a>
<span class="snippet"><span>Comment</span>LLMの事前学習における知見がまとまっている記事とのこと・Megatron LMで学習
　→ 3D Parallelismなどの分散学習手法によりHF Trainerより高速
　→ Data Parallelim、Tensor Parallelism、 Pipeline Parallelismを組み合わせたもの
・GPUメンテナンス、不良で学習が継続できなかった場合はcheckpointをロードして学習
・学習曲線が安定しているように見えるがSpikeは発生している。発生時はgradient normが急激に上昇する
・LlamaなどのLLMからの継続的事前学習ではなくfrom scratchから学習しているので透明性が高い
・Transformer engineを利用
・AdamWを利用
・attention dropout, hidden dropoutは0.0

&gt;この際、 通信を多く必要とする分散手法のワーカー（Tensor Parallelワーカー）はノード内に配置するようにMegatron-LMのデフォルトではなっているため、今回もそれを利用しました。このようにする理由は、ノード内の通信はNVLinkにより、ノード間通信よりも高速であるためです。また、Data Parallelの勾配平均化のための通信を考慮して、Data Parallelワーカーも可能な限りノード内に配置するMegatron-LMデフォルトの挙動を利用しました。
Pipeline Parallelismは他の並列化手法と比較して通信量が少ないP2P(Point-to-Point)通信であるため、パイプラインステージはノード間で配置するようにしました。これも、Megatron-LMデフォルトの挙動です。

勉強になる

・通常のデータ並列はoptimizer stateをworker間で複製するので遅い。Deep Speed Zero 1のように分散して保有することで高速化
・Tensor Parallelでself attention, MLPの計算を並列化できる
・LayerNormalization, Dropoutの演算もメモリ効率の観点から並列化
・学習を安定させるためにz-lossを利用
・batch skippingとは、gradient clippingを行っていてもなおspikeが生じる場合に、100 step前に戻り、spikeが生じた付近のデータを数百iteration程度スキップすること
</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/Blog.html">#Blog</a><br /><span class="issue_date">Issue Date: 2024-04-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1294">The End of Finetuning — with Jeremy Howard of Fast.ai, 2023.11</a>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/InstructionTuning.html">#InstructionTuning</a><a class="button" href="articles/Repository.html">#Repository</a><a class="button" href="articles/Japanese.html">#Japanese</a><br /><span class="issue_date">Issue Date: 2023-12-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1183">A Review of Public Japanese Training Sets, shisa, 2023.12</a>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a><a class="button" href="articles/Chain-of-Thought.html">#Chain-of-Thought</a><a class="button" href="articles/Evaluation.html">#Evaluation</a><a class="button" href="articles/Blog.html">#Blog</a><a class="button" href="articles/Reasoning.html">#Reasoning</a><br /><span class="issue_date">Issue Date: 2023-05-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/626">Towards Complex Reasoning: the Polaris of Large Language Models, Yao Fu, 2023.05</a>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a><a class="button" href="articles/Survey.html">#Survey</a><br /><span class="issue_date">Issue Date: 2022-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/495">A Paper List for Recommend-system PreTrained Models</a>
<button onclick="hideContent(0)" style="display: none;">hide</button>
</div>


    </div>

</article>
<div class="post-nav"><a class="previous" href="/paper_notes/articles/PostTraining.html" title="PostTrainingに関する論文・技術記事メモの一覧">PostTrainingに関する論文・技術記事メモの一覧</a><a class="next" href="/paper_notes/articles/Privacy.html" title="Privacyに関する論文・技術記事メモの一覧">Privacyに関する論文・技術記事メモの一覧</a></div><div class="post-related">
      <div>Related Articles</div>
      <ul>
        <li class="">
          <a class="post-link"
            href="/paper_notes/articles/KnowledgeEditing.html"
            title="KnowledgeEditingに関する論文・技術記事メモの一覧">
            KnowledgeEditingに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li><li class="">
          <a class="post-link"
            href="/paper_notes/articles/UnitTest.html"
            title="UnitTestに関する論文・技術記事メモの一覧">
            UnitTestに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li><li class="">
          <a class="post-link"
            href="/paper_notes/articles/Coding.html"
            title="Codingに関する論文・技術記事メモの一覧">
            Codingに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li><li class="">
          <a class="post-link"
            href="/paper_notes/articles/ProprietaryLLM.html"
            title="ProprietaryLLMに関する論文・技術記事メモの一覧">
            ProprietaryLLMに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li></ul>
    </div><div class="post-comments"></div></section>
</div>


  </section>
  <section class="sidebar" style="margin-left: 15px;">
    <!-- Get sidebar items --><style type="text/css" media="screen">
.post-menu ul {
  list-style: none;
  padding: 0;
  margin: 0;
}
</style>

<div class="post-menu">
  <div class="post-menu-title">TOC</div>
  <div class="post-menu-content"></div>
</div>

<script>
  function generateContent() {
    var menu = document.querySelector(".post-menu");
    var menuContent =  menu.querySelector(".post-menu-content");
    var headings = document.querySelector(".post-content").querySelectorAll("h2, h3, h4, h5, h6");

    // Hide menu when no headings
    if (headings.length === 0) {
      return menu.style.display = "none";
    }

    // Generate post menu
    var menuHTML = '';
    for (var i = 0; i < headings.length; i++) {
      var h = headings[i];
      menuHTML += (
        '<li class="h-' + h.tagName.toLowerCase() + '">'
        + '<a href="#h-' + h.getAttribute('id') + '">' + h.textContent + '</a></li>');
    }

    menuContent.innerHTML = '<ul>' + menuHTML + '</ul>';

    // The header element
    var header = document.querySelector('header.site-header');

    function doMenuCollapse(index, over_items) {
      var items = menuContent.firstChild.children;

      if (over_items == undefined) {
        over_items = 20;
      }

      if (items.length < over_items) {
        return;
      }

      var activeItem = items[index];
      var beginItem = activeItem
      var endItem = activeItem
      var beginIndex = index;
      var endIndex = index + 1;
      while (beginIndex >= 0
        && !items[beginIndex].classList.contains('h-h2')) {
        beginIndex -= 1;
      }
      while (endIndex < items.length
        && !items[endIndex].classList.contains('h-h2')) {
        endIndex += 1;
      }
      for (var i = 0; i < beginIndex; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
      for (var i = beginIndex + 1; i < endIndex; i++) {
        item = items[i]
        // if (!item.classList.contains('h-h2')) {
          item.style.display = '';
        // }
      }
      for (var i = endIndex; i < items.length; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
    }

    // Init menu collapsed
    doMenuCollapse(-1);

    // Active the menu item
    window.addEventListener('scroll', function (event) {
      var lastActive = menuContent.querySelector('.active');
      var changed = true;
      var activeIndex = -1;
      for (var i = headings.length - 1; i >= 0; i--) {
        var h = headings[i];
        var headingRect = h.getBoundingClientRect();
        var headerRect = header.getBoundingClientRect();
        var headerTop = Math.floor(headerRect.top);
        var headerHeight = Math.floor(headerRect.height);
        var headerHeight = headerTop + headerHeight + 20;
        if (headingRect.top <= headerHeight) {
          var id = 'h-' + h.getAttribute('id');
          var a = menuContent.querySelector('a[href="#' + id  + '"]');
          var curActive = a.parentNode;
          if (curActive) {
            curActive.classList.add('active');
            activeIndex = i;
          }
          if (lastActive == curActive) {
            changed = false;
          }
          break;
        }
      }
      if (changed) {
        if (lastActive) {
          lastActive.classList.remove('active');
        }
        doMenuCollapse(activeIndex);
      }
      event.preventDefault();
    });
  }
  generateContent();
</script>
</section>
</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/paper_notes/"></data>

  <div class="wrapper">
    <div class="site-footer-inner"><div>Copyright © 2023-current AkihikoWATANABE. The header images and any thumbnail images for the posts were generated by ChatGPT's DALL-E3.</div>
      <div>Powered by <a title="Jekyll is a simple, blog-aware, static site
      generator." href="https://jekyllrb.com/">Jekyll</a> &amp; <a title="Yat, yet
      another theme." href="https://github.com/jeffreytse/jekyll-theme-yat">Yat Theme</a>.</div>
      <div class="footer-col rss-subscribe">Subscribe <a href="/paper_notes/feed.xml">via RSS</a></div>
    </div>
  </div>
</footer>
</body>
</html>
