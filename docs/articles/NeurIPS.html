<!DOCTYPE html>
<html lang="ja">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-translate-customization" content="108d9124921d80c3-80e20d618ff053c8-g4f02ec6f3dba68b7-c">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>NeurIPSに関する論文・技術記事メモの一覧 | わたしのべんきょうノート</title>
<meta name="generator" content="Jekyll v3.10.0">
<meta property="og:title" content="NeurIPSに関する論文・技術記事メモの一覧">
<meta name="author" content="AkihikoWATANABE">
<meta property="og:locale" content="ja">
<meta name="description" content="NeurIPS #Pocket #NLP #Dataset #LanguageModel #Evaluation">
<meta property="og:description" content="NeurIPS #Pocket #NLP #Dataset #LanguageModel #Evaluation">
<link rel="canonical" href="http://akihikowatanabe.github.io/paper_notes/articles/NeurIPS.html">
<meta property="og:url" content="http://akihikowatanabe.github.io/paper_notes/articles/NeurIPS.html">
<meta property="og:site_name" content="わたしのべんきょうノート">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-09-15T00:48:09+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="NeurIPSに関する論文・技術記事メモの一覧">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"AkihikoWATANABE"},"dateModified":"2025-09-15T00:48:09+00:00","datePublished":"2025-09-15T00:48:09+00:00","description":"NeurIPS #Pocket #NLP #Dataset #LanguageModel #Evaluation","headline":"NeurIPSに関する論文・技術記事メモの一覧","mainEntityOfPage":{"@type":"WebPage","@id":"http://akihikowatanabe.github.io/paper_notes/articles/NeurIPS.html"},"url":"http://akihikowatanabe.github.io/paper_notes/articles/NeurIPS.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="icon" href="">
  <link rel="canonical" href="http://akihikowatanabe.github.io">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/main.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/custom_button.css">
  <script src="/paper_notes/assets/js/main.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script><link type="application/atom+xml" rel="alternate" href="http://akihikowatanabe.github.io/paper_notes/feed.xml" title="わたしのべんきょうノート">
<script>
  function showMore(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "block";
          
      // このボタンの参照を取得して非表示にします
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "none";
      }
          
      // hideボタンの参照を取得して表示します
      const hideButton = contentDiv.querySelector('button[onclick^="hideContent"]');
      if (hideButton) {
        hideButton.style.display = "block";
      }
    }
  }

  function hideContent(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "none";
  
      // moreボタンの参照を取得して表示します
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "block";
      }
  
      // このボタンを隠します
      const hideButtons = document.querySelectorAll('button[onclick^="hideContent"]');
      const hideButton = hideButtons[index];
      if (hideButton) {
        hideButton.style.display = "none";
      }
    }
  }
</script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script charset="UTF-8" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/go.min.js" async></script>



















<script>
// Init highlight js
document.addEventListener('DOMContentLoaded', function(event) {
  var els = document.querySelectorAll('pre code')

  function addLangData(block) {
    var outer = block.parentElement.parentElement.parentElement;
    var lang = block.getAttribute('data-lang');
    for (var i = 0; i < outer.classList.length; i++) {
      var cls = outer.classList[i];
      if (cls.startsWith('language-')) {
        lang = cls;
        break;
      }
    }
    if (!lang) {
      cls = block.getAttribute('class');
      lang = cls ? cls.replace('hljs ', '') : '';
    }
    if (lang.startsWith('language-')) {
      lang = lang.substr(9);
    }
    block.setAttribute('class', 'hljs ' + lang);
    block.parentNode.setAttribute('data-lang', lang);
  }

  function addBadge(block) {
    var enabled = ('true' || 'true').toLowerCase();
    if (enabled == 'true') {
      var pre = block.parentElement;
      pre.classList.add('badge');
    }
  }

  function handle(block) {
    addLangData(block);
    addBadge(block)
    hljs.highlightBlock(block);
  }

  for (var i = 0; i < els.length; i++) {
    var el = els[i];
    handle(el);
  }
});
</script>

<style>
  /* code language badge */
  pre.badge::before {
    content: attr(data-lang);
    color: #fff;
    background-color: #ff4e00;
    padding: 0 .5em;
    border-radius: 0 2px;
    text-transform: uppercase;
    text-align: center;
    min-width: 32px;
    display: inline-block;
    position: absolute;
    right: 0;
  }

  /* fix wrong badge display for firefox browser */
  code > table pre::before {
    display: none;
  }
</style>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe-lightbox.umd.min.js" async></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe.umd.min.js" async></script>
<link href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/photoswipe.min.css" rel="stylesheet">
<style>
  .pswp .pswp__container .pswp__img {
    background-color: white;
  }
</style>

<script>
  function initPhotoSwipe() {
    let customOptions = {};

    try {
      const data = `{"gallery"=>"section.main", "children"=>"a.photo-swipe", "bgOpacity"=>0.8, "padding"=>{"top"=>20, "bottom"=>40, "left"=>100, "right"=>100}}`.replaceAll("=>", ":");
      customOptions = JSON.parse(data);
    } catch (e) {
      console.info("Invalid custom photo previewer options! " + e.message);
    }

    // Define object and gallery options
    const options = Object.assign(
      {
        gallery: "section.main",
        children: "a.photo-swipe",
        photo_scale: 2,
        // dynamic import is not supported in UMD version
        pswpModule: PhotoSwipe,
      },
      customOptions
    );

    const galleryEl = document.querySelector(options.gallery);
    if (!galleryEl) {
      return;
    }

    const imgEls = [];
    const els = galleryEl.querySelectorAll("img:not(.emoji)");
    els.forEach((el) => {
      if (el.src.trim() == "") {
        return;
      }
      if (!imgEls.includes(el)) {
        imgEls.push(el);
      }
    });

    if (imgEls.length === 0) {
      return;
    }

    imgEls.forEach((imgEl) => {
      imgEl.outerHTML = `
      <a class="photo-swipe"
        href="${imgEl.src}"
        data-pswp-width="${
          Math.max(imgEl.naturalWidth, imgEl.width) * options.photo_scale
        }"
        data-pswp-height="${
          Math.max(imgEl.naturalHeight, imgEl.height) * options.photo_scale
        }"
        data-pswp-caption="${imgEl.getAttribute("caption") || imgEl.alt}"
        target="_blank">
        ${imgEl.outerHTML}
      </a>`;
    });

    // Initialize PhotoSwipe 5
    var lightbox = new PhotoSwipeLightbox(options);

    lightbox.init();
  }

  window.addEventListener("load", initPhotoSwipe);
</script>
<meta name="google-site-verification" content="u_DTTPcCZ806iq51zgirHyWq3556HUKGq8AQfH91iFI">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-P70KSB88WH"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-P70KSB88WH');
  </script>

<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>





































































































































<header class="site-header site-header-transparent" role="banner">

  <div class="wrapper">
    <div class="site-header-inner">
<span class="site-brand"><a class="site-brand-inner" rel="author" href="/paper_notes/">
  <img class="site-favicon" title="わたしのべんきょうノート" src="" onerror="this.style.display='none'">
  わたしのべんきょうノート
</a>
</span><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger">
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewbox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
              </svg>
            </span>
          </label>

          <div class="trigger">









<span class="page-link">



<div id="google_translate_element" style="display: none;">
</div>

<span class="ct-language">
  <ul class="list-unstyled ct-language-dropdown">
    
      <li>
        <a href="#" class="lang-select" data-lang="en">
          
          <img src="https://cdn.countryflags.com/thumbs/united-states-of-america/flag-400.png" title="English">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="fr">
          
          <img src="https://cdn.countryflags.com/thumbs/france/flag-400.png" title="French">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="zh-CN">
          
          <img src="https://cdn.countryflags.com/thumbs/china/flag-400.png" title="Chinese(Simple)">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ja">
          
          <img src="https://cdn.countryflags.com/thumbs/japan/flag-400.png" title="Japanese">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ko">
          
          <img src="https://cdn.countryflags.com/thumbs/south-korea/flag-400.png" title="Korean">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ru">
          
          <img src="https://cdn.countryflags.com/thumbs/russia/flag-400.png" title="Russian">
          
        </a>
      </li>
    
  </ul>
</span>

<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({
    pageLanguage: 'ja',
    autoDisplay: false,
    layout: google.translate.TranslateElement.InlineLayout.VERTICAL
  }, 'google_translate_element');

  // Links to cross-origin destinations are unsafe
  var gll = document.getElementsByClassName('goog-logo-link')[0];
  if (gll) {
    gll.setAttribute('rel', 'noopener');
  }

  function restoreLang() {
    var iframe = document.getElementsByClassName('goog-te-banner-frame')[0];
    if (!iframe) return;

    var innerDoc = iframe.contentDocument || iframe.contentWindow.document;
    var restore_el = innerDoc.getElementsByTagName("button");

    for (var i = 0; i < restore_el.length; i++) {
      if (restore_el[i].id.indexOf("restore") >= 0) {
        restore_el[i].click();
        var close_el = innerDoc.getElementsByClassName("goog-close-link");
        close_el[0].click();
        return;
      }
    }
  }

  function triggerHtmlEvent(element, eventName) {
    var event;
    if (document.createEvent) {
      event = document.createEvent('HTMLEvents');
      event.initEvent(eventName, true, true);
      element.dispatchEvent(event);
    } else {
      event = document.createEventObject();
      event.eventType = eventName;
      element.fireEvent('on' + event.eventType, event);
    }
  }

  var googleCombo = document.querySelector("select.goog-te-combo");
  var langSelect = document.querySelector('.ct-language');
  langSelect.addEventListener('click', function(event) {
    if (!event.target) {
      return;
    }

    var selected = document.querySelector('.ct-language .ct-language-selected');
    if (selected) {
      selected.classList.remove('ct-language-selected');
    }

    var target = event.target;
    while (target && target !== langSelect ) {
      if (target.matches('.lang-select')) {
        break;
      }
      target = target.parentElement;
    }

    if (target && target.matches('.lang-select')) {
      var lang = target.getAttribute('data-lang');
      if (googleCombo.value == lang) {
        restoreLang();
      } else {
        target.parentElement.classList.add('ct-language-selected');
        googleCombo.value = lang;
        triggerHtmlEvent(googleCombo, 'change');
      }
    }

    event.preventDefault();
  });
}
</script>

<script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit" async></script>
</span>
</div>
        </nav>
</div>
  </div>
</header>

<script>
  function initHeader() {
    var lastScrollY = getScrollPos().y;
    var documentElement = document.documentElement;

    function storeScrollData() {
      var y = getScrollPos().y;documentElement.setAttribute("data-header-transparent", "");var scrollStatus = "";

      if (y <= 0) {
        scrollStatus = "top";
      } else if ((window.innerHeight + y) >= document.body.offsetHeight) {
        scrollStatus = "bottom";
      } else {
        var isScrollDown = (y - lastScrollY > 0) ? true : false;
        scrollStatus = isScrollDown ? "down" : "up";
      }

      lastScrollY = y;
      documentElement.setAttribute("data-scroll-status", scrollStatus);
    }

    window.addEventListener('scroll', function(e) {
      storeScrollData();
    });

    storeScrollData();
  }
  document.addEventListener('DOMContentLoaded', initHeader);
</script>


























































































































































<style>
    html .page-banner .page-banner-img > *:first-child {
      opacity: 0.4;
    }

    html[data-theme="dark"] .page-banner .page-banner-img > *:first-child {
      opacity: 0.2872;
    }
  </style>
<section class="page-banner">
    <div class="page-banner-img">
<div style="background-image: url(/paper_notes/assets/images/banner.png)"></div>
        <img class="img-placeholder" src="/paper_notes/assets/images/banner.png">
</div>
    <div class="wrapper">
      <div class="page-banner-inner">
<header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">わたしのべんきょうノート</h1>
  <h2 class="post-subtitle">勉強した論文や技術等の情報をGithubのIssueにメモっているひとのブログ。
それなりにメモの量が蓄積されてきたので、一度整理したいなと思いブログはじめてみました！
自然言語処理(NLP), 推薦システム(RecommenderSystem), Educational Data Mining (EDM), Learning Analytics (LA)などの分野のメモが多いと思います。
最近は特にLLMの勉強が多めです :)</h2>

  <div class="post-meta">
    <time class="dt-published" datetime="2025-09-15T00:48:09+00:00" itemprop="datePublished"><i class="fa fa-calendar"></i> Sep 15, 2025
    </time><span class="post-author left-vsplit"><i class="fa fa-pencil"></i> AkihikoWATANABE</span>
    
































    <span class="post-reading-time left-vsplit"><i class="fa fa-clock-o"></i> About 2 hours 13 mins</span>
  </div></header>
</div>
    </div>
  </section><script>
  function hashLocate(hashValue) {
    hashValue = hashValue.replace(/^.*#h-/, '');
    hashValue = decodeURIComponent(hashValue);
    var element = document.getElementById(hashValue);

    if (!element) {
      return;
    }

    var header = document.querySelector('header.site-header');
    var headerRect = header.getBoundingClientRect();
    var headerTop = Math.floor(headerRect.top);
    var headerHeight = Math.floor(headerRect.height);
    var scrollPos = getScrollPos();
    var offsetY = element.offsetTop - (headerTop + headerHeight + 20);

    if (offsetY == scrollPos.y) {
      return;
    }

    if (headerTop == 0  && offsetY > scrollPos.y) {
      offsetY += headerHeight + 2;
    } else if (headerTop < 0  && offsetY < scrollPos.y) {
      offsetY -= headerHeight - 2;
    }

    smoothScrollTo(offsetY);
  }

  // The first event occurred
  window.addEventListener('load', function(event) {
    if (window.location.hash) {
      hashLocate(window.location.hash);
    }
  });

  // The first event occurred
  window.addEventListener('click', function(event) {
    if (event.target.tagName.toLowerCase() == 'a') {
      hashLocate(event.target.getAttribute('href'));
    }
  });
</script>
<div class="theme-toggle">
  <input type="checkbox" id="theme-switch">
  <label for="theme-switch">
    <div class="toggle"></div>
    <div class="names">
      <p class="light">Light</p>
      <p class="dark">Dark</p>
    </div>
  </label>
</div>




<script>
  (function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var nightModeOption = ('off' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
      storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
      var data = storage.getItem('theme');
      try {
        data = JSON.parse(data ? data : '');
      } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
      }
      return data;
    }

    function handleThemeToggle(nightShift) {
      themeData.nightShift = nightShift;
      saveThemeData(themeData);
      html.dataset.theme = nightShift ? 'dark' : 'light';
      setTimeout(function() {
        sw.checked = nightShift ? true : false;
      }, 50);
    }

    function autoThemeToggle() {
      // Next time point of theme toggle
      var now = new Date();
      var toggleAt = new Date();
      var hours = now.getHours();
      var nightShift = hours >= 19 || hours <=7;

      if (nightShift) {
        if (hours > 7) {
          toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
      } else {
        toggleAt.setHours(19);
      }

      toggleAt.setMinutes(0);
      toggleAt.setSeconds(0);
      toggleAt.setMilliseconds(0)

      var delay = toggleAt.getTime() - now.getTime();

      // auto toggle theme mode
      setTimeout(function() {
        handleThemeToggle(!nightShift);
      }, delay);

      return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
      };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
      handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
      var data = autoThemeToggle();

      // Toggle theme by local setting
      if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
      } else {
        handleThemeToggle(themeData.nightShift);
      }
    } else if (nightModeOption == 'manual') {
      handleThemeToggle(themeData.nightShift);
    } else {
      var nightShift = themeData.nightShift;
      if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
      }
      handleThemeToggle(nightShift);
    }
  })();
</script>
<div id="click-to-top" class="click-to-top">
  <i class="fa fa-arrow-up"></i>
</div>
<script>
  (function () {
    const clickToTop = document.getElementById('click-to-top');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 100) {
        clickToTop.classList.add('show')
      }else {
        clickToTop.classList.remove('show')
      }
    });
    clickToTop.addEventListener('click', () => {
      window.smoothScrollTo(0);
    });
  })();
</script>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="framework">
  <section class="main">

     <div class="post">
  <section>









<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="post-content e-content" itemprop="articleBody">

      <h2 id="NeurIPS"> NeurIPS</h2>
<div class="visible-content">
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>


<br>


<span class="issue_date">Issue Date: 2025-09-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2739" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] MixEval: Deriving Wisdom of the Crowd from LLM Benchmark Mixtures, Jinjie Ni+, NeurIPS'24</a>
<span class="snippet"><span>Summary</span>- MixEvalは、LLM評価の新しいパラダイムであり、実世界のユーザークエリと真実に基づくベンチマークを組み合わせることで、効率的かつ公正な評価を実現する。これにより、Chatbot Arenaとの高い相関を持ち、迅速かつ安価な評価が可能となる。さらに、動的評価を通じてLLM評価の理解を深め、今後の研究方向を示す。</span>
<span class="snippet"><span>Comment</span><p>openreview: 


<a href="https://openreview.net/forum?id=6A29LUZhfv&referrer=%5Bthe%20profile%20of%20Yang%20You%5D(%2Fprofile%3Fid%3D~Yang_You1)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=6A29LUZhfv&referrer=%5Bthe%20profile%20of%20Yang%20You%5D(%2Fprofile%3Fid%3D~Yang_You1)</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>


<br>


<span class="issue_date">Issue Date: 2025-09-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2734" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] MMLU-Pro: A More Robust and Challenging Multi-Task Language   Understanding Benchmark, Yubo Wang+, NeurIPS'24</a>
<span class="snippet"><span>Summary</span>- MMLUベンチマークの限界を克服するため、推論に焦点を当てた質問を統合し、選択肢を4から10に増やした強化データセットMMLU-Proを提案。MMLU-Proは些細な質問を排除し、精度が16%から33%低下する一方で、プロンプトに対する安定性が向上。Chain of Thought推論を利用するモデルは、MMLU-Proでより良いパフォーマンスを示し、複雑な推論問題を含むことを示唆。MMLU-Proは、より識別的なベンチマークとして分野の進展を追跡するのに適している。</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=y10DM6R2r3&referrer=%5Bthe%20profile%20of%20Ge%20Zhang%5D(%2Fprofile%3Fid%3D~Ge_Zhang5)#discussion" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=y10DM6R2r3&referrer=%5Bthe%20profile%20of%20Ge%20Zhang%5D(%2Fprofile%3Fid%3D~Ge_Zhang5)#discussion</a>


</p>
<p>MMLUはこちら:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/901" target="_blank" rel="noopener noreferrer">Measuring Massive Multitask Language Understanding, Dan Hendrycks+, N/A, ICLR'21</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Alignment.html" target="_blank" rel="noopener noreferrer">#Alignment</a>
<a class="button" href="articles/Safety.html" target="_blank" rel="noopener noreferrer">#Safety</a>


<br>


<span class="issue_date">Issue Date: 2025-09-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2732" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Stepwise Alignment for Constrained Language Model Policy Optimization, Akifumi Wachi+, arXiv'24</a>
<span class="snippet"><span>Summary</span>- 安全性と信頼性はLLMを用いるAIシステムにおいて重要であり、本研究では報酬最大化を人間の価値に基づく安全性制約の下で定式化し、逐次整合性アルゴリズム（SACPO）を提案。SACPOは報酬と安全性を組み込んだ最適ポリシーを段階的に整合させ、シンプルで強力な整合性アルゴリズムを活用。理論的分析により最適性と安全性制約違反の上限を示し、実験結果ではSACPOがAlpaca-7Bのファインチューニングにおいて最先端手法を上回ることを確認。</span>
<span class="snippet"><span>Comment</span><p>NLPコロキウムでのスライドを参照のこと: <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1603" target="_blank" rel="noopener noreferrer">【NLPコロキウム】Stepwise Alignment for Constrained Language Model Policy Optimization (NeurIPS 2024)  , 2024.12</a>
</p>
<p>openreview: 


<a href="https://openreview.net/forum?id=VrVx83BkQX&referrer=%5Bthe%20profile%20of%20Takumi%20Tanabe%5D(%2Fprofile%3Fid%3D~Takumi_Tanabe1)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=VrVx83BkQX&referrer=%5Bthe%20profile%20of%20Takumi%20Tanabe%5D(%2Fprofile%3Fid%3D~Takumi_Tanabe1)</a>


</p></span><br><br>
</div>
<p><button onclick="showMore(0)">more</button></p>
<div class="hidden-content">
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<span class="issue_date">Issue Date: 2025-09-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2731" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Unpacking DPO and PPO: Disentangling Best Practices for Learning from   Preference Feedback, Hamish Ivison+, NeurIPS'24</a>
<span class="snippet"><span>Summary</span>- 好みのフィードバックから学ぶことは、言語モデルの生成品質向上に重要である。本研究では、好みに基づく学習の4つの核心的側面（好みデータ、学習アルゴリズム、報酬モデル、ポリシートレーニングプロンプト）を特定し、それぞれの影響を調査。特に、良質な好みデータが最も大きな改善をもたらし、次いで学習アルゴリズムや報酬モデルの改善が続くことを示した。PPOを用いることで数学分野で最大2.5%、一般分野で1.2%の改善が見られ、高品質の好みデータは指示遵守能力に最大8%の向上をもたらした。</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=JMBWTlazjW" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=JMBWTlazjW</a>


</p>
<p>以下のオンライン vs. オフラインRLのポストで本研究が引用されている:<br>


</p>
<div class="tweet-embed" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/cwolferesearch/status/1965088925510520853?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>&lt;img alt="loading..." src="/assets/images/load-31_128.gif class="tweet-loading" /&gt;</div>


<p>関連:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2730" target="_blank" rel="noopener noreferrer">[Paper Note] Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study, Shusheng Xu+, ICML'24</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2090" target="_blank" rel="noopener noreferrer">[Paper Note] Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy   Data, Fahim Tajwar+, ICML'24</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/Routing.html" target="_blank" rel="noopener noreferrer">#Routing</a>
<span class="issue_date">Issue Date: 2025-09-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2690" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Multi-Head Mixture-of-Experts, Xun Wu+, NeurIPS'24</a>
<span class="snippet"><span>Summary</span>- MH-MoEは、マルチヘッドメカニズムを用いてトークンを複数のサブトークンに分割し、専門家の活性化を向上させる新しい手法です。これにより、文脈理解が深まり、過学習が軽減されます。MH-MoEは実装が簡単で、他のSMoEモデルと統合可能であり、広範な実験でその有効性が示されています。</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=dyZ8GJZjtX&referrer=%5Bthe%20profile%20of%20Shaohan%20Huang%5D(%2Fprofile%3Fid%3D~Shaohan_Huang1)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=dyZ8GJZjtX&referrer=%5Bthe%20profile%20of%20Shaohan%20Huang%5D(%2Fprofile%3Fid%3D~Shaohan_Huang1)</a>


</p>
<p>SNLP'24での解説スライド:


<a href="https://speakerdeck.com/takase/snlp2024-multiheadmoe" target="_blank" rel="noopener noreferrer">https://speakerdeck.com/takase/snlp2024-multiheadmoe</a>


</p>
<p>MoEのRouting Collapseに対して、Expertsの表現力を落とすことで特定のExpertsにルーティングが偏らないようにする、というコンセプトな模様。具体的には、inputを複数headに分割してhead単位でExpertsを選択し、出力をconcatする、といったアーキテクチャらしい。</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Memorization.html" target="_blank" rel="noopener noreferrer">#Memorization</a>
<span class="issue_date">Issue Date: 2025-09-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2679" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Be like a Goldfish, Don't Memorize Mitigating Memorization in   Generative LLMs, Abhimanyu Hans+, NeurIPS'24</a>
<span class="snippet"><span>Summary</span>- 「ゴールドフィッシュロス」を導入し、トレーニング中にランダムに選ばれたトークンをロス計算から除外することで、プライバシーや著作権リスクを軽減。10億規模のLlama-2モデルの実験により、下流のベンチマークに影響を与えずに記憶の削減を実証。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:


</p>
<div class="tweet-embed" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/vikhyatk/status/1962954696500674908?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>&lt;img alt="loading..." src="/assets/images/load-31_128.gif class="tweet-loading" /&gt;</div>


<p>クロスエントロピーのloss計算からランダムにtokenを除外せることでdownstream taskの性能を損なうことなくmemorizationを防げますよ、という話らしい</p>
<p>openreview:


<a href="https://openreview.net/forum?id=DylSyAfmWs&referrer=%5Bthe%20profile%20of%20Jonas%20Geiping%5D(%2Fprofile%3Fid%3D~Jonas_Geiping1)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=DylSyAfmWs&referrer=%5Bthe%20profile%20of%20Jonas%20Geiping%5D(%2Fprofile%3Fid%3D~Jonas_Geiping1)</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Zero/Few/ManyShotPrompting.html" target="_blank" rel="noopener noreferrer">#Zero/Few/ManyShotPrompting</a>
<a class="button" href="articles/Prompting.html" target="_blank" rel="noopener noreferrer">#Prompting</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2025-09-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2638" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Many-Shot In-Context Learning, Rishabh Agarwal+, NeurIPS'24</a>
<span class="snippet"><span>Summary</span>- 大規模言語モデル（LLMs）は、少数ショットから多くのショットのインコンテキスト学習（ICL）において顕著な性能向上を示す。新たな設定として、モデル生成の思考過程を用いる強化されたICLと、ドメイン特有の質問のみを用いる無監督ICLを提案。これらは特に複雑な推論タスクに効果的であり、多くのショット学習は事前学習のバイアスを覆し、ファインチューニングと同等の性能を発揮することが示された。また、推論コストは線形に増加し、最前線のLLMsは多くのショットのICLから恩恵を受けることが確認された。</span>
<span class="snippet"><span>Comment</span><p>many-shotを提案</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/Mathematics.html" target="_blank" rel="noopener noreferrer">#Mathematics</a>
<span class="issue_date">Issue Date: 2025-08-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2615" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] DART-Math: Difficulty-Aware Rejection Tuning for Mathematical  Problem-Solving, Yuxuan Tong+, NeurIPS'24</a>
<span class="snippet"><span>Summary</span>- 数学問題解決には高度な推論が必要であり、従来のモデルは難しいクエリに対して偏りがあることが明らかになった。そこで、Difficulty-Aware Rejection Tuning（DART）を提案し、難しいクエリに多くの試行を割り当てることでトレーニングを強化。新たに作成した小規模な数学問題データセットで、7Bから70BのモデルをファインチューニングしたDART-MATHは、従来の手法を上回る性能を示した。合成データセットが数学問題解決において効果的でコスト効率の良いリソースであることが確認された。</span>
<span class="snippet"><span>Comment</span><p>openreview: 


<a href="https://openreview.net/forum?id=zLU21oQjD5&referrer=%5Bthe%20profile%20of%20Rui%20Wang%5D(%2Fprofile%3Fid%3D~Rui_Wang1)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=zLU21oQjD5&referrer=%5Bthe%20profile%20of%20Rui%20Wang%5D(%2Fprofile%3Fid%3D~Rui_Wang1)</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/ReversalCurse.html" target="_blank" rel="noopener noreferrer">#ReversalCurse</a>
<span class="issue_date">Issue Date: 2025-08-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2395" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] The Factorization Curse: Which Tokens You Predict Underlie the Reversal   Curse and More, Ouail Kitouni+, NeurIPS'24</a>
<span class="snippet"><span>Summary</span>- 最先端の言語モデルは幻覚に悩まされ、情報取得において逆転の呪いが問題となる。これを因数分解の呪いとして再定義し、制御実験を通じてこの現象が次トークン予測の固有の失敗であることを発見。信頼性のある情報取得は単純な手法では解決できず、ファインチューニングも限界がある。異なるタスクでの結果は、因数分解に依存しないアプローチが逆転の呪いを軽減し、知識の保存と計画能力の向上に寄与する可能性を示唆している。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:


</p>
<div class="tweet-embed" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/scaling01/status/1954682957798715669?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>&lt;img alt="loading..." src="/assets/images/load-31_128.gif class="tweet-loading" /&gt;</div>


<p>openreview:


<a href="https://openreview.net/forum?id=f70e6YYFHF" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=f70e6YYFHF</a>


</p>
<p>Reversal Curseを提言した研究は下記:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1059" target="_blank" rel="noopener noreferrer">[Paper Note] The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A", Lukas Berglund+, arXiv'23</a>
</p>
<p>関連:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2399" target="_blank" rel="noopener noreferrer">[Paper Note] Physics of Language Models: Part 3.2, Knowledge Manipulation, Zeyuan Allen-Zhu+, ICLR'25</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/DPO.html" target="_blank" rel="noopener noreferrer">#DPO</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-07-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2127" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Iterative Reasoning Preference Optimization, Richard Yuanzhe Pang+, NeurIPS'24</a>
<span class="snippet"><span>Summary</span>- 反復的な好み最適化手法を用いて、Chain-of-Thought（CoT）候補間の推論ステップを最適化するアプローチを開発。修正DPO損失を使用し、推論の改善を示す。Llama-2-70B-ChatモデルでGSM8K、MATH、ARC-Challengeの精度を向上させ、GSM8Kでは55.6%から81.6%に改善。多数決による精度は88.7%に達した。</span>
<span class="snippet"><span>Comment</span><p>OpenReview:


<a href="https://openreview.net/forum?id=4XIKfvNYvx&referrer=%5Bthe%20profile%20of%20He%20He%5D(%2Fprofile%3Fid%3D~He_He2)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=4XIKfvNYvx&referrer=%5Bthe%20profile%20of%20He%20He%5D(%2Fprofile%3Fid%3D~He_He2)</a>


</p>
<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1212" target="_blank" rel="noopener noreferrer">Self-Rewarding Language Models, Weizhe Yuan+, N/A, ICML'24</a>
<br><br>と似たようにiterativeなmannerでreasoning能力を向上させる。<br><br><img src="https://github.com/user-attachments/assets/a0f10e8e-454d-40e8-ae67-8c6c2da6a0ed" alt="image" loading="lazy"><br><br>ただし、loss functionとしては、chosenなCoT+yのresponseに対して、reasoning traceを生成する能力を高めるために、NLL Lossも適用している点に注意。<br><img src="https://github.com/user-attachments/assets/5ae2dcba-09c8-4618-9b63-ae6aed5b234d" alt="image" loading="lazy"><br><br>32 samplesのmajority votingによってより高い性能が達成できているので、多様なreasoning traceが生成されていることが示唆される。</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-05-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1942" target="_blank" rel="noopener noreferrer" class="title-link">The FineWeb Datasets: Decanting the Web for the Finest Text Data at   Scale, Guilherme Penedo+, NeurIPS'24</a>
<span class="snippet"><span>Summary</span>- 本研究では、15兆トークンからなるFineWebデータセットを紹介し、LLMの性能向上に寄与することを示します。FineWebは高品質な事前学習データセットのキュレーション方法を文書化し、重複排除やフィルタリング戦略を詳細に調査しています。また、FineWebから派生した1.3兆トークンのFineWeb-Eduを用いたLLMは、MMLUやARCなどのベンチマークで優れた性能を発揮します。データセット、コードベース、モデルは公開されています。</span>
<span class="snippet"><span>Comment</span><p>日本語解説:


<a href="https://zenn.dev/deepkawamura/articles/da9aeca6d6d9f9" target="_blank" rel="noopener noreferrer">https://zenn.dev/deepkawamura/articles/da9aeca6d6d9f9</a>


</p>
<p>openreview:


<a href="https://openreview.net/forum?id=n6SCkn2QaG#discussion" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=n6SCkn2QaG#discussion</a>


</p></span><br><br>
<a class="button" href="articles/Tools.html" target="_blank" rel="noopener noreferrer">#Tools</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/API.html" target="_blank" rel="noopener noreferrer">#API</a>
<span class="issue_date">Issue Date: 2025-04-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1874" target="_blank" rel="noopener noreferrer" class="title-link">Gorilla: Large Language Model Connected with Massive APIs, Shishir G. Patil+, NeurIPS'24</a>
<span class="snippet"><span>Summary</span>- Gorillaは、API呼び出しの生成においてGPT-4を上回るLLaMAベースのモデルであり、文書検索システムと組み合わせることで、テスト時の文書変更に適応し、ユーザーの柔軟な更新を可能にします。幻覚の問題を軽減し、APIをより正確に使用する能力を示します。Gorillaの評価には新たに導入したデータセット「APIBench」を使用し、信頼性と適用性の向上を実現しています。</span>
<span class="snippet"><span>Comment</span><p>APIBench:


<a href="https://huggingface.co/datasets/gorilla-llm/APIBench" target="_blank" rel="noopener noreferrer">https://huggingface.co/datasets/gorilla-llm/APIBench</a>


</p>
<p>OpenReview:


<a href="https://openreview.net/forum?id=tBRNC6YemY" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=tBRNC6YemY</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Pruning.html" target="_blank" rel="noopener noreferrer">#Pruning</a>
<a class="button" href="articles/Distillation.html" target="_blank" rel="noopener noreferrer">#Distillation</a>
<span class="issue_date">Issue Date: 2025-03-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1804" target="_blank" rel="noopener noreferrer" class="title-link">Compact Language Models via Pruning and Knowledge Distillation, Saurav Muralidharan+, NeurIPS'24</a>
<span class="snippet"><span>Summary</span>- 本論文では、既存の大規模言語モデル（LLMs）をプルーニングし、少量のトレーニングデータで再トレーニングする手法を提案。深さ、幅、注意、MLPプルーニングを知識蒸留と組み合わせた圧縮ベストプラクティスを開発し、Nemotron-4ファミリーのLLMを2-4倍圧縮。これにより、トレーニングに必要なトークン数を最大40倍削減し、計算コストを1.8倍削減。Minitronモデルは、ゼロからトレーニングした場合と比較してMMLUスコアが最大16%改善され、他のモデルと同等の性能を示す。モデルの重みはオープンソース化され、補足資料も提供。</span>
<span class="snippet"><span>Comment</span><p>OpenReview:


<a href="https://openreview.net/forum?id=9U0nLnNMJ7&referrer=%5Bthe%20profile%20of%20Pavlo%20Molchanov%5D(%2Fprofile%3Fid%3D~Pavlo_Molchanov1)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=9U0nLnNMJ7&referrer=%5Bthe%20profile%20of%20Pavlo%20Molchanov%5D(%2Fprofile%3Fid%3D~Pavlo_Molchanov1)</a>


</p>
<p><img src="https://github.com/user-attachments/assets/76ab1107-bf94-4cf1-9ad1-e9f494b917e7" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/d1bf8a84-5365-4d35-aae0-146b1860ed9d" alt="image" loading="lazy"><br><br>（あとでメモを追記）</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/LLMAgent.html" target="_blank" rel="noopener noreferrer">#LLMAgent</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2025-01-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1729" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Chain of Agents: Large language models collaborating on long-context tasks, Google Research, 2025.01, NeurIPS'24</a>
<span class="snippet"><span>Comment</span><p>元ポスト:


</p>
<div class="tweet-embed" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/googleai/status/1882554959272849696?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>&lt;img alt="loading..." src="/assets/images/load-31_128.gif class="tweet-loading" /&gt;</div>


<p>LLMがどこまでいってもcontext長の制約に直面する問題に対してLLM Agentを組み合わせて対処しました、的な話な模様</p>
<p>ブログ中にアプローチを解説した動画があるのでわかりやすい</p>
<p>Is the experimental code open source?</p>
<p>Thank you for your comment. I tried to find an official open-source implementation provided by the authors, but I was not able to locate one. In fact, I also checked the personal webpage of the first author, but there was no link to any released code.<br><br>Is seems that an unofficial implementation is listed under the “Code” tab on the NeurIPS page. I hope this is helpful. Thank you.<br><br>NeurIPS link: 


<a href="https://nips.cc/virtual/2024/poster/95563" target="_blank" rel="noopener noreferrer">https://nips.cc/virtual/2024/poster/95563</a>


<br>openreview: 


<a href="https://openreview.net/forum?id=LuCLf4BJsr" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=LuCLf4BJsr</a>


</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<span class="issue_date">Issue Date: 2024-12-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1584" target="_blank" rel="noopener noreferrer" class="title-link">Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale   Prediction, Keyu Tian+, NeurIPS'24</a>
<span class="snippet"><span>Summary</span>- Visual AutoRegressive modeling (VAR)を提案し、画像生成において自己回帰学習を次のスケール予測として再定義。VARは、GPTのようなARモデルが拡散トランスフォーマーを上回ることを実現し、ImageNet 256x256ベンチマークでFIDを18.65から1.73、ISを80.4から350.2に改善。推論速度は約20倍向上し、画像品質やデータ効率でも優れた性能を示す。VARはゼロショット一般化能力を持ち、スケーリング法則を示す。全モデルとコードを公開し、視覚生成の研究を促進。</span>
<span class="snippet"><span>Comment</span><p>NeurIPS2024のベストペーパー</p>
<p>第一著者がByteDance社から訴訟を起こされている模様…？<br>


<a href="https://var-integrity-report.github.io" target="_blank" rel="noopener noreferrer">https://var-integrity-report.github.io</a>


</p>
<p>OpenReview:


<a href="https://openreview.net/forum?id=gojL67CfS8" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=gojL67CfS8</a>


</p>
<p>Next Token Prediction, Next Image Token Generation (従来手法）, Next Scale (resolution) prediction (提案手法)の違いの図解。非常に分かりやすい。next token predictionでは次トークンのみを予測するがVARでは、次の解像度画像の全体のトークンマップを予測する。<br><br><img src="https://github.com/user-attachments/assets/668d7523-f262-45c1-a1d0-2dd479c0a708" alt="image" loading="lazy"><br><br>学習方法の概要。2-Stageで学習される。最初のステージでK種類の解像度の画像（＝K種類のマルチスケールのtoken maps r_k）を得るためにAutoEncoderを学習し、次のステージでblock-wiseのcausal attention maskを用いて、K_&lt;k個目の解像度の画像からK個目の解像度の画像を予測する（図を見るとイメージを掴みやすい）。inference時はKV Cacheを利用し、maskは不要となる。<br>各r_kをデコードする際にr_&lt;kのみに依存する設計にすることでcoase-to-fineに画像を生成することに相当し、これは人間の粗く捉えてから詳細を見る認知プロセスと合致する。また、flatten操作が存在せず、それぞれのr_&lt;k内のトークンがr_k生成時に全て考慮されるため空間的局所性も担保される。また、r_k内のトークンは並列に生成可能なので計算量のオーダーが大幅に削減される（O(n^4)。<br><img src="https://github.com/user-attachments/assets/e1a85712-e66a-4c9a-9cf1-6556f2b8e687" alt="image" loading="lazy"><br><br>従来手法と比べより小さいパラメータで高い性能を実現し、inference timeも非常に早い。<br><img src="https://github.com/user-attachments/assets/90a6a7de-995d-49e6-94a2-cd709e68777f" alt="image" loading="lazy"><br><br>ScalingLawsも成立する。<br><img src="https://github.com/user-attachments/assets/351c2a7b-85aa-4cc7-8ba2-a5e9528cabd4" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<span class="issue_date">Issue Date: 2024-02-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1236" target="_blank" rel="noopener noreferrer" class="title-link">Linear Transformers are Versatile In-Context Learners, Max Vladymyrov+, N_A, NeurIPS'24</a>
<span class="snippet"><span>Summary</span>- 研究では、線形transformersが複雑な問題に対して効果的な最適化アルゴリズムを見つける能力を持つことが示された。特に、トレーニングデータが異なるノイズレベルで破損している場合でも、線形transformersは合理的なベースラインを上回るか匹敵する結果を示した。新しいアプローチとして、運動量と再スケーリングを組み込んだ最適化戦略が提案された。これにより、線形transformersが洗練された最適化戦略を発見する能力を持つことが示された。</span>
<span class="snippet"><span>Comment</span><p>openreview: 


<a href="https://openreview.net/forum?id=MWV9zfgW9s" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=MWV9zfgW9s</a>


</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-09-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2769" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] GenEval: An Object-Focused Framework for Evaluating Text-to-Image   Alignment, Dhruba Ghosh+, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>- テキストから画像への生成モデルの自動評価方法「GenEval」を提案。物体の共起、位置、数、色などの特性を評価し、現在の物体検出モデルを活用して生成タスクを分析。最近のモデルは改善を示すが、複雑な能力には課題が残る。GenEvalは失敗モードの発見にも寄与し、次世代モデルの開発に役立つ。コードは公開中。</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=Wbr51vK331&noteId=NpvYJlJFqK" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=Wbr51vK331&noteId=NpvYJlJFqK</a>


</p></span><br><br>
<a class="button" href="articles/RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/VariationalAutoEncoder.html" target="_blank" rel="noopener noreferrer">#VariationalAutoEncoder</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ColdStart.html" target="_blank" rel="noopener noreferrer">#ColdStart</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<a class="button" href="articles/SemanticID.html" target="_blank" rel="noopener noreferrer">#SemanticID</a>
<span class="issue_date">Issue Date: 2025-07-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2309" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Recommender Systems with Generative Retrieval, Shashank Rajput+, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>- 新しい生成的検索アプローチを提案し、アイテムのセマンティックIDを用いて次のアイテムを予測するTransformerベースのモデルを訓練。これにより、従来のレコメンダーシステムを大幅に上回る性能を達成し、過去の対話履歴がないアイテムに対しても改善された検索性能を示す。</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=BJ0fQUU32w" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=BJ0fQUU32w</a>


</p>
<p>Semantic IDを提案した研究</p>
<p>アイテムを意味的な情報を保持したdiscrete tokenのタプル（＝Semantic ID)で表現し、encoder-decoderでNext ItemのSemantic IDを生成するタスクに落としこむことで推薦する。SemanticIDの作成方法は後で読んで理解したい。<br><br><img src="https://github.com/user-attachments/assets/38606c7c-011f-46b0-ab14-8d213626be3d" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/a239bac7-c273-4681-a102-65e88c9c65d2" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/7d1822ce-462e-43f6-bb33-af77914919f6" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Hallucination.html" target="_blank" rel="noopener noreferrer">#Hallucination</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/ActivationSteering/ITI.html" target="_blank" rel="noopener noreferrer">#ActivationSteering/ITI</a>
<a class="button" href="articles/Probing.html" target="_blank" rel="noopener noreferrer">#Probing</a>
<a class="button" href="articles/Trustfulness.html" target="_blank" rel="noopener noreferrer">#Trustfulness</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-05-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1941" target="_blank" rel="noopener noreferrer" class="title-link">Inference-Time Intervention: Eliciting Truthful Answers from a Language   Model, Kenneth Li+, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>- Inference-Time Intervention (ITI)を提案し、LLMsの真実性を向上させる技術を紹介。ITIは推論中にモデルの活性化を調整し、LLaMAモデルの性能をTruthfulQAベンチマークで大幅に改善。Alpacaモデルでは真実性が32.5%から65.1%に向上。真実性と有用性のトレードオフを特定し、介入の強度を調整する方法を示す。ITIは低コストでデータ効率が高く、数百の例で真実の方向性を特定可能。LLMsが虚偽を生成しつつも真実の内部表現を持つ可能性を示唆。</span>
<span class="snippet"><span>Comment</span><p>Inference Time Interventionを提案した研究。Attention Headに対して線形プロービング[^1]を実施し、真実性に関連するであろうHeadをtopKで特定できるようにし、headの出力に対し真実性を高める方向性のベクトルvを推論時に加算することで（＝intervention）、モデルの真実性を高める。vは線形プロービングによって学習された重みを使う手法と、正答と誤答の活性化の平均ベクトルを計算しその差分をvとする方法の二種類がある。後者の方が性能が良い。topKを求める際には、線形プロービングをしたモデルのvalidation setでの性能から決める。Kとαはハイパーパラメータである。<br><br>[^1]: headのrepresentationを入力として受け取り、線形モデルを学習し、線形モデルの2値分類性能を見ることでheadがどの程度、プロービングの学習に使ったデータに関する情報を保持しているかを測定する手法<br><br>日本語解説スライド:


<a href="https://www.docswell.com/s/DeepLearning2023/Z38P8D-2024-06-20-131813#p1" target="_blank" rel="noopener noreferrer">https://www.docswell.com/s/DeepLearning2023/Z38P8D-2024-06-20-131813#p1</a>


</p>
<p>これは相当汎用的に使えそうな話だから役に立ちそう</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/PositionalEncoding.html" target="_blank" rel="noopener noreferrer">#PositionalEncoding</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-04-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1865" target="_blank" rel="noopener noreferrer" class="title-link">The Impact of Positional Encoding on Length Generalization in   Transformers, Amirhossein Kazemnejad+, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>- 長さ一般化はTransformerベースの言語モデルにおける重要な課題であり、位置エンコーディング（PE）がその性能に影響を与える。5つの異なるPE手法（APE、T5の相対PE、ALiBi、Rotary、NoPE）を比較した結果、ALiBiやRotaryなどの一般的な手法は長さ一般化に適しておらず、NoPEが他の手法を上回ることが明らかになった。NoPEは追加の計算を必要とせず、絶対PEと相対PEの両方を表現可能である。さらに、スクラッチパッドの形式がモデルの性能に影響を与えることも示された。この研究は、明示的な位置埋め込みが長いシーケンスへの一般化に必須でないことを示唆している。</span>
<span class="snippet"><span>Comment</span><p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1863" target="_blank" rel="noopener noreferrer">Llama 4 Series, Meta, 2025.04</a>
<br><br>において、Llama4 Scoutが10Mコンテキストウィンドウを実現できる理由の一つとのこと。<br><br>元ポスト:


</p>
<div class="tweet-embed" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/drjimfan/status/1908615861650547081?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>&lt;img alt="loading..." src="/assets/images/load-31_128.gif class="tweet-loading" /&gt;</div>


<br><br>Llama4のブログポストにもその旨記述されている:<br>&gt;A key innovation in the Llama 4 architecture is the use of interleaved attention layers without positional embeddings. Additionally, we employ inference time temperature scaling of attention to enhance length generalization.<br><br>[The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation](


<a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/?utm_source=twitter&utm_medium=organic_social&utm_content=image&utm_campaign=llama4)" target="_blank" rel="noopener noreferrer">https://ai.meta.com/blog/llama-4-multimodal-intelligence/?utm_source=twitter&utm_medium=organic_social&utm_content=image&utm_campaign=llama4)</a>


<p>斜め読みだが、length generalizationを評価する上でdownstream taskに焦点を当て、3つの代表的なカテゴリに相当するタスクで評価したところ、この観点においてはT5のrelative positinal encodingとNoPE（位置エンコードディング無し）のパフォーマンスが良く、<br><br><img src="https://github.com/user-attachments/assets/dddadfff-ab28-4073-96c3-831eb16845a0" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/c6ec8e0e-7abb-4330-be23-2261486a477c" alt="image" loading="lazy"><br><br>NoPEは絶対位置エンコーディングと相対位置エンコーディングを理論上実現可能であり[^1]<br><img src="https://github.com/user-attachments/assets/bbcf797a-d394-42d4-b017-08d7dba4261c" alt="image" loading="lazy"><br><br>実際に学習された異なる2つのモデルに対して同じトークンをそれぞれinputし、同じ深さのLayerの全てのattention distributionの組み合わせからJensen Shannon Divergenceで距離を算出し、最も小さいものを2モデル間の当該layerの距離として可視化すると下記のようになり、NoPEとT5のrelative positional encodingが最も類似していることから、NoPEが学習を通じて（実用上は）相対位置エンコーディングのようなものを学習することが分かった。<br><img src="https://github.com/user-attachments/assets/9619c7e5-0612-45de-8717-1634bee509b7" alt="image" loading="lazy"><br><br>[^1]:深さ1のLayerのHidden State H^1から絶対位置の復元が可能であり（つまり、当該レイヤーのHが絶対位置に関する情報を保持している）、この前提のもと、後続のLayerがこの情報を上書きしないと仮定した場合に、相対位置エンコーディングを実現できる。</p>
<p>また、CoT/Scratchpadはlong sequenceに対する汎化性能を向上させることがsmall scaleではあるが先行研究で示されており、Positional Encodingを変化させた時にCoT/Scratchpadの性能にどのような影響を与えるかを調査。<br><br>具体的には、CoT/Scratchpadのフォーマットがどのようなものが有効かも明らかではないので、5種類のコンポーネントの組み合わせでフォーマットを構成し、mathematical reasoningタスクで以下のような設定で訓練し<br><br>- さまざまなコンポーネントの組み合わせで異なるフォーマットを作成し、<br>- 全ての位置エンコーディングあり/なしモデルを訓練<br><br>これらを比較した。この結果、CoT/Scratchpadはフォーマットに関係なく、特定のタスクでのみ有効（有効かどうかはタスク依存）であることが分かった。このことから、CoT/Scratcpad（つまり、モデルのinputとoutputの仕方）単体で、long contextに対する汎化性能を向上させることができないので、Positional Encoding（≒モデルのアーキテクチャ）によるlong contextに対する汎化性能の向上が非常に重要であることが浮き彫りになった。<br><img src="https://github.com/user-attachments/assets/e23c4fbf-84de-4344-a01e-1e7e9e66fa7e" alt="image" loading="lazy"><br><br>また、CoT/Scratchpadが有効だったAdditionに対して各Positional Embeddingモデルを学習し、生成されたトークンのattentionがどの位置のトークンを指しているかを相対距離で可視化したところ（0が当該トークン、つまり現在のScratchpadに着目しており、1が遠いトークン、つまりinputに着目していることを表すように正規化）、NoPEとRelative Positional Encodingがshort/long rangeにそれぞれフォーカスするようなbinomialな分布なのに対し、他のPositional Encodingではよりuniformな分布であることが分かった。このタスクにおいてはNoPEとRelative POの性能が高かったため、binomialな分布の方がより最適であろうことが示唆された。<br><img src="https://github.com/user-attachments/assets/833e6a81-8611-4e79-9d2e-473f7ebee2d0" alt="image" loading="lazy"><br></p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Scaling%20Laws.html" target="_blank" rel="noopener noreferrer">#Scaling Laws</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-03-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1829" target="_blank" rel="noopener noreferrer" class="title-link">Scaling Data-Constrained Language Models, Niklas Muennighoff+, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>- 言語モデルのスケーリングにおいて、データ制約下でのトレーニングを調査。9000億トークンと90億パラメータのモデルを用いた実験で、繰り返しデータを使用しても損失に大きな変化は見られず、繰り返しの価値が減少することを確認。計算最適性のスケーリング法則を提案し、データ不足を軽減するアプローチも実験。得られたモデルとデータセットは公開。</span>
<span class="snippet"><span>Comment</span><p>OpenReview:


<a href="https://openreview.net/forum?id=j5BuTrEj35" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=j5BuTrEj35</a>


</p>
<p>チンチラ則のようなScaling Lawsはパラメータとデータ量の両方をスケールさせた場合の前提に立っており、かつデータは全てuniqueである前提だったが、データの枯渇が懸念される昨今の状況に合わせて、データ量が制限された状況で、同じデータを繰り返し利用する（＝複数エポック学習する）ことが一般的になってきた。このため、データのrepetitionに関して性能を事前学習による性能の違いを調査して、repetitionとパラメータ数に関するスケーリング則を提案（$3.1)しているようである。<br><br>Takeawayとしては、データが制限された環境下では、repetitionは上限4回までが効果的（コスパが良い）であり（左図）、小さいモデルを複数エポック訓練する方が固定されたBudgetの中で低いlossを達成できる右図）。<br><img src="https://github.com/user-attachments/assets/4e62cd1b-fe83-4d6e-a40d-df992c85def3" alt="image" loading="lazy"><br><br>学習データの半分をコードにしても性能の劣化はなく、様々なタスクの性能が向上しパフォーマンスの分散も小さくなる、といったことが挙げられるようだ。<br><img src="https://github.com/user-attachments/assets/d404156f-7416-4f22-aa7e-d342065435ee" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<span class="issue_date">Issue Date: 2023-11-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1132" target="_blank" rel="noopener noreferrer" class="title-link">Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small   Scorer, Bowen Tan+, N_A, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>- 大規模言語モデル（LLMs）はマルチタスキングに優れた性能を示していますが、パラメータ数が多く計算リソースを必要とし、効率的ではありません。そこで、小規模なスコアラーであるCappyを導入し、独立して機能するかLLMsの補助として使用することでパフォーマンスを向上させました。Cappyはファインチューニングやパラメータへのアクセスを必要とせず、さまざまなタスクで高い性能を発揮します。実験結果では、Cappyは独立したタスクや複雑なタスクで大きなLLMsを上回り、他のLLMsとの連携も可能です。</span>
<span class="snippet"><span>Comment</span><p>360MパラメータでさまざまなタスクでLLMに勝つっぽいのでおもしろそうだし実用性もありそう</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Quantization.html" target="_blank" rel="noopener noreferrer">#Quantization</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2023-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/881" target="_blank" rel="noopener noreferrer" class="title-link">QLoRA: Efficient Finetuning of Quantized LLMs, Tim Dettmers+, N_A, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>- 私たちは、QLoRAという効率的なファインチューニング手法を提案します。この手法は、メモリ使用量を削減し、48GBの単一のGPU上で65Bパラメータモデルをファインチューニングすることができます。また、16ビットのファインチューニングタスクのパフォーマンスを維持します。QLoRAは、凍結された4ビット量子化された事前学習済み言語モデルの勾配をLow Rank Adapters（LoRA）に逆伝播させます。私たちの最良のモデルファミリーであるGuanacoは、Vicunaベンチマークで以前に公開されたすべてのモデルを上回り、ChatGPTのパフォーマンスレベルの99.3%に達します。また、単一のGPU上でのファインチューニングには24時間しかかかりません。QLoRAは、パフォーマンスを犠牲にすることなくメモリを節約するためのいくつかの革新を導入しています。具体的には、4ビットNormalFloat（NF4）という情報理論的に最適な新しいデータ型、ダブル量子化による平均メモリフットプリントの削減、およびページドオプティマイザによるメモリスパイクの管理です。私たちはQLoRAを使用して1,000以上のモデルをファインチューニングし、8つの命令データセット、複数のモデルタイプ（LLaMA、T5）、および従来のファインチューニングでは実行不可能なモデルスケール（33Bおよび65Bパラメータモデル）にわたる命令の追跡とチャットボットのパフォーマンスの詳細な分析を提供します。私たちの結果は、QLoRAを使用して小規模な高品質のデータセットでのファインチューニングが、以前のSoTAよりも小さいモデルを使用しても最先端の結果をもたらすことを示しています。また、人間の評価とGPT-4の評価に基づいたチャットボットのパフォーマンスの詳細な分析を提供し、GPT-4の評価が安価で合理的な人間の評価の代替手段であることを示します。さらに、現在のチャットボットのベンチマークは、チャットボットのパフォーマンスレベルを正確に評価するためには信頼性がないことがわかります。GuanacoがChatGPTと比較してどこで失敗するかを示す分析も行っています。私たちは、4ビットトレーニングのためのCUDAカーネルを含む、すべてのモデルとコードを公開しています。</span>
<span class="snippet"><span>Comment</span><p>実装: 


<a href="https://github.com/artidoro/qlora" target="_blank" rel="noopener noreferrer">https://github.com/artidoro/qlora</a>


<br>PEFTにもある</p>
<p>参考: 


</p>
<div class="tweet-embed" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1662946722690236417?s=46&t=TDHYK31QiXKxggPzhZbcAQ"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>&lt;img alt="loading..." src="/assets/images/load-31_128.gif class="tweet-loading" /&gt;</div>


<p>OpenReview:


<a href="https://openreview.net/forum?id=OUIFPHEgJU&referrer=%5Bthe%20profile%20of%20Ari%20Holtzman%5D(%2Fprofile%3Fid%3D~Ari_Holtzman1)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=OUIFPHEgJU&referrer=%5Bthe%20profile%20of%20Ari%20Holtzman%5D(%2Fprofile%3Fid%3D~Ari_Holtzman1)</a>


</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2023-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/787" target="_blank" rel="noopener noreferrer" class="title-link">Transformers learn to implement preconditioned gradient descent for   in-context learning, Kwangjun Ahn+, N_A, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>- トランスフォーマーは勾配降下法のアルゴリズムを学習できるかどうかについての研究があります。この研究では、トランスフォーマーが勾配降下法の反復をシミュレートすることができることが示されています。さらに、線形トランスフォーマーについての分析から、訓練目的のグローバル最小値が事前条件付き勾配降下法の単一の反復を実装することが証明されました。また、k個のアテンション層を持つトランスフォーマーについても、特定の臨界点が事前条件付き勾配降下法のk回の反復を実装することが証明されました。これらの結果は、トランスフォーマーを訓練して学習アルゴリズムを実装するための将来の研究を促しています。</span>
<span class="snippet"><span>Comment</span><p>参考: 


</p>
<div class="tweet-embed" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1678525778492018688?s=46&t=5BO_qSlNBSEGSugyUlP5Hw"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>&lt;img alt="loading..." src="/assets/images/load-31_128.gif class="tweet-loading" /&gt;</div>


<p>つまり、事前学習の段階でIn context learningが可能なように学習がなされているということなのか。<br>それはどのような学習かというと、プロンプトとそれによって与えられた事例を前条件とした場合の勾配降下法によって実現されていると。<br><br>つまりどういうことかというと、プロンプトと与えられた事例ごとに、それぞれ最適なパラメータが学習されているというイメージだろうか。条件付き分布みたいなもの？<br><br>なので、未知のプロンプトと事例が与えられたときに、事前学習時に前条件として与えられているものの中で類似したものがあれば、良い感じに汎化してうまく生成ができる、ということかな？</p>
<p>いや違うな。1つのアテンション層が勾配降下法の1ステップをシミュレーションしており、k個のアテンション層があったらkステップの勾配降下法をシミュレーションしていることと同じ結果になるということ?<br>そしてその購買降下法では、プロンプトによって与えられた事例が最小となるように学習される（シミュレーションされる）ということなのか。<br><br>つまり、ネットワーク上で本当に与えられた事例に基づいて学習している（のと等価な結果）を得ているということなのか？😱</p>
<p>openreview:


<a href="https://openreview.net/forum?id=LziniAXEI9" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=LziniAXEI9</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<span class="issue_date">Issue Date: 2023-06-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/710" target="_blank" rel="noopener noreferrer" class="title-link">Deductive Verification of Chain-of-Thought Reasoning, Zhan Ling+, N_A, NeuriPS'23</a>
<span class="snippet"><span>Summary</span>- 大規模言語モデル（LLMs）を使用して、Chain-of-Thought（CoT）プロンプティングによる推論タスクを解決するために、自己検証を通じて推論プロセスの信頼性を確保するNatural Programを提案する。このアプローチにより、モデルは正確な推論ステップを生成し、各演繹的推論段階に統合された検証プロセスにより、生成された推論ステップの厳密性と信頼性を向上させることができる。コードはhttps://github.com/lz1oceani/verify_cotで公開される。</span>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Alignment.html" target="_blank" rel="noopener noreferrer">#Alignment</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/DataDistillation.html" target="_blank" rel="noopener noreferrer">#DataDistillation</a>
<span class="issue_date">Issue Date: 2023-05-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/700" target="_blank" rel="noopener noreferrer" class="title-link">LIMA: Less Is More for Alignment, Chunting Zhou+, N_A, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>- 本研究では、65BパラメータのLLaMa言語モデルであるLIMAを訓練し、強化学習や人間の好みモデリングなしに、厳選された1,000のプロンプトとレスポンスのみで標準的な教師あり損失で微調整しました。LIMAは、幅広いクエリに対応する驚くべき強力なパフォーマンスを示し、トレーニングデータに現れなかった未知のタスクにも一般化する傾向があります。制御された人間の研究では、LIMAのレスポンスは、GPT-4、Bard、DaVinci003と比較して優れていることが示されました。これらの結果から、大規模言語モデルのほとんどの知識は事前トレーニング中に学習され、高品質の出力を生成するためには限られた指示調整データしか必要ないことが示唆されます。</span>
<span class="snippet"><span>Comment</span><p>LLaMA65Bをたった1kのdata point（厳選された物）でRLHF無しでfinetuningすると、旅行プランの作成や、歴史改変の推測（？）幅広いタスクで高いパフォーマンスを示し、未知のタスクへの汎化能力も示した。最終的にGPT3,4,BARD,CLAUDEよりも人間が好む回答を返した。<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/db025381-0bf0-47a3-bd18-5d88bff666df" alt="image" loading="lazy"></p>
<p>LLaMAのようなオープンでパラメータ数が少ないモデルに対して、少量のサンプルでfinetuningするとGPT4に迫れるというのはgamechangerになる可能性がある</p>
<p>openreview: 


<a href="https://openreview.net/forum?id=KBMOKmX2he" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=KBMOKmX2he</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<span class="issue_date">Issue Date: 2023-05-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/691" target="_blank" rel="noopener noreferrer" class="title-link">Language Models Meet World Models: Embodied Experiences Enhance Language   Models, Jiannan Xiang+, N_A, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>- 本論文では、大規模言語モデル（LMs）が物理的な環境での単純な推論や計画に苦労することを解決するため、LMsを世界モデルで微調整する新しいパラダイムを提案しています。具体的には、物理的な世界のシミュレータでエージェントを展開し、目的指向の計画とランダムな探索を通じて多様な具現化された経験を獲得することで、LMsを微調整して物理的な世界での推論や行動の多様な能力を教えます。また、重みの選択的な更新のための古典的な弾性重み結合（EWC）を導入し、トレーニング効率のための低ランクアダプタ（LoRA）と組み合わせています。徹底的な実験により、提案手法は18の下流タスクでベースLMsを平均64.28％改善することが示されました。</span>
<span class="snippet"><span>Comment</span><p><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/59f96416-a0ab-4371-b060-ccc6358a867c" alt="image" loading="lazy"></p>
<p>OpenReview:


<a href="https://openreview.net/forum?id=SVBR6xBaMl" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=SVBR6xBaMl</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<span class="issue_date">Issue Date: 2023-05-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/667" target="_blank" rel="noopener noreferrer" class="title-link">Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing   Important Tokens, Zhanpeng Zeng+, N_A, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>- 本論文では、Transformerモデルの二次コストを削減するために、各層でサイズ$r$が$n$に独立した表現に入力を圧縮する方法を提案する。VIPトークン中心の圧縮（Vcc）スキームを使用し、VIPトークンの表現を近似するために入力シーケンスを選択的に圧縮する。提案されたアルゴリズムは、競合するベースラインと比較して効率的であり、多数のタスクにおいて競争力のあるまたはより優れたパフォーマンスを発揮する。また、アルゴリズムは128Kトークンにスケーリングでき、一貫して精度の向上を提供することが示された。</span>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<a class="button" href="articles/Faithfulness.html" target="_blank" rel="noopener noreferrer">#Faithfulness</a>
<span class="issue_date">Issue Date: 2023-05-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/666" target="_blank" rel="noopener noreferrer" class="title-link">Language Models Don't Always Say What They Think: Unfaithful   Explanations in Chain-of-Thought Prompting, Miles Turpin+, N_A, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>- LLMsによる推論において、chain-of-thought reasoning（CoT）と呼ばれる説明を生成することができるが、この説明がモデルの予測の真の理由を誤って表現することがあることがわかった。バイアスのある特徴をモデルの入力に追加することで、CoT説明が大きく影響を受けることが示された。この結果は、LLMsに対する信頼を高めるために、説明の忠実度を評価し、改善する必要があることを示唆している。</span>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<span class="issue_date">Issue Date: 2023-04-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/563" target="_blank" rel="noopener noreferrer" class="title-link">Stable and low-precision training for large-scale vision-language models, Wortsman+, University of Washington, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>- 大規模な言語-視覚モデルのトレーニングを加速し安定させる新手法を提案。SwitchBackを用いたint8量子化で、CLIP ViT-Hugeのトレーニング速度を13-25%向上させ、bfloat16と同等の性能を維持。float8トレーニングも効果的であることを示し、初期化方法が成功に寄与。損失のスパイクを分析し、AdamW-Adafactorハイブリッドを推奨することで、トレーニングの安定性を向上させた。</span>
<span class="snippet"><span>Comment</span><p><img src="https://user-images.githubusercontent.com/12249301/235149432-1c818dc6-174c-4666-a26c-2ab9683b438b.png" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<span class="issue_date">Issue Date: 2023-03-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/512" target="_blank" rel="noopener noreferrer" class="title-link">Reflexion: Language Agents with Verbal Reinforcement Learning, Noah Shinn+, N_A, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>- 本研究では、言語エージェントを強化するための新しいフレームワークであるReflexionを提案しています。Reflexionエージェントは、言語的フィードバックを通じて自己反省し、より良い意思決定を促すために反省的なテキストを保持します。Reflexionはさまざまなタスクでベースラインエージェントに比べて大幅な改善を実現し、従来の最先端のGPT-4を上回る精度を達成しました。さらに、異なるフィードバック信号や統合方法、エージェントタイプの研究を行い、パフォーマンスへの影響についての洞察を提供しています。</span>
<span class="snippet"><span>Comment</span><p>なぜ回答を間違えたのか自己反省させることでパフォーマンスを向上させる研究</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Scaling%20Laws.html" target="_blank" rel="noopener noreferrer">#Scaling Laws</a>
<a class="button" href="articles/Deduplication.html" target="_blank" rel="noopener noreferrer">#Deduplication</a>
<span class="issue_date">Issue Date: 2025-09-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2688" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Beyond neural scaling laws: beating power law scaling via data pruning, Ben Sorscher+, NeurIPS'22</a>
<span class="snippet"><span>Summary</span>- データセットサイズに対する誤差のスケーリングを研究し、高品質なデータプルーニングメトリックを用いることで誤差を指数スケーリングに減少させる可能性を示す。CIFAR-10、SVHN、ImageNetでの実験により、冪法則スケーリングを超える改善を確認。ImageNetにおける10種類のデータプルーニングメトリックのベンチマークを実施し、従来のメトリックに代わる新しい自己教師ありプルーニングメトリックを開発。良好なデータプルーニングメトリックがニューラルスケーリング法則の改善とリソースコスト削減に寄与する可能性を示唆。</span>
<span class="snippet"><span>Comment</span><p>openreview: 


<a href="https://openreview.net/forum?id=UmvSlP-PyV" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=UmvSlP-PyV</a>


</p>
<p>日本語解説スライド: 


<a href="https://speakerdeck.com/takase/snlp2023-beyond-neural-scaling-laws" target="_blank" rel="noopener noreferrer">https://speakerdeck.com/takase/snlp2023-beyond-neural-scaling-laws</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/KnowledgeEditing.html" target="_blank" rel="noopener noreferrer">#KnowledgeEditing</a>
<span class="issue_date">Issue Date: 2025-08-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2557" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Locating and Editing Factual Associations in GPT, Kevin Meng+, NeurIPS'22</a>
<span class="snippet"><span>Summary</span>- 自回帰型トランスフォーマー言語モデルにおける事実の関連付けの保存と想起を分析し、局所的な計算に対応することを示した。因果介入を用いて事実予測に関与するニューロンを特定し、フィードフォワードモジュールの役割を明らかにした。Rank-One Model Editing（ROME）を用いて特定の事実の関連付けを更新し、他の方法と同等の効果を確認。新しいデータセットに対する評価でも特異性と一般化を両立できることを示した。中間層のフィードフォワードモジュールが事実の関連付けに重要であり、モデル編集の実行可能性を示唆している。</span>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/RepresentationLearning.html" target="_blank" rel="noopener noreferrer">#RepresentationLearning</a>
<a class="button" href="articles/Length.html" target="_blank" rel="noopener noreferrer">#Length</a>
<span class="issue_date">Issue Date: 2025-07-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2311" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Matryoshka Representation Learning, Aditya Kusupati+, NeurIPS'22</a>
<span class="snippet"><span>Summary</span>- マトリョーシカ表現学習（MRL）は、異なる計算リソースに適応可能な柔軟な表現を設計する手法であり、既存の表現学習パイプラインを最小限に修正して使用します。MRLは、粗から細への表現を学習し、ImageNet-1K分類で最大14倍小さい埋め込みサイズを提供し、実世界のスピードアップを実現し、少数ショット分類で精度向上を達成します。MRLは視覚、視覚+言語、言語のモダリティにわたるデータセットに拡張可能で、コードとモデルはオープンソースで公開されています。</span>
<span class="snippet"><span>Comment</span><p>日本語解説:


<a href="https://speakerdeck.com/hpprc/lun-jiang-zi-liao-matryoshka-representation-learning" target="_blank" rel="noopener noreferrer">https://speakerdeck.com/hpprc/lun-jiang-zi-liao-matryoshka-representation-learning</a>


</p>
<p>単一のモデルから複数のlengthのEmbeddingを出力できるような手法。</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/CLIP.html" target="_blank" rel="noopener noreferrer">#CLIP</a>
<span class="issue_date">Issue Date: 2025-05-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1928" target="_blank" rel="noopener noreferrer" class="title-link">LAION-5B: An open large-scale dataset for training next generation   image-text models, Christoph Schuhmann+, NeurIPS'22</a>
<span class="snippet"><span>Summary</span>- LAION-5Bは、5.85億のCLIPフィルタリングされた画像-テキストペアから成る大規模データセットで、英語のペアが2.32B含まれています。このデータセットは、CLIPやGLIDEなどのモデルの再現とファインチューニングに利用され、マルチモーダルモデルの研究を民主化します。また、データ探索やサブセット生成のためのインターフェースや、コンテンツ検出のためのスコアも提供されます。</span>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Scaling%20Laws.html" target="_blank" rel="noopener noreferrer">#Scaling Laws</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-03-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1827" target="_blank" rel="noopener noreferrer" class="title-link">Training Compute-Optimal Large Language Models, Jordan Hoffmann+, NeurIPS'22</a>
<span class="snippet"><span>Summary</span>- トランスフォーマー言語モデルの訓練において、計算予算内で最適なモデルサイズとトークン数を調査。モデルサイズと訓練トークン数は同等にスケールする必要があり、倍増するごとにトークン数も倍増すべきと提案。Chinchillaモデルは、Gopherなどの大規模モデルに対して優れた性能を示し、ファインチューニングと推論の計算量を削減。MMLUベンチマークで67.5%の精度を達成し、Gopherに対して7%以上の改善を実現。</span>
<span class="snippet"><span>Comment</span><p>OpenReview: 


<a href="https://openreview.net/forum?id=iBBcRUlOAPR" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=iBBcRUlOAPR</a>


</p>
<p>chinchilla則</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Zero/Few/ManyShotPrompting.html" target="_blank" rel="noopener noreferrer">#Zero/Few/ManyShotPrompting</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<a class="button" href="articles/Prompting.html" target="_blank" rel="noopener noreferrer">#Prompting</a>
<span class="issue_date">Issue Date: 2023-04-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/551" target="_blank" rel="noopener noreferrer" class="title-link">Chain of thought prompting elicits reasoning in large language models, Wei+, Google Research, NeurIPS'22</a>
<span class="snippet"><span>Comment</span><p>Chain-of-Thoughtを提案した論文。CoTをする上でパラメータ数が100B未満のモデルではあまり効果が発揮されないということは念頭に置いた方が良さそう。<br><br><img src="https://user-images.githubusercontent.com/12249301/234739470-be1c9299-0dd6-4483-901a-0bf855e73f0f.png" alt="image" loading="lazy"><br><br></p>
<p>先行研究では、reasoningが必要なタスクの性能が低い問題をintermediate stepを明示的に作成し、pre-trainedモデルをfinetuningすることで解決していた。しかしこの方法では、finetuning用の高品質なrationaleが記述された大規模データを準備するのに多大なコストがかかるという問題があった。<br><br>このため、few-shot promptingによってこの問題を解決することが考えられるが、reasoning能力が必要なタスクでは性能が悪いという問題あがった。そこで、両者の強みを組み合わせた手法として、chain-of-thought promptingは提案された。</p>
<p># CoTによる実験結果<br><br>以下のベンチマークを利用<br><br>- math word problem: GSM8K, SVAMP, ASDiv, AQuA, MAWPS<br><br>- commonsense reasoning: CSQA, StrategyQA, Big-bench Effort (Date, Sports), SayCan<br><br>- Symbolic Reasoning: Last Letter concatenation, Coin Flip<br><br>  - Last Letter concatnation: 名前の単語のlast wordをconcatするタスク（"Amy Brown" -&gt; "yn"）<br><br>  - Coin Flip: コインをひっくり返す、 あるいはひっくり返さない動作の記述の後に、コインが表向きであるかどうかをモデルに回答するよう求めるタスク<br><br> <br><br>## math word problem benchmark<br><br>- モデルのサイズが大きくなるにつれ性能が大きく向上（emergent ability）することがあることがわかる<br><br>  - 言い換えるとCoTは&lt;100Bのモデルではパフォーマンスに対してインパクトを与えない<br><br>  - モデルサイズが小さいと、誤ったCoTを生成してしまうため<br><br>- 複雑な問題になればなるほど、CoTによる恩恵が大きい<br><br>  - ベースラインの性能が最も低かったGSM8Kでは、パフォーマンスの2倍向上しており、1 stepのreasoningで解決できるSingleOpやMAWPSでは、性能の向上幅が小さい<br><br>- Task specificなモデルをfinetuningした以前のSoTAと比較してcomparable, あるいはoutperformしている<br><br>- <img src="https://user-images.githubusercontent.com/12249301/236394200-826ba167-8ec7-4abb-ba4d-fe44bf247b41.png" alt="image" loading="lazy"><br><br>## Ablation Study<br><br>CoTではなく、他のタイプのpromptingでも同じような効果が得られるのではないか？という疑問に回答するために、3つのpromptingを実施し、CoTと性能比較した：<br><br>- Equation Only: 回答するまえに数式を記載するようなprompt<br><br>  - promptの中に数式が書かれているから性能改善されているのでは？という疑問に対する検証<br><br>  - =&gt; GSM8Kによる結果を見ると、equation onlyでは性能が低かった。これは、これは数式だけでreasoning stepsを表現できないことに起因している<br><br>- Variable compute only: dotのsequence (...) のみのprompt<br><br>  - CoTは難しい問題に対してより多くの計算（intermediate token）をすることができているからでは？という疑問に対する検証<br><br>  - variable computationとCoTの影響を分離するために、dotのsequence (...) のみでpromptingする方法を検証<br><br>  - =&gt; 結果はbaselineと性能変わらず。このことから、variableの計算自体が性能向上に寄与しているわけではないことがわかる。<br><br>- Chain of Thought after answer: 回答の後にCoTを出力するようなprompting<br><br>  - 単にpretrainingの際のrelevantな知識にアクセスしやすくなっているだけなのでは？という疑問を検証<br><br>  - =&gt; baselineと性能は変わらず、単に知識を活性化させるだけでは性能が向上しないことがわかる。<br><br><br><br><img src="https://user-images.githubusercontent.com/12249301/236396383-877a26ae-20c2-42a4-a023-1eb66abf8320.png" alt="image" loading="lazy"><br><br><br><br>## CoTのロバスト性<br><br>人間のAnnotatorにCoTを作成させ、それらを利用したCoTpromptingとexamplarベースな手法によって性能がどれだけ変わるかを検証。standard promptingを全ての場合で上回る性能を獲得した。このことから、linguisticなstyleにCoTは影響を受けていないことがわかる。<br><br><img src="https://user-images.githubusercontent.com/12249301/236397864-073dd88f-95c0-47f0-af3c-ed7288ca967d.png" alt="image" loading="lazy"><br><br><br><br># commonsense reasoning<br><br>全てのデータセットにおいて、CoTがstandard promptingをoutperformした。<br><br><img src="https://user-images.githubusercontent.com/12249301/236398447-6c58a3f3-7461-4109-9a96-8f8092831dd1.png" alt="image" loading="lazy"><br><br><br><br># Symbolic Reasoning<br><br>in-domain test setとout-of-domain test setの2種類を用意した。前者は必要なreasoning stepがfew-shot examplarと同一のもの、後者は必要なreasoning stepがfew-shot examplarよりも多いものである。<br><br>CoTがStandard proimptingを上回っている。特に、standard promptingではOOV test setではモデルをスケールさせても性能が向上しなかったのに対し、CoTではより大きなgainを得ている。このことから、CoTにはreasoning stepのlengthに対しても汎化能力があることがわかる。<br><br><br><br><img src="https://user-images.githubusercontent.com/12249301/236399389-30e62218-3e59-4912-983c-818de457fa04.png" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/ZeroshotHyperparameterTransfer.html" target="_blank" rel="noopener noreferrer">#ZeroshotHyperparameterTransfer</a>
<span class="issue_date">Issue Date: 2025-08-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2582" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Tensor Programs V: Tuning Large Neural Networks via Zero-Shot  Hyperparameter Transfer, Greg Yang+, NeurIPS'21</a>
<span class="snippet"><span>Summary</span>- ハイパーパラメータチューニングは高コストであり、特に大規模なニューラルネットワークにおいて負担が大きい。新たに提案するmuTransferは、最大更新パラメータ化（muP）を利用し、小さなモデルでチューニングしたHPをフルサイズモデルにゼロショットで転送する手法である。実験により、1300万パラメータのモデルからBERT-largeを超える性能を達成し、4000万パラメータからはGPT-3を上回る結果を得た。チューニングコストはそれぞれ事前学習コストの同等または7%に抑えられた。</span>
<span class="snippet"><span>Comment</span><p>openreview: 


<a href="https://openreview.net/forum?id=Bx6qKuBM2AD" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=Bx6qKuBM2AD</a>


</p>
<p>小規模なモデルに対してハイパーパラメータのチューニングを実施し、同様のベースモデルで、**各layerのwidthが大きいもの**に対しても、小規模モデルで最適であったハイパーパラメータをzero-shotで転移することで near optimalなハイパーパラメータで学習できるmu Transferを提案。<br><br>モデルの深さ（以外にも下表中の*印のパラメータ）に対しても限定的に転移可能な模様。Post-Layer NormのTransformerやではあまりうまくいかないことが11節に記述されている（実験はpre-Layer Norm Transformer, ResNetに対して行われている模様）。<br>また、6.1節では、（実験的に）利用する小規模モデルのスケールとして幅256, 深さ4, バッチサイズ32, sequence長128, 訓練ステップ数5000を最低満たしており、かつスケールさせる幅が妥当な範囲内である必要がある、といった話が記述されている。<br><br>前提知識（muP）や条件が多そうな気がするので、しっかり確認した方がよさそう。<br>たとえば、muPで初期化されている必要があることや、転送可能なハイパーパラメータに限りがある（e.g. 学習率）、異なるデータに対するfinetuningなどは転送できないなど。<br><br><br>&lt;img width="872" height="336" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/e5aeb152-5c9e-4ba2-9152-4bfef0d7c27c"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/e5aeb152-5c9e-4ba2-9152-4bfef0d7c27c"&lt;/a&gt;


/&gt;</p>
<p>muP:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2583" target="_blank" rel="noopener noreferrer">[Paper Note] Feature Learning in Infinite-Width Neural Networks, Greg Yang+, PMLR'21</a>
</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<span class="issue_date">Issue Date: 2021-11-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/431" target="_blank" rel="noopener noreferrer" class="title-link">ResNet strikes back: An improved training procedure in timm, Wightman+, NeurIPS'21 Workshop ImageNet PPF</a>
<span class="snippet"><span>Summary</span>- 本論文では、Residual Networks（ResNet-50）の性能を新たなトレーニング手法を用いて再評価し、競争力のある設定で80.4%のトップ1精度を達成したことを報告します。これにより、将来の研究のためのより良いベースラインを提供することを目指しています。</span>
<span class="snippet"><span>Comment</span><p>2015年以後、様々な最適化アルゴリズム、正則化手法、データ拡張などが提案される中で、最新アーキテクチャのモデルにはそれらが適用される一方ベースラインとなるResNetではそれらが適用されず、論文の値のみが参照される現状はフェアではないので、ResNetの性能を向上させるような訓練手法を追求した研究。<br><br><br><br>ResNetにおける有効な訓練手法として下記を模索：<br><br><br><br>損失関数として、MixUp（訓練画像を重ね合わせ、組み合わせた画像のラベルをミックスして新しい学習インスタンスを作るデータ拡張手法）と、CutMix（画像を切り貼りして、切り貼り部分の面積に応じてラベルのスコアを調整するデータ拡張手法）を適用し、CutMixによって大幅に性能が改善することを示した。このとき、ラベルの確率の和が1となる前提の元クロスエントロピーで学習するのではなく、元画像に含まれる物体が両方存在するという全体の元BinaryCrossEntropyを適用しマルチラベル問題として学習することで、性能が向上。<br><br><br><br>データ拡張手法として、MixUp, CutMixだけでなく、通常のリサイズ・切り抜きと、水平方向の反転を適用しデータ拡張する。加えてRandAugment（14種類のデータ拡張操作から、N個サンプルし、強さMで順番に適用するデータ拡張手法。N,Mはそれぞれ0〜10の整数なので、10の二乗オーダーでグリッドサーチすれば、最適なN,Mを得る。グリッドサーチするだけでお手軽だが非常に強力）を適用した。<br><br><br><br>正則化として、Weight Decay（学習過程で重みが大きくなりすぎないようにペナルティを課し、過学習を防止する手法。L2正則化など。）と、label smoothing（正解ラベルが1、その他は0とラベル付けするのではなく、ラベルに一定のノイズを入れ、正解ラベル以外にも重みが入っている状態にし、ラベル付けのノイズにロバストなモデルを学習する手法。ノイズの強さは定数で調整する）、Repeated Augmentation（同じバッチ内の画像にデータ拡張を適用しバッチサイズを大きくする）、Stochastic Depth（ランダムでレイヤーを削除し、その間を恒等関数で繋ぎ訓練することで、モデルの汎化能力と訓練時間を向上する）を適用。<br><br></p>
<p>Optimizerとして、オリジナルのResNetでは、SGDやAdamWで訓練されることが多いが、Repeated Augmentationとバイナリクロスエントロピーを組み合わせた場合はLAMBが有効であった。また、従来よりも長い訓練時間（600epoch、様々な正則化手法を使っているので過学習しづらいため）で学習し、最初にウォームアップを使い徐々に学習率を上げ（finetuningの再認識これまでのweightをなるべく壊したくないから小さい学習率から始める、あるいはMomentumやAdamといった移動平均を使う手法では移動平均を取るための声倍の蓄積が足りない場合学習の信頼度が低いので最初の方は学習率小さくするみたいな、イメージ）その後コサイン関数に従い学習率を減らしていくスケジューリング法で学習。<br><br><br><br>論文中では上記手法の3種類の組み合わせ（A1,A2,A3）を提案し実験している。<br><br>ResNet-50に対してA1,2,3を適用した結果、A1を適用した場合にImageNetのトップ1精度が80.4%であり、これはResNet-50を使った場合のSoTA。元のResNetの精度が76%程度だったので大幅に向上した。<br><br>同じ実験設定を使った場合の他のアーキテクチャ（ViTやEfficientNetなど）と比べても遜色のない性能を達成。<br><br><br><br><img src="https://user-images.githubusercontent.com/12249301/140302112-05392bbb-7014-4518-a001-55e91933a065.png" alt="image" loading="lazy"><br><br><br><br>また、本論文で提案されているA2と、DeiTと呼ばれるアーキテクチャで提案されている訓練手法（T2）をそれぞれのモデルに適用した結果、ResNetではA2、DeiTではT2の性能が良かった。つまり、「アーキテクチャと訓練方法は同時に最適化する必要がある」ということ。これがこの論文のメッセージの肝とのこと。<br><br><br><br>（ステートオブAIガイドの内容を一部補足して記述しました。いつもありがとうございます。）<br><br><br><br><img src="https://user-images.githubusercontent.com/12249301/140302160-c31717ae-a225-47a4-ae33-f1cd081c419b.png" alt="image" loading="lazy"></p>
<p>画像系でどういった訓練手法が利用されるか色々書かれていたので勉強になった。特に画像系のデータ拡張手法なんかは普段触らないので勉強になる。</p>
<p>OpenReview:


<a href="https://openreview.net/forum?id=NG6MJnVl6M5" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=NG6MJnVl6M5</a>


</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Zero/Few/ManyShotPrompting.html" target="_blank" rel="noopener noreferrer">#Zero/Few/ManyShotPrompting</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2023-04-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/552" target="_blank" rel="noopener noreferrer" class="title-link">Language Models are Few-Shot Learners, Tom B. Brown+, NeurIPS'20</a>
<span class="snippet"><span>Summary</span>- GPT-3は1750億パラメータを持つ自己回帰型言語モデルで、少数ショット設定においてファインチューニングなしで多くのNLPタスクで強力な性能を示す。翻訳や質問応答などで優れた結果を出し、即時推論やドメイン適応が必要なタスクでも良好な性能を発揮する一方、依然として苦手なデータセットや訓練に関する問題も存在する。また、GPT-3は人間が書いた記事と区別が難しいニュース記事を生成できることが確認され、社会的影響についても議論される。</span>
<span class="snippet"><span>Comment</span><p>In-Context Learningを提案した論文</p>
<p>論文に記載されているIn-Context Learningの定義は、しっかり押さえておいた方が良い。<br><br>下図はmeta-learningの観点から見たときの、in-contextの位置付け。事前学習時にSGDでパラメータをupdateするのをouter loopとし、そこで広いスキルとパターン認識の能力を身につける。一方で、in-context learningは、Inference時に事前学習時に得たそれらのスキルを用いて、求めるタスクを認識、あるいは適応するInner loopのことを指す。<br><img src="https://github.com/user-attachments/assets/679129f3-93e3-445f-b9e8-5d909261737b" alt="image" loading="lazy"><br><br>この上で、論文中では In-Context Learningについて:<br>&gt; Recent work [RWC+19] attempts to do this via what we call “in-context learning”, using the text input of a pretrained language model as a form of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.<br><br>と定義している。</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<span class="issue_date">Issue Date: 2021-06-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/381" target="_blank" rel="noopener noreferrer" class="title-link">All Word Embeddings from One Embedding, Takase+, NeurIPS'20</a>
<span class="snippet"><span>Comment</span><p>NLPのためのNN-basedなモデルのパラメータの多くはEmbeddingによるもので、従来は個々の単語ごとに異なるembeddingをMatrixの形で格納してきた。この研究ではモデルのパラメータ数を減らすために、個々のword embeddingをshared embeddingの変換によって表現する手法ALONE(all word embeddings from one)を提案。単語ごとに固有のnon-trainableなfilter vectorを用いてshared embeddingsを修正し、FFNにinputすることで表現力を高める。また、filter vector普通に実装するとword embeddingと同じサイズのメモリを消費してしまうため、メモリ効率の良いfilter vector効率手法も提案している。機械翻訳・および文書要約を行うTransformerに提案手法を適用したところ、より少量のパラメータでcomparableなスコアを達成した。</p>
<p>Embedidngのパラメータ数とBLEUスコアの比較。より少ないパラメータ数でcomparableな性能を達成している。<br><br><br><br><img src="https://user-images.githubusercontent.com/12249301/121308824-700c3100-c93c-11eb-8d15-629d896f9db8.png" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2357" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Deep Equilibrium Models, Shaojie Bai+, NeurIPS'19</a>
<span class="snippet"><span>Summary</span>- 深い平衡モデル（DEQ）を提案し、逐次データのモデル化において平衡点を直接見つけるアプローチを示す。DEQは無限の深さのフィードフォワードネットワークを解析的に逆伝播可能にし、定数メモリでトレーニングと予測を行える。自己注意トランスフォーマーやトレリスネットワークに適用し、WikiText-103ベンチマークでパフォーマンス向上、計算要件の維持、メモリ消費の最大88%削減を実証。</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<span class="issue_date">Issue Date: 2025-07-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2170" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Neural Ordinary Differential Equations, Ricky T. Q. Chen+, arXiv'18</a>
<span class="snippet"><span>Summary</span>- 新しい深層ニューラルネットワークモデルを提案し、隠れ状態の導関数をパラメータ化。ブラックボックスの微分方程式ソルバーを用いて出力を計算し、メモリコストを一定に保ちながら評価戦略を適応。連続深度残差ネットワークや連続時間潜在変数モデルで特性を実証。最大尤度で学習可能な連続正規化フローを構築し、ODEソルバーを逆伝播する方法を示すことで、エンドツーエンドの学習を実現。</span>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<span class="issue_date">Issue Date: 2018-02-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/253" target="_blank" rel="noopener noreferrer" class="title-link">Deep Learning: Practice and Trends, NIPS'17</a>
<span class="snippet"><span>Comment</span><p>基礎から最新まで幅広いトピックがまとまったtutorial</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/GenerativeAdversarialNetwork.html" target="_blank" rel="noopener noreferrer">#GenerativeAdversarialNetwork</a>
<span class="issue_date">Issue Date: 2018-02-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/247" target="_blank" rel="noopener noreferrer" class="title-link">Adversarial Ranking for Language Generation, Lin+, NIPS'17</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineTranslation.html" target="_blank" rel="noopener noreferrer">#MachineTranslation</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/PositionalEncoding.html" target="_blank" rel="noopener noreferrer">#PositionalEncoding</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2018-01-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/245" target="_blank" rel="noopener noreferrer" class="title-link">Attention is all you need, Vaswani+, NIPS'17</a>
<span class="snippet"><span>Comment</span><p>Transformer (self-attentionを利用) 論文<br><br>解説スライド：


<a href="https://www.slideshare.net/DeepLearningJP2016/dlattention-is-all-you-need" target="_blank" rel="noopener noreferrer">https://www.slideshare.net/DeepLearningJP2016/dlattention-is-all-you-need</a>


<br><br>解説記事：


<a href="https://qiita.com/nishiba/items/1c99bc7ddcb2d62667c6" target="_blank" rel="noopener noreferrer">https://qiita.com/nishiba/items/1c99bc7ddcb2d62667c6</a>


<br><br><br><br>* 新しい翻訳モデル(Transformer)を提案。既存のモデルよりも並列化に対応しており、短時間の訓練で（既存モデルの1/4以下のコスト）高いBLEUスコアを達成した。<br><br>* TransformerはRNNやCNNを使わず、attentionメカニズムに基づいている。<br><br><br><br>（解説より）</p>
<p>分かりやすい:<br>


<a href="https://qiita.com/halhorn/items/c91497522be27bde17ce" target="_blank" rel="noopener noreferrer">https://qiita.com/halhorn/items/c91497522be27bde17ce</a>


</p>
<p>Transformerの各コンポーネントでのoutputのshapeや、attention_maskの形状、実装について記述されており有用:<br>


<a href="https://qiita.com/FuwaraMiyasaki/items/239f3528053889847825" target="_blank" rel="noopener noreferrer">https://qiita.com/FuwaraMiyasaki/items/239f3528053889847825</a>


</p>
<p>集合知</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Word.html" target="_blank" rel="noopener noreferrer">#Word</a>
<a class="button" href="articles/RepresentationLearning.html" target="_blank" rel="noopener noreferrer">#RepresentationLearning</a>
<span class="issue_date">Issue Date: 2017-12-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/78" target="_blank" rel="noopener noreferrer" class="title-link">Poincar'e Embeddings for Learning Hierarchical Representations, Nickel+, NIPS'17</a>
<span class="snippet"><span>Comment</span><p>解説: 


<a href="http://tech-blog.abeja.asia/entry/poincare-embeddings" target="_blank" rel="noopener noreferrer">http://tech-blog.abeja.asia/entry/poincare-embeddings</a>


<br><br>解説スライド：


<a href="https://speakerdeck.com/eumesy/poincare-embeddings-for-learning-hierarchical-representations" target="_blank" rel="noopener noreferrer">https://speakerdeck.com/eumesy/poincare-embeddings-for-learning-hierarchical-representations</a>


<br><br>実装：


<a href="https://github.com/TatsuyaShirakawa/poincare-embedding" target="_blank" rel="noopener noreferrer">https://github.com/TatsuyaShirakawa/poincare-embedding</a>


<br><br></p>
<p>・階層構造を持つデータ（WordNet上の上位語下位語、is-a関係など）を埋め込むために、双曲空間を使った話（通常はユークリッド空間）。<br><br>・階層構造・べき分布を持つデータはユークリッド空間ではなく双曲空間の方が効率的に埋め込める。<br><br>・階層構造・べき分布を持つデータを双曲空間（ポアンカレ球モデル）に埋め込むための学習手法（リーマン多様体上でSGD）を提案<br><br>・WordNet hypernymyの埋め込み：低次元でユークリッド埋め込みに圧勝<br><br>・Social Networkの埋め込み：低次元だと圧勝<br><br>・Lexical Entailment：2つのデータセットでSoTA<br><br>（解説スライドより）</p>
<p><img src="https://user-images.githubusercontent.com/12249301/34452953-0e124ad6-ed8d-11e7-800d-0c2712df116a.png" alt="image" loading="lazy"><br><br><br><br>データとして上位・下位概念を与えていないのに、原点付近には上位語・円周付近には下位語が自然に埋め込まれている（意図した通りになっている）。<br><br>ポアンカレ円板では、原点からの距離に応じて指数的に円周長が増加していくので、指数的に数が増えていく下位語などは外側に配置されると効率的だけど、その通りになっている。<br><br><br><br><img src="https://user-images.githubusercontent.com/12249301/34452994-7c17a738-ed8d-11e7-8a56-13929c55c07e.png" alt="image" loading="lazy"><br><br><br><br><br><br></p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineTranslation.html" target="_blank" rel="noopener noreferrer">#MachineTranslation</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/DualLearning.html" target="_blank" rel="noopener noreferrer">#DualLearning</a>
<span class="issue_date">Issue Date: 2025-08-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2508" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Dual Learning for Machine Translation, Yingce Xia+, NIPS'16</a>
<span class="snippet"><span>Summary</span>- デュアルラーニングメカニズムを用いたニューラル機械翻訳（dual-NMT）を提案。プライマルタスク（英語からフランス語）とデュアルタスク（フランス語から英語）を通じて、ラベルのないデータから自動的に学習。強化学習を用いて互いに教え合い、モデルを更新。実験により、モノリンガルデータから学習しつつ、バイリンガルデータと同等の精度を達成することが示された。</span>
<span class="snippet"><span>Comment</span><p>モノリンガルコーパスD_A, D_Bで学習した言語モデルLM_A, LM_Bが与えられた時、翻訳モデルΘ_A, Θ_Bのの翻訳の自然さ（e.g., 尤度）をrewardとして与え、互いのモデルの翻訳（プライマルタスク）・逆翻訳（デュアルタスク）の性能が互いに高くなるように強化学習するような枠組みを提案。パラレルコーパス不要でモノリンガルコーパスのみで、人手によるアノテーション無しで学習ができる。</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/AdaptiveLearning.html" target="_blank" rel="noopener noreferrer">#AdaptiveLearning</a>
<a class="button" href="articles/EducationalDataMining.html" target="_blank" rel="noopener noreferrer">#EducationalDataMining</a>
<a class="button" href="articles/LearningAnalytics.html" target="_blank" rel="noopener noreferrer">#LearningAnalytics</a>
<a class="button" href="articles/KnowledgeTracing.html" target="_blank" rel="noopener noreferrer">#KnowledgeTracing</a>
<span class="issue_date">Issue Date: 2022-04-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/445" target="_blank" rel="noopener noreferrer" class="title-link">Estimating student proficiency: Deep learning is not the panacea, Wilson+, Knewton+, NIPS'16 workshop</a>
<span class="snippet"><span>Comment</span><p>DKTの性能をBKTやPFA等の手法と比較した研究<br><br><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/355" target="_blank" rel="noopener noreferrer">How Deep is Knowledge Tracing?, Mozer+, EDM'16</a>
 を引用し、DKTとBKTのAUCの計算方法の違いについて言及している</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/GraphConvolutionalNetwork.html" target="_blank" rel="noopener noreferrer">#GraphConvolutionalNetwork</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2018-03-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/265" target="_blank" rel="noopener noreferrer" class="title-link">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering, Defferrard+, NIPS'16</a>
<span class="snippet"><span>Comment</span><p>GCNを勉強する際は読むと良いらしい。<br><br>あわせてこのへんも：<br><br>Semi-Supervised Classification with Graph Convolutional Networks, Kipf+, ICLR'17<br><br>


<a href="https://github.com/tkipf/gcn" target="_blank" rel="noopener noreferrer">https://github.com/tkipf/gcn</a>


</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/GenerativeAdversarialNetwork.html" target="_blank" rel="noopener noreferrer">#GenerativeAdversarialNetwork</a>
<span class="issue_date">Issue Date: 2018-02-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/254" target="_blank" rel="noopener noreferrer" class="title-link">Generative Adversarial Networks （GANS）, NIPS'16</a>
<span class="snippet"><span>Comment</span><p>Goodfellow氏によるGANチュートリアル</p></span><br><br>
<a class="button" href="articles/RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MatrixFactorization.html" target="_blank" rel="noopener noreferrer">#MatrixFactorization</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2018-01-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/224" target="_blank" rel="noopener noreferrer" class="title-link">Deep content-based music recommendation, Oord+, NIPS'13</a>
<span class="snippet"><span>Comment</span><p>Contents-Basedな音楽推薦手法(cold-start problemに強い)。<br><br>Weighted Matrix Factorization (WMF) (Implicit Feedbackによるデータに特化したMatrix Factorization手法) <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/225" target="_blank" rel="noopener noreferrer">Collaborative filtering for implicit feedback datasets, Hu+, International Conference on Data Mining, 2008</a>
 に、Convolutional Neural Networkによるmusic audioのlatent vectorの情報が組み込まれ、item vectorが学習されるような仕組みになっている。<br><br><br><br><img src="https://user-images.githubusercontent.com/12249301/34815522-01679f0e-f6f5-11e7-8534-22e5b5edd7a6.png" alt="image" loading="lazy"><br><br><br><br>CNNでmusic audioのrepresentationを生成する際には、audioのtime-frequencyの情報をinputとする。学習を高速化するために、window幅を3秒に設定しmusic clipをサンプルしinputする。music clip全体のrepresentationを求める際には、consecutive windowからpredictionしたrepresentationを平均したものを使用する。</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ImageClassification.html" target="_blank" rel="noopener noreferrer">#ImageClassification</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-05-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1958" target="_blank" rel="noopener noreferrer" class="title-link">ImageNet Classification with Deep Convolutional Neural Networks, Krizhevsky+, NIPS'12</a>
<span class="snippet"><span>Comment</span><p>ILSVRC 2012において圧倒的な性能示したことで現代のDeepLearningの火付け役となった研究AlexNet。メモってなかったので今更ながら追加した。</p>
<p>AlexNet以前の画像認識技術については牛久先生がまとめてくださっている（当時の課題とそれに対する解決法、しかしまだ課題が…と次々と課題に直面し解決していく様子が描かれており非常に興味深かった)。現在でも残っている技術も紹介されている。:<br>


<a href="https://speakerdeck.com/yushiku/pre_alexnet" target="_blank" rel="noopener noreferrer">https://speakerdeck.com/yushiku/pre_alexnet</a>


<br><br>&gt; 過去の技術だからといって聞き流していると時代背景の変化によってなし得たイノベーションを逃すかも<br><br>これは肝に銘じたい。</p></span><br><br>
<a class="button" href="articles/RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="articles/MatrixFactorization.html" target="_blank" rel="noopener noreferrer">#MatrixFactorization</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2018-01-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/227" target="_blank" rel="noopener noreferrer" class="title-link">Probabilistic Matrix Factorization, Salakhutdinov+, NIPS'08</a>
<span class="snippet"><span>Comment</span><p>Matrix Factorizationを確率モデルとして表した論文。<br><br>解説：


<a href="http://yamaguchiyuto.hatenablog.com/entry/2017/07/13/080000" target="_blank" rel="noopener noreferrer">http://yamaguchiyuto.hatenablog.com/entry/2017/07/13/080000</a>


<br><br></p>
<p>既存のMFは大規模なデータに対してスケールしなかったが、PMFではobservationの数に対して線形にスケールし、さらには、large, sparse, imbalancedなNetflix datasetで良い性能が出た（Netflixデータセットは、rating件数が少ないユーザとかも含んでいる。MovieLensとかは含まれていないのでより現実的なデータセット）。<br><br><img src="https://user-images.githubusercontent.com/12249301/34817061-30757582-f6fa-11e7-90fb-ad5e5fc65781.png" alt="image" loading="lazy"><br><br><br><br>また、Constrained PMF（同じようなsetの映画にrateしているユーザは似ているといった仮定に基づいたモデル ※1）を用いると、少ないratingしかないユーザに対しても良い性能が出た。<br><br><br><br>※1　ratingの少ないユーザの潜在ベクトルは平均から動きにくい、つまりなんの特徴もない平均的なユーザベクトルになってしまうので、同じ映画をratingした人は似た事前分布を持つように制約を導入したモデル<br><br><br><br>（解説ブログ、解説スライドより）</p></span><br><br>
<a class="button" href="articles/InformationRetrieval.html" target="_blank" rel="noopener noreferrer">#InformationRetrieval</a>
<a class="button" href="articles/LearningToRank.html" target="_blank" rel="noopener noreferrer">#LearningToRank</a>
<a class="button" href="articles/PointWise.html" target="_blank" rel="noopener noreferrer">#PointWise</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/190" target="_blank" rel="noopener noreferrer" class="title-link">PRanking with Ranking, Crammer+, NIPS'01</a>
<span class="snippet"><span>Comment</span><p>Point-WiseなLearning2Rankの有名手法</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/AdaptiveLearning.html" target="_blank" rel="noopener noreferrer">#AdaptiveLearning</a>
<a class="button" href="articles/StudentPerformancePrediction.html" target="_blank" rel="noopener noreferrer">#StudentPerformancePrediction</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2018-12-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/297" target="_blank" rel="noopener noreferrer" class="title-link">Deep Knowledge Tracing, Piech+, NIPS, 2015</a>
&lt;span class=\"snippet\"&gt;<span>Comment</span><p>Knowledge Tracingタスクとは：<br><br>　特定のlearning taskにおいて、生徒によってとられたインタラクションの系列x0, ..., xtが与えられたとき、次のインタラクションxt+1を予測するタスク<br><br>　典型的な表現としては、xt={qt, at}, where qt=knowledge component \(KC) ID \(あるいは問題ID)、at=正解したか否か<br><br>　モデルが予測するときは、qtがgivenな時に、atを予測することになる<br><br><br><br>&lt;img src=\"https://user-images.githubusercontent.com/12249301/50377468-2989c580-0661-11e9-97c9-328056fbd692.png\" alt=\"image\" loading=\"lazy\" /&gt;<br><br><br><br>Contribution:<br><br>　1. A novel way to encode student interactions as input to a recurrent neural network.<br><br>　2. A 25% gain in AUC over the best previous result on a knowledge tracing benchmark.<br><br>　3. Demonstration that our knowledge tracing model does not need expert annotations.<br><br>　4. Discovery of exercise influence and generation of improved exercise curricula.<br><br><br><br>モデル：<br><br><br><br>&lt;img src=\"https://user-images.githubusercontent.com/12249301/50377473-432b0d00-0661-11e9-97e1-a60a68a6ef32.png\" alt=\"image\" loading=\"lazy\" /&gt;<br><br><br><br>Inputは、ExerciseがM個あったときに、M個のExerciseがcorrectか否かを表すベクトル（長さ2Mベクトルのone-hot）。separateなrepresentationにするとパフォーマンスが下がるらしい。<br><br>Output ytの長さは問題数Mと等しく、各要素は、生徒が対応する問題を正答する確率。<br><br><br><br>InputとしてExerciseを用いるか、ExerciseのKCを用いるかはアプリケーション次第っぽいが、典型的には各スキルの潜在的なmasteryを測ることがモチベーションなのでKCを使う。<br><br><br><br>（もし問題数が膨大にあるような設定の場合は、各問題-正/誤答tupleに対して、random vectorを正規分布からサンプリングして、one-hot high-dimensional vectorで表現する。）<br><br><br><br>hidden sizeは200, mini-batch sizeは100としている。<br><br><br><br>\[Educational Applicationsへの応用]<br><br><br><br>生徒へ最適なパスの学習アイテムを選んで提示することができること<br><br>　生徒のknowledge stateを予測し、その後特定のアイテムを生徒にassignすることができる。たとえば、生徒が50個のExerciseに回答した場合、生徒へ次に提示するアイテムを計算するだけでなく、その結果期待される生徒のknowledge stateも推測することができる<br><br><br><br>Exercises間の関係性を見出すことができる<br><br>　y\( j | i )を考える。y\( j | i )は、はじめにexercise iを正答した後に、second time stepでjを正答する確率。これによって、pre-requisiteを明らかにすることができる。<br><br><br><br>\[評価]<br><br>3種類のデータセットを用いる。<br><br>　1. simulated Data<br><br>　　2000人のvirtual studentを作り、1〜5つのコンセプトから生成された、50問を、同じ順番で解かせた。このとき、IRTモデルを用いて、シミュレーションは実施した。このとき、hidden stateのラベルには何も使わないで、inputは問題のIDと正誤データだけを与えた。さらに、2000人のvirtual studentをテストデータとして作り、それぞれのコンセプト（コンセプト数を1〜5に変動させる）に対して、20回ランダムに生成したデータでaccuracyの平均とstandard errorを測った。<br><br>　2. Khan Academy Data<br><br>　　1.4MのExerciseと、69の異なるExercise Typeがあり、47495人の生徒がExerciseを行なっている。<br><br>　　PersonalなInformationは含んでいない。<br><br>　3. Assistsments bemchmark Dataset<br><br>　　2009-2011のskill builder public benchmark datasetを用いた。Assistmentsは、online tutorが、数学を教えて、教えるのと同時に生徒を評価するような枠組みである。<br><br><br><br>それぞれのデータセットに対して、AUCを計算。<br><br>ベースラインは、BKTと生徒がある問題を正答した場合の周辺確率？<br><br><br><br>&lt;img src=\"https://user-images.githubusercontent.com/12249301/50377495-b0d73900-0661-11e9-9ca2-1cb97393d698.png\" alt=\"image\" loading=\"lazy\" /&gt;<br><br><br><br>&lt;img src=\"https://user-images.githubusercontent.com/12249301/50377501-b92f7400-0661-11e9-87ce-9f836c860209.png\" alt=\"image\" loading=\"lazy\" /&gt;<br><br><br><br>simulated dataの場合、問題番号5がコンセプト1から生成され、問題番号22までの問題は別のコンセプトから生成されていたにもかかわらず、きちんと二つの問題の関係をとらえられていることがわかる。<br><br>Khan Datasetについても同様の解析をした。これは、この結果は専門家が見たら驚くべきものではないかもしれないが、モデルが一貫したものを学習したと言える。<br><br><br><br>\[Discussion]<br><br>提案モデルの特徴として、下記の２つがある：<br><br><br><br>専門家のアノテーションを必要としない（concept patternを勝手に学習してくれる）<br><br>ベクトル化された生徒のinputであれば、なんでもoperateすることができる<br><br>drawbackとしては、大量のデータが必要だということ。small classroom environmentではなく、online education environmentに向いている。<br><br>今後の方向性としては、<br><br>・incorporate other feature as inputs \(such as time taken)<br><br>・explore other educational impacts \(hint generation, dropout prediction)<br><br>・validate hypotheses posed in education literature \(such as spaced repetition, modeling how students forget)<br><br>・open-ended programmingとかへの応用とか（proramのvectorizationの方法とかが最近提案されているので）<br><br>などがある。</p>
<p>knewtonのグループが、DKTを既存手法であるIRTの変種やBKTの変種などでoutperformすることができることを示す：<br><br>


<a href="https://arxiv.org/pdf/1604.02336.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/1604.02336.pdf</a>


<br><br><br><br>vanillaなDKTはかなりナイーブなモデルであり、今後の伸びが結構期待できると思うので、単純にoutperformしても、今後の発展性を考えるとやはりDKTには注目せざるを得ない感</p>
<p>DKT元論文では、BKTを大幅にoutperformしており、割と衝撃的な結果だったようだが、<br><br>後に論文中で利用されているAssistmentsデータセット中にdupilcate entryがあり、<br><br>それが原因で性能が不当に上がっていることが判明。<br><br><br><br>結局DKTの性能的には、BKTとどっこいみたいなことをRyan Baker氏がedXで言っていた気がする。</p>
<p>Deep Knowledge TracingなどのKnowledge Tracingタスクにおいては、<br><br>基本的に問題ごとにKnowledge Component(あるいは知識タグ, その問題を解くのに必要なスキルセット）が付与されていることが前提となっている。<br><br>ただし、このような知識タグを付与するには専門家によるアノテーションが必要であり、<br><br>適用したいデータセットに対して必ずしも付与されているとは限らない。<br><br><br><br>このような場合は、DKTは単なる”問題”の正答率予測モデルとして機能させることしかできないが、<br><br>知識タグそのものもNeural Networkに学習させてしまおうという試みが行われている：<br><br>


<a href="https://www.jstage.jst.go.jp/article/tjsai/33/3/33_C-H83/_article/-char/ja" target="_blank" rel="noopener noreferrer">https://www.jstage.jst.go.jp/article/tjsai/33/3/33_C-H83/_article/-char/ja</a>


</p>
<p>DKTに関する詳細な説明が書かれているブログポスト：<br><br>expectimaxアルゴリズムの説明や、最終的なoutput vector y_i の図解など、説明が省略されガチなところが詳細に書いてあって有用。（英語に翻訳して読むと良い）<br><br>


<a href="https://hcnoh.github.io/2019-06-14-deep-knowledge-tracing" target="_blank" rel="noopener noreferrer">https://hcnoh.github.io/2019-06-14-deep-knowledge-tracing</a>


</p>
<p>こちらのリポジトリではexpectimaxアルゴリズムによってvirtualtutorを実装している模様。<br><br>詳細なレポートもアップロードされている。<br><br>


<a href="https://github.com/alessandroscoppio/VirtualIntelligentTutor" target="_blank" rel="noopener noreferrer">https://github.com/alessandroscoppio/VirtualIntelligentTutor</a>


</p>
<p>DKTのinputの次元数が 2 * num_skills, outputの次元数がnum_skillsだと明記されているスライド。<br><br>元論文だとこの辺が言及されていなくてわかりづらい・・・<br><br>


<a href="http://gdac.uqam.ca/Workshop@EDM20/slides/LSTM_tutorial_Application.pdf" target="_blank" rel="noopener noreferrer">http://gdac.uqam.ca/Workshop@EDM20/slides/LSTM_tutorial_Application.pdf</a>


<br><br>


<a href="http://gdac.uqam.ca/Workshop@EDM20/slides/LSTM_Tutorial.pdf" target="_blank" rel="noopener noreferrer">http://gdac.uqam.ca/Workshop@EDM20/slides/LSTM_Tutorial.pdf</a>


<br><br><br><br>こちらのページが上記チュートリアルのページ<br><br>


<a href="http://gdac.uqam.ca/Workshop@EDM20/" target="_blank" rel="noopener noreferrer">http://gdac.uqam.ca/Workshop@EDM20/</a>


</p>&lt;/span&gt;<br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/InformationRetrieval.html" target="_blank" rel="noopener noreferrer">#InformationRetrieval</a>
<a class="button" href="articles/LearningToRank.html" target="_blank" rel="noopener noreferrer">#LearningToRank</a>
<a class="button" href="articles/PairWise.html" target="_blank" rel="noopener noreferrer">#PairWise</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/191" target="_blank" rel="noopener noreferrer" class="title-link">Large Scale Learning to Rank, Sculley+, NIPS 2009</a>
<span class="snippet"><span>Comment</span><p>sofia-mlの実装内容について記述されている論文<br><br><br><br>よくonline学習の文脈で触れられるが、気をつけないと罠にはまる。<br><br>というのは、sofia-ml内のMethodsによって、最適化している目的関数が異なるからだ。<br><br>実装をみると、全てのmethodsがonlineでできちゃいそうに見える（学習済みのモデルをinputして学習を再開させられるため）が、落とし穴。<br><br><br><br>まず、SGD SVM, Pegasos SVM,については、最適化している目的関数がbatchになっているため、online learningではない。<br><br>passive-aggressive perceptrionは目的関数が個別の事例に対して定式化される(要確認)のでonline learningといえる。<br><br>(ROMMAは調べないとわからん)<br><br><br><br>pairwiseのlearning to rankでは、サンプルのペアを使って学習するので、最悪の場合O(n^2)の計算量がかかってしまってめっちゃ遅いのだが、実は学習データを一部サンプリングして重みを更新するってのをたくさん繰り返すだけで、高速に学習できちゃうという話。<br><br><br><br>実際、sofia-mlを使って見たら、liblinearのranking SVM実装で40分かかった学習が数秒で終わり、なおかつ精度も良かった。<br><br></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Document.html" target="_blank" rel="noopener noreferrer">#Document</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/QuestionAnswering.html" target="_blank" rel="noopener noreferrer">#QuestionAnswering</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/77" target="_blank" rel="noopener noreferrer" class="title-link">Teaching Machines to Read and Comprehend, Hermann+, NIPS 2015</a>
<span class="snippet"><span>Comment</span><p>だいぶ前に読んだので割とうろおぼえ。<br><br><br><br>CNN/DailyMailデータセットの作成を行なった論文（最近Neuralな文”書”要約の学習でよく使われるやつ）。<br><br>CNN/DailyMailにはニュース記事に対して、人手で作成した要約が付与されており、要約中のEntityを穴埋めにするなどして、穴埋め問題を作成。<br><br>言文書をNeuralなモデルに与えて、どれだけ回答できるかという話。<br><br><br><br>[スタンフォードによる追試がある](


<a href="https://cs.stanford.edu/people/danqi/papers/acl2016.pdf)" target="_blank" rel="noopener noreferrer">https://cs.stanford.edu/people/danqi/papers/acl2016.pdf)</a>


<br><br>[詳しい解説 by 久保さん](


<a href="https://www.slideshare.net/takahirokubo7792/machine-comprehension)" target="_blank" rel="noopener noreferrer">https://www.slideshare.net/takahirokubo7792/machine-comprehension)</a>


<br><br><br><br>追試によると、評価で使用している穴埋め問題は単純なモデルで提案モデルの性能を上回ったりしている。また、この穴埋め問題のうち54%は単純な質問とのマッチで回答可能であり、25%は人でも正解不能らしい（正解率のupper boundは75%）。by 久保さんのスライド<br><br>のちの研究で、ほぼこの上限に達する精度が達成されてしまったので、このデータセットはQAタスクではほぼ攻略された状態だという。</p></span><br><br>
<button onclick="hideContent(0)" style="display: none;">hide</button>
</div>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<script>
  document.addEventListener('DOMContentLoaded', function() {
    const tweets = document.querySelectorAll('.tweet-embed[data-embed]');

    if ('IntersectionObserver' in window) {
      const observer = new IntersectionObserver((entries, obs) => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            const el = entry.target;
            const html = el.getAttribute('data-embed');
            if (html) {
              const loadingImg = el.querySelector('.tweet-loading');
              if (loadingImg) loadingImg.remove();

              el.innerHTML = html.trim();

              if (window.twttr?.widgets?.load) {
                window.twttr.widgets.load(el);
              }
            }
            obs.unobserve(el); // 処理済みは監視解除
          }
        });
      }, {
        rootMargin: '500px 0px', // 画面手前200pxで読み込み開始
        threshold: 0
      });

      tweets.forEach(tweet => observer.observe(tweet));

    } else {
      // IntersectionObserver未対応ブラウザ用のフォールバック
      function lazyLoadFallback() {
        tweets.forEach(el => {
          if (el.getAttribute('data-embed') && el.getBoundingClientRect().top < window.innerHeight) {
            const html = el.getAttribute('data-embed');
            const loadingImg = el.querySelector('.tweet-loading');
            if (loadingImg) loadingImg.remove();
            el.innerHTML = html.trim();
            el.removeAttribute('data-embed');
            if (window.twttr?.widgets?.load) {
              window.twttr.widgets.load(el);
            }
          }
        });
      }
      window.addEventListener('scroll', lazyLoadFallback);
      lazyLoadFallback();
    }
  });
</script>



    </div>

</article>
<div class="post-nav">
<a class="previous" href="/paper_notes/articles/Navigation.html" title="Navigationに関する論文・技術記事メモの一覧">Navigationに関する論文・技術記事メモの一覧</a><a class="next" href="/paper_notes/articles/NeuralArchitectureSearch.html" title="NeuralArchitectureSearchに関する論文・技術記事メモの一覧">NeuralArchitectureSearchに関する論文・技術記事メモの一覧</a>
</div>
<div class="post-related">
      <div>Related Articles</div>
      <ul>
        <li class="">
          <a class="post-link" href="/paper_notes/articles/RewardModel.html" title="RewardModelに関する論文・技術記事メモの一覧">
            RewardModelに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/Self-SupervisedLearning.html" title="Self-SupervisedLearningに関する論文・技術記事メモの一覧">
            Self-SupervisedLearningに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/ScorePrediction.html" title="ScorePredictionに関する論文・技術記事メモの一覧">
            ScorePredictionに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/DiffusionModel.html" title="DiffusionModelに関する論文・技術記事メモの一覧">
            DiffusionModelに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
</ul>
    </div>
<div class="post-comments"></div></section>
</div>


  </section>
  <section class="sidebar" style="margin-left: 15px;">
    <!-- Get sidebar items --><style type="text/css" media="screen">
.post-menu ul {
  list-style: none;
  padding: 0;
  margin: 0;
}
</style>

<div class="post-menu">
  <div class="post-menu-title">TOC</div>
  <div class="post-menu-content"></div>
</div>

<script>
  function generateContent() {
    var menu = document.querySelector(".post-menu");
    var menuContent =  menu.querySelector(".post-menu-content");
    var headings = document.querySelector(".post-content").querySelectorAll("h2, h3, h4, h5, h6");

    // Hide menu when no headings
    if (headings.length === 0) {
      return menu.style.display = "none";
    }

    // Generate post menu
    var menuHTML = '';
    for (var i = 0; i < headings.length; i++) {
      var h = headings[i];
      menuHTML += (
        '<li class="h-' + h.tagName.toLowerCase() + '">'
        + '<a href="#h-' + h.getAttribute('id') + '">' + h.textContent + '</a></li>');
    }

    menuContent.innerHTML = '<ul>' + menuHTML + '</ul>';

    // The header element
    var header = document.querySelector('header.site-header');

    function doMenuCollapse(index, over_items) {
      var items = menuContent.firstChild.children;

      if (over_items == undefined) {
        over_items = 20;
      }

      if (items.length < over_items) {
        return;
      }

      var activeItem = items[index];
      var beginItem = activeItem
      var endItem = activeItem
      var beginIndex = index;
      var endIndex = index + 1;
      while (beginIndex >= 0
        && !items[beginIndex].classList.contains('h-h2')) {
        beginIndex -= 1;
      }
      while (endIndex < items.length
        && !items[endIndex].classList.contains('h-h2')) {
        endIndex += 1;
      }
      for (var i = 0; i < beginIndex; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
      for (var i = beginIndex + 1; i < endIndex; i++) {
        item = items[i]
        // if (!item.classList.contains('h-h2')) {
          item.style.display = '';
        // }
      }
      for (var i = endIndex; i < items.length; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
    }

    // Init menu collapsed
    doMenuCollapse(-1);

    // Active the menu item
    window.addEventListener('scroll', function (event) {
      var lastActive = menuContent.querySelector('.active');
      var changed = true;
      var activeIndex = -1;
      for (var i = headings.length - 1; i >= 0; i--) {
        var h = headings[i];
        var headingRect = h.getBoundingClientRect();
        var headerRect = header.getBoundingClientRect();
        var headerTop = Math.floor(headerRect.top);
        var headerHeight = Math.floor(headerRect.height);
        var headerHeight = headerTop + headerHeight + 20;
        if (headingRect.top <= headerHeight) {
          var id = 'h-' + h.getAttribute('id');
          var a = menuContent.querySelector('a[href="#' + id  + '"]');
          var curActive = a.parentNode;
          if (curActive) {
            curActive.classList.add('active');
            activeIndex = i;
          }
          if (lastActive == curActive) {
            changed = false;
          }
          break;
        }
      }
      if (changed) {
        if (lastActive) {
          lastActive.classList.remove('active');
        }
        doMenuCollapse(activeIndex);
      }
      event.preventDefault();
    });
  }
  generateContent();
</script>
</section>
</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/paper_notes/"></data>

  <div class="wrapper">
    <div class="site-footer-inner">
<div>Copyright © 2023-current AkihikoWATANABE. The header images and any thumbnail images for the posts were generated by ChatGPT's DALL-E3.</div>
      <div>Powered by <a title="Jekyll is a simple, blog-aware, static site
      generator." href="https://jekyllrb.com/">Jekyll</a> &amp; <a title="Yat, yet
      another theme." href="https://github.com/jeffreytse/jekyll-theme-yat">Yat Theme</a>.</div>
      <div class="footer-col rss-subscribe">Subscribe <a href="/paper_notes/feed.xml">via RSS</a>
</div>
    </div>
  </div>
</footer>
</body>
  </html>
