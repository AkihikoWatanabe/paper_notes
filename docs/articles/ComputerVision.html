<!DOCTYPE html>
<html lang="ja">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-translate-customization" content="108d9124921d80c3-80e20d618ff053c8-g4f02ec6f3dba68b7-c">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>ComputerVisionã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§ | ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ</title>
<meta name="generator" content="Jekyll v4.3.2">
<meta property="og:title" content="ComputerVisionã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
<meta name="author" content="AkihikoWATANABE">
<meta property="og:locale" content="ja">
<meta name="description" content="ComputerVision">
<meta property="og:description" content="ComputerVision">
<link rel="canonical" href="http://akihikowatanabe.github.io/paper_notes/articles/ComputerVision.html">
<meta property="og:url" content="http://akihikowatanabe.github.io/paper_notes/articles/ComputerVision.html">
<meta property="og:site_name" content="ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-08-04T01:00:15+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="ComputerVisionã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"AkihikoWATANABE"},"dateModified":"2025-08-04T01:00:15+00:00","datePublished":"2025-08-04T01:00:15+00:00","description":"ComputerVision","headline":"ComputerVisionã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§","mainEntityOfPage":{"@type":"WebPage","@id":"http://akihikowatanabe.github.io/paper_notes/articles/ComputerVision.html"},"url":"http://akihikowatanabe.github.io/paper_notes/articles/ComputerVision.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="icon" href="">
  <link rel="canonical" href="http://akihikowatanabe.github.io">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/main.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/custom_button.css">
  <script src="/paper_notes/assets/js/main.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script><link type="application/atom+xml" rel="alternate" href="http://akihikowatanabe.github.io/paper_notes/feed.xml" title="ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ">
<script>
  function showMore(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "block";
          
      // ã“ã®ãƒœã‚¿ãƒ³ã®å‚ç…§ã‚’å–å¾—ã—ã¦éè¡¨ç¤ºã«ã—ã¾ã™
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "none";
      }
          
      // hideãƒœã‚¿ãƒ³ã®å‚ç…§ã‚’å–å¾—ã—ã¦è¡¨ç¤ºã—ã¾ã™
      const hideButton = contentDiv.querySelector('button[onclick^="hideContent"]');
      if (hideButton) {
        hideButton.style.display = "block";
      }
    }
  }

  function hideContent(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "none";
  
      // moreãƒœã‚¿ãƒ³ã®å‚ç…§ã‚’å–å¾—ã—ã¦è¡¨ç¤ºã—ã¾ã™
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "block";
      }
  
      // ã“ã®ãƒœã‚¿ãƒ³ã‚’éš ã—ã¾ã™
      const hideButtons = document.querySelectorAll('button[onclick^="hideContent"]');
      const hideButton = hideButtons[index];
      if (hideButton) {
        hideButton.style.display = "none";
      }
    }
  }
</script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script charset="UTF-8" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/go.min.js" async></script>



















<script>
// Init highlight js
document.addEventListener('DOMContentLoaded', function(event) {
  var els = document.querySelectorAll('pre code')

  function addLangData(block) {
    var outer = block.parentElement.parentElement.parentElement;
    var lang = block.getAttribute('data-lang');
    for (var i = 0; i < outer.classList.length; i++) {
      var cls = outer.classList[i];
      if (cls.startsWith('language-')) {
        lang = cls;
        break;
      }
    }
    if (!lang) {
      cls = block.getAttribute('class');
      lang = cls ? cls.replace('hljs ', '') : '';
    }
    if (lang.startsWith('language-')) {
      lang = lang.substr(9);
    }
    block.setAttribute('class', 'hljs ' + lang);
    block.parentNode.setAttribute('data-lang', lang);
  }

  function addBadge(block) {
    var enabled = ('true' || 'true').toLowerCase();
    if (enabled == 'true') {
      var pre = block.parentElement;
      pre.classList.add('badge');
    }
  }

  function handle(block) {
    addLangData(block);
    addBadge(block)
    hljs.highlightBlock(block);
  }

  for (var i = 0; i < els.length; i++) {
    var el = els[i];
    handle(el);
  }
});
</script>

<style>
  /* code language badge */
  pre.badge::before {
    content: attr(data-lang);
    color: #fff;
    background-color: #ff4e00;
    padding: 0 .5em;
    border-radius: 0 2px;
    text-transform: uppercase;
    text-align: center;
    min-width: 32px;
    display: inline-block;
    position: absolute;
    right: 0;
  }

  /* fix wrong badge display for firefox browser */
  code > table pre::before {
    display: none;
  }
</style>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe-lightbox.umd.min.js" async></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe.umd.min.js" async></script>
<link href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/photoswipe.min.css" rel="stylesheet">
<style>
  .pswp .pswp__container .pswp__img {
    background-color: white;
  }
</style>

<script>
  function initPhotoSwipe() {
    let customOptions = {};

    try {
      const data = `{"gallery"=>"section.main", "children"=>"a.photo-swipe", "bgOpacity"=>0.8, "padding"=>{"top"=>20, "bottom"=>40, "left"=>100, "right"=>100}}`.replaceAll("=>", ":");
      customOptions = JSON.parse(data);
    } catch (e) {
      console.info("Invalid custom photo previewer options! " + e.message);
    }

    // Define object and gallery options
    const options = Object.assign(
      {
        gallery: "section.main",
        children: "a.photo-swipe",
        photo_scale: 2,
        // dynamic import is not supported in UMD version
        pswpModule: PhotoSwipe,
      },
      customOptions
    );

    const galleryEl = document.querySelector(options.gallery);
    if (!galleryEl) {
      return;
    }

    const imgEls = [];
    const els = galleryEl.querySelectorAll("img:not(.emoji)");
    els.forEach((el) => {
      if (el.src.trim() == "") {
        return;
      }
      if (!imgEls.includes(el)) {
        imgEls.push(el);
      }
    });

    if (imgEls.length === 0) {
      return;
    }

    imgEls.forEach((imgEl) => {
      imgEl.outerHTML = `
      <a class="photo-swipe"
        href="${imgEl.src}"
        data-pswp-width="${
          Math.max(imgEl.naturalWidth, imgEl.width) * options.photo_scale
        }"
        data-pswp-height="${
          Math.max(imgEl.naturalHeight, imgEl.height) * options.photo_scale
        }"
        data-pswp-caption="${imgEl.getAttribute("caption") || imgEl.alt}"
        target="_blank">
        ${imgEl.outerHTML}
      </a>`;
    });

    // Initialize PhotoSwipe 5
    var lightbox = new PhotoSwipeLightbox(options);

    lightbox.init();
  }

  window.addEventListener("load", initPhotoSwipe);
</script>
<meta name="google-site-verification" content="u_DTTPcCZ806iq51zgirHyWq3556HUKGq8AQfH91iFI">
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>





































































































































<header class="site-header site-header-transparent" role="banner">

  <div class="wrapper">
    <div class="site-header-inner">
<span class="site-brand"><a class="site-brand-inner" rel="author" href="/paper_notes/">
  <img class="site-favicon" title="ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ" src="" onerror="this.style.display='none'">
  ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ
</a>
</span><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger">
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewbox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
              </svg>
            </span>
          </label>

          <div class="trigger">









<span class="page-link">



<div id="google_translate_element" style="display: none;">
</div>

<span class="ct-language">
  <ul class="list-unstyled ct-language-dropdown">
    
      <li>
        <a href="#" class="lang-select" data-lang="en">
          
          <img src="https://cdn.countryflags.com/thumbs/united-states-of-america/flag-400.png" title="English">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="fr">
          
          <img src="https://cdn.countryflags.com/thumbs/france/flag-400.png" title="French">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="zh-CN">
          
          <img src="https://cdn.countryflags.com/thumbs/china/flag-400.png" title="Chinese(Simple)">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ja">
          
          <img src="https://cdn.countryflags.com/thumbs/japan/flag-400.png" title="Japanese">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ko">
          
          <img src="https://cdn.countryflags.com/thumbs/south-korea/flag-400.png" title="Korean">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ru">
          
          <img src="https://cdn.countryflags.com/thumbs/russia/flag-400.png" title="Russian">
          
        </a>
      </li>
    
  </ul>
</span>

<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({
    pageLanguage: 'ja',
    autoDisplay: false,
    layout: google.translate.TranslateElement.InlineLayout.VERTICAL
  }, 'google_translate_element');

  // Links to cross-origin destinations are unsafe
  var gll = document.getElementsByClassName('goog-logo-link')[0];
  if (gll) {
    gll.setAttribute('rel', 'noopener');
  }

  function restoreLang() {
    var iframe = document.getElementsByClassName('goog-te-banner-frame')[0];
    if (!iframe) return;

    var innerDoc = iframe.contentDocument || iframe.contentWindow.document;
    var restore_el = innerDoc.getElementsByTagName("button");

    for (var i = 0; i < restore_el.length; i++) {
      if (restore_el[i].id.indexOf("restore") >= 0) {
        restore_el[i].click();
        var close_el = innerDoc.getElementsByClassName("goog-close-link");
        close_el[0].click();
        return;
      }
    }
  }

  function triggerHtmlEvent(element, eventName) {
    var event;
    if (document.createEvent) {
      event = document.createEvent('HTMLEvents');
      event.initEvent(eventName, true, true);
      element.dispatchEvent(event);
    } else {
      event = document.createEventObject();
      event.eventType = eventName;
      element.fireEvent('on' + event.eventType, event);
    }
  }

  var googleCombo = document.querySelector("select.goog-te-combo");
  var langSelect = document.querySelector('.ct-language');
  langSelect.addEventListener('click', function(event) {
    if (!event.target) {
      return;
    }

    var selected = document.querySelector('.ct-language .ct-language-selected');
    if (selected) {
      selected.classList.remove('ct-language-selected');
    }

    var target = event.target;
    while (target && target !== langSelect ) {
      if (target.matches('.lang-select')) {
        break;
      }
      target = target.parentElement;
    }

    if (target && target.matches('.lang-select')) {
      var lang = target.getAttribute('data-lang');
      if (googleCombo.value == lang) {
        restoreLang();
      } else {
        target.parentElement.classList.add('ct-language-selected');
        googleCombo.value = lang;
        triggerHtmlEvent(googleCombo, 'change');
      }
    }

    event.preventDefault();
  });
}
</script>

<script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit" async></script>
</span>
</div>
        </nav>
</div>
  </div>
</header>

<script>
  function initHeader() {
    var lastScrollY = getScrollPos().y;
    var documentElement = document.documentElement;

    function storeScrollData() {
      var y = getScrollPos().y;documentElement.setAttribute("data-header-transparent", "");var scrollStatus = "";

      if (y <= 0) {
        scrollStatus = "top";
      } else if ((window.innerHeight + y) >= document.body.offsetHeight) {
        scrollStatus = "bottom";
      } else {
        var isScrollDown = (y - lastScrollY > 0) ? true : false;
        scrollStatus = isScrollDown ? "down" : "up";
      }

      lastScrollY = y;
      documentElement.setAttribute("data-scroll-status", scrollStatus);
    }

    window.addEventListener('scroll', function(e) {
      storeScrollData();
    });

    storeScrollData();
  }
  document.addEventListener('DOMContentLoaded', initHeader);
</script>


























































































































































<style>
    html .page-banner .page-banner-img > *:first-child {
      opacity: 0.4;
    }

    html[data-theme="dark"] .page-banner .page-banner-img > *:first-child {
      opacity: 0.2872;
    }
  </style>
<section class="page-banner">
    <div class="page-banner-img">
<div style="background-image: url(/paper_notes/assets/images/banner.png)"></div>
        <img class="img-placeholder" src="/paper_notes/assets/images/banner.png">
</div>
    <div class="wrapper">
      <div class="page-banner-inner">
<header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ</h1>
  <h2 class="post-subtitle">å‹‰å¼·ã—ãŸè«–æ–‡ã‚„æŠ€è¡“ç­‰ã®æƒ…å ±ã‚’Githubã®Issueã«ãƒ¡ãƒ¢ã£ã¦ã„ã‚‹ã²ã¨ã®ãƒ–ãƒ­ã‚°ã€‚
ãã‚Œãªã‚Šã«ãƒ¡ãƒ¢ã®é‡ãŒè“„ç©ã•ã‚Œã¦ããŸã®ã§ã€ä¸€åº¦æ•´ç†ã—ãŸã„ãªã¨æ€ã„ãƒ–ãƒ­ã‚°ã¯ã˜ã‚ã¦ã¿ã¾ã—ãŸï¼
è‡ªç„¶è¨€èªå‡¦ç†(NLP), æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ (RecommenderSystem), Educational Data Mining (EDM), Learning Analytics (LA)ãªã©ã®åˆ†é‡ã®ãƒ¡ãƒ¢ãŒå¤šã„ã¨æ€ã„ã¾ã™ã€‚
æœ€è¿‘ã¯ç‰¹ã«LLMã®å‹‰å¼·ãŒå¤šã‚ã§ã™ :)</h2>

  <div class="post-meta">
    <time class="dt-published" datetime="2025-08-04T01:00:15+00:00" itemprop="datePublished"><i class="fa fa-calendar"></i> Aug 4, 2025
    </time><span class="post-author left-vsplit"><i class="fa fa-pencil"></i> AkihikoWATANABE</span>
    
































    <span class="post-reading-time left-vsplit"><i class="fa fa-clock-o"></i> About 4 hours 15 mins</span>
  </div></header>
</div>
    </div>
  </section><script>
  function hashLocate(hashValue) {
    hashValue = hashValue.replace(/^.*#h-/, '');
    hashValue = decodeURIComponent(hashValue);
    var element = document.getElementById(hashValue);

    if (!element) {
      return;
    }

    var header = document.querySelector('header.site-header');
    var headerRect = header.getBoundingClientRect();
    var headerTop = Math.floor(headerRect.top);
    var headerHeight = Math.floor(headerRect.height);
    var scrollPos = getScrollPos();
    var offsetY = element.offsetTop - (headerTop + headerHeight + 20);

    if (offsetY == scrollPos.y) {
      return;
    }

    if (headerTop == 0  && offsetY > scrollPos.y) {
      offsetY += headerHeight + 2;
    } else if (headerTop < 0  && offsetY < scrollPos.y) {
      offsetY -= headerHeight - 2;
    }

    smoothScrollTo(offsetY);
  }

  // The first event occurred
  window.addEventListener('load', function(event) {
    if (window.location.hash) {
      hashLocate(window.location.hash);
    }
  });

  // The first event occurred
  window.addEventListener('click', function(event) {
    if (event.target.tagName.toLowerCase() == 'a') {
      hashLocate(event.target.getAttribute('href'));
    }
  });
</script>
<div class="theme-toggle">
  <input type="checkbox" id="theme-switch">
  <label for="theme-switch">
    <div class="toggle"></div>
    <div class="names">
      <p class="light">Light</p>
      <p class="dark">Dark</p>
    </div>
  </label>
</div>




<script>
  (function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var nightModeOption = ('off' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
      storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
      var data = storage.getItem('theme');
      try {
        data = JSON.parse(data ? data : '');
      } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
      }
      return data;
    }

    function handleThemeToggle(nightShift) {
      themeData.nightShift = nightShift;
      saveThemeData(themeData);
      html.dataset.theme = nightShift ? 'dark' : 'light';
      setTimeout(function() {
        sw.checked = nightShift ? true : false;
      }, 50);
    }

    function autoThemeToggle() {
      // Next time point of theme toggle
      var now = new Date();
      var toggleAt = new Date();
      var hours = now.getHours();
      var nightShift = hours >= 19 || hours <=7;

      if (nightShift) {
        if (hours > 7) {
          toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
      } else {
        toggleAt.setHours(19);
      }

      toggleAt.setMinutes(0);
      toggleAt.setSeconds(0);
      toggleAt.setMilliseconds(0)

      var delay = toggleAt.getTime() - now.getTime();

      // auto toggle theme mode
      setTimeout(function() {
        handleThemeToggle(!nightShift);
      }, delay);

      return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
      };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
      handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
      var data = autoThemeToggle();

      // Toggle theme by local setting
      if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
      } else {
        handleThemeToggle(themeData.nightShift);
      }
    } else if (nightModeOption == 'manual') {
      handleThemeToggle(themeData.nightShift);
    } else {
      var nightShift = themeData.nightShift;
      if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
      }
      handleThemeToggle(nightShift);
    }
  })();
</script>
<div id="click-to-top" class="click-to-top">
  <i class="fa fa-arrow-up"></i>
</div>
<script>
  (function () {
    const clickToTop = document.getElementById('click-to-top');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 100) {
        clickToTop.classList.add('show')
      }else {
        clickToTop.classList.remove('show')
      }
    });
    clickToTop.addEventListener('click', () => {
      window.smoothScrollTo(0);
    });
  })();
</script>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="framework">
  <section class="main">

     <div class="post">
  <section>









<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="post-content e-content" itemprop="articleBody">

      <h2 id="computervision">ComputerVision</h2>

<div class="visible-content">
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/ICCV.html">#ICCV</a>


<br>


<span class="issue_date">Issue Date: 2025-08-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2344">[Paper Note] BUFFER-X: Towards Zero-Shot Point Cloud Registration in Diverse Scenes, Minkyun Seo+, ICCV'25</a>
<span class="snippet"><span>Summary</span>BUFFER-Xã¨ã„ã†ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆç™»éŒ²ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ææ¡ˆã—ã€ç’°å¢ƒç‰¹æœ‰ã®ãƒœã‚¯ã‚»ãƒ«ã‚µã‚¤ã‚ºã‚„æ¢ç´¢åŠå¾„ã¸ã®ä¾å­˜ã€ãƒ‰ãƒ¡ã‚¤ãƒ³å¤–ãƒ­ãƒã‚¹ãƒˆæ€§ã®ä½ã•ã€ã‚¹ã‚±ãƒ¼ãƒ«ä¸ä¸€è‡´ã®å•é¡Œã«å¯¾å‡¦ã€‚ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã®ãƒ‘ãƒƒãƒãƒ™ãƒ¼ã‚¹ã®è¨˜è¿°å­ç”Ÿæˆã¨éšå±¤çš„ã‚¤ãƒ³ãƒ©ã‚¤ã‚¢æ¤œç´¢ã‚’ç”¨ã„ã¦ã€ã•ã¾ã–ã¾ãªã‚·ãƒ¼ãƒ³ã§ã®ãƒ­ãƒã‚¹ãƒˆæ€§ã‚’å‘ä¸Šã€‚æ–°ã—ã„ä¸€èˆ¬åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ç”¨ã„ã¦ã€BUFFER-XãŒæ‰‹å‹•èª¿æ•´ãªã—ã§å¤§å¹…ãªä¸€èˆ¬åŒ–ã‚’é”æˆã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/rsasaki0109/status/1951478059002966159?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qã“ã®è¾ºã®åˆ†é‡ã±ã£ã¨è¦‹ã§å…¨ç„¶ã‚ã‹ã‚‰ãªã„â€¦</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/MultiLingual.html">#MultiLingual</a>
<a class="button" href="articles/CLIP.html">#CLIP</a>


<br>


<span class="issue_date">Issue Date: 2025-07-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2320">[Paper Note] MetaCLIP 2: A Worldwide Scaling Recipe, Yung-Sung Chuang+, arXiv'25</a>
<span class="snippet"><span>Summary</span>MetaCLIP 2ã‚’ææ¡ˆã—ã€CLIPã‚’ã‚¼ãƒ­ã‹ã‚‰è¨“ç·´ã™ã‚‹ãŸã‚ã®æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç¤ºã™ã€‚è‹±èªã¨éè‹±èªãƒ‡ãƒ¼ã‚¿ã®ç›¸äº’åˆ©ç›Šã‚’å¾—ã‚‹ãŸã‚ã®æœ€å°é™ã®å¤‰æ›´ã‚’åŠ ãˆã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã®ImageNetåˆ†é¡ã§è‹±èªå°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’é”æˆã€‚å¤šè¨€èªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã‚‚æ–°ãŸãªæœ€å…ˆç«¯ã‚’è¨˜éŒ²ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/jaseweston/status/1950366185742016935?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/SpeechProcessing.html">#SpeechProcessing</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a>


<br>


<span class="issue_date">Issue Date: 2025-07-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2300">[Paper Note] Ming-Omni: A Unified Multimodal Model for Perception and Generation, Inclusion AI+, arXiv'25</a>
<span class="snippet"><span>Summary</span>Ming-Omniã¯ã€ç”»åƒã€ãƒ†ã‚­ã‚¹ãƒˆã€éŸ³å£°ã€å‹•ç”»ã‚’å‡¦ç†ã§ãã‚‹çµ±ä¸€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã€éŸ³å£°ç”Ÿæˆã¨ç”»åƒç”Ÿæˆã«ãŠã„ã¦å„ªã‚ŒãŸèƒ½åŠ›ã‚’ç¤ºã™ã€‚å°‚ç”¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’ç”¨ã„ã¦ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡ºã—ã€MoEã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§å‡¦ç†ã™ã‚‹ã“ã¨ã§ã€åŠ¹ç‡çš„ã«ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ã‚’èåˆã€‚éŸ³å£°ãƒ‡ã‚³ãƒ¼ãƒ€ã¨é«˜å“è³ªãªç”»åƒç”Ÿæˆã‚’çµ±åˆã—ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¿œã˜ãŸãƒãƒ£ãƒƒãƒˆã‚„ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°ã¸ã®å¤‰æ›ã€ç”»åƒç·¨é›†ãŒå¯èƒ½ã€‚Ming-Omniã¯ã€GPT-4oã«åŒ¹æ•µã™ã‚‹åˆã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ç ”ç©¶ã¨é–‹ç™ºã‚’ä¿ƒé€²ã™ã‚‹ãŸã‚ã«ã‚³ãƒ¼ãƒ‰ã¨ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span><img src="https://github.com/user-attachments/assets/62fe9563-ed6b-40bf-ad95-067407534626" alt="image" loading="lazy">å…ƒãƒã‚¹ãƒˆ:https://x.com/gm8xx8/status/1948878025757446389?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>



<br>

ç¾åœ¨ã¯v1.5ã‚‚å…¬é–‹ã•ã‚Œã¦ãŠã‚Šã•ã‚‰ã«æ€§èƒ½ãŒå‘ä¸Šã—ã¦ã„ã‚‹æ¨¡æ§˜ï¼ŸHF:https://huggingface.co/inclusionAI/Ming-Lite-Omni</span>
</div>
<p><button onclick="showMore(0)">more</button></p>
<div class="hidden-content">
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/Controllable.html">#Controllable</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2297">[Paper Note] CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning, Kuniaki Saito+, arXiv'25</a>
<span class="snippet"><span>Summary</span>CaptionSmithsã¯ã€ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ãŒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®ç‰¹æ€§ï¼ˆé•·ã•ã€è¨˜è¿°æ€§ã€å˜èªã®ç‹¬è‡ªæ€§ï¼‰ã‚’æŸ”è»Ÿã«åˆ¶å¾¡ã§ãã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã€‚äººé–“ã®æ³¨é‡ˆãªã—ã§ç‰¹æ€§ã‚’å®šé‡åŒ–ã—ã€çŸ­ã„ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã¨é•·ã„ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®é–“ã§è£œé–“ã™ã‚‹ã“ã¨ã§æ¡ä»¶ä»˜ã‘ã‚’å®Ÿç¾ã€‚å®Ÿè¨¼çµæœã§ã¯ã€å‡ºåŠ›ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®ç‰¹æ€§ã‚’ã‚¹ãƒ ãƒ¼ã‚ºã«å¤‰åŒ–ã•ã›ã€èªå½™çš„æ•´åˆæ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã€èª¤å·®ã‚’506%å‰Šæ¸›ã€‚ã‚³ãƒ¼ãƒ‰ã¯GitHubã§å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/a_hasimoto/status/1948258269668970782?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qå¾“æ¥ã¯Discreteã«è¡¨ç¾ã•ã‚Œã¦ã„ãŸcaptioningã«ãŠã‘ã‚‹ç‰¹æ€§ã‚’Condition Caluculatorã‚’å°å…¥ã™ã‚‹ã“ã¨ã§continuousãªrepresentationã«ã‚ˆã£ã¦è¡¨ç¾ã—ã€Caluculatorã«äººé–“ã«ã‚ˆã‚‹input, ã‚ã‚‹ã„ã¯è¡¨ç¾ã—ãŸã„Conditionã‚’æŒã¤exampleã‚’inputã™ã‚‹ã“ã¨ã§ã€ç”Ÿæˆæ™‚ã«åæ˜ ã•ã›ã‚‹ã‚ˆã†ãªæ‰‹æ³•ã‚’ææ¡ˆã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚Conditionã§åˆ©ç”¨ã™ã‚‹propertyã«ã¤ã„ã¦ã¯ã€ææ¡ˆæ‰‹æ³•ã§ã¯Length, Descriptive, Uniqueness of Vocabulariesã®3ã¤ã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹ï¼ˆãŒã€ä»–ã®propertyã§ã‚‚æœ¬æ‰‹æ³•ã¯é©ç”¨å¯èƒ½ã¨æ€ã‚ã‚Œã‚‹ï¼‰ã€‚ã“ã®ã¨ãã€ã‚ã‚‹propertyã®å€¤ã‚’å¤‰ãˆã‚‹ã“ã¨ã§ä»–ã®propertyãŒå¤‰åŒ–ã—ã¦ã—ã¾ã†ã¨åˆ¶å¾¡ãŒã§ããªããªã‚‹ãŸã‚ã€propertyé–“ã®decorrelationã‚’å®Ÿæ–½ã—ã¦ã„ã‚‹ã€‚ã“ã‚Œã¯ã€ã‚ã‚‹property Aã‹ã‚‰åˆ¥ã®property Bã®å€¤ã‚’äºˆæ¸¬ã—ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã®propertyã®å€¤ã‹ã‚‰subtractã™ã‚‹ã€ã¨ã„ã£ãŸå‡¦ç†ã‚’é †æ¬¡propertyã”ã¨ã«å®Ÿæ–½ã™ã‚‹ã“ã¨ã§å®Ÿç¾ã•ã‚Œã‚‹ã€‚Appendixã«è©³ç´°ãŒè¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ã€‚

<br>



<br>

<img src="https://github.com/user-attachments/assets/673a2b9d-d630-4328-b619-f5382bb74f27" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/a90aa9d1-27f1-45c0-819e-c81b93364c68" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/4DReconstruction.html">#4DReconstruction</a>
<span class="issue_date">Issue Date: 2025-07-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2246">[Paper Note] Streaming 4D Visual Geometry Transformer, Dong Zhuo+, arXiv'25</a>
<span class="snippet"><span>Summary</span>å‹•ç”»ã‹ã‚‰4Dç©ºé–“-æ™‚é–“å¹¾ä½•å­¦ã‚’èªè­˜ãƒ»å†æ§‹ç¯‰ã™ã‚‹ãŸã‚ã«ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°4Dãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¸ã‚ªãƒ¡ãƒˆãƒªãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ææ¡ˆã€‚å› æœãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç”¨ã„ã¦ã€éå»ã®æƒ…å ±ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ãªãŒã‚‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§4Då†æ§‹ç¯‰ã‚’å®Ÿç¾ã€‚åŠ¹ç‡çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŸã‚ã«ã€åŒæ–¹å‘ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¸ã‚ªãƒ¡ãƒˆãƒªã‹ã‚‰ã®çŸ¥è­˜è’¸ç•™ã‚’è¡Œã„ã€æ¨è«–é€Ÿåº¦ã‚’å‘ä¸Šã•ã›ã¤ã¤ç«¶äº‰åŠ›ã®ã‚ã‚‹æ€§èƒ½ã‚’ç¶­æŒã€‚ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãª4Dãƒ“ã‚¸ãƒ§ãƒ³ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿç¾ã«å¯„ä¸ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/zhenjun_zhao/status/1945427634642424188?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

<br>

<img src="https://github.com/user-attachments/assets/4aafda63-cbdb-4823-908b-6ef0732f339b" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<a class="button" href="articles/Finetuning.html">#Finetuning</a>
<span class="issue_date">Issue Date: 2025-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2206">[Paper Note] ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision   Transformers under Domain Shifts, Samar Khanna+, ICML'25</a>
<span class="snippet"><span>Summary</span>PEFTæŠ€è¡“ã‚’ç”¨ã„ãŸExPLoRAã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ“ã‚¸ãƒ§ãƒ³ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ï¼ˆViTï¼‰ã‚’æ–°ã—ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ã«é©å¿œã•ã›ã‚‹æ‰‹æ³•ã§ã€æ•™å¸«ãªã—äº‹å‰å­¦ç¿’ã‚’é€šã˜ã¦åŠ¹ç‡çš„ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†ã€‚å®Ÿé¨“ã§ã¯ã€è¡›æ˜Ÿç”»åƒã«ãŠã„ã¦æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã—ã€å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚ˆã‚Šã‚‚å°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ç²¾åº¦ã‚’æœ€å¤§8%å‘ä¸Šã•ã›ãŸã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/samar_a_khanna/status/1944781066591748336?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qã“ã‚Œã¾ã§ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œã™ã‚‹å ´åˆã«ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿+LoRAã§Finetuningã—ã¦ã„ãŸã®ã‚’ã€ãƒ©ãƒ™ãƒ«ç„¡ã—ãƒ‡ãƒ¼ã‚¿+ç¶™ç¶šäº‹å‰å­¦ç¿’ã®æ çµ„ã¿ã§ã‚„ã‚Šã¾ã—ã‚‡ã†ã€ã¨ã„ã†è©±ã®ã‚ˆã†ã§ã‚ã‚‹ã€‚

<br>

<img src="https://github.com/user-attachments/assets/dcae10cf-6b5d-4b29-8d9a-a94227f29a11" alt="image" loading="lazy">

<br>



<br>

æ‰‹æ³•ã¯ä¸‹è¨˜ã§ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦LoRAã‚’é©ç”¨ã—ç¶™ç¶šäº‹å‰å­¦ç¿’ã™ã‚‹ã€‚ãŸã ã—ã€æœ€å¾Œå°¾ã®Layerã€ã‚ã‚‹ã„ã¯æœ€åˆã¨æœ€å¾Œå°¾ã®Layerã®ä¸¡æ–¹ã‚’unfreezeã—ã¦ã€trainableã«ã™ã‚‹ã€‚ã¾ãŸã€LoRAã¯freezeã—ãŸLayerã®Q,Vã«é©ç”¨ã—ã€ãã‚Œã‚‰ã®Layerã®normalization layerã‚‚unfreezeã™ã‚‹ã€‚æœ€çµ‚çš„ã«ã€ç¶™ç¶šäº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã«ãƒ˜ãƒƒãƒ‰ã‚’concatã—ã¦finetuningã™ã‚‹ã“ã¨ã§ç›®çš„ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚

<br>



<br>

<img src="https://github.com/user-attachments/assets/6b7ef497-2253-46c9-bbe7-ffdd50765fa3" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/84596039-0a10-4556-896b-1fee164a153b" alt="image" loading="lazy">

<br>



<br>

åŒã˜ãƒ¢ãƒ‡ãƒ«ã§å˜ã«LoRAã‚’é©ç”¨ã—ãŸã ã‘ã®æ‰‹æ³•ã‚„ã€æ—¢å­˜æ‰‹æ³•ã‚’outperform

<br>



<br>

<img width="679" height="364" alt="Image" src="https://github.com/user-attachments/assets/14935879-75a4-4e4a-a176-1b1eabc4b8fd">ç”»åƒ+ViTç³»ã®ãƒ¢ãƒ‡ãƒ«ã ã‘ã§å®Ÿé¨“ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã‚‹ãŒã€LLMã¨ã‹ã«ã‚‚å¿œç”¨å¯èƒ½ã ã¨æ€ã‚ã‚Œã‚‹ã€‚

<br>

</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2205">[Paper Note] VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain  Knowledge, Yueqi Song+, arXiv'25</a>
<span class="snippet"><span>Summary</span>VisualPuzzlesã¯ã€å°‚é–€çŸ¥è­˜ã¸ã®ä¾å­˜ã‚’æœ€å°é™ã«æŠ‘ãˆãŸè¦–è¦šçš„æ¨è«–ã‚’è©•ä¾¡ã™ã‚‹æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã€5ã¤ã®æ¨è«–ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰æˆã‚‹å¤šæ§˜ãªè³ªå•ã‚’å«ã‚€ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€VisualPuzzlesã¯ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹æœ‰ã®çŸ¥è­˜ã‚’å¤§å¹…ã«æ¸›å°‘ã•ã›ã€ã‚ˆã‚Šè¤‡é›‘ãªæ¨è«–ã‚’è¦æ±‚ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚æœ€å…ˆç«¯ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã¯ã€VisualPuzzlesã§äººé–“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«é…ã‚Œã‚’ã¨ã‚Šã€çŸ¥è­˜é›†ç´„å‹ã‚¿ã‚¹ã‚¯ã§ã®æˆåŠŸãŒæ¨è«–ã‚¿ã‚¹ã‚¯ã§ã®æˆåŠŸã«å¿…ãšã—ã‚‚ã¤ãªãŒã‚‰ãªã„ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚ã¾ãŸã€ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®é–“ã«æ˜ç¢ºãªç›¸é–¢ã¯è¦‹ã‚‰ã‚Œãšã€VisualPuzzlesã¯äº‹å®Ÿã®è¨˜æ†¶ã‚’è¶…ãˆãŸæ¨è«–èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹æ–°ãŸãªè¦–ç‚¹ã‚’æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/yueqi_song/status/1912510869491101732?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qç”»åƒã¯PJãƒšãƒ¼ã‚¸ã‚ˆã‚Šå¼•ç”¨ã€‚æ–°ãŸã«Visual Puzzleã¨å‘¼ã°ã‚Œã‚‹ç‰¹å®šã®ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ãŒã»ã¨ã‚“ã©å¿…è¦ãªã„ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªreasoningãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã€‚o1ã§ã™ã‚‰ã€äººé–“ã®5th percentileã«æº€ãŸãªã„æ€§èƒ½ã¨ã®ã“ã¨ã€‚

<br>



<br>

Chinese Civil Service Examinationä¸­ã®logical reasoning questionã‚’æ‰‹ä½œæ¥­ã§ç¿»è¨³ã—ãŸã¨ã®ã“ã¨ã€‚

<br>



<br>

<img src="https://github.com/user-attachments/assets/4ee1cd31-2d47-46a2-861b-2a72c5df8529" alt="image" loading="lazy">

<br>



<br>

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆé‡ã¯ä»¥ä¸‹ã§ã€åˆè¨ˆ1168å•ã§ã€é›£æ˜“åº¦ã¯3æ®µéšã«åˆ†ã‹ã‚Œã¦ã„ã‚‹æ¨¡æ§˜ã€‚

<br>

<img src="https://github.com/user-attachments/assets/332246e3-075f-4d98-b528-c8e4ec865068" alt="image" loading="lazy">

<br>



<br>

project page:https://neulab.github.io/VisualPuzzles/</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2200">[Paper Note] Kimi-VL Technical Report, Kimi Team+, arXiv'25</a>
<span class="snippet"><span>Summary</span>Kimi-VLã¯ã€åŠ¹ç‡çš„ãªã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®Mixture-of-Expertsãƒ“ã‚¸ãƒ§ãƒ³ãƒ»ãƒ©ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€2.8Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨€èªãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’æ´»æ€§åŒ–ã—ã¦é«˜åº¦ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–ã‚’å®Ÿç¾ã€‚ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¿ã‚¹ã‚¯ã‚„å¤§å­¦ãƒ¬ãƒ™ãƒ«ã®ç”»åƒãƒ»å‹•ç”»ç†è§£ã«ãŠã„ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ã€æœ€å…ˆç«¯ã®VLMã¨ç«¶äº‰ã€‚128Kã®æ‹¡å¼µã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’æŒã¡ã€é•·ã„å…¥åŠ›ã‚’å‡¦ç†å¯èƒ½ã€‚Kimi-VL-Thinking-2506ã¯ã€é•·æœŸçš„æ¨è«–èƒ½åŠ›ã‚’å¼·åŒ–ã™ã‚‹ãŸã‚ã«æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨å¼·åŒ–å­¦ç¿’ã‚’ç”¨ã„ã¦é–‹ç™ºã•ã‚Œã€å …ç‰¢ãªä¸€èˆ¬èƒ½åŠ›ã‚’ç²å¾—ã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span>ãƒ»2201 

<br>

ã§ã®æ€§èƒ½ï¼ˆVision+ãƒ†ã‚­ã‚¹ãƒˆã®æ•°å­¦ã®å•é¡Œï¼‰ã€‚ä»–ã®å·¨å¤§ãªãƒ¢ãƒ‡ãƒ«ã¨æ¯”ã¹2.8Bã®Activation paramsã§é«˜ã„æ€§èƒ½ã‚’é”æˆ

<br>



<br>

<img width="831" height="431" alt="Image" src="https://github.com/user-attachments/assets/3ec08621-f269-4f1d-97bb-3ebca537f2ea">

<br>



<br>

ãã®ä»–ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã‚‚é«˜ã„æ€§èƒ½ã‚’ç²å¾—

<br>



<br>

<img width="833" height="558" alt="Image" src="https://github.com/user-attachments/assets/b30afc4f-efce-4206-b499-f4f089d97226">

<br>



<br>

ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚MoonViT (Image Encoder, 1Dã®patchã‚’input, æ§˜ã€…ãªè§£åƒåº¦ã®ã‚µãƒãƒ¼ãƒˆ, FlashAttention,  SigLIP-SO-400Mã‚’ç¶™ç¶šäº‹å‰å­¦ç¿’, RoPEã‚’æ¡ç”¨) + Linear Projector + MoE Language Decoderã®æ§‹æˆ

<br>

<img width="851" height="590" alt="Image" src="https://github.com/user-attachments/assets/f59d7655-c1c7-4284-b79c-9d62739da889">

<br>



<br>

å­¦ç¿’ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚ViTã®äº‹å‰å­¦ç¿’ã§ã¯SigLIP loss (contrastive lossã®äºœç¨®)ã¨captionç”Ÿæˆã®cross-entropy lossã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã€‚joint cooldown stageã«ãŠã„ã¦ã¯ã€é«˜å“è³ªãªQAãƒ‡ãƒ¼ã‚¿ã‚’åˆæˆã™ã‚‹ã“ã¨ã§å®Ÿé¨“çš„ã«å¤§å¹…ã«æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ãŸã®ã§ã€ãã‚Œã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã¨ã®ã“ã¨ã€‚optimizerã¯ 

<br>

ãƒ»2202

<br>



<br>

<img width="849" height="213" alt="Image" src="https://github.com/user-attachments/assets/720b02f7-a260-497f-85c5-04cf382c2f98">

<br>



<br>

<img width="828" height="402" alt="Image" src="https://github.com/user-attachments/assets/bb78d799-5db4-4904-8669-540d2142c95c">

<br>



<br>

post-trainingã«ãŠã‘ã‚‹RLã§ã¯ä»¥ä¸‹ã®ç›®çš„é–¢æ•°ã‚’ç”¨ã„ã¦ãŠã‚Šã€RLVRã‚’ç”¨ã„ã¤ã¤ã€ç¾åœ¨ã®ãƒãƒªã‚·ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’referenceã¨ã—æ›´æ–°ã‚’ã™ã‚‹ã‚ˆã†ãªç›®çš„é–¢æ•°ã«ãªã£ã¦ã„ã‚‹ã€‚curriculum sampling, prioritize samplingã‚’difficulty labelã«åŸºã¥ã„ã¦å®Ÿæ–½ã—ã¦ã„ã‚‹ã€‚

<br>

<img width="842" height="152" alt="Image" src="https://github.com/user-attachments/assets/298fdef8-9807-4511-96f6-02241393ab9f">

<br>



<br>

<img width="822" height="187" alt="Image" src="https://github.com/user-attachments/assets/4ad0d815-ef1c-4945-ae08-ab2b072ec63f"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<a class="button" href="articles/On-Policy.html">#On-Policy</a>
<a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2191">[Paper Note] Perception-Aware Policy Optimization for Multimodal Reasoning, Zhenhailong Wang+, arXiv'25</a>
<span class="snippet"><span>Summary</span>å¼·åŒ–å­¦ç¿’ã«ãŠã‘ã‚‹æ¤œè¨¼å¯èƒ½ãªå ±é…¬ï¼ˆRLVRï¼‰ã¯ã€LLMsã«å¤šæ®µéšæ¨è«–èƒ½åŠ›ã‚’ä¸ãˆã‚‹ãŒã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–ã§ã¯æœ€é©ãªæ€§èƒ½ã‚’ç™ºæ®ã§ããªã„ã€‚è¦–è¦šå…¥åŠ›ã®èªè­˜ãŒä¸»ãªã‚¨ãƒ©ãƒ¼åŸå› ã§ã‚ã‚‹ãŸã‚ã€çŸ¥è¦šã‚’æ„è­˜ã—ãŸãƒãƒªã‚·ãƒ¼æœ€é©åŒ–ï¼ˆPAPOï¼‰ã‚’ææ¡ˆã€‚PAPOã¯GRPOã®æ‹¡å¼µã§ã€å†…éƒ¨ç›£è¦–ä¿¡å·ã‹ã‚‰å­¦ç¿’ã—ã€è¿½åŠ ã®ãƒ‡ãƒ¼ã‚¿ã‚„å¤–éƒ¨å ±é…¬ã«ä¾å­˜ã—ãªã„ã€‚KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹é …ã‚’å°å…¥ã—ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§4.4%ã®æ”¹å–„ã€è¦–è¦šä¾å­˜ã‚¿ã‚¹ã‚¯ã§ã¯8.0%ã®æ”¹å–„ã‚’é”æˆã€‚çŸ¥è¦šã‚¨ãƒ©ãƒ¼ã‚‚30.5%æ¸›å°‘ã—ã€PAPOã®åŠ¹æœã‚’ç¤ºã™ã€‚ç ”ç©¶ã¯è¦–è¦šã«åŸºã¥ãæ¨è«–ã‚’ä¿ƒé€²ã™ã‚‹æ–°ã—ã„RLãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®åŸºç›¤ã‚’ç¯‰ãã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/aicia_solid/status/1943507735489974596?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QVLMã«ãŠã„ã¦ã€ç”»åƒã‚’ãƒã‚¹ã‚¯ã—ãŸå ´åˆã®ãƒãƒªã‚·ãƒ¼ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã¨ã€ç”»åƒã‚’ãƒã‚¹ã‚¯ã—ãªã„å ´åˆã®ãƒãƒªã‚·ãƒ¼ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã®KL Divergenceã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã§ã€ç”»åƒã®èªçŸ¥èƒ½åŠ›ãŒå‘ä¸Šã—æ€§èƒ½å‘ä¸Šã™ã‚‹ã‚ˆã€ã¿ãŸã„ãªè©±ãªæ¨¡æ§˜ã€‚

<br>

<img src="https://github.com/user-attachments/assets/d7844321-d979-497f-84da-5d69fd13233f" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/afe8919c-ea16-48a1-b33b-79b7a3b1ccb0" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/04a3d23c-2eb0-40e2-aa2c-363498976320" alt="image" loading="lazy"></span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<a class="button" href="articles/VideoGeneration/Understandings.html">#VideoGeneration/Understandings</a>
<a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2146">[Paper Note] Energy-Based Transformers are Scalable Learners and Thinkers, Alexi Gladstone+, arXiv'25</a>
<span class="snippet"><span>Summary</span>ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ï¼ˆEBTsï¼‰ã‚’ç”¨ã„ã¦ã€ç„¡ç›£ç£å­¦ç¿’ã‹ã‚‰æ€è€ƒã‚’å­¦ã¶ãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã€‚EBTsã¯ã€å…¥åŠ›ã¨å€™è£œäºˆæ¸¬ã®äº’æ›æ€§ã‚’æ¤œè¨¼ã—ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ–ã‚’é€šã˜ã¦äºˆæ¸¬ã‚’è¡Œã†ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚ˆã‚Šã‚‚é«˜ã„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç‡ã‚’é”æˆã—ã€è¨€èªã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’29%å‘ä¸Šã•ã›ã€ç”»åƒã®ãƒã‚¤ã‚ºé™¤å»ã§ã‚‚å„ªã‚ŒãŸçµæœã‚’ç¤ºã™ã€‚EBTsã¯ä¸€èˆ¬åŒ–èƒ½åŠ›ãŒé«˜ãã€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’èƒ½åŠ›ã¨æ€è€ƒèƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§ã‚ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/hillbig/status/1941657099567845696?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QProject Page:https://energy-based-transformers.github.ioFirst Authorã®æ–¹ã«ã‚ˆã‚‹è§£èª¬ãƒã‚¹ãƒˆ:https://x.com/alexiglad/status/1942231878305714462?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a>
<a class="button" href="articles/2D.html">#2D</a>
<a class="button" href="articles/3D.html">#3D</a>
<a class="button" href="articles/FeatureMatching.html">#FeatureMatching</a>
<span class="issue_date">Issue Date: 2025-07-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2132">[Paper Note] Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space, Yingping Liang+, arXiv'25</a>
<span class="snippet"><span>Summary</span>æ–°ã—ã„äºŒæ®µéšãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€ŒLift to Match (L2M)ã€ã‚’ææ¡ˆã—ã€2Dç”»åƒã‚’3Dç©ºé–“ã«æŒã¡ä¸Šã’ã‚‹ã“ã¨ã§ã€ç‰¹å¾´ãƒãƒƒãƒãƒ³ã‚°ã®ä¸€èˆ¬åŒ–ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ç¬¬ä¸€æ®µéšã§3Dç‰¹å¾´ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’å­¦ç¿’ã—ã€ç¬¬äºŒæ®µéšã§ç‰¹å¾´ãƒ‡ã‚³ãƒ¼ãƒ€ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€å …ç‰¢ãªç‰¹å¾´ãƒãƒƒãƒãƒ³ã‚°ã‚’å®Ÿç¾ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆè©•ä¾¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å„ªã‚ŒãŸä¸€èˆ¬åŒ–æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/zhenjun_zhao/status/1940399755827270081?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/RLHF.html">#RLHF</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/mid-training.html">#mid-training</a>
<a class="button" href="articles/RewardHacking.html">#RewardHacking</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<a class="button" href="articles/CurriculumLearning.html">#CurriculumLearning</a>
<a class="button" href="articles/RLVR.html">#RLVR</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2128">[Paper Note] GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable  Reinforcement Learning, GLM-V Team+, arXiv'25</a>
<span class="snippet"><span>Summary</span>è¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«GLM-4.1V-Thinkingã‚’ç™ºè¡¨ã—ã€æ¨è«–ä¸­å¿ƒã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’é–‹ç™ºã€‚å¼·åŠ›ãªè¦–è¦šåŸºç›¤ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã€ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ç”¨ã„ãŸå¼·åŒ–å­¦ç¿’ã§å¤šæ§˜ãªã‚¿ã‚¹ã‚¯ã®èƒ½åŠ›ã‚’å‘ä¸Šã€‚28ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã€ç‰¹ã«é›£ã—ã„ã‚¿ã‚¹ã‚¯ã§ç«¶äº‰åŠ›ã®ã‚ã‚‹çµæœã‚’ç¤ºã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã¨ã—ã¦å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/sinclairwang1/status/1940331927724232712?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QQwen2.5-VLã‚ˆã‚Šã‚‚æ€§èƒ½ãŒè‰¯ã„VLM

<br>

<img src="https://github.com/user-attachments/assets/1215d0cf-3776-4631-a5d5-2c514e7d5a2e" alt="image" loading="lazy">ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã“ã¡ã‚‰ã€‚ãŒã€pretraining(ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°, ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«â†’long contextç¶™ç¶šäº‹å‰å­¦ç¿’)-&gt;SFT(cold startã¸ã®å¯¾å‡¦, reasoningèƒ½åŠ›ã®ç²å¾—)-&gt;RL(RLVRã¨RLHFã®ä½µç”¨ã«ã‚ˆã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã¨Alignment, RewardHackingã¸ã®å¯¾å‡¦,curriculum sampling)ãªã©ã€å…¨ä½“ã®å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ç´°ã‹ã„ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã®ç©ã¿é‡ã­ã§é«˜ã„æ€§èƒ½ãŒç²å¾—ã•ã‚Œã¦ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€‚

<br>

<img src="https://github.com/user-attachments/assets/a692b5de-5f4e-42c6-938e-3718dd2fc0e6" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a>
<a class="button" href="articles/Findings.html">#Findings</a>
<span class="issue_date">Issue Date: 2025-07-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2125">[Paper Note] Do Vision-Language Models Have Internal World Models? Towards an Atomic   Evaluation, Qiyue Gao+, ACLï¼ˆFindingsï¼‰'25</a>
<span class="snippet"><span>Summary</span>å†…éƒ¨ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ï¼ˆWMsï¼‰ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç†è§£ã¨äºˆæ¸¬ã‚’æ”¯ãˆã‚‹ãŒã€æœ€è¿‘ã®å¤§è¦æ¨¡ãƒ“ã‚¸ãƒ§ãƒ³ãƒ»ãƒ©ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ï¼ˆVLMsï¼‰ã®åŸºæœ¬çš„ãªWMèƒ½åŠ›ã«é–¢ã™ã‚‹è©•ä¾¡ã¯ä¸è¶³ã—ã¦ã„ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€çŸ¥è¦šã¨äºˆæ¸¬ã‚’è©•ä¾¡ã™ã‚‹äºŒæ®µéšã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€WM-ABenchã¨ã„ã†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å°å…¥ã€‚15ã®VLMsã«å¯¾ã™ã‚‹660ã®å®Ÿé¨“ã§ã€ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ãŒåŸºæœ¬çš„ãªWMèƒ½åŠ›ã«é¡•è‘—ãªåˆ¶é™ã‚’ç¤ºã—ã€ç‰¹ã«é‹å‹•è»Œé“ã®è­˜åˆ¥ã«ãŠã„ã¦ã»ã¼ãƒ©ãƒ³ãƒ€ãƒ ãªç²¾åº¦ã§ã‚ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚VLMsã¨äººé–“ã®WMã¨ã®é–“ã«ã¯é‡è¦ãªã‚®ãƒ£ãƒƒãƒ—ãŒå­˜åœ¨ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/qiyuegao123/status/1940097188220297613?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<span class="issue_date">Issue Date: 2025-07-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2122">[Paper Note] MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning, Yulun Jiang+, arXiv'25</a>
<span class="snippet"><span>Summary</span>MARBLEã¨ã„ã†æ–°ã—ã„ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€MLLMsã®è¤‡é›‘ãªæ¨è«–èƒ½åŠ›ã‚’è©•ä¾¡ã€‚MARBLEã¯ã€ç©ºé–“çš„ãƒ»è¦–è¦šçš„ãƒ»ç‰©ç†çš„åˆ¶ç´„ä¸‹ã§ã®å¤šæ®µéšè¨ˆç”»ã‚’å¿…è¦ã¨ã™ã‚‹M-Portalã¨M-Cubeã®2ã¤ã®ã‚¿ã‚¹ã‚¯ã‹ã‚‰æˆã‚‹ã€‚ç¾åœ¨ã®MLLMsã¯ä½ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã€è¦–è¦šçš„å…¥åŠ›ã‹ã‚‰ã®æƒ…å ±æŠ½å‡ºã«ãŠã„ã¦ã‚‚å¤±æ•—ãŒè¦‹ã‚‰ã‚Œã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ¬¡ä¸–ä»£ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–èƒ½åŠ›å‘ä¸ŠãŒæœŸå¾…ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/michael_d_moor/status/1940062842742526445?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QPortal2ã‚’ä½¿ã£ãŸæ–°ãŸãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€‚ç­†è€…ã¯æ˜”ã“ã®ã‚²ãƒ¼ãƒ ã‚’å°‘ã—ã ã‘ãƒ—ãƒ¬ã‚¤ã—ãŸã“ã¨ãŒã‚ã‚‹ãŒã€æ™®é€šã«é›£ã—ã‹ã£ãŸè¨˜æ†¶ãŒã‚ã‚‹ğŸ˜…

<br>



<br>

ç´°ã‹ã„ãŒè¡¨ä¸­ã®GPT-o3ã¯æ­£ã—ãã¯o3ã ã¨æ€ã‚ã‚Œã‚‹ã€‚

<br>

æ™‚é–“ãŒãªãã¦å…¨ç„¶ã—ã£ã‹ã‚Šã¨èª­ã‚ã¦ã„ãªã„ãŒã€reasoning effortã‚„thinkingãƒ¢ãƒ¼ãƒ‰ã¯ã©ã®ã‚ˆã†ã«è¨­å®šã—ã¦è©•ä¾¡ã—ãŸã®ã ã‚ã†ã‹ã€‚

<br>

<img src="https://github.com/user-attachments/assets/a7647007-b718-4b1c-8d8a-396c36d7811d" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/4b996864-7bf8-4ea9-aa3e-84d4e9f3f5d2" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Zero/FewShotPrompting.html">#Zero/FewShotPrompting</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2025-07-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2121">[Paper Note] SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context  Learning, Melanie Rieff+, arXiv'25</a>
<span class="snippet"><span>Summary</span>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ï¼ˆICLï¼‰ã¯åŒ»ç™‚åˆ†é‡ã§ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŒã€ååˆ†ã«æ¢æ±‚ã•ã‚Œã¦ã„ãªã„ã€‚SMMILEã¨ã„ã†åŒ»ç™‚ã‚¿ã‚¹ã‚¯å‘ã‘ã®åˆã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ICLãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å°å…¥ã—ã€111ã®å•é¡Œã‚’å«ã‚€ã€‚15ã®MLLMã®è©•ä¾¡ã§ã€åŒ»ç™‚ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹ICLèƒ½åŠ›ãŒä¸­ç¨‹åº¦ã‹ã‚‰ä½ã„ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ICLã¯SMMILEã§å¹³å‡8%ã€SMMILE++ã§9.4%ã®æ”¹å–„ã‚’ã‚‚ãŸã‚‰ã—ã€ç„¡é–¢ä¿‚ãªä¾‹ãŒãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€å¤§9.5%ä½ä¸‹ã•ã›ã‚‹ã“ã¨ã‚‚ç¢ºèªã€‚ä¾‹ã®é †åºã«ã‚ˆã‚‹æœ€è¿‘æ€§ãƒã‚¤ã‚¢ã‚¹ãŒãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã«å¯„ä¸ã™ã‚‹ã“ã¨ã‚‚æ˜ã‚‰ã‹ã«ãªã£ãŸã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/michael_d_moor/status/1939664155813839114?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<span class="issue_date">Issue Date: 2025-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2105">[Paper Note] OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision  Encoders for Multimodal Learning, Xianhang Li+, arXiv'25</a>
<span class="snippet"><span>Summary</span>OpenVisionã¯ã€å®Œå…¨ã«ã‚ªãƒ¼ãƒ—ãƒ³ã§ã‚³ã‚¹ãƒˆåŠ¹æœã®é«˜ã„ãƒ“ã‚¸ãƒ§ãƒ³ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’ææ¡ˆã—ã€CLIPã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚æ—¢å­˜ã®ç ”ç©¶ã‚’åŸºã«æ§‹ç¯‰ã•ã‚Œã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®é€²å±•ã«å®Ÿç”¨çš„ãªåˆ©ç‚¹ã‚’ç¤ºã—ã¾ã™ã€‚5.9Mã‹ã‚‰632.1Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’æä¾›ã—ã€å®¹é‡ã¨åŠ¹ç‡ã®æŸ”è»Ÿãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å®Ÿç¾ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/cihangxie/status/1920575141849030882?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/pretrained-LM.html">#pretrained-LM</a>
<a class="button" href="articles/Scaling%20Laws.html">#Scaling Laws</a>
<a class="button" href="articles/TMLR.html">#TMLR</a>
<span class="issue_date">Issue Date: 2025-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2100">[Paper Note] An Empirical Study of Pre-trained Model Selection for   Out-of-Distribution Generalization and Calibration, Hiroki Naganuma+, TMLR'25</a>
<span class="snippet"><span>Summary</span>äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒåˆ†å¸ƒå¤–ä¸€èˆ¬åŒ–ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦é‡è¦ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã€ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®é¸æŠãŒOODç²¾åº¦ã¨ä¿¡é ¼æ€§ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¿æŸ»ã€‚120,000æ™‚é–“ä»¥ä¸Šã®å®Ÿé¨“ã‚’é€šã˜ã¦ã€å¤§ããªãƒ¢ãƒ‡ãƒ«ã¨å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒOODãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ”¹å–„ã™ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã€‚ã“ã‚Œã¯ã€å¾“æ¥ã®ç ”ç©¶ã¨å¯¾ç…§çš„ã§ã‚ã‚Šã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®é¸æŠã®é‡è¦æ€§ã‚’å¼·èª¿ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span>OpenReview:https://openreview.net/forum?id=tYjoHjShxFå…ƒãƒã‚¹ãƒˆ:https://x.com/_hiroki11x/status/1938052113466323134?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/VideoGeneration/Understandings.html">#VideoGeneration/Understandings</a>
<a class="button" href="articles/ICCV.html">#ICCV</a>
<span class="issue_date">Issue Date: 2025-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2099">[Paper Note] Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers, Weiming Ren+, arXiv'25</a>
<span class="snippet"><span>Summary</span>VAMBAãƒ¢ãƒ‡ãƒ«ã¯ã€Mamba-2ãƒ–ãƒ­ãƒƒã‚¯ã‚’ç”¨ã„ã¦ãƒ“ãƒ‡ã‚ªãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç·šå½¢ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›ãªã—ã§1024ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†å¯èƒ½ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’50%å‰Šæ¸›ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã‚’å€å¢—ã€‚1æ™‚é–“ã®ãƒ“ãƒ‡ã‚ªç†è§£ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯LVBenchã§4.3%ã®ç²¾åº¦å‘ä¸Šã‚’é”æˆã—ã€æ§˜ã€…ãªãƒ“ãƒ‡ã‚ªç†è§£ã‚¿ã‚¹ã‚¯ã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/wenhuchen/status/1938064510369280136?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/Tokenizer.html">#Tokenizer</a>
<span class="issue_date">Issue Date: 2025-06-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2082">[Paper Note] Vision as a Dialect: Unifying Visual Understanding and Generation via  Text-Aligned Representations, Jiaming Han+, arXiv'25</a>
<span class="snippet"><span>Summary</span>æœ¬è«–æ–‡ã§ã¯ã€è¦–è¦šç†è§£ã¨ç”Ÿæˆã‚’çµ±ä¸€ã™ã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯Tarã‚’ææ¡ˆã€‚Text-Aligned Tokenizerï¼ˆTA-Tokï¼‰ã‚’ç”¨ã„ã¦ç”»åƒã‚’é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›ã—ã€è¦–è¦šã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ±ä¸€ç©ºé–“ã«çµ±åˆã€‚ã‚¹ã‚±ãƒ¼ãƒ«é©å¿œå‹ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å°å…¥ã—ã€é«˜å¿ å®Ÿåº¦ã®è¦–è¦šå‡ºåŠ›ã‚’ç”Ÿæˆã€‚è¿…é€Ÿãªè‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã¨æ‹¡æ•£ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸãƒ‡ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æ´»ç”¨ã—ã€è¦–è¦šç†è§£ã¨ç”Ÿæˆã®æ”¹å–„ã‚’å®Ÿç¾ã€‚å®Ÿé¨“çµæœã§ã¯ã€TarãŒæ—¢å­˜æ‰‹æ³•ã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’ç¤ºã—ã€åŠ¹ç‡çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é”æˆã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/_akhaliq/status/1937345768223859139?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qtext modalityã¨vision modalityã‚’å…±é€šã®ç©ºé–“ã§è¡¨ç¾ã™ã‚‹

<br>

<img src="https://github.com/user-attachments/assets/356e86e1-cad9-4bee-8398-d68c4fc6ad46" alt="image" loading="lazy">Visual Understanding/Generationã®ãƒ™ãƒ³ãƒã§å…¨ä½“çš„ã«é«˜ã„æ€§èƒ½ã‚’é”æˆ

<br>

<img src="https://github.com/user-attachments/assets/6e45aec0-ae0b-4327-923f-fdfce8e83ca0" alt="image" loading="lazy"></span>
<a class="button" href="articles/Embeddings.html">#Embeddings</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/RepresentationLearning.html">#RepresentationLearning</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<span class="issue_date">Issue Date: 2025-06-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2079">[Paper Note] jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual  Retrieval, Michael GÃ¼nther+, arXiv'25</a>
<span class="snippet"><span>Summary</span>3.8å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã€Œjina-embeddings-v4ã€ã‚’ææ¡ˆã€‚æ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚Šã€ã‚¯ã‚¨ãƒªãƒ™ãƒ¼ã‚¹ã®æƒ…å ±æ¤œç´¢ã‚„ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«ã®é¡ä¼¼æ€§æ¤œç´¢ã‚’æœ€é©åŒ–ã€‚ã‚¿ã‚¹ã‚¯ç‰¹åŒ–å‹ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’çµ„ã¿è¾¼ã¿ã€è¦–è¦šçš„ã«è±Šã‹ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å‡¦ç†ã«å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã€‚æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ŒJina-VDRã€ã‚‚å°å…¥ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/arankomatsuzaki/status/1937342962075378014?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/VideoGeneration/Understandings.html">#VideoGeneration/Understandings</a>
<span class="issue_date">Issue Date: 2025-06-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2074">[Paper Note] Sekai: A Video Dataset towards World Exploration, Zhen Li+, arXiv'25</a>
<span class="snippet"><span>Summary</span>é«˜å“è³ªãªä¸€äººç§°è¦–ç‚¹ã®ãƒ“ãƒ‡ã‚ªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ŒSekaiã€ã‚’ç´¹ä»‹ã€‚750ã®éƒ½å¸‚ã‹ã‚‰5,000æ™‚é–“ä»¥ä¸Šã®ãƒ“ãƒ‡ã‚ªã‚’åé›†ã—ã€ä½ç½®ã‚„ã‚·ãƒ¼ãƒ³ãªã©ã®è±Šå¯Œãªæ³¨é‡ˆã‚’ä»˜ä¸ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ“ãƒ‡ã‚ªä¸–ç•Œæ¢æŸ»ãƒ¢ãƒ‡ãƒ«ã€ŒYUMEã€ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€‚Sekaiã¯ãƒ“ãƒ‡ã‚ªç”Ÿæˆã¨ä¸–ç•Œæ¢æŸ»ã«è²¢çŒ®ã™ã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/yongyuanxi/status/1936846469346251068?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/CVPR.html">#CVPR</a>
<a class="button" href="articles/3D%20Reconstruction.html">#3D Reconstruction</a>
<span class="issue_date">Issue Date: 2025-06-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2068">[Paper Note] VGGT: Visual Geometry Grounded Transformer, Jianyuan Wang+, CVPR'25</a>
<span class="snippet"><span>Summary</span>VGGTã¯ã€ã‚·ãƒ¼ãƒ³ã®ä¸»è¦ãª3Då±æ€§ã‚’è¤‡æ•°ã®ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰ç›´æ¥æ¨æ¸¬ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚Šã€3Dã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®åˆ†é‡ã«ãŠã„ã¦æ–°ãŸãªé€²å±•ã‚’ç¤ºã—ã¾ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯åŠ¹ç‡çš„ã§ã€1ç§’æœªæº€ã§ç”»åƒã‚’å†æ§‹ç¯‰ã—ã€è¤‡æ•°ã®3Dã‚¿ã‚¹ã‚¯ã§æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã—ã¾ã™ã€‚ã¾ãŸã€VGGTã‚’ç‰¹å¾´ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ä¸‹æµã‚¿ã‚¹ã‚¯ã®æ€§èƒ½ãŒå¤§å¹…ã«å‘ä¸Šã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/hillbig/status/1936711294956265820?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a>
<a class="button" href="articles/VideoGeneration/Understandings.html">#VideoGeneration/Understandings</a>
<span class="issue_date">Issue Date: 2025-06-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2037">[Paper Note] Seedance 1.0: Exploring the Boundaries of Video Generation Models, Yu Gao+, arXiv'25</a>
<span class="snippet"><span>Summary</span>Seedance 1.0ã¯ã€å‹•ç”»ç”Ÿæˆã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéµå®ˆã€å‹•ãã®å¦¥å½“æ€§ã€è¦–è¦šçš„å“è³ªã‚’åŒæ™‚ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚ä¸»ãªæŠ€è¡“æ”¹å–„ã¨ã—ã¦ã€æ„å‘³ã®ã‚ã‚‹å‹•ç”»ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ç”¨ã„ãŸãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒãƒ«ãƒã‚·ãƒ§ãƒƒãƒˆç”Ÿæˆã®ã‚µãƒãƒ¼ãƒˆã€å‹•ç”»ç‰¹æœ‰ã®RLHFã‚’æ´»ç”¨ã—ãŸãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€æ¨è«–é€Ÿåº¦ã®ç´„10å€å‘ä¸Šã‚’å®Ÿç¾ã™ã‚‹è’¸ç•™æˆ¦ç•¥ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚Seedance 1.0ã¯ã€1080pè§£åƒåº¦ã®5ç§’é–“ã®å‹•ç”»ã‚’41.4ç§’ã§ç”Ÿæˆã—ã€é«˜å“è³ªã‹ã¤è¿…é€Ÿãªå‹•ç”»ç”Ÿæˆã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/scaling01/status/1933048431775527006?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a>
<a class="button" href="articles/CVPR.html">#CVPR</a>
<span class="issue_date">Issue Date: 2025-06-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2021">[Paper Note] Generative Omnimatte: Learning to Decompose Video into Layers, Yao-Chih Lee+, CVPR'25</a>
<span class="snippet"><span>Summary</span>ã‚ªãƒ ãƒ‹ãƒãƒƒãƒˆæ‰‹æ³•ã¯ã€ãƒ“ãƒ‡ã‚ªã‚’æ„å‘³çš„ã«æœ‰æ„ç¾©ãªå±¤ã«åˆ†è§£ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ãŒã€æ—¢å­˜æ‰‹æ³•ã¯é™çš„èƒŒæ™¯ã‚„æ­£ç¢ºãªãƒãƒ¼ã‚ºã‚’å‰æã¨ã—ã¦ãŠã‚Šã€ã“ã‚ŒãŒç ´ã‚‰ã‚Œã‚‹ã¨æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹ã€‚æ–°ãŸã«ææ¡ˆã™ã‚‹ç”Ÿæˆçš„å±¤çŠ¶ãƒ“ãƒ‡ã‚ªåˆ†è§£ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€é™æ­¢ã‚·ãƒ¼ãƒ³ã‚„æ·±åº¦æƒ…å ±ã‚’å¿…è¦ã¨ã›ãšã€å‹•çš„é ˜åŸŸã®è£œå®Œã‚’è¡Œã†ã€‚æ ¸å¿ƒçš„ãªã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã€ãƒ“ãƒ‡ã‚ªæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€ã‚·ãƒ¼ãƒ³åŠ¹æœã‚’ç‰¹å®šãƒ»é™¤å»ã™ã‚‹ã“ã¨ã§ã‚ã‚Šã€ã“ã‚Œã«ã‚ˆã‚Šé«˜å“è³ªãªåˆ†è§£ã¨ç·¨é›†çµæœã‚’å®Ÿç¾ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/yaochihlee/status/1930473521081397253?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qã–ã£ãã‚Šã—ã‹èª­ã‚ã¦ã„ãªã„ãŒã€Inputã¨ã—ã¦å‹•ç”»ã¨maskï¼ˆç™½:æ®‹ã™, é»’:æ¶ˆã™, ã‚°ãƒ¬ãƒ¼: ä¸ç¢ºå®šãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚„ã‚¨ãƒ•ã‚§ã‚¯ãƒˆãŒå«ã¾ã‚Œã‚‹ã‚¨ãƒªã‚¢â‰’èƒŒæ™¯ï¼Ÿ)ã‚’å—ã‘å–ã‚Šã€Casperã¨å‘¼ã°ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¶ˆã—æ¶ˆã—ãŸéƒ¨åˆ†ã‚’inpaintingã™ã‚‹ã“ã¨ã§ã€layerã£ã½ã„ã‚‚ã®ã‚’ä½œæˆã™ã‚‹ã£ã½ã„ï¼ŸCasperã¯&lt;Inputç”»åƒ, maskã€maskã‹ã‚‰ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å‰Šé™¤ã—ãŸç”»åƒï¼ˆå‰Šé™¤ã—ãŸéƒ¨åˆ†ã‚‚ãã¡ã‚“ã¨èƒŒæ™¯ãŒã‚ã‚‹ï¼‰&gt;ã®3çµ„ãƒ‡ãƒ¼ã‚¿ã§Finetuningã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚project pageãŒã‚µãƒ³ãƒ—ãƒ«ã‚‚ã‚ã‚Šã¨ã¦ã‚‚ã‚ã‹ã‚Šã‚„ã™ã„:https://gen-omnimatte.github.io</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/RLVR.html">#RLVR</a>
<a class="button" href="articles/DataMixture.html">#DataMixture</a>
<span class="issue_date">Issue Date: 2025-06-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2015">[Paper Note] MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement  Learning, Yiqing Liang+, arXiv'25</a>
<span class="snippet"><span>Summary</span>æ¤œè¨¼å¯èƒ½ãªå ±é…¬ã‚’ç”¨ã„ãŸå¼·åŒ–å­¦ç¿’ï¼ˆRLVRï¼‰ã‚’ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMsã«é©ç”¨ã™ã‚‹ãŸã‚ã®ãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã€‚ç•°ãªã‚‹è¦–è¦šã¨è¨€èªã®å•é¡Œã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã€æœ€é©ãªãƒ‡ãƒ¼ã‚¿æ··åˆæˆ¦ç•¥ã‚’å°å…¥ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ææ¡ˆã—ãŸæˆ¦ç•¥ãŒMLLMã®æ¨è«–èƒ½åŠ›ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç¤ºã—ã€åˆ†å¸ƒå¤–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å¹³å‡5.24%ã®ç²¾åº¦å‘ä¸Šã‚’é”æˆã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/_vztu/status/1930312780701413498?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªè¨­å®šã§RLVRã‚’é©ç”¨ã™ã‚‹ã¨ã€ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å­¦ç¿’ã«åˆ©ç”¨ã™ã‚‹å ´åˆã‚ˆã‚Šã€ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã®ã¿ã®ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ãŸæ–¹ãŒå½“è©²ã‚¿ã‚¹ã‚¯ã§ã¯æ€§èƒ½ãŒé«˜ããªã£ãŸã‚Šï¼ˆã¤ã¾ã‚Šãƒ‡ãƒ¼ã‚¿ãŒå¤šã‘ã‚Œã°å¤šã„ã»ã©è‰¯ã„ã‚ã‘ã§ã¯ç„¡ã„ï¼‰ã€ç‰¹å®šã®ãƒ‡ãƒ¼ã‚¿ã‚’ablationã™ã‚‹ã¨OODã«å¯¾ã™ã‚‹äºˆæ¸¬æ€§èƒ½ãŒæ”¹å–„ã—ãŸã‚Šã™ã‚‹ãªã©ã€ãƒ‡ãƒ¼ã‚¿é–“ã§å¹²æ¸‰ãŒèµ·ãã¦æ•µå¯¾çš„ã«ãªã£ã¦ã—ã¾ã†ã‚ˆã†ãªç¾è±¡ãŒèµ·ãã‚‹ã€‚ã“ã®ã“ã¨ã‹ã‚‰ã€ã©ã®ã‚ˆã†ã«é©åˆ‡ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ··åˆã§ãã‚‹ã‹ï¼Ÿã¨ã„ã†æˆ¦ç•¥ã®å¿…è¦æ€§ãŒæµ®ãå½«ã‚Šã«ãªã‚Šã€ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ãªMixtureæˆ¦ç•¥ï¼ˆã©ã†ã‚„ã‚‰ãƒ‡ãƒ¼ã‚¿ã®æ··åˆåˆ†å¸ƒã‹ã‚‰å­¦ç¿’å¾Œã®æ€§èƒ½ã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãªæ¨¡æ§˜ï¼‰ã®æ€§èƒ½ãŒuniformã«mixã™ã‚‹ã‚ˆã‚Šã‚‚é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ãŸã€ã¿ãŸã„ãªè©±ã‚‰ã—ã„ã€‚</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2025-05-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1985">LaViDa: A Large Diffusion Language Model for Multimodal Understanding, Shufan Li+, arXiv'25</a>
<span class="snippet"><span>Summary</span>LaViDaã¯ã€é›¢æ•£æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ï¼ˆDMï¼‰ã‚’åŸºã«ã—ãŸãƒ“ã‚¸ãƒ§ãƒ³ãƒ»ãƒ©ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰ã§ã€é«˜é€Ÿãªæ¨è«–ã¨åˆ¶å¾¡å¯èƒ½ãªç”Ÿæˆã‚’å®Ÿç¾ã€‚æ–°æŠ€è¡“ã‚’å–ã‚Šå…¥ã‚Œã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦AR VLMã¨ç«¶äº‰åŠ›ã®ã‚ã‚‹æ€§èƒ½ã‚’é”æˆã€‚COCOã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã§é€Ÿåº¦å‘ä¸Šã¨æ€§èƒ½æ”¹å–„ã‚’ç¤ºã—ã€AR VLMã®å¼·åŠ›ãªä»£æ›¿æ‰‹æ®µã§ã‚ã‚‹ã“ã¨ã‚’è¨¼æ˜ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/iscienceluvr/status/1925749919312159167?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QDiffusion Modelã®æ³¢ãŒæ¥ãŸåŒç¨‹åº¦ã®ã‚µã‚¤ã‚ºã®ARãƒ¢ãƒ‡ãƒ«ã‚’outperform [^1]

<br>

<img src="https://github.com/user-attachments/assets/aeb12147-48ba-4b64-917c-9976ec1ffa0a" alt="image" loading="lazy">

<br>



<br>

[^1]:ãŸã ã—ã€ã“ã‚ŒãŒæœ¬å½“ã«Diffusion Modelã‚’ä½¿ã£ãŸã“ã¨ã«ã‚ˆã‚‹æ©æµãªã®ã‹ã¯ã¾ã è«–æ–‡ã‚’èª­ã‚“ã§ã„ãªã„ã®ã§ã‚ã‹ã‚‰ãªã„ã€‚å¿…è¦ã«ãªã£ãŸã‚‰èª­ã‚€ã€‚ãŸã ã€Physics of Language Modelã®ã‚ˆã†ã«ã€å®Œå…¨ã«ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§ç•°ãªã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¯”è¼ƒã—ãªã„ã¨ãã®è¾ºã¯ã‚ã‹ã‚‰ãªãã†ã§ã¯ã‚ã‚‹ã€‚</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/SyntheticData.html">#SyntheticData</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<a class="button" href="articles/DPO.html">#DPO</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<a class="button" href="articles/Probing.html">#Probing</a>
<span class="issue_date">Issue Date: 2025-05-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1974">Why Vision Language Models Struggle with Visual Arithmetic? Towards   Enhanced Chart and Geometry Understanding, Kung-Hsiang Huang+, ACL'25</a>
<span class="snippet"><span>Summary</span>Vision Language Models (VLMs)ã¯è¦–è¦šçš„ç®—è¡“ã«è‹¦åŠ´ã—ã¦ã„ã‚‹ãŒã€CogAlignã¨ã„ã†æ–°ã—ã„ãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã‚’ææ¡ˆã—ã€VLMã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚CogAlignã¯è¦–è¦šçš„å¤‰æ›ã®ä¸å¤‰ç‰¹æ€§ã‚’èªè­˜ã™ã‚‹ã‚ˆã†ã«è¨“ç·´ã—ã€CHOCOLATEã§4.6%ã€MATH-VISIONã§2.9%ã®æ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’60%å‰Šæ¸›ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€åŸºæœ¬çš„ãªè¦–è¦šçš„ç®—è¡“èƒ½åŠ›ã®å‘ä¸Šã¨ä¸‹æµã‚¿ã‚¹ã‚¯ã¸ã®è»¢é€ã®åŠ¹æœãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/steeve__huang/status/1923543884367306763?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qæ—¢å­˜ã®LLM (proprietary, openweightãã‚Œãã‚Œ)ãŒã€ã‚·ãƒ³ãƒ—ãƒ«ãªvisual arithmeticã‚¿ã‚¹ã‚¯(e.g., ç·šåˆ†ã®é•·ã•æ¯”è¼ƒ, Chartä¸Šã®dotã®ç†è§£)ãªã©ã®æ€§èƒ½ãŒä½ã„ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã—ã€

<br>

<img src="https://github.com/user-attachments/assets/039a48de-67a5-4c81-ba59-174acd508479" alt="image" loading="lazy">

<br>

ãã‚Œã‚‰ã®åŸå› ã‚’(1)Vision Encoderã®representationã¨(2)Vision Encoderã‚’Freezeã—ãŸä¸Šã§ã®Text Decoderã®finetuningã§åˆ†æã—ãŸã€‚ãã®çµæœã€(1)ã§ã¯ã„ãã¤ã‹ã®ã‚¿ã‚¹ã‚¯ã§linear layerã®probingã§ã¯é«˜ã„æ€§èƒ½ãŒé”æˆã§ããªã„ã“ã¨ãŒã‚ã‹ã£ãŸã€‚ã“ã®ã“ã¨ã‹ã‚‰ã€Vision Encoderã«ã‚ˆã‚‹representationãŒã‚¿ã‚¹ã‚¯ã«é–¢ã™ã‚‹æƒ…å ±ã‚’å†…åŒ…ã§ãã¦ã„ãªã„ã‹ã€ã‚¿ã‚¹ã‚¯ã«é–¢ã™ã‚‹æƒ…å ±ã¯å†…åŒ…ã—ã¦ã„ã‚‹ãŒlinear layerã§ã¯ãã‚Œã‚’ååˆ†ã«å¯èƒ½ã§ããªã„å¯èƒ½æ€§ãŒç¤ºå”†ã•ã‚ŒãŸã€‚

<br>

<img src="https://github.com/user-attachments/assets/0eb90fa2-7b6a-43b6-81d9-b5f7e6fb3ea8" alt="image" loading="lazy">

<br>



<br>

ã“ã‚Œã‚’ã•ã‚‰ã«åˆ†æã™ã‚‹ãŸã‚ã«(2)ã‚’å®Ÿæ–½ã—ãŸã¨ã“ã‚ã€Vision Encoderã‚’freezeã—ã¦ã„ã¦ã‚‚finetuningã«ã‚ˆã‚Šquery stringã«é–¢ã‚ã‚‰ãšé«˜ã„æ€§èƒ½ã‚’ç²å¾—ã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ã“ã®ã“ã¨ã‹ã‚‰ã€Vision Encoderå´ã®representationã®å•é¡Œã§ã¯ãªãã€Text Decoderã¨å´ã§ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹éš›ã«Finetuningã—ãªã„ã¨ã†ã¾ãæ´»ç”¨ã§ããªã„ã“ã¨ãŒåˆ¤æ˜ã—ãŸã€‚

<br>

<img src="https://github.com/user-attachments/assets/cd122d99-9228-44b1-9827-cdb56f49d492" alt="image" loading="lazy">æ‰‹æ³•ã®ã¨ã“ã‚ã¯ã¾ã å…¨ç„¶ã—ã£ã‹ã‚Šèª­ã‚ã¦ã„ãªã„ã®ã ãŒã€ç”»åƒã«é–¢ã™ã‚‹ç‰¹å®šã®å±æ€§ã«é–¢ã™ã‚‹ã‚¯ã‚¨ãƒªã¨å›ç­”ã®ãƒšã‚¢ã‚’åˆæˆã—ã€DPOã™ã‚‹ã“ã¨ã§ã€zero-shotã®æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã€ã¨ã„ã†æ„Ÿã˜ã£ã½ã„ï¼Ÿ

<br>

<img src="https://github.com/user-attachments/assets/707b1cc9-8bbf-45a5-b564-f654503c836e" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/281da17b-c8c3-455a-aa51-043ed297ae1f" alt="image" loading="lazy"></span>
<a class="button" href="articles/Embeddings.html">#Embeddings</a>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/RepresentationLearning.html">#RepresentationLearning</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/Chain-of-Thought.html">#Chain-of-Thought</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-05-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1924">Layer by Layer: Uncovering Hidden Representations in Language Models, Oscar Skean+, ICML'25</a>
<span class="snippet"><span>Summary</span>ä¸­é–“å±¤ã®åŸ‹ã‚è¾¼ã¿ãŒæœ€çµ‚å±¤ã‚’è¶…ãˆã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã™ã“ã¨ã‚’åˆ†æã—ã€æƒ…å ±ç†è«–ã‚„å¹¾ä½•å­¦ã«åŸºã¥ããƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ææ¡ˆã€‚32ã®ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚¿ã‚¹ã‚¯ã§ä¸­é–“å±¤ãŒå¼·åŠ›ãªç‰¹å¾´ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã—ã€AIã‚·ã‚¹ãƒ†ãƒ ã®æœ€é©åŒ–ã«ãŠã‘ã‚‹ä¸­é–“å±¤ã®é‡è¦æ€§ã‚’å¼·èª¿ã€‚</span>
<span class="snippet"><span>Comment</span>ç¾ä»£ã®ä»£è¡¨çš„ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆdecoder-only model, encoder-only model, SSMï¼‰ã«ã¤ã„ã¦ã€æœ€çµ‚å±¤ã®embeddingã‚ˆã‚Šã‚‚ä¸­é–“å±¤ã®embeddingã®æ–¹ãŒdownstream taskï¼ˆMTEBã®32Taskã®å¹³å‡ï¼‰ã«ã€ä¸€è²«ã—ã¦ï¼ˆãŸã ã—ã€ã“ã‚Œã¯MTEBã®å¹³å‡ã§è¦‹ãŸã‚‰ãã†ã¨ã„ã†è©±ã§ã‚ã‚Šã€å€‹åˆ¥ã®ã‚¿ã‚¹ã‚¯ã§ä¸€è²«ã—ã¦å¼·ã„ã‹ã¯èª­ã‚“ã§ã¿ãªã„ã¨ã‚ã‹ã‚‰ãªã„ï¼‰å¼·ã„ã“ã¨ã‚’ç¤ºã—ãŸç ”ç©¶ã€‚

<br>



<br>

ã“ã®ã“ã¨è‡ªä½“ã¯çµŒé¨“çš„ã«çŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ã®ã§ã‚ã¾ã‚Šé©šãã§ã¯ãªã„ã®ã ãŒï¼ˆãŸã ã€SSMã§ã‚‚ãã†ãªã®ã‹ã€ã¨ã„ã†ã®ã¨ã€ä¸€è²«ã—ã¦å¼·ã„ã¨ã„ã†ã®ã¯èˆˆå‘³æ·±ã„ï¼‰ã€ã“ã®ç ”ç©¶ã¯Matrix Based Entropyã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã«åŸºã¥ã„ã¦ã€ã“ã‚Œã‚‰ã‚’åˆ†æã™ã‚‹ãŸã‚ã®æ§˜ã€…ãªæŒ‡æ¨™ã‚’å®šç¾©ã—ç†è«–çš„ãªæ ¹æ‹ ã‚’ç¤ºã—ã€Autoregressiveãªå­¦ç¿’ã‚ˆã‚Šã‚‚Masked Languageã«ã‚ˆã‚‹å­¦ç¿’ã®æ–¹ãŒã“ã®ã‚ˆã†ãªMiddle Layerã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãŒç·©å’Œã•ã‚Œã€åŒæ§˜ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãŒç”»åƒã®å ´åˆã§ã‚‚èµ·ãã‚‹ã“ã¨ã‚’ç¤ºã—ã€CoTãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸFinetuningã«ã¤ã„ã¦ã‚‚åˆ†æã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚ã“ã®è¾ºã®è²¢çŒ®ãŒéå¸¸ã«å¤§ãã„ã¨æ€ã‚ã‚Œã‚‹ã®ã§ã“ã“ã‚’ç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã ã¨æ€ã‚ã‚Œã‚‹ã€‚ã‚ã¨ã§èª­ã‚€ã€‚

<br>



<br>

<img src="https://github.com/user-attachments/assets/bda00c50-c97b-45e0-97a5-d98dd98599fd" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/x-Use.html">#x-Use</a>
<span class="issue_date">Issue Date: 2025-04-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1897">AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents, Christopher Rawles+, ICLR'25</a>
<span class="snippet"><span>Summary</span>æœ¬ç ”ç©¶ã§ã¯ã€116ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦å ±é…¬ä¿¡å·ã‚’æä¾›ã™ã‚‹ã€ŒAndroidWorldã€ã¨ã„ã†å®Œå…¨ãªAndroidç’°å¢ƒã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è‡ªç„¶è¨€èªã§è¡¨ç¾ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚’å‹•çš„ã«æ§‹ç¯‰ã—ã€ç¾å®Ÿçš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿç¾ã€‚åˆæœŸçµæœã§ã¯ã€æœ€è‰¯ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒ30.6%ã®ã‚¿ã‚¹ã‚¯ã‚’å®Œäº†ã—ã€ã•ã‚‰ãªã‚‹ç ”ç©¶ã®ä½™åœ°ãŒç¤ºã•ã‚ŒãŸã€‚ã¾ãŸã€ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—Webã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®Androidé©å¿œãŒåŠ¹æœè–„ã§ã‚ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã€ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿç¾ã«ã¯ã•ã‚‰ãªã‚‹ç ”ç©¶ãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚ŒãŸã€‚ã‚¿ã‚¹ã‚¯ã®å¤‰å‹•ãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ã‚‚ç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span>Androidç’°å¢ƒã§ã®Phone Useã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/FoundationModel.html">#FoundationModel</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<a class="button" href="articles/CVPR.html">#CVPR</a>
<span class="issue_date">Issue Date: 2025-04-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1883">AM-RADIO: Agglomerative Vision Foundation Model -- Reduce All Domains   Into One, Mike Ranzinger+, CVPR'25</a>
<span class="snippet"><span>Summary</span>è¦–è¦šåŸºç›¤ãƒ¢ãƒ‡ãƒ«ï¼ˆVFMï¼‰ã‚’ãƒãƒ«ãƒãƒ†ã‚£ãƒ¼ãƒãƒ£ãƒ¼è’¸ç•™ã‚’é€šã˜ã¦çµ±åˆã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒAM-RADIOã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã®è¦–è¦š-è¨€èªç†è§£ã‚„ãƒ”ã‚¯ã‚»ãƒ«ãƒ¬ãƒ™ãƒ«ã®ç†è§£ã‚’å‘ä¸Šã•ã›ã€å€‹ã€…ã®ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è¶…ãˆã‚‹ã€‚æ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£E-RADIOã¯ã€ãƒ†ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚å°‘ãªãã¨ã‚‚7å€é€Ÿã„ã€‚åŒ…æ‹¬çš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æ§˜ã€…ãªä¸‹æµã‚¿ã‚¹ã‚¯ã‚’è©•ä¾¡ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/pavlomolchanov/status/1910391609927360831?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qvisionç³»ã®foundation modelã¯ãã‚Œãã‚Œç•°ãªã‚‹ç›®çš„é–¢æ•°ã§è¨“ç·´ã•ã‚Œã¦ãã¦ãŠã‚Šï¼ˆCLIPã¯å¯¾ç…§å­¦ç¿’ 550, DINOv2ã¯è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ 1884, SAMã¯segmentation 1885)ãã‚Œãã‚Œåˆ¥ã®èƒ½åŠ›ã‚’æŒã£ã¦ãŸãŒã€ãã‚Œã‚‰ã‚’ä¸€å€‹ã®ãƒ¢ãƒ‡ãƒ«ã«è’¸ç•™ã—ã¾ã—ãŸã€ã¨ã„ã†è©±ã‚‰ã—ã„

<br>

<img src="https://github.com/user-attachments/assets/929aaa47-ab88-4912-a59a-579d2f34e886" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/SpeechProcessing.html">#SpeechProcessing</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<a class="button" href="articles/Video.html">#Video</a>
<span class="issue_date">Issue Date: 2025-03-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1843">Qwen2.5-Omni Technical Report, Jin Xu+, arXiv'25</a>
<span class="snippet"><span>Summary</span>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã€ŒQwen2.5-Omniã€ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ã€å‹•ç”»ã‚’èªè­˜ã—ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ–¹å¼ã§è‡ªç„¶ãªéŸ³å£°å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ã€‚éŸ³å£°ã¨è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯ãƒ–ãƒ­ãƒƒã‚¯å‡¦ç†ã‚’ç”¨ã„ã€TMRoPEã«ã‚ˆã‚‹æ–°ã—ã„ä½ç½®åŸ‹ã‚è¾¼ã¿ã§éŸ³å£°ã¨å‹•ç”»ã®åŒæœŸã‚’å®Ÿç¾ã€‚Thinker-Talkerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã¨éŸ³å£°å‡ºåŠ›ã‚’å¹²æ¸‰ãªãè¡Œã†ã€‚Qwen2.5-Omniã¯ã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§è¨“ç·´ã•ã‚Œã€éŸ³å£°æŒ‡ç¤ºã«å¯¾ã™ã‚‹æ€§èƒ½ãŒãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã¨åŒç­‰ã§ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°Talkerã¯æ—¢å­˜æ‰‹æ³•ã‚’ä¸Šå›ã‚‹è‡ªç„¶ã•ã‚’æŒã¤ã€‚</span>
<span class="snippet"><span>Comment</span>Qwen Teamã«ã‚ˆã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMã€‚ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€å‹•ç”»éŸ³å£°ã‚’inputã¨ã—ã¦å—ã‘å–ã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆã€éŸ³å£°ã‚’outputã™ã‚‹ã€‚

<br>

<img src="https://github.com/user-attachments/assets/03e54fd7-2011-4069-aa1b-38d1610169ec" alt="image" loading="lazy">

<br>



<br>

weight:https://huggingface.co/collections/Qwen/qwen25-omni-67de1e5f0f9464dc6314b36eå…ƒãƒã‚¹ãƒˆ:https://www.linkedin.com/posts/niels-rogge-a3b7a3127_alibabas-qwen-team-has-done-it-again-this-activity-7311036679627132929-HUqy?utm_source=share&utm_medium=member_ios&rcm=ACoAACzQvjwB2FeLVE3yukDiUYtr5J4k-6nlNG4</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2025-03-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1776">Large Language Diffusion Models, Shen Nie+, arXiv'25</a>
<span class="snippet"><span>Summary</span>LLaDAã¯ã€è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆARMsï¼‰ã«ä»£ã‚ã‚‹æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ã‚¼ãƒ­ã‹ã‚‰è¨“ç·´ã•ã‚Œã€ãƒ‡ãƒ¼ã‚¿ãƒã‚¹ã‚­ãƒ³ã‚°ã‚’é€šã˜ã¦åˆ†å¸ƒã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã€‚åºƒç¯„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å¼·åŠ›ãªã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’ç¤ºã—ã€è‡ªå·±æ§‹ç¯‰ã—ãŸARMãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚‹ã€‚ç‰¹ã«ã€LLaDA 8Bã¯æ–‡è„ˆå†…å­¦ç¿’ã‚„æŒ‡ç¤ºè¿½å¾“èƒ½åŠ›ã«å„ªã‚Œã€é€†è©©ã®å®Œæˆã‚¿ã‚¹ã‚¯ã§GPT-4oã‚’è¶…ãˆã‚‹æ€§èƒ½ã‚’ç™ºæ®ã€‚æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ãŒARMsã®å®Ÿè¡Œå¯èƒ½ãªä»£æ›¿æ‰‹æ®µã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/dair_ai/status/1893698288328602022?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qå‚è€ƒ:https://x.com/karpathy/status/1894923254864978091</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-01-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1740">SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model  Post-training, Tianzhe Chu+, arXiv'25</a>
<span class="snippet"><span>Summary</span>SFTã¨RLã®ä¸€èˆ¬åŒ–èƒ½åŠ›ã®é•ã„ã‚’ç ”ç©¶ã—ã€GeneralPointsã¨V-IRLã‚’ç”¨ã„ã¦è©•ä¾¡ã€‚RLã¯ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆã¨è¦–è¦šå¤‰ç¨®ã«å¯¾ã—ã¦å„ªã‚ŒãŸä¸€èˆ¬åŒ–ã‚’ç¤ºã™ä¸€æ–¹ã€SFTã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜æ†¶ã—åˆ†å¸ƒå¤–ã‚·ãƒŠãƒªã‚ªã«è‹¦åŠ´ã€‚RLã¯è¦–è¦šèªè­˜èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ãŒã€SFTã¯RLè¨“ç·´ã«ä¸å¯æ¬ ã§ã‚ã‚Šã€å‡ºåŠ›å½¢å¼ã‚’å®‰å®šã•ã›ã‚‹ã“ã¨ã§æ€§èƒ½å‘ä¸Šã‚’ä¿ƒé€²ã€‚ã“ã‚Œã‚‰ã®çµæœã¯ã€è¤‡é›‘ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹RLã®ä¸€èˆ¬åŒ–èƒ½åŠ›ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/hillbig/status/1884731381517082668?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/CVPR.html">#CVPR</a>
<a class="button" href="articles/Scaling%20Laws.html">#Scaling Laws</a>
<a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a>
<a class="button" href="articles/DataFiltering.html">#DataFiltering</a>
<span class="issue_date">Issue Date: 2025-07-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2262">[Paper Note] Scaling Laws for Data Filtering -- Data Curation cannot be Compute   Agnostic, Sachin Goyal+, CVPR'24</a>
<span class="snippet"><span>Summary</span>è¦–è¦šã¨è¨€èªã®ãƒ¢ãƒ‡ãƒ«ï¼ˆVLMsï¼‰ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦ã€é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãŒé‡è¦ã§ã‚ã‚‹ãŒã€è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã¨ã¯ç„¡é–¢ä¿‚ã«è¡Œã‚ã‚Œã‚‹ã“ã¨ãŒå¤šã„ã€‚æœ¬ç ”ç©¶ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®å“è³ªã¨é‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼ˆQQTï¼‰ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€ã‚¦ã‚§ãƒ–ãƒ‡ãƒ¼ã‚¿ã®éå‡è³ªæ€§ã‚’è€ƒæ…®ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ‡ãƒ¼ã‚¿ã®æœ‰ç”¨æ€§ã®é•ã„ã‚„ç¹°ã‚Šè¿”ã—ä½¿ç”¨ã«ã‚ˆã‚‹åŠ£åŒ–ã‚’è©•ä¾¡ã—ã€è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¼ãƒ«ã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¨å®šå¯èƒ½ã«ã™ã‚‹ã€‚æœ€é©ãªãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¼ãƒ«ã®ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é€šã˜ã¦ã€è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã«å¿œã˜ãŸæœ€é«˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/cloneofsimo/status/1946241642572448174?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qé«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§å¤šãã®ç ”ç©¶ãŒãƒ¢ãƒ‡ãƒ«ãŒã‚ˆã‚Šé«˜ã„æ€§èƒ½ã‚’é”æˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ãŒã€é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã«ã¯é™ã‚ŠãŒã‚ã‚‹ã“ã¨ã¨ã€ç¹°ã‚Šè¿”ã—å­¦ç¿’ã‚’ã™ã‚‹ã“ã¨ã§ã™ãã«ãã®åŠ¹ç”¨ãŒä½ä¸‹ã™ã‚‹ï¼ˆQuality-Quantity tradeoff!)ã¨ã„ã†ç‰¹æ€§ãŒã‚ã‚‹ã€‚ã“ã®ã‚ˆã†ãªçŠ¶æ³ã«ãŠã„ã¦ã€ãŸã¨ãˆã°è¨ˆç®—ã®äºˆç®—ãŒãƒ‡ãƒ¼ã‚¿6ãƒ‘ã‚±ãƒƒãƒˆåˆ†ã®æ™‚ã«ã€ã‚ã¡ã‚ƒã‚ã¡ã‚ƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’é ‘å¼µã£gé«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¼ãƒ«Eã®ã¿ã‚’ä½¿ã£ã¦6 epochå­¦ç¿’ã™ã‚‹ã®ãŒè‰¯ã„ã®ã‹ã€å°‘ã—å“è³ªã¯è½ã¡ã‚‹ãƒ‡ãƒ¼ã‚¿Dã‚‚æ··ãœã¦E+Dã‚’3 epochå­¦ç¿’ã™ã‚‹ã®ãŒè‰¯ã„ã®ã‹ã€ã¨ãã«ã©ã¡ã‚‰ãŒè‰¯ã„ã®ã‹ï¼Ÿã¨ã„ã†è©±ã®ã‚ˆã†ã§ã‚ã‚‹ã€‚

<br>

<img src="https://github.com/user-attachments/assets/06812781-7212-415e-bc7a-dd19ac4ca0d7" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/Mathematics.html">#Mathematics</a>
<a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2201">[Paper Note] Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset, Ke Wang+, NeurIPS'24 Datasets and Benchmarks Track</a>
<span class="snippet"><span>Summary</span>MATH-Visionï¼ˆMATH-Vï¼‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ææ¡ˆã—ã€3,040ã®è¦–è¦šçš„æ–‡è„ˆã‚’æŒã¤æ•°å­¦å•é¡Œã‚’åé›†ã€‚16ã®æ•°å­¦åˆ†é‡ã¨5ã¤ã®é›£æ˜“åº¦ã§æ§‹æˆã•ã‚Œã€LMMsã®æ•°å­¦çš„æ¨è«–èƒ½åŠ›ã‚’è©•ä¾¡ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€LMMsã¨äººé–“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é–“ã«é¡•è‘—ãªã‚®ãƒ£ãƒƒãƒ—ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã€ã•ã‚‰ãªã‚‹é€²å±•ã®å¿…è¦æ€§ã‚’å¼·èª¿ã€‚ã‚¨ãƒ©ãƒ¼åˆ†æã‚’é€šã˜ã¦ä»Šå¾Œã®ç ”ç©¶ã«è²´é‡ãªæ´å¯Ÿã‚’æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span>openreview: https://openreview.net/forum?id=QWTCcxMpPAdiscussion

<br>

project page: https://mathllm.github.io/mathvision/Project Pageã®ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒšãƒ¼ã‚¸ãŒéå¸¸ã«ã‚ã‹ã‚Šã‚„ã™ã„ã€‚ã“ã¡ã‚‰ã¯äººé–“ã®æ–¹ãŒã¾ã ã¾ã æ€§èƒ½ãŒé«˜ãã†ã€‚

<br>



<br>

<img width="671" height="806" alt="Image" src="https://github.com/user-attachments/assets/586edf6d-cd77-48cb-b209-8ea819e725fc"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/FoundationModel.html">#FoundationModel</a>
<a class="button" href="articles/Self-SupervisedLearning.html">#Self-SupervisedLearning</a>
<a class="button" href="articles/TMLR.html">#TMLR</a>
<span class="issue_date">Issue Date: 2025-04-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1884">DINOv2: Learning Robust Visual Features without Supervision, Maxime Oquab+, TMLR'24</a>
<span class="snippet"><span>Summary</span>è‡ªå·±æ•™å¸«ã‚ã‚Šæ‰‹æ³•ã‚’ç”¨ã„ã¦ã€å¤šæ§˜ãªã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ±ç”¨çš„ãªè¦–è¦šç‰¹å¾´ã‚’ç”Ÿæˆã™ã‚‹æ–°ã—ã„äº‹å‰å­¦ç¿’æ‰‹æ³•ã‚’ææ¡ˆã€‚1Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ViTãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€å°å‹ãƒ¢ãƒ‡ãƒ«ã«è’¸ç•™ã™ã‚‹ã“ã¨ã§ã€OpenCLIPã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’é”æˆã€‚</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1699">OlympiadBench: A Challenging Benchmark for Promoting AGI with   Olympiad-Level Bilingual Multimodal Scientific Problems, Chaoqun He+, ACL'24</a>
<span class="snippet"><span>Summary</span>å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã‚„ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆLMMsï¼‰ã®èƒ½åŠ›ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã«ã€ã‚ªãƒªãƒ³ãƒ”ã‚¢ãƒ‰ãƒ¬ãƒ™ãƒ«ã®ãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ç§‘å­¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ŒOlympiadBenchã€ã‚’ææ¡ˆã€‚8,476ã®æ•°å­¦ã¨ç‰©ç†ã®å•é¡Œã‚’å«ã¿ã€å°‚é–€å®¶ãƒ¬ãƒ™ãƒ«ã®æ³¨é‡ˆãŒä»˜ã‘ã‚‰ã‚Œã¦ã„ã‚‹ã€‚ãƒˆãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ã®GPT-4Vã¯å¹³å‡17.97%ã®ã‚¹ã‚³ã‚¢ã‚’é”æˆã—ãŸãŒã€ç‰©ç†ã§ã¯10.74%ã«ã¨ã©ã¾ã‚Šã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®å³ã—ã•ã‚’ç¤ºã™ã€‚ä¸€èˆ¬çš„ãªå•é¡Œã¨ã—ã¦å¹»è¦šã‚„è«–ç†çš„èª¤è¬¬ãŒæŒ‡æ‘˜ã•ã‚Œã€ä»Šå¾Œã®AGIç ”ç©¶ã«è²´é‡ãªãƒªã‚½ãƒ¼ã‚¹ã¨ãªã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã‚‹ã€‚</span>
<a class="button" href="articles/InformationRetrieval.html">#InformationRetrieval</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/RAG(RetrievalAugmentedGeneration).html">#RAG(RetrievalAugmentedGeneration)</a>
<span class="issue_date">Issue Date: 2024-12-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1598">VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval  Augmented Generation, Hyeonseok Lim+, arXiv'24</a>
<span class="snippet"><span>Summary</span>è¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯VLR-Benchã‚’ææ¡ˆã€‚ã“ã‚Œã¯5ã¤ã®å…¥åŠ›ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”¨ã„ã¦ã€ç‰¹å®šã®ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹æœ‰ç”¨ãªæƒ…å ±ã®åˆ¤æ–­èƒ½åŠ›ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã€‚32,000ã®è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸæŒ‡ç¤ºã‹ã‚‰ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆVLR-IFã‚’æ§‹ç¯‰ã—ã€VLMã®RAGèƒ½åŠ›ã‚’å¼·åŒ–ã€‚Llama3ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã§æ€§èƒ½ã‚’æ¤œè¨¼ã—ã€ä¸¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span>Multilingual VLMã‚’ç”¨ã„ãŸRAGã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<span class="issue_date">Issue Date: 2024-12-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1584">Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale   Prediction, Keyu Tian+, NeurIPS'24</a>
<span class="snippet"><span>Summary</span>Visual AutoRegressive modeling (VAR)ã‚’ææ¡ˆã—ã€ç”»åƒç”Ÿæˆã«ãŠã„ã¦è‡ªå·±å›å¸°å­¦ç¿’ã‚’æ¬¡ã®ã‚¹ã‚±ãƒ¼ãƒ«äºˆæ¸¬ã¨ã—ã¦å†å®šç¾©ã€‚VARã¯ã€GPTã®ã‚ˆã†ãªARãƒ¢ãƒ‡ãƒ«ãŒæ‹¡æ•£ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’å®Ÿç¾ã—ã€ImageNet 256x256ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§FIDã‚’18.65ã‹ã‚‰1.73ã€ISã‚’80.4ã‹ã‚‰350.2ã«æ”¹å–„ã€‚æ¨è«–é€Ÿåº¦ã¯ç´„20å€å‘ä¸Šã—ã€ç”»åƒå“è³ªã‚„ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã§ã‚‚å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã€‚VARã¯ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆä¸€èˆ¬åŒ–èƒ½åŠ›ã‚’æŒã¡ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã‚’ç¤ºã™ã€‚å…¨ãƒ¢ãƒ‡ãƒ«ã¨ã‚³ãƒ¼ãƒ‰ã‚’å…¬é–‹ã—ã€è¦–è¦šç”Ÿæˆã®ç ”ç©¶ã‚’ä¿ƒé€²ã€‚</span>
<span class="snippet"><span>Comment</span>NeurIPS2024ã®ãƒ™ã‚¹ãƒˆãƒšãƒ¼ãƒ‘ãƒ¼ç¬¬ä¸€è‘—è€…ãŒByteDanceç¤¾ã‹ã‚‰è¨´è¨Ÿã‚’èµ·ã“ã•ã‚Œã¦ã„ã‚‹æ¨¡æ§˜â€¦ï¼Ÿ

<br>

https://var-integrity-report.github.ioOpenReview:https://openreview.net/forum?id=gojL67CfS8Next Token Prediction, Next Image Token Generation (å¾“æ¥æ‰‹æ³•ï¼‰, Next Scale (resolution) prediction (ææ¡ˆæ‰‹æ³•)ã®é•ã„ã®å›³è§£ã€‚éå¸¸ã«åˆ†ã‹ã‚Šã‚„ã™ã„ã€‚next token predictionã§ã¯æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã‚’äºˆæ¸¬ã™ã‚‹ãŒVARã§ã¯ã€æ¬¡ã®è§£åƒåº¦ç”»åƒã®å…¨ä½“ã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒãƒƒãƒ—ã‚’äºˆæ¸¬ã™ã‚‹ã€‚

<br>



<br>

<img src="https://github.com/user-attachments/assets/668d7523-f262-45c1-a1d0-2dd479c0a708" alt="image" loading="lazy">

<br>



<br>

å­¦ç¿’æ–¹æ³•ã®æ¦‚è¦ã€‚2-Stageã§å­¦ç¿’ã•ã‚Œã‚‹ã€‚æœ€åˆã®ã‚¹ãƒ†ãƒ¼ã‚¸ã§Kç¨®é¡ã®è§£åƒåº¦ã®ç”»åƒï¼ˆï¼Kç¨®é¡ã®ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã®token maps r_kï¼‰ã‚’å¾—ã‚‹ãŸã‚ã«AutoEncoderã‚’å­¦ç¿’ã—ã€æ¬¡ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã§block-wiseã®causal attention maskã‚’ç”¨ã„ã¦ã€K_&lt;kå€‹ç›®ã®è§£åƒåº¦ã®ç”»åƒã‹ã‚‰Kå€‹ç›®ã®è§£åƒåº¦ã®ç”»åƒã‚’äºˆæ¸¬ã™ã‚‹ï¼ˆå›³ã‚’è¦‹ã‚‹ã¨ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’æ´ã¿ã‚„ã™ã„ï¼‰ã€‚inferenceæ™‚ã¯KV Cacheã‚’åˆ©ç”¨ã—ã€maskã¯ä¸è¦ã¨ãªã‚‹ã€‚

<br>

å„r_kã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹éš›ã«r_&lt;kã®ã¿ã«ä¾å­˜ã™ã‚‹è¨­è¨ˆã«ã™ã‚‹ã“ã¨ã§coase-to-fineã«ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã«ç›¸å½“ã—ã€ã“ã‚Œã¯äººé–“ã®ç²—ãæ‰ãˆã¦ã‹ã‚‰è©³ç´°ã‚’è¦‹ã‚‹èªçŸ¥ãƒ—ãƒ­ã‚»ã‚¹ã¨åˆè‡´ã™ã‚‹ã€‚ã¾ãŸã€flattenæ“ä½œãŒå­˜åœ¨ã›ãšã€ãã‚Œãã‚Œã®r_&lt;kå†…ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒr_kç”Ÿæˆæ™‚ã«å…¨ã¦è€ƒæ…®ã•ã‚Œã‚‹ãŸã‚ç©ºé–“çš„å±€æ‰€æ€§ã‚‚æ‹…ä¿ã•ã‚Œã‚‹ã€‚ã¾ãŸã€r_kå†…ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯ä¸¦åˆ—ã«ç”Ÿæˆå¯èƒ½ãªã®ã§è¨ˆç®—é‡ã®ã‚ªãƒ¼ãƒ€ãƒ¼ãŒå¤§å¹…ã«å‰Šæ¸›ã•ã‚Œã‚‹ï¼ˆO(n^4)ã€‚

<br>

<img src="https://github.com/user-attachments/assets/e1a85712-e66a-4c9a-9cf1-6556f2b8e687" alt="image" loading="lazy">

<br>



<br>

å¾“æ¥æ‰‹æ³•ã¨æ¯”ã¹ã‚ˆã‚Šå°ã•ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§é«˜ã„æ€§èƒ½ã‚’å®Ÿç¾ã—ã€inference timeã‚‚éå¸¸ã«æ—©ã„ã€‚

<br>

<img src="https://github.com/user-attachments/assets/90a6a7de-995d-49e6-94a2-cd709e68777f" alt="image" loading="lazy">

<br>



<br>

ScalingLawsã‚‚æˆç«‹ã™ã‚‹ã€‚

<br>

<img src="https://github.com/user-attachments/assets/351c2a7b-85aa-4cc7-8ba2-a5e9528cabd4" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<span class="issue_date">Issue Date: 2024-11-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1542">Multimodal Autoregressive Pre-training of Large Vision Encoders, Enrico Fini+, arXiv'24</a>
<span class="snippet"><span>Summary</span>æ–°ã—ã„æ‰‹æ³•AIMV2ã‚’ç”¨ã„ã¦ã€å¤§è¦æ¨¡ãªãƒ“ã‚¸ãƒ§ãƒ³ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®äº‹å‰å­¦ç¿’ã‚’è¡Œã†ã€‚ã“ã‚Œã¯ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ„ã¿åˆã‚ã›ãŸãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è¨­å®šã«æ‹¡å¼µã•ã‚Œã€ã‚·ãƒ³ãƒ—ãƒ«ãªäº‹å‰å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã¨å„ªã‚ŒãŸæ€§èƒ½ã‚’ç‰¹å¾´ã¨ã™ã‚‹ã€‚AIMV2-3Bã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯ImageNet-1kã§89.5%ã®ç²¾åº¦ã‚’é”æˆã—ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ç”»åƒç†è§£ã«ãŠã„ã¦æœ€å…ˆç«¯ã®ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã€‚</span>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2024-11-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1526">Tutorial on Diffusion Models for Imaging and Vision, Stanley H. Chan, arXiv'24</a>
<span class="snippet"><span>Summary</span>ç”Ÿæˆãƒ„ãƒ¼ãƒ«ã®æˆé•·ã«ã‚ˆã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã‚„å‹•ç”»ã‚’ç”Ÿæˆã™ã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒå¯èƒ½ã«ã€‚æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®åŸç†ãŒã“ã‚Œã‚‰ã®ç”Ÿæˆãƒ„ãƒ¼ãƒ«ã®åŸºç›¤ã§ã‚ã‚Šã€å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ¬ ç‚¹ã‚’å…‹æœã€‚ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬çš„ãªã‚¢ã‚¤ãƒ‡ã‚¢ã‚’å­¦éƒ¨ç”Ÿã‚„å¤§å­¦é™¢ç”Ÿå‘ã‘ã«è§£èª¬ã€‚</span>
<span class="snippet"><span>Comment</span>ã„ã¤ã‹èª­ã¾ãªã‘ã‚Œã°ãªã‚‰ãªã„</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/SpeechProcessing.html">#SpeechProcessing</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<span class="issue_date">Issue Date: 2024-11-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1505">Mixture-of-Transformers: A Sparse and Scalable Architecture for  Multi-Modal Foundation Models, Weixin Liang+, arXiv'24</a>
<span class="snippet"><span>Summary</span>å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å‡¦ç†ã‚’åŠ¹ç‡åŒ–ã™ã‚‹ãŸã‚ã«ã€Mixture-of-Transformersï¼ˆMoTï¼‰ã‚’ææ¡ˆã€‚MoTã¯è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã—ã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã”ã¨ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆ†é›¢ã—ã¦ç‰¹åŒ–ã—ãŸå‡¦ç†ã‚’å®Ÿç¾ã€‚Chameleon 7Bè¨­å®šã§ã¯ã€55.8%ã®FLOPsã§å¯†ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«åŒ¹æ•µã™ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€éŸ³å£°ã‚’å«ã‚€å ´åˆã‚‚37.2%ã®FLOPsã§åŒæ§˜ã®çµæœã‚’é”æˆã€‚ã•ã‚‰ã«ã€Transfusionè¨­å®šã§ã¯ã€7Bã®MoTãƒ¢ãƒ‡ãƒ«ãŒå¯†ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ç”»åƒæ€§èƒ½ã«å¯¾ã—ã¦FLOPsã®3åˆ†ã®1ã§åŒ¹æ•µã—ã€760Mã®ãƒ¢ãƒ‡ãƒ«ã¯ä¸»è¦ãªç”»åƒç”ŸæˆæŒ‡æ¨™ã§ä¸Šå›ã‚‹çµæœã‚’å¾—ãŸã€‚MoTã¯å®Ÿç”¨çš„ãªåˆ©ç‚¹ã‚‚ç¤ºã—ã€ç”»åƒå“è³ªã‚’47.2%ã€ãƒ†ã‚­ã‚¹ãƒˆå“è³ªã‚’75.6%ã®çµŒéæ™‚é–“ã§é”æˆã€‚</span>
<span class="snippet"><span>Comment</span><img src="https://github.com/user-attachments/assets/340ab176-7b17-467a-8731-20d1594d6951" alt="image" loading="lazy"></span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/InstructionTuning.html">#InstructionTuning</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/Catastrophic%20Forgetting.html">#Catastrophic Forgetting</a>
<span class="issue_date">Issue Date: 2024-11-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1502">Online-LoRA: Task-free Online Continual Learning via Low Rank Adaptation, Xiwen Wei+, arXiv'24</a>
<span class="snippet"><span>Summary</span>ç ´æ»…çš„å¿˜å´ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€ã‚¿ã‚¹ã‚¯ãƒ•ãƒªãƒ¼ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç¶™ç¶šå­¦ç¿’ï¼ˆOCLï¼‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯Online-LoRAã‚’ææ¡ˆã€‚ãƒªãƒãƒ¼ã‚µãƒ«ãƒãƒƒãƒ•ã‚¡ã®åˆ¶ç´„ã‚’å…‹æœã—ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ“ã‚¸ãƒ§ãƒ³ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ï¼ˆViTï¼‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å¾®èª¿æ•´ã€‚æ–°ã—ã„ã‚ªãƒ³ãƒ©ã‚¤ãƒ³é‡ã¿æ­£å‰‡åŒ–æˆ¦ç•¥ã‚’ç”¨ã„ã¦é‡è¦ãªãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç‰¹å®šã—ã€ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®å¤‰åŒ–ã‚’è‡ªå‹•èªè­˜ã€‚å¤šæ§˜ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><img src="https://github.com/user-attachments/assets/b789ba71-3941-4d60-9397-46607ddc7712" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<span class="issue_date">Issue Date: 2024-09-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1435">COM Kitchens: An Unedited Overhead-view Video Dataset as a   Vision-Language Benchmark, Koki Maeda+, N_A, ECCV'24</a>
<span class="snippet"><span>Summary</span>æ‰‹ç¶šãçš„ãªãƒ“ãƒ‡ã‚ªç†è§£ã®ãŸã‚ã«ã€COM Kitchensã¨ã„ã†æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ææ¡ˆã€‚ã“ã‚Œã¯ã€å‚åŠ è€…ãŒãƒ¬ã‚·ãƒ”ã«åŸºã¥ã„ã¦é£Ÿæã‚’æº–å‚™ã™ã‚‹æ§˜å­ã‚’ä¸Šæ–¹è¦–ç‚¹ã§æ’®å½±ã—ãŸç·¨é›†ã•ã‚Œã¦ã„ãªã„ãƒ“ãƒ‡ã‚ªã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹ã€‚å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿åé›†ã®ãŸã‚ã«ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã‚’ä½¿ç”¨ã—ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¬ã‚·ãƒ”æ¤œç´¢ï¼ˆOnRRï¼‰ã¨å¯†ãªãƒ“ãƒ‡ã‚ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ï¼ˆDVC-OVï¼‰ã¨ã„ã†æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’ææ¡ˆã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€æ—¢å­˜ã®ã‚¦ã‚§ãƒ–ãƒ“ãƒ‡ã‚ªãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã®èƒ½åŠ›ã¨é™ç•Œã‚’æ¤œè¨¼ã€‚</span>
<span class="snippet"><span>Comment</span>ã¨ã¦ã‚‚ãŠã‚‚ã—ã‚ãã†ï¼</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<span class="issue_date">Issue Date: 2024-09-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1434">What matters when building vision-language models?, Hugo LaurenÃ§on+, N_A, arXiv'24</a>
<span class="snippet"><span>Summary</span>è¦–è¦šã¨è¨€èªã®ãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰ã®è¨­è¨ˆã«ãŠã‘ã‚‹è£ä»˜ã‘ã®ãªã„æ±ºå®šãŒæ€§èƒ½å‘ä¸Šã®ç‰¹å®šã‚’å¦¨ã’ã¦ã„ã‚‹ã¨æŒ‡æ‘˜ã€‚äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€ãƒ‡ãƒ¼ã‚¿ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã«é–¢ã™ã‚‹å®Ÿé¨“ã‚’è¡Œã„ã€80å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åŸºç›¤VLMã€ŒIdefics2ã€ã‚’é–‹ç™ºã€‚Idefics2ã¯ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’é”æˆã—ã€4å€ã®ã‚µã‚¤ã‚ºã®ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã®æ€§èƒ½ã‚’ç¤ºã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆã«OpenVLMã®é€²å±•ã®æ­´å²ãŒè¼‰ã£ã¦ã„ã‚‹ã€‚æ§‹ç¯‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚å…¬é–‹ã•ã‚Œã‚‹æ¨¡æ§˜ã€‚

<br>

<img src="https://github.com/user-attachments/assets/9675c2ad-650a-460b-9655-1c6347d07f58" alt="image" loading="lazy">

<br>

å…ƒãƒã‚¹ãƒˆ:https://x.com/thom_wolf/status/1840372428855280045?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/CLIP.html">#CLIP</a>
<span class="issue_date">Issue Date: 2024-09-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1432">Long-CLIP: Unlocking the Long-Text Capability of CLIP, Beichen Zhang+, N_A, ECCV'24</a>
<span class="snippet"><span>Summary</span>Long-CLIPã¯ã€CLIPã®ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã®é•·ã•åˆ¶é™ã‚’å…‹æœã—ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã®ä¸€èˆ¬åŒ–èƒ½åŠ›ã‚’ä¿æŒã¾ãŸã¯è¶…ãˆã‚‹æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã‚’ç”¨ã„ã¦ã€CLIPã®æ€§èƒ½ã‚’ç¶­æŒã—ã¤ã¤ã€é•·æ–‡ãƒ†ã‚­ã‚¹ãƒˆ-ç”»åƒãƒšã‚¢ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ†ã‚­ã‚¹ãƒˆ-ç”»åƒæ¤œç´¢ã‚¿ã‚¹ã‚¯ã§ç´„20%ã®æ€§èƒ½å‘ä¸Šã‚’é”æˆã—ã¾ã—ãŸã€‚ã¾ãŸã€Long-CLIPã¯è©³ç´°ãªãƒ†ã‚­ã‚¹ãƒˆèª¬æ˜ã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆã™ã‚‹èƒ½åŠ›ã‚’å¼·åŒ–ã—ã¾ã™ã€‚</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2024-09-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1369">Diffusion Models Are Real-Time Game Engines, Dani Valevski+, N_A, arXiv'24</a>
<span class="snippet"><span>Summary</span>GameNGenã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å®Œå…¨ã«å‹•ä½œã™ã‚‹ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³ã§ã‚ã‚Šã€é«˜å“è³ªã§é•·ã„è»Œè·¡ä¸Šã§è¤‡é›‘ãªç’°å¢ƒã¨ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚GameNGenã¯ã€å˜ä¸€ã®TPUä¸Šã§ç§’é–“20ãƒ•ãƒ¬ãƒ¼ãƒ ä»¥ä¸Šã§ã‚¯ãƒ©ã‚·ãƒƒã‚¯ã‚²ãƒ¼ãƒ DOOMã‚’ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚æ¬¡ãƒ•ãƒ¬ãƒ¼ãƒ äºˆæ¸¬ã§ã¯ã€PSNRãŒ29.4ã«é”ã—ã€åŠ£åŒ–JPEGåœ§ç¸®ã¨æ¯”è¼ƒå¯èƒ½ã§ã™ã€‚GameNGenã¯ã€2ã¤ã®æ®µéšã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¾ã™ï¼šï¼ˆ1ï¼‰RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚²ãƒ¼ãƒ ã‚’ãƒ—ãƒ¬ã‚¤ã™ã‚‹ã“ã¨ã‚’å­¦ã³ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¨˜éŒ²ã•ã‚Œã€ï¼ˆ2ï¼‰æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ãŒéå»ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¿œã˜ã¦æ¬¡ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç”Ÿæˆã™ã‚‹ã‚ˆã†ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¾ã™ã€‚æ¡ä»¶ä»˜ãã®æ‹¡å¼µã«ã‚ˆã‚Šã€é•·ã„è»Œè·¡ä¸Šã§å®‰å®šã—ãŸè‡ªå·±å›å¸°ç”ŸæˆãŒå¯èƒ½ã¨ãªã‚Šã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span>Diffusion Modelã§ã‚²ãƒ¼ãƒ æ˜ åƒã‚’ç”Ÿæˆã™ã‚‹å–ã‚Šçµ„ã¿ã‚‰ã—ã„ã€‚ã‚²ãƒ¼ãƒ ã®environmentã«å¯¾ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ã®Actionã¨frameã®ç³»åˆ—ã‚’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã¿ãªã—ã¦ç”Ÿæˆã™ã‚‹ã£ã½ã„ï¼Ÿproject pageã«ãƒ‡ãƒ¢ãŒã®ã£ã¦ã„ã‚‹

<br>



<br>

https://gamengen.github.io/</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Chain-of-Thought.html">#Chain-of-Thought</a>
<span class="issue_date">Issue Date: 2024-04-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1275">Visualization-of-Thought Elicits Spatial Reasoning in Large Language  Models, Wenshan Wu+, N_A, arXiv'24</a>
<span class="snippet"><span>Summary</span>LLMsã®ç©ºé–“æ¨è«–èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€Visualization-of-Thoughtï¼ˆVoTï¼‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã‚’ææ¡ˆã€‚VoTã¯ã€LLMsã®æ¨è«–ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’å¯è¦–åŒ–ã—ã€ç©ºé–“æ¨è«–ã‚¿ã‚¹ã‚¯ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€æ—¢å­˜ã®MLLMsã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã€‚VoTã¯ã€ç©ºé–“æ¨è«–ã‚’ä¿ƒé€²ã™ã‚‹ãŸã‚ã«ã€Œãƒ¡ãƒ³ã‚¿ãƒ«ã‚¤ãƒ¡ãƒ¼ã‚¸ã€ã‚’ç”Ÿæˆã™ã‚‹èƒ½åŠ›ã‚’æŒã¡ã€MLLMsã§ã®æœ‰åŠ¹æ€§ã‚’ç¤ºå”†ã™ã‚‹ã€‚</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ModelMerge.html">#ModelMerge</a>
<span class="issue_date">Issue Date: 2024-03-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1257">Evolutionary Optimization of Model Merging Recipes, Takuya Akiba+, N_A, arXiv'24</a>
<span class="snippet"><span>Summary</span>é€²åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ãŸæ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã€å¼·åŠ›ãªåŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•ç”Ÿæˆã‚’å®Ÿç¾ã€‚LLMã®é–‹ç™ºã«ãŠã„ã¦ã€äººé–“ã®ç›´æ„Ÿã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«ä¾å­˜ã›ãšã€å¤šæ§˜ãªã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®åŠ¹æœçš„ãªçµ„ã¿åˆã‚ã›ã‚’è‡ªå‹•çš„ã«ç™ºè¦‹ã™ã‚‹ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€æ—¥æœ¬èªã®LLMã¨æ•°å­¦æ¨è«–èƒ½åŠ›ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ãªã©ã€ç•°ãªã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³é–“ã®çµ±åˆã‚’å®¹æ˜“ã«ã—ã€æ—¥æœ¬èªVLMã®æ€§èƒ½å‘ä¸Šã«ã‚‚è²¢çŒ®ã€‚ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¸ã®è²¢çŒ®ã¨è‡ªå‹•ãƒ¢ãƒ‡ãƒ«æ§‹æˆã®æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ å°å…¥ã«ã‚ˆã‚Šã€åŸºç›¤ãƒ¢ãƒ‡ãƒ«é–‹ç™ºã«ãŠã‘ã‚‹åŠ¹ç‡çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¨¡ç´¢ã€‚</span>
<span class="snippet"><span>Comment</span>è¤‡æ•°ã®LLMã‚’èåˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ã®è©±ã€‚æ—¥æœ¬èªLLMã¨è‹±èªã®æ•°å­¦LLNã‚’ãƒãƒ¼ã‚¸ã•ã›ã‚‹ã“ã¨ã§æ—¥æœ¬èªã®æ•°å­¦æ€§èƒ½ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ãŸã‚Šã€LLMã¨VLMã‚’èåˆã—ãŸã‚Šã™ã‚‹ã“ã¨ã§ã€æ—¥æœ¬ã«ã—ã‹å­˜åœ¨ã—ãªã„æ¦‚å¿µã®ç”»åƒã‚‚ã€ãã¡ã‚“ã¨å›ç­”ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚

<br>



<br>

è‘—è€…ã‚¹ãƒ©ã‚¤ãƒ‰ã«ã‚ˆã‚‹ã¨ã€å¾“æ¥ã®ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ã«ã¯base modelãŒåŒä¸€ã§ãªã„ã¨ã†ã¾ãã„ã‹ãªã‹ã£ãŸã‚Šï¼ˆé‡ã¿ã®ç·šå‹çµåˆã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ï¼‰ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¢—æ¸›ã—ãŸã‚Šï¼ˆè¤‡æ•°LLMã®Layerã‚’é‡ã¿ã¯å¼„ã‚‰ãšå†é…ç½®ã™ã‚‹ï¼‰ã€‚ã¾ãŸæ—¥æœ¬èªLLMã«å¯¾ã—ã¦ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ã‚’å®Ÿæ–½ã—ã‚ˆã†ã¨ã™ã‚‹ã¨ã€ãƒãƒ¼ã‚¸å…ƒã®LLMãŒå°‘ãªã‹ã£ãŸã‚Šã€åºƒç¯„å›²ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ‰±ã†ã¨ãƒãƒ¼ã‚¸ãŒã†ã¾ãã„ã‹ãªã„ã€ã¨ã„ã£ãŸèª²é¡ŒãŒã‚ã£ãŸã€‚æœ¬ç ”ç©¶ã§ã¯ã“ã‚Œã‚‰èª²é¡Œã‚’è§£æ±ºã§ãã‚‹ã€‚è‘—è€…ã«ã‚ˆã‚‹è³‡æ–™ï¼ˆNLPã‚³ãƒ­ã‚­ã‚¦ãƒ ï¼‰:

<br>

https://speakerdeck.com/iwiwi/17-nlpkorokiumu</span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/InstructionTuning.html">#InstructionTuning</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/SpeechProcessing.html">#SpeechProcessing</a>
<a class="button" href="articles/CVPR.html">#CVPR</a>
<a class="button" href="articles/Encoder-Decoder.html">#Encoder-Decoder</a>
<a class="button" href="articles/Robotics.html">#Robotics</a>
<span class="issue_date">Issue Date: 2023-12-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1202">Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision,   Language, Audio, and Action, Jiasen Lu+, N_A, CVPR'24</a>
<span class="snippet"><span>Summary</span>Unified-IO 2ã¯ã€æœ€åˆã®è‡ªå·±å›å¸°å‹ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ç”»åƒã€ãƒ†ã‚­ã‚¹ãƒˆã€éŸ³å£°ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç†è§£ã—ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’çµ±ä¸€ã™ã‚‹ãŸã‚ã«ã€å…±æœ‰ã®æ„å‘³ç©ºé–“ã«å…¥åŠ›ã¨å‡ºåŠ›ã‚’é…ç½®ã—ã€å˜ä¸€ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã§å‡¦ç†ã—ã¾ã™ã€‚ã•ã¾ã–ã¾ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ”¹å–„ã‚’ææ¡ˆã—ã€å¤§è¦æ¨¡ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªäº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚Unified-IO 2ã¯ã€GRITãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å«ã‚€35ä»¥ä¸Šã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span>ç”»åƒã€ãƒ†ã‚­ã‚¹ãƒˆã€éŸ³å£°ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç†è§£ã§ãã‚‹åˆã‚ã¦ã®autoregressive modelã€‚AllenAI<img src="https://github.com/user-attachments/assets/fa54f7bf-6689-4346-a1ca-031e4e5516ea" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/4282ffb0-18f1-40c9-b6d7-f004d03b8382" alt="image" loading="lazy">ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã«æ‹¡å¼µã—ãŸã“ã¨ã§ã€è¨“ç·´ãŒéå¸¸ã«ä¸å®‰å®šã«ãªã£ãŸãŸã‚ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä¸Šã§ã„ãã¤ã‹ã®å·¥å¤«ã‚’åŠ ãˆã¦ã„ã‚‹:

<br>



<br>

ãƒ»2D Rotary Embedding

<br>

  ãƒ»Positional Encodingã¨ã—ã¦RoPEã‚’æ¡ç”¨

<br>

  ãƒ»ç”»åƒã®ã‚ˆã†ãª2æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®å ´åˆã¯RoPEã‚’2æ¬¡å…ƒã«æ‹¡å¼µã™ã‚‹ã€‚å…·ä½“çš„ã«ã¯ã€ä½ç½®(i, j)ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ã¤ã„ã¦ã¯ã€Q, Kã®embeddingã‚’åŠåˆ†ã«åˆ†å‰²ã—ã¦ã€ãã‚Œãã‚Œã«å¯¾ã—ã¦ç‹¬ç«‹ã«i, jã®RoPE Embeddingã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§i, jåŒæ–¹ã®æƒ…å ±ã‚’çµ„ã¿è¾¼ã‚€ã€‚

<br>

ãƒ»QK Normalization

<br>

  ãƒ»image, audioã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§MHAã®logitsãŒéå¸¸ã«å¤§ãããªã‚Šatteetion weightãŒ0/1ã®æ¥µç«¯ãªå€¤ã‚’ã¨ã‚‹ã‚ˆã†ã«ãªã‚Šè¨“ç·´ã®ä¸å®‰å®šã•ã«ã¤ãªãŒã£ãŸã€‚ã“ã®ãŸã‚ã€dot product attentionã‚’é©ç”¨ã™ã‚‹å‰ã«LayerNormã‚’çµ„ã¿è¾¼ã‚“ã ã€‚

<br>

ãƒ»Scaled Cosine Attention

<br>

  ãƒ»Image Historyãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«ãŠã„ã¦å›ºå®šé•·ã®Embeddingã‚’å¾—ã‚‹ãŸã‚ã«Perceiver Resamplerã‚’æ‰±ã£ãŸã¦ã„ã‚‹ãŒã€ã“ã¡ã‚‰ã‚‚ä¸Šè¨˜ã¨åŒæ§˜ã«Attentionã®logitsãŒæ¥µç«¯ã«å¤§ãããªã£ãŸãŸã‚ã€cosineé¡ä¼¼åº¦ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸScaled Cosine Attention 2259 ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€å¤§å¹…ã«è¨“ç·´ã®å®‰å®šæ€§ãŒæ”¹å–„ã•ã‚ŒãŸã€‚

<br>

ãƒ»ãã®ä»–

<br>

  ãƒ»attention logitsã«ã¯fp32ã‚’é©ç”¨

<br>

  ãƒ»äº‹å‰å­¦ç¿’ã•ã‚ŒãŸViTã¨ASTã‚’åŒæ™‚ã«æ›´æ–°ã™ã‚‹ã¨ä¸å®‰å®šã«ã¤ãªãŒã£ãŸãŸã‚ã€äº‹å‰å­¦ç¿’ã®æ®µéšã§ã¯freezeã—ã€instruction tuningã®æœ€å¾Œã«finetuningã‚’å®Ÿæ–½

<br>



<br>

<img src="https://github.com/user-attachments/assets/74c8fa3a-8fb5-4785-8dd3-6a8cf3c7cfeb" alt="image" loading="lazy">ç›®çš„é–¢æ•°ã¨ã—ã¦ã¯ã€Mixture of Denoisers (1424)ã«ç€æƒ³ã‚’å¾—ã¦ã€Multimodal Mixture of Denoisersã‚’ææ¡ˆã€‚MoDã§ã¯ã€

<br>

ãƒ»\[R\]: é€šå¸¸ã®span corruption (1--5 tokenç¨‹åº¦ã®spanã‚’maskã™ã‚‹)

<br>

ãƒ»\[S\]: causal language modeling (inputã‚’2ã¤ã®ã‚µãƒ–ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«åˆ†å‰²ã—ã€å‰æ–¹ã‹ã‚‰å¾Œæ–¹ã‚’äºˆæ¸¬ã™ã‚‹ã€‚å‰æ–¹éƒ¨åˆ†ã¯Bi-directionalã§ã‚‚å¯)

<br>

ãƒ»\[X\]: extreme span corruption (12&gt;=tokenç¨‹åº¦ã®spanã‚’maskã™ã‚‹)

<br>



<br>

ã®3ç¨®é¡ãŒææ¡ˆã•ã‚Œã¦ãŠã‚Šã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã”ã¨ã«ã“ã‚Œã‚‰ã‚’ä½¿ã„åˆ†ã‘ã‚‹:

<br>

ãƒ»text modality: UL2 (1424)ã‚’è¸è¥²

<br>

ãƒ»image, audioãŒtargetã®å ´åˆ: 2ã¤ã®é¡ä¼¼ã—ãŸãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’å®šç¾©ã—åˆ©ç”¨

<br>

  ãƒ»\[R\]: patchã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«x%ãƒã‚¹ã‚¯ã—re-constructã™ã‚‹

<br>

  ãƒ»\[S\]: inputã®targetã¨ã¯ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®ã¿ã®æƒ…å ±ã‹ã‚‰ã€targetãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’ç”Ÿæˆã™ã‚‹

<br>



<br>

è¨“ç·´æ™‚ã«ã¯ prefixã¨ã—ã¦modality token \[Text\], \[Image\], \[Audio\] ã¨paradigm token \[R\], \[S\], \[X\] ã‚’ã‚¿ã‚¹ã‚¯ã‚’æŒ‡ç¤ºã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦åˆ©ç”¨ã—ã¦ã„ã‚‹ã€‚ã¾ãŸã€image, audioã®ãƒã‚¹ã‚¯éƒ¨åˆ†ã®denoisingã‚’autoregressive modelã§å®Ÿæ–½ã™ã‚‹éš›ã«ã¯æ™®é€šã«ã‚„ã‚‹ã¨decoderå´ã§ãƒªãƒ¼ã‚¯ãŒç™ºç”Ÿã™ã‚‹(a)ã€‚ã“ã‚Œã‚’é˜²ãã«ã¯ã€Encoderå´ã§ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã€Decoderå´ã§teacher-forcingã™ã‚‹éš›ã«ã®å…¨ã¦ãƒã‚¹ã‚¯ã™ã‚‹æ–¹æ³•(b)ãŒã‚ã‚‹ãŒã€ã“ã®å ´åˆã€ç”Ÿæˆã‚¿ã‚¹ã‚¯ã¨denoisingã‚¿ã‚¹ã‚¯ãŒç›¸äº’ã«å¹²æ¸‰ã—ã¦ã—ã¾ã„ã†ã¾ãå­¦ç¿’ã§ããªããªã£ã¦ã—ã¾ã†ï¼ˆç”Ÿæˆã‚¿ã‚¹ã‚¯ã§ã¯é€šå¸¸Decoderã®inputã¨ã—ã¦[mask]ãŒå…¥åŠ›ã•ã‚Œæ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã™ã‚‹ã€ã¨ã„ã£ãŸã“ã¨ã¯èµ·ããˆãªã„ãŒã€æ„šç›´ã«(b)ã‚’ã‚„ã‚‹ã¨ãã†ãªã£ã¦ã—ã¾ã†ï¼‰ã€‚ã®ã§ã€(c)ã«ç¤ºã—ãŸã‚ˆã†ã«ã€ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’inputã¨ã—ã¦ç”Ÿæˆã—ãªã‘ã‚Œã°ãªã‚‰ãªã„æ™‚ã ã‘ã€ãƒã‚¹ã‚¯ã‚’è§£é™¤ã—ã¦decoderå´ã«inputã™ã‚‹ã€ã¨ã„ã†æ–¹æ³• (Dynamic Masking) ã§ã“ã®å•é¡Œã«å¯¾å‡¦ã—ã¦ã„ã‚‹ã€‚

<br>

<img width="597" height="394" alt="Image" src="https://github.com/user-attachments/assets/0dba8d5d-0c93-4c56-852b-fce9869428e7"></span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/ImageSegmentation.html">#ImageSegmentation</a>
<a class="button" href="articles/FoundationModel.html">#FoundationModel</a>
<span class="issue_date">Issue Date: 2023-04-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/600">Segment Anything in Medical Images, Jun Ma+, N_A, Nature Communications'24</a>
<span class="snippet"><span>Summary</span>æœ¬ç ”ç©¶ã§ã¯ã€è‡ªç„¶ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã«é©æ–°çš„ãªæ‰‹æ³•ã§ã‚ã‚‹Segment anything model (SAM)ã‚’åŒ»ç™‚ç”»åƒã«æ‹¡å¼µã™ã‚‹ãŸã‚ã®MedSAMã‚’ææ¡ˆã—ã€æ§˜ã€…ãªåŒ»ç™‚ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®æ±ç”¨ãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚MedSAMã¯ã€å¤§è¦æ¨¡ãªåŒ»ç™‚ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦é–‹ç™ºã•ã‚Œã€SAMã‚’ä¸€èˆ¬çš„ãªåŒ»ç™‚ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã«é©å¿œã™ã‚‹ãŸã‚ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã‚’é–‹ç™ºã—ã¾ã—ãŸã€‚21ã®3Dã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã¨9ã®2Dã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹åŒ…æ‹¬çš„ãªå®Ÿé¨“ã«ã‚ˆã‚Šã€MedSAMã¯ã€å¹³å‡Diceé¡ä¼¼ä¿‚æ•°ï¼ˆDSCï¼‰ãŒãã‚Œãã‚Œ22.5ï¼…ã¨17.6ï¼…ã§ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®SAMãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚ã‚³ãƒ¼ãƒ‰ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯ã€\url{https://github.com/bowang-lab/MedSAM}ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span>SAMã®æ€§èƒ½ã¯åŒ»ç™‚ç”»åƒã«å¯¾ã—ã¦ã¯é™å®šçš„ã ã£ãŸãŸã‚ã€11ã®ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«å¯¾ã—ã¦200kã®ãƒã‚¹ã‚¯ã‚’ã—ãŸåŒ»ç™‚ç”»åƒã‚’ç”¨æ„ã—finetuningã—ãŸMedSAMã«ã‚ˆã£ã¦ã€åŒ»ç™‚ç”»åƒã®ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®æ€§èƒ½ã‚’å¤§å¹…ã«å‘ä¸Šã€‚

<br>

ã‚³ãƒ¼ãƒ‰ã¨ãƒ¢ãƒ‡ãƒ«ã¯publicly available<img src="https://github.com/user-attachments/assets/ea394adc-b1da-4764-bf29-534323bfc443" alt="image" loading="lazy"></span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/SpeechProcessing.html">#SpeechProcessing</a>
<a class="button" href="articles/AAAI.html">#AAAI</a>
<span class="issue_date">Issue Date: 2023-04-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/547">AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head, AAAI'24</a>
<span class="snippet"><span>Summary</span>AudioGPTã¯ã€è¤‡é›‘ãªéŸ³å£°æƒ…å ±ã‚’å‡¦ç†ã—ã€éŸ³å£°å¯¾è©±ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIã‚·ã‚¹ãƒ†ãƒ ã§ã‚ã‚‹ã€‚åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã¨ASRã€TTSã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’çµ„ã¿åˆã‚ã›ã€éŸ³å£°ã€éŸ³æ¥½ã€ãƒˆãƒ¼ã‚­ãƒ³ã‚°ãƒ˜ãƒƒãƒ‰ã®ç†è§£ã¨ç”Ÿæˆã‚’è¡Œã†ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€AudioGPTãŒå¤šæ§˜ãªã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å‰µé€ ã‚’å®¹æ˜“ã«ã™ã‚‹èƒ½åŠ›ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span>text, audio, imageã¨ã„ã£ãŸãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªpromptã‹ã‚‰ã€audioã«é–¢ã™ã‚‹æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã‚’å®Ÿç¾ã§ãã‚‹ã‚·ã‚¹ãƒ†ãƒ ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’jointã§å­¦ç¿’ã—ãŸã¨ã„ã†ã‚ã‘ã§ã¯ãªãã€è‰²ã€…ãªãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›ã¦ã‚¿ã‚¹ã‚¯ã‚’å®Ÿç¾ã—ã¦ã„ã‚‹ã£ã½ã„

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/234739859-f833706a-6040-484a-b015-553a719484d7.png" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<a class="button" href="articles/ICCV.html">#ICCV</a>
<span class="issue_date">Issue Date: 2025-06-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2111">[Paper Note] Sigmoid Loss for Language Image Pre-Training, Xiaohua Zhai+, ICCV'23</a>
<span class="snippet"><span>Summary</span>ã‚·ãƒ³ãƒ—ãƒ«ãªãƒšã‚¢ãƒ¯ã‚¤ã‚ºã‚·ã‚°ãƒ¢ã‚¤ãƒ‰æå¤±ï¼ˆSigLIPï¼‰ã‚’ææ¡ˆã—ã€ç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ã«åŸºã¥ãè¨€èª-ç”»åƒäº‹å‰å­¦ç¿’ã‚’æ”¹å–„ã€‚ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰æå¤±ã¯ãƒãƒƒãƒã‚µã‚¤ã‚ºã®æ‹¡å¤§ã‚’å¯èƒ½ã«ã—ã€å°ã•ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã‚‚æ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ã€‚SigLiTãƒ¢ãƒ‡ãƒ«ã¯84.5%ã®ImageNetã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆç²¾åº¦ã‚’é”æˆã€‚ãƒãƒƒãƒã‚µã‚¤ã‚ºã®å½±éŸ¿ã‚’ç ”ç©¶ã—ã€32kãŒåˆç†çš„ãªã‚µã‚¤ã‚ºã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã€‚ãƒ¢ãƒ‡ãƒ«ã¯å…¬é–‹ã•ã‚Œã€ã•ã‚‰ãªã‚‹ç ”ç©¶ã®ä¿ƒé€²ã‚’æœŸå¾…ã€‚</span>
<span class="snippet"><span>Comment</span>SigLIPè«–æ–‡</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/SpeechProcessing.html">#SpeechProcessing</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<a class="button" href="articles/Normalization.html">#Normalization</a>
<span class="issue_date">Issue Date: 2025-04-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1899">Foundation Transformers, Hongyu Wang+, PMLR'23</a>
<span class="snippet"><span>Summary</span>è¨€èªã€è¦–è¦šã€éŸ³å£°ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã«ãŠã‘ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®åæŸãŒé€²ã‚€ä¸­ã€ç•°ãªã‚‹å®Ÿè£…ã®ã€ŒTransformersã€ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚æ±ç”¨ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ãŸã‚ã«ã€å®‰å®šæ€§ã‚’æŒã¤Foundation Transformerã®é–‹ç™ºãŒæå”±ã•ã‚Œã€Magnetoã¨ã„ã†æ–°ã—ã„Transformerå¤‰ç¨®ãŒç´¹ä»‹ã•ã‚Œã‚‹ã€‚Sub-LayerNormã¨ç†è«–ã«åŸºã¥ãåˆæœŸåŒ–æˆ¦ç•¥ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ã•ã¾ã–ã¾ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ãŠã„ã¦å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨å®‰å®šæ€§ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><img src="https://github.com/user-attachments/assets/2847f982-3266-4394-9920-01d9977e505e" alt="image" loading="lazy">é–¢é€£:

<br>

ãƒ»1900</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/ImageSegmentation.html">#ImageSegmentation</a>
<a class="button" href="articles/FoundationModel.html">#FoundationModel</a>
<span class="issue_date">Issue Date: 2025-04-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1885">Segment Anything, Alexander Kirillov+, arXiv'23</a>
<span class="snippet"><span>Summary</span>Segment Anything (SA)ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã€ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ææ¡ˆã—ã€1å„„ä»¥ä¸Šã®ãƒã‚¹ã‚¯ã‚’å«ã‚€1,100ä¸‡ã®ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼å°Šé‡ã—ãŸç”»åƒã‹ã‚‰ãªã‚‹æœ€å¤§ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã¯ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã§æ–°ã—ã„ç”»åƒåˆ†å¸ƒã‚„ã‚¿ã‚¹ã‚¯ã«é©å¿œã§ãã€è©•ä¾¡ã®çµæœã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ€§èƒ½ãŒé«˜ãã€å¾“æ¥ã®ç›£è¦–ã•ã‚ŒãŸçµæœã‚’ä¸Šå›ã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ã€‚SAMã¨SA-1Bãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ç ”ç©¶ä¿ƒé€²ã®ãŸã‚ã«å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span>SAMè«–æ–‡</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<span class="issue_date">Issue Date: 2025-04-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1881">PaLI-3 Vision Language Models: Smaller, Faster, Stronger, Xi Chen+, arXiv'23</a>
<span class="snippet"><span>Summary</span>PaLI-3ã¯ã€å¾“æ¥ã®ãƒ¢ãƒ‡ãƒ«ã«æ¯”ã¹ã¦10å€å°å‹ã§é«˜é€Ÿãªè¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰ã§ã‚ã‚Šã€ç‰¹ã«ãƒ­ãƒ¼ã‚«ãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚„è¦–è¦šçš„ãƒ†ã‚­ã‚¹ãƒˆç†è§£ã«ãŠã„ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã€‚SigLIPãƒ™ãƒ¼ã‚¹ã®PaLIã¯ã€20å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã•ã‚Œã€å¤šè¨€èªã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«æ¤œç´¢ã§æ–°ãŸãªæœ€å…ˆç«¯ã‚’é”æˆã€‚50å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®PaLI-3ã¯ã€VLMã®ç ”ç©¶ã‚’å†ç‡ƒã•ã›ã‚‹ã“ã¨ã‚’æœŸå¾…ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span>OpenReview:https://openreview.net/forum?id=JpyWPfzu0b

<br>



<br>

å®Ÿé¨“çš„ã«ç´ æ™´ã‚‰ã—ã„æ€§èƒ½ãŒå®Ÿç¾ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã¯èªã‚ã‚‰ã‚Œã¤ã¤ã‚‚

<br>

ãƒ»æ¯”è¼ƒå¯¾è±¡ãŒSigLIPã®ã¿ã§ã‚ˆã‚Šåºƒç¯„ãªæ¯”è¼ƒå®Ÿé¨“ã¨åˆ†æãŒå¿…è¦ãªã“ã¨

<br>

ãƒ»Backboneãƒ¢ãƒ‡ãƒ«ã‚’Contrastive Learningã™ã‚‹ã“ã¨è‡ªä½“ã®æœ‰ç”¨æ€§ã¯æ—¢ã«çŸ¥ã‚‰ã‚Œã¦ãŠã‚Šã€æ–°è¦æ€§ã«ä¹ã—ã„ã“ã¨

<br>



<br>

ã¨ã—ã¦ICLR'24ã«Rejectã•ã‚Œã¦ã„ã‚‹</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Zero/FewShotPrompting.html">#Zero/FewShotPrompting</a>
<a class="button" href="articles/Self-SupervisedLearning.html">#Self-SupervisedLearning</a>
<span class="issue_date">Issue Date: 2024-10-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1448">SINC: Self-Supervised In-Context Learning for Vision-Language Tasks, Yi-Syuan Chen+, N_A, ICCV'23</a>
<span class="snippet"><span>Summary</span>è‡ªå·±æ•™å¸«ã‚ã‚Šæ–‡è„ˆå†…å­¦ç¿’ï¼ˆSINCï¼‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«ä¾å­˜ã›ãšã«æ–‡è„ˆå†…å­¦ç¿’ã‚’å®Ÿç¾ã€‚ç‰¹åˆ¥ã«èª¿æ•´ã•ã‚ŒãŸãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”¨ã„ãŸãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ãŒã€è¦–è¦šã¨è¨€èªã®ã‚¿ã‚¹ã‚¯ã§å°‘æ•°ã‚·ãƒ§ãƒƒãƒˆè¨­å®šã«ãŠã„ã¦å‹¾é…ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã€‚SINCã¯æ–‡è„ˆå†…å­¦ç¿’ã®åˆ©ç‚¹ã‚’æ¢æ±‚ã—ã€é‡è¦ãªè¦ç´ ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ã€‚</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/GenerativeAI.html">#GenerativeAI</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<span class="issue_date">Issue Date: 2023-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1169">SEINE: Short-to-Long Video Diffusion Model for Generative Transition and  Prediction, Xinyuan Chen+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>æœ¬ç ”ç©¶ã§ã¯ã€ãƒ“ãƒ‡ã‚ªç”Ÿæˆã«ãŠã„ã¦é€£ç¶šã—ãŸé•·ã„ãƒ“ãƒ‡ã‚ªã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ã‚¸ã‚§ãƒãƒ¬ãƒ¼ãƒ†ã‚£ãƒ–ãªãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³ã¨äºˆæ¸¬ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸãƒ¢ãƒ‡ãƒ«SEINEã‚’ææ¡ˆã™ã‚‹ã€‚SEINEã¯ãƒ†ã‚­ã‚¹ãƒˆã®èª¬æ˜ã«åŸºã¥ã„ã¦ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã—ã€ä¸€è²«æ€§ã¨è¦–è¦šçš„å“è³ªã‚’ç¢ºä¿ã—ãŸé•·ã„ãƒ“ãƒ‡ã‚ªã‚’ç”Ÿæˆã™ã‚‹ã€‚ã•ã‚‰ã«ã€ææ¡ˆæ‰‹æ³•ã¯ä»–ã®ã‚¿ã‚¹ã‚¯ã«ã‚‚æ‹¡å¼µå¯èƒ½ã§ã‚ã‚Šã€å¾¹åº•çš„ãªå®Ÿé¨“ã«ã‚ˆã‚Šãã®æœ‰åŠ¹æ€§ãŒæ¤œè¨¼ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span>https://huggingface.co/spaces/Vchitect/SEINE

<br>



<br>

ç”»åƒ + ãƒ†ã‚­ã‚¹ãƒˆpromptã§ã€å‹•ç”»ã‚’ç”Ÿæˆã™ã‚‹ãƒ‡ãƒ¢</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/AutomaticPromptEngineering.html">#AutomaticPromptEngineering</a>
<span class="issue_date">Issue Date: 2023-11-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1161">NeuroPrompts: An Adaptive Framework to Optimize Prompts for  Text-to-Image Generation, Shachar Rosenman+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>æœ¬ç ”ç©¶ã§ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å“è³ªã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®é©å¿œå‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯NeuroPromptsã‚’ææ¡ˆã—ã¾ã™ã€‚ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€äº‹å‰å­¦ç¿’ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦åˆ¶ç´„ä»˜ããƒ†ã‚­ã‚¹ãƒˆãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¡Œã„ã€äººé–“ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒç”Ÿæˆã™ã‚‹ã‚‚ã®ã«é¡ä¼¼ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é«˜å“è³ªãªãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®ç”ŸæˆãŒå¯èƒ½ã¨ãªã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã‚¹ã‚¿ã‚¤ãƒ«ã®ç‰¹å¾´ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚ã¾ãŸã€å¤§è¦æ¨¡ãªäººé–“ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ãŸå®Ÿé¨“ã«ã‚ˆã‚Šã€å½“ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒè‡ªå‹•çš„ã«å“è³ªã®é«˜ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã€å„ªã‚ŒãŸç”»åƒå“è³ªã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã—ãŸã€‚</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/ImageSegmentation.html">#ImageSegmentation</a>
<a class="button" href="articles/Prompting.html">#Prompting</a>
<a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2023-11-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1160">Visual In-Context Prompting, Feng Li+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>æœ¬ç ”ç©¶ã§ã¯ã€ãƒ“ã‚¸ãƒ§ãƒ³é ˜åŸŸã«ãŠã‘ã‚‹æ±ç”¨çš„ãªãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã¾ã™ã€‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½¿ç”¨ã—ã€ã•ã¾ã–ã¾ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’é–‹ç™ºã—ã¾ã—ãŸã€‚ã•ã‚‰ã«ã€ä»»æ„ã®æ•°ã®å‚ç…§ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å—ã‘å–ã‚‹ã‚ˆã†ã«æ‹¡å¼µã—ã¾ã—ãŸã€‚å®Ÿé¨“çµæœã‹ã‚‰ã€ææ¡ˆæ‰‹æ³•ãŒéå‡¡ãªå‚ç…§ãŠã‚ˆã³ä¸€èˆ¬çš„ãªã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³èƒ½åŠ›ã‚’å¼•ãå‡ºã—ã€ç«¶äº‰åŠ›ã®ã‚ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã™ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span>Image Segmentationã«ã¯ã€ãƒ¦ãƒ¼ã‚¶ãŒä¸ãˆãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å…±é€šã®ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’æŒã¤ã™ã¹ã¦ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹ã‚¿ã‚¹ã‚¯ã¨ã€ãƒ¦ãƒ¼ã‚¶ã®å…¥åŠ›ã®ç‰¹å®šã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹ã‚¿ã‚¹ã‚¯ãŒã‚ã‚‹ã€‚å¾“æ¥ã¯å€‹åˆ¥ã®ã‚¿ã‚¹ã‚¯ã”ã¨ã«ã€ç‰¹å®šã®å…¥åŠ›æ–¹æ³•ï¼ˆVisual Prompt, Image Promptï¼‰ã‚’å‰æã¨ã—ãŸæ‰‹æ³•ã‚„ã€å€‹ã€…ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿæ–½ã§ãã‚‹ãŒIn-Context Promptã—ã‹ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„æ‰‹æ³•ã—ã‹ãªã‹ã£ãŸãŒã€ã“ã®ç ”ç©¶ã§ã¯ã€Visual Prompt, Image Prompt, In-Context Promptã‚’ãã‚Œãã‚Œã‚µãƒãƒ¼ãƒˆã—ä¸¡ã‚¿ã‚¹ã‚¯ã‚’å®Ÿæ–½ã§ãã‚‹ã¨ã„ã†ä½ç½®ä»˜ã‘ã®æ¨¡æ§˜ã€‚ã¾ãŸã€ææ¡ˆæ‰‹æ³•ã§ã¯ã‚¹ãƒˆãƒ­ãƒ¼ã‚¯ã€ç‚¹ã€ãƒœãƒƒã‚¯ã‚¹ã¨ã„ã£ãŸãƒ¦ãƒ¼ã‚¶ã®ç”»åƒã«å¯¾ã™ã‚‹æç”»ã«åŸºã¥ãPromptingã‚’ã‚µãƒãƒ¼ãƒˆã—ã€Promptingã«ãŠã‘ã‚‹å‚ç…§ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°ã‚‚ä»»æ„ã®æ•°æŒ‡å®šã§ãã‚‹ã¨ã®ã“ã¨ã€‚

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/f5da3d7b-68aa-4120-a37c-7c42be1704f8" alt="image" loading="lazy">

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/e6992dde-e10b-41cb-a190-eb78376bef31" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LayoutGeneration.html">#LayoutGeneration</a>
<span class="issue_date">Issue Date: 2023-11-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1133">LayoutPrompter: Awaken the Design Ability of Large Language Models, Jiawei Lin+, N_A, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>LayoutPrompterã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã‚’ä½¿ç”¨ã—ã¦æ¡ä»¶ä»˜ãã®ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆç”Ÿæˆã‚’è¡Œã†æ‰‹æ³•ã§ã‚ã‚Šã€å…¥åŠ›-å‡ºåŠ›ã®ã‚·ãƒªã‚¢ãƒ«åŒ–ã€å‹•çš„ãªæ¨¡ç¯„çš„é¸æŠã€ãŠã‚ˆã³ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®3ã¤ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚LayoutPrompterã¯ã€æ—¢å­˜ã®æ‰‹æ³•ã¨ç«¶åˆã—ãŸã‚Šä¸Šå›ã£ãŸã‚Šã™ã‚‹æ€§èƒ½ã‚’æŒã¡ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„å¾®èª¿æ•´ãªã—ã§ä½¿ç”¨ã§ãã‚‹æ±ç”¨æ€§ã®ã‚ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã‚ã‚‹ã“ã¨ãŒå®Ÿé¨“çµæœã‹ã‚‰ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚ã¾ãŸã€ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã«ã‚‚å„ªã‚Œã¦ãŠã‚Šã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ˆã‚Šã‚‚æœ‰æ„ã«å„ªã‚Œã¦ã„ã‚‹ã“ã¨ã‚‚ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€https://github.com/microsoft/LayoutGeneration/tree/main/LayoutPrompterã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚</span>
<span class="snippet"><span>Comment</span>Conditional Graphic Layout Generation</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/MultitaskLearning.html">#MultitaskLearning</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/FoundationModel.html">#FoundationModel</a>
<span class="issue_date">Issue Date: 2023-11-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1127">Florence-2: Advancing a Unified Representation for a Variety of Vision  Tasks, Bin Xiao+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>Florence-2ã¯ã€ãƒ“ã‚¸ãƒ§ãƒ³åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ã•ã¾ã–ã¾ãªãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã«å¯¾å¿œã™ã‚‹ãŸã‚ã®çµ±ä¸€ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ™ãƒ¼ã‚¹ã®è¡¨ç¾ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å—ã‘å–ã‚Šã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡ºã€ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã€ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãªã©ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã€ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§çµæœã‚’ç”Ÿæˆã—ã¾ã™ã€‚ã¾ãŸã€FLD-5Bã¨ã„ã†å¤§è¦æ¨¡ãªæ³¨é‡ˆä»˜ããƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚é–‹ç™ºã•ã‚Œã¾ã—ãŸã€‚Florence-2ã¯ã€å¤šç›®çš„ã‹ã¤åŒ…æ‹¬çš„ãªãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ„ãƒ¼ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ§‹é€ ã‚’æ¡ç”¨ã—ã¦ãŠã‚Šã€å‰ä¾‹ã®ãªã„ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãŠã‚ˆã³ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®èƒ½åŠ›ã‚’æŒã¤å¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</span>
<span class="snippet"><span>Comment</span>Vison Foundation Modelã€‚Spatialãªéšå±¤æ§‹é€ ã‚„ã€Semanticã‚’æ‰ãˆã‚‰ã‚Œã‚‹ã‚ˆã†ã«è¨“ç·´ã€‚Image/Prompt Encoderã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã€outputã¯text + location informationã¨ãªã‚‹ã€‚

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/9fbfba62-190f-46eb-a893-5ebe76dda030" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/f7497161-6b9a-4adc-aa6b-53debe1e9318" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<span class="issue_date">Issue Date: 2023-10-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1093">Exploring OCR Capabilities of GPT-4Vï¼ˆisionï¼‰ : A Quantitative and  In-depth Evaluation, Yongxin Shi+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>ã“ã®è«–æ–‡ã§ã¯ã€GPT-4Vã¨ã„ã†å¤§è¦æ¨¡ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å…‰å­¦æ–‡å­—èªè­˜ï¼ˆOCRï¼‰èƒ½åŠ›ã‚’è©•ä¾¡ã—ã¾ã™ã€‚ã•ã¾ã–ã¾ãªOCRã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã—ã€ãƒ©ãƒ†ãƒ³æ–‡å­—ã®èªè­˜ã¨ç†è§£ã«ãŠã„ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ä¸€æ–¹ã€å¤šè¨€èªã‚„è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã«ã¯è‹¦æˆ¦ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚ã“ã‚Œã«åŸºã¥ã„ã¦ã€å°‚é–€ã®OCRãƒ¢ãƒ‡ãƒ«ã®å¿…è¦æ€§ã‚„GPT-4Vã‚’æ´»ç”¨ã™ã‚‹æˆ¦ç•¥ã«ã¤ã„ã¦ã‚‚æ¤œè¨ã—ã¾ã™ã€‚ã“ã®ç ”ç©¶ã¯ã€å°†æ¥ã®LMMã‚’ç”¨ã„ãŸOCRã®ç ”ç©¶ã«å½¹ç«‹ã¤ã‚‚ã®ã§ã™ã€‚è©•ä¾¡ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨çµæœã¯ã€GitHubã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚</span>
<span class="snippet"><span>Comment</span>GPT4-Vã‚’ã•ã¾ã–ã¾ãªOCRã‚¿ã‚¹ã‚¯ã€Œæ‰‹æ›¸ãã€æ•°å¼ã€ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ èªè­˜ç­‰ã‚’å«ã‚€ï¼‰ã§æ€§èƒ½æ¤œè¨¼ã—ãŸç ”ç©¶ã€‚

<br>

MLT19ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã£ãŸè©•ä¾¡ã§ã¯ã€æ—¥æœ¬èªã®æ€§èƒ½ã¯éå¸¸ã«ä½ãã€è‹±èªã¨ãƒ•ãƒ©ãƒ³ã‚¹èªãŒæ€§èƒ½é«˜ã„ã€‚æ‰‹æ›¸ãæ–‡å­—èªè­˜ã§ã¯è‹±èªã¨ä¸­å›½èªã§ã®ã¿è©•ä¾¡ã€‚

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/c433b921-c527-441f-8925-00f4ac5fc6c3" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/QuestionAnswering.html">#QuestionAnswering</a>
<span class="issue_date">Issue Date: 2023-10-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1068">Improved Baselines with Visual Instruction Tuning, Haotian Liu+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>LLaVAã¯ã€ãƒ“ã‚¸ãƒ§ãƒ³ã¨è¨€èªã®ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚³ãƒã‚¯ã‚¿ã§ã‚ã‚Šã€ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ãŒé«˜ãå¼·åŠ›ãªæ€§èƒ½ã‚’æŒã¤ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚CLIP-ViT-L-336pxã‚’ä½¿ç”¨ã—ã€å­¦è¡“ã‚¿ã‚¹ã‚¯æŒ‡å‘ã®VQAãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€11ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ç¢ºç«‹ã—ã¾ã—ãŸã€‚13Bã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ã‚ãšã‹120ä¸‡ã®å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã€1æ—¥ã§å®Œå…¨ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’çµ‚ãˆã¾ã™ã€‚ã‚³ãƒ¼ãƒ‰ã¨ãƒ¢ãƒ‡ãƒ«ã¯å…¬é–‹ã•ã‚Œã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span>ç”»åƒåˆ†æãŒå¯èƒ½ãªã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLMã¨ã®ã“ã¨ã€‚Overview

<br>



<br>

ç”»åƒç”Ÿæˆã‚’ã§ãã‚‹ã‚ã‘ã§ã¯ãªãã€inputã¨ã—ã¦ç”»åƒã‚’æ‰±ãˆã‚‹ã®ã¿ã€‚

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/8d0382b0-8c2b-438d-8de8-ee451f5e2649" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/FoundationModel.html">#FoundationModel</a>
<span class="issue_date">Issue Date: 2023-08-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/914">Foundational Models Defining a New Era in Vision: A Survey and Outlook, Muhammad Awais+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>æœ¬ç ”ç©¶ã§ã¯ã€è¦–è¦šã‚·ã‚¹ãƒ†ãƒ ã®åŸºç¤ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦åŒ…æ‹¬çš„ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æä¾›ã—ã¾ã™ã€‚ã“ã‚Œã«ã¯ã€ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’çµ„ã¿åˆã‚ã›ã‚‹ãŸã‚ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆã‚„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç›®æ¨™ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãªã©ãŒå«ã¾ã‚Œã¾ã™ã€‚ã¾ãŸã€åŸºç¤ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚„èª²é¡Œã€æœ€è¿‘ã®ç™ºå±•ã«ã¤ã„ã¦ã‚‚è­°è«–ã—ã¾ã™ã€‚è©³ç´°ãªãƒªã‚¹ãƒˆã¯ã€\url{https://github.com/awaisrauf/Awesome-CV-Foundational-Models}ã§å…¥æ‰‹ã§ãã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span>CVã«ãŠã‘ã‚‹foundation modelã®surveyã€‚æ®‹ã•ã‚ŒãŸãƒãƒ£ãƒ¬ãƒ³ã‚¸ã¨ç ”ç©¶ã®æ–¹å‘æ€§ãŒè­°è«–ã•ã‚Œã¦ã„ã‚‹</span>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<span class="issue_date">Issue Date: 2023-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/891">InfoMetIC: An Informative Metric for Reference-free Image Caption Evaluation, ACL'23</a>
<span class="snippet"><span>Summary</span>è‡ªå‹•ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®è©•ä¾¡ã«ã¯ã€æƒ…å ±è±Šã‹ãªãƒ¡ãƒˆãƒªãƒƒã‚¯ï¼ˆInfoMetICï¼‰ãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®èª¤ã‚Šã‚„æ¬ è½ã—ãŸæƒ…å ±ã‚’è©³ç´°ã«ç‰¹å®šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚InfoMetICã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã®ç²¾åº¦ã‚¹ã‚³ã‚¢ã€ãƒ“ã‚¸ãƒ§ãƒ³ã®å†ç¾ã‚¹ã‚³ã‚¢ã€ãŠã‚ˆã³å…¨ä½“ã®å“è³ªã‚¹ã‚³ã‚¢ã‚’æä¾›ã—ã€äººé–“ã®åˆ¤æ–­ã¨ã®ç›¸é–¢ã‚‚é«˜ã„ã§ã™ã€‚ã¾ãŸã€ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚è©³ç´°ã¯GitHubã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/LLMAgent.html">#LLMAgent</a>
<span class="issue_date">Issue Date: 2023-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/883">Towards A Unified Agent with Foundation Models, Norman Di Palo+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>æœ¬ç ”ç©¶ã§ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ãƒ“ã‚¸ãƒ§ãƒ³è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«çµ„ã¿è¾¼ã¿ã€åŠ¹ç‡çš„ãªæ¢ç´¢ã‚„çµŒé¨“ãƒ‡ãƒ¼ã‚¿ã®å†åˆ©ç”¨ãªã©ã®èª²é¡Œã«å–ã‚Šçµ„ã‚€æ–¹æ³•ã‚’èª¿æŸ»ã—ã¾ã—ãŸã€‚ã‚¹ãƒ‘ãƒ¼ã‚¹ãªå ±é…¬ã®ãƒ­ãƒœãƒƒãƒˆæ“ä½œç’°å¢ƒã§ã®ãƒ†ã‚¹ãƒˆã«ãŠã„ã¦ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«æ¯”ã¹ã¦å¤§å¹…ãªæ€§èƒ½å‘ä¸Šã‚’å®Ÿè¨¼ã—ã€å­¦ç¿’æ¸ˆã¿ã®ã‚¹ã‚­ãƒ«ã‚’æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã®è§£æ±ºã‚„äººé–“ã®å°‚é–€å®¶ã®ãƒ“ãƒ‡ã‚ªã®æ¨¡å€£ã«æ´»ç”¨ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span>

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/aa40d0e3-9499-4804-9046-a9ad795c2d52" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Personalization.html">#Personalization</a>
<a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2023-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/878">FABRIC: Personalizing Diffusion Models with Iterative Feedback, Dimitri von RÃ¼tte+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>æœ¬ç ”ç©¶ã§ã¯ã€æ‹¡æ•£ãƒ™ãƒ¼ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®å¤‰æ›ãƒ¢ãƒ‡ãƒ«ã«äººé–“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’çµ„ã¿è¾¼ã‚€æˆ¦ç•¥ã‚’ææ¡ˆã™ã‚‹ã€‚è‡ªå·±æ³¨æ„å±¤ã‚’åˆ©ç”¨ã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒªãƒ¼ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã‚ã‚‹FABRICã‚’ææ¡ˆã—ã€ã•ã¾ã–ã¾ãªæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«é©ç”¨å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ã¾ãŸã€åŒ…æ‹¬çš„ãªè©•ä¾¡æ–¹æ³•ã‚’å°å…¥ã—ã€äººé–“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’çµ±åˆã—ãŸç”Ÿæˆãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®šé‡åŒ–ã™ã‚‹ãŸã‚ã®å …ç‰¢ãªãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æä¾›ã™ã‚‹ã€‚å¾¹åº•çš„ãªåˆ†æã«ã‚ˆã‚Šã€åå¾©çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®è¤‡æ•°ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã‚’é€šã˜ã¦ç”ŸæˆçµæœãŒæ”¹å–„ã•ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å€‹åˆ¥åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä½œæˆã‚„ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãªã©ã®é ˜åŸŸã«å¿œç”¨ãŒå¯èƒ½ã¨ãªã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span>upvote downvoteã‚’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã—ã€iterativeãªmannerã§Diffusionãƒ¢ãƒ‡ãƒ«ã®ç”Ÿæˆçµæœã‚’æ”¹å–„ã§ãã‚‹æ‰‹æ³•ã€‚å¤šãã®Diffusion based Modelã«å¯¾ã—ã¦é©ç”¨å¯èƒ½

<br>

ãƒ‡ãƒ¢: https://huggingface.co/spaces/dvruette/fabric</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/SpokenLanguageProcessing.html">#SpokenLanguageProcessing</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/SpeechProcessing.html">#SpeechProcessing</a>
<span class="issue_date">Issue Date: 2023-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/875">Meta-Transformer: A Unified Framework for Multimodal Learning, Yiyuan Zhang+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>æœ¬ç ”ç©¶ã§ã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å­¦ç¿’ã®ãŸã‚ã®Meta-Transformerã¨ã„ã†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®æƒ…å ±ã‚’å‡¦ç†ã—é–¢é€£ä»˜ã‘ã‚‹ãŸã‚ã®çµ±ä¸€ã•ã‚ŒãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚Meta-Transformerã¯ã€å¯¾å¿œã®ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦12ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ã§çµ±ä¸€ã•ã‚ŒãŸå­¦ç¿’ã‚’è¡Œã†ã“ã¨ãŒã§ãã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€ãƒã‚¤ãƒ³ãƒˆã‚¯ãƒ©ã‚¦ãƒ‰ã€éŸ³å£°ã€ãƒ“ãƒ‡ã‚ªãªã©ã®åŸºæœ¬çš„ãªãƒ‘ãƒ¼ã‚»ãƒ—ã‚·ãƒ§ãƒ³ã‹ã‚‰ã€Xç·šã€èµ¤å¤–ç·šã€é«˜åˆ†å…‰ã€IMUãªã©ã®å®Ÿç”¨çš„ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€ã‚°ãƒ©ãƒ•ã€è¡¨å½¢å¼ã€æ™‚ç³»åˆ—ãªã©ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°ã¾ã§ã€å¹…åºƒã„ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚Meta-Transformerã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ç”¨ã„ãŸçµ±ä¸€ã•ã‚ŒãŸãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ã‚¹ã®é–‹ç™ºã«å‘ã‘ãŸæœ‰æœ›ãªæœªæ¥ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span>12ç¨®é¡ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«å¯¾ã—ã¦å­¦ç¿’ã§ãã‚‹Transformerã‚’ææ¡ˆ

<br>

Dataã‚’sequenceã«tokenizeã—ã€unifiedã«featureã‚’encodingã—ã€ãã‚Œãã‚Œã®downstreamã‚¿ã‚¹ã‚¯ã§å­¦ç¿’

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/8734073a-573e-442e-8b9f-fed559199d56" alt="image" loading="lazy"></span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/Personalization.html">#Personalization</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/Conversation.html">#Conversation</a>
<span class="issue_date">Issue Date: 2023-07-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/839">MPCHAT: Towards Multimodal Persona-Grounded Conversation, ACL'23</a>
<span class="snippet"><span>Summary</span>æœ¬ç ”ç©¶ã§ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®ä¸¡æ–¹ã‚’ä½¿ç”¨ã—ã¦ãƒ‘ãƒ¼ã‚½ãƒŠã‚’æ‹¡å¼µã—ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªå¯¾è©±ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹MPCHATã‚’ææ¡ˆã—ã¾ã™ã€‚ã•ã‚‰ã«ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‘ãƒ¼ã‚½ãƒŠã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€å¿œç­”äºˆæ¸¬ã€ãƒ‘ãƒ¼ã‚½ãƒŠã®ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°äºˆæ¸¬ã€è©±è€…ã®è­˜åˆ¥ã¨ã„ã£ãŸã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’çµ±è¨ˆçš„ã«æœ‰æ„ã«æ”¹å–„ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚ã“ã®ç ”ç©¶ã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªå¯¾è©±ç†è§£ã«ãŠã„ã¦ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‘ãƒ¼ã‚½ãƒŠã®é‡è¦æ€§ã‚’å¼·èª¿ã—ã€MPCHATãŒé«˜å“è³ªãªãƒªã‚½ãƒ¼ã‚¹ã¨ã—ã¦å½¹ç«‹ã¤ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚</span>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/TabularData.html">#TabularData</a>
<a class="button" href="articles/TextToImageGeneration.html">#TextToImageGeneration</a>
<span class="issue_date">Issue Date: 2023-07-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/835">Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models, ACL'23</a>
<span class="snippet"><span>Summary</span>æœ¬ç ”ç©¶ã§ã¯ã€Visionï¼†Languageï¼ˆVï¼†Lï¼‰ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®çŸ¥è­˜ã®ä¿æŒæ–¹æ³•ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã«ã€ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ç”»åƒã®ç”Ÿæˆã‚¿ã‚¹ã‚¯ã‚’ææ¡ˆã—ã¾ã™ã€‚ã“ã®ã‚¿ã‚¹ã‚¯ã§ã¯ã€ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨é–¢é€£ã™ã‚‹ç”»åƒã®çŸ¥è­˜ã‚’å«ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ç¬¬ä¸€ã®éƒ¨åˆ†ã¨ã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã¨ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®é–¢é€£çŸ¥è­˜ã‚’å«ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ç¬¬äºŒã®éƒ¨åˆ†ãŒã‚ã‚Šã¾ã™ã€‚ææ¡ˆã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«ã€Wikipediaã®ç´„20ä¸‡ã®infoboxã‹ã‚‰WikiTIGãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã—ãŸã€‚æœ€å…ˆç«¯ã®Vï¼†Lãƒ¢ãƒ‡ãƒ«OFAã‚’ä½¿ç”¨ã—ã¦ã€ææ¡ˆã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚å®Ÿé¨“çµæœã¯ã€OFAãŒä¸€éƒ¨ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£çŸ¥è­˜ã‚’å¿˜ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚</span>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a>
<a class="button" href="articles/TextToImageGeneration.html">#TextToImageGeneration</a>
<span class="issue_date">Issue Date: 2023-07-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/831">Learning to Imagine: Visually-Augmented Natural Language Generation, ACL'23</a>
<span class="snippet"><span>Summary</span>æœ¬ç ”ç©¶ã§ã¯ã€è¦–è¦šæƒ…å ±ã‚’æ´»ç”¨ã—ãŸè‡ªç„¶è¨€èªç”Ÿæˆã®ãŸã‚ã®LIVEã¨ã„ã†æ‰‹æ³•ã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚LIVEã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦å ´é¢ã‚’æƒ³åƒã—ã€é«˜å“è³ªãªç”»åƒã‚’åˆæˆã™ã‚‹æ–¹æ³•ã§ã™ã€‚ã¾ãŸã€CLIPã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®æƒ³åƒåŠ›ã‚’è©•ä¾¡ã—ã€æ®µè½ã”ã¨ã«ç”»åƒã‚’ç”Ÿæˆã—ã¾ã™ã€‚ã•ã¾ã–ã¾ãªå®Ÿé¨“ã«ã‚ˆã‚Šã€LIVEã®æœ‰åŠ¹æ€§ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚³ãƒ¼ãƒ‰ã€ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã¯å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span>&gt;ã¾ãšã€ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦å ´é¢ã‚’æƒ³åƒã—ã¾ã™ã€‚å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦é«˜å“è³ªãªç”»åƒã‚’åˆæˆã™ã‚‹ãŸã‚ã«æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚æ¬¡ã«ã€CLIPã‚’ä½¿ç”¨ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆãŒæƒ³åƒåŠ›ã‚’å–šèµ·ã§ãã‚‹ã‹ã‚’äº‹å¾Œçš„ã«åˆ¤æ–­ã—ã¾ã™ã€‚æœ€å¾Œã«ã€ç§ãŸã¡ã®æƒ³åƒåŠ›ã¯å‹•çš„ã§ã‚ã‚Šã€æ®µè½å…¨ä½“ã«1ã¤ã®ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã®ã§ã¯ãªãã€å„æ–‡ã«å¯¾ã—ã¦åˆæˆã‚’è¡Œã„ã¾ã™ã€‚

<br>



<br>



<br>



<br>

èˆˆå‘³æ·±ã„</span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<span class="issue_date">Issue Date: 2023-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/806">Generative Pretraining in Multimodality, Quan Sun+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>Emuã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€å˜ä¸€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã¾ãŸã¯ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªãƒ‡ãƒ¼ã‚¿å…¥åŠ›ã‚’å—ã‘å…¥ã‚Œã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚Emuã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã€ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã¸ã®ã‚¿ã‚¹ã‚¯ã‚„ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®ã‚¿ã‚¹ã‚¯ãªã©ã€ã•ã¾ã–ã¾ãªã‚¿ã‚¹ã‚¯ã§å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã¾ã™ã€‚ã¾ãŸã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãªã©ã®æ‹¡å¼µæ©Ÿèƒ½ã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚</span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<span class="issue_date">Issue Date: 2023-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/805">EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the  Backbone, Shraman Pramanick+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>ã‚¨ã‚´ã‚»ãƒ³ãƒˆãƒªãƒƒã‚¯ãƒ“ãƒ‡ã‚ªè¨€èªã®äº‹å‰å­¦ç¿’ã®ç¬¬2ä¸–ä»£ï¼ˆEgoVLPv2ï¼‰ã¯ã€ãƒ“ãƒ‡ã‚ªã¨è¨€èªã®ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã«ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«ã®èåˆã‚’ç›´æ¥çµ„ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã‚‹ã€‚EgoVLPv2ã¯å¼·åŠ›ãªãƒ“ãƒ‡ã‚ªãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¾ã‚’å­¦ç¿’ã—ã€æŸ”è»Ÿã‹ã¤åŠ¹ç‡çš„ãªæ–¹æ³•ã§ã•ã¾ã–ã¾ãªãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¿ã‚¹ã‚¯ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã€‚ã•ã‚‰ã«ã€ææ¡ˆã•ã‚ŒãŸãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³æˆ¦ç•¥ã¯è»½é‡ã§è¨ˆç®—åŠ¹ç‡ãŒé«˜ã„ã€‚EgoVLPv2ã¯å¹…åºƒã„VLã‚¿ã‚¹ã‚¯ã§æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã¦ã„ã‚‹ã€‚è©³ç´°ã¯https://shramanpramanick.github.io/EgoVLPv2/ã‚’å‚ç…§ã€‚</span>
<a class="button" href="articles/FoundationModel.html">#FoundationModel</a>
<a class="button" href="articles/Navigation.html">#Navigation</a>
<span class="issue_date">Issue Date: 2023-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/802">ViNT: A Foundation Model for Visual Navigation, Dhruv Shah+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>æœ¬ç ”ç©¶ã§ã¯ã€æ±ç”¨äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹Visual Navigation Transformerï¼ˆViNTï¼‰ã‚’ææ¡ˆã—ã€ãƒ“ã‚¸ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ã®ãƒ­ãƒœãƒƒãƒˆãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã«æˆåŠŸã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚ViNTã¯ã€å¤§è¦æ¨¡ãªãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¨“ç·´ã•ã‚Œã€æŸ”è»ŸãªTransformerãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½¿ç”¨ã—ã¦ã•ã¾ã–ã¾ãªãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã«é©å¿œã—ã¾ã™ã€‚ViNTã¯ã€æ‹¡æ•£ãƒ™ãƒ¼ã‚¹ã®ã‚µãƒ–ã‚´ãƒ¼ãƒ«ææ¡ˆã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€æ–°ã—ã„ç’°å¢ƒã‚’æ¢ç´¢ã—ã€ã‚­ãƒ­ãƒ¡ãƒ¼ãƒˆãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³å•é¡Œã‚’è§£æ±ºã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€ViNTã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«è§¦ç™ºã•ã‚ŒãŸæŠ€è¡“ã‚’ä½¿ç”¨ã—ã¦ã€æ–°ã—ã„ã‚¿ã‚¹ã‚¯ä»•æ§˜ã«é©å¿œã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ViNTã¯ãƒ¢ãƒã‚¤ãƒ«ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã®ãŸã‚ã®åŠ¹æœçš„ãªåŸºç¤ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ç¢ºç«‹ã•ã‚Œã¦ã„ã¾ã™ã€‚è©³ç´°ã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒšãƒ¼ã‚¸ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚</span>
<span class="snippet"><span>Comment</span>äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è¦–è¦šãƒ™ãƒ¼ã‚¹ã®ãƒ­ãƒœãƒƒãƒˆãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã«æ´»ç”¨ã™ã‚‹Foundation Modelã€‚FlexibleãªTransformerãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ã„ã¦æ§‹ç¯‰ã•ã‚Œã¦ãŠã‚Šã€ã•ã¾ã–ã¾ãªãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã«å–ã‚Šçµ„ã‚€ã“ã¨ãŒå¯èƒ½

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/fcb59d61-9a89-4ac8-989c-ffb125e90cbd" alt="image" loading="lazy"></span>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/QuestionAnswering.html">#QuestionAnswering</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<span class="issue_date">Issue Date: 2023-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/800">SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen  LLMs, Lijun Yu+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>ã“ã®ç ”ç©¶ã§ã¯ã€Semantic Pyramid AutoEncoderï¼ˆSPAEï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€å‡çµã•ã‚ŒãŸLLMsãŒéè¨€èªçš„ãªãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’å«ã‚€ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚SPAEã¯ã€LLMã®èªå½™ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã¨ç”Ÿã®ãƒ”ã‚¯ã‚»ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›ã‚’è¡Œã„ã¾ã™ã€‚ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã¯ã€è¦–è¦šå†æ§‹æˆã«å¿…è¦ãªæ„å‘³ã¨è©³ç´°ã‚’æ‰ãˆã€LLMãŒç†è§£ã§ãã‚‹è¨€èªã«å¤‰æ›ã—ã¾ã™ã€‚å®Ÿé¨“çµæœã§ã¯ã€æˆ‘ã€…ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒç”»åƒç†è§£ã¨ç”Ÿæˆã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’25ï¼…ä»¥ä¸Šä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span>ç”»åƒã‚’LLMã®tokenã‚¹ãƒšãƒ¼ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€LLMãŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°ãªã—ã«visual taskã‚’è§£ãã“ã¨ã‚’å¯èƒ½ã«ã—ãŸã€‚in context learningã«ã‚ˆã£ã¦ã€æ§˜ã€…ãªvisuataskã‚’è§£ãã“ã¨ãŒã§ãã‚‹ã€‚

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/1e0f962f-e661-44e6-bc59-73d9ae87d6dd" alt="image" loading="lazy"></span>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/QuestionAnswering.html">#QuestionAnswering</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<span class="issue_date">Issue Date: 2023-06-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/775">Towards Language Models That Can See: Computer Vision Through the LENS  of Natural Language, William Berrios+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>ç§ãŸã¡ã¯ã€LENSã¨ã„ã†ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®å•é¡Œã«å–ã‚Šçµ„ã¿ã¾ã™ã€‚LENSã¯ã€ç‹¬ç«‹ã—ãŸãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å‡ºåŠ›ã«å¯¾ã—ã¦è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦æ¨è«–ã‚’è¡Œã„ã¾ã™ã€‚ç§ãŸã¡ã¯ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãŠã‚ˆã³ãƒ•ãƒ¥ãƒ¼ã‚·ãƒ§ãƒƒãƒˆã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆèªè­˜ãªã©ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®è¨­å®šã§LENSã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚LENSã¯å¸‚è²©ã®LLMã«é©ç”¨ã§ãã€éå¸¸ã«ç«¶äº‰åŠ›ã®ã‚ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã¾ã™ã€‚ã‚³ãƒ¼ãƒ‰ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§æä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span>å‚è€ƒ: https://twitter.com/hillbig/status/1674878733264781312?s=46&t=KFT8cWTu8vV69iD6Qt0NGw<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/e96f9a8a-6ce2-4985-8b0a-8daf4a6e477c" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Personalization.html">#Personalization</a>
<span class="issue_date">Issue Date: 2023-06-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/751">Photoswap: Personalized Subject Swapping in Images, Jing Gu+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>æœ¬ç ”ç©¶ã§ã¯ã€Photoswapã¨ã„ã†æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã€æ—¢å­˜ã®ç”»åƒã«ãŠã„ã¦å€‹äººçš„ãªå¯¾è±¡ç‰©ã®äº¤æ›ã‚’å¯èƒ½ã«ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚Photoswapã¯ã€å‚ç…§ç”»åƒã‹ã‚‰å¯¾è±¡ç‰©ã®è¦–è¦šçš„ãªæ¦‚å¿µã‚’å­¦ç¿’ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒªãƒ¼ã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”»åƒã«äº¤æ›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€PhotoswapãŒåŠ¹æœçš„ã§åˆ¶å¾¡å¯èƒ½ã§ã‚ã‚Šã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ‰‹æ³•ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹äººé–“ã®è©•ä¾¡ã‚’å¾—ã¦ã„ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚Photoswapã¯ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒ—ãƒ­ã®ç·¨é›†ã¾ã§å¹…åºƒã„å¿œç”¨å¯èƒ½æ€§ã‚’æŒã£ã¦ã„ã¾ã™ã€‚</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Personalization.html">#Personalization</a>
<a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a>
<a class="button" href="articles/TextToImageGeneration.html">#TextToImageGeneration</a>
<span class="issue_date">Issue Date: 2023-06-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/741">ViCo: Detail-Preserving Visual Condition for Personalized Text-to-Image  Generation, Shaozhe Hao+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸç”»åƒç”Ÿæˆã«ãŠã„ã¦ã€é«˜é€Ÿã§è»½é‡ãªãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãƒ¡ã‚½ãƒƒãƒ‰ã§ã‚ã‚‹ViCoã‚’ææ¡ˆã€‚æ³¨ç›®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å°å…¥ã—ã€æ³¨ç›®ãƒ™ãƒ¼ã‚¹ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒã‚¹ã‚¯ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ä¸€èˆ¬çš„ãªéå­¦ç¿’ã®åŠ£åŒ–ã‚’è»½æ¸›ã€‚å…ƒã®æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¾®èª¿æ•´ã›ãšã€è»½é‡ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã ã‘ã§ã€æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã¾ãŸã¯ãã‚Œä»¥ä¸Šã®æ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/QuestionAnswering.html">#QuestionAnswering</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<span class="issue_date">Issue Date: 2023-06-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/732">AVIS: Autonomous Visual Information Seeking with Large Language Models, Ziniu Hu+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>æœ¬è«–æ–‡ã§ã¯ã€è‡ªå¾‹çš„ãªæƒ…å ±åé›†ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«è³ªå•å¿œç­”ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚‹AVISã‚’ææ¡ˆã™ã‚‹ã€‚AVISã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’æ´»ç”¨ã—ã¦å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ã®åˆ©ç”¨æˆ¦ç•¥ã‚’å‹•çš„ã«æ±ºå®šã—ã€è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã«å¿…è¦ãªä¸å¯æ¬ ãªçŸ¥è­˜ã‚’ç²å¾—ã™ã‚‹ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¹ã‚¿ãƒ‡ã‚£ã‚’å®Ÿæ–½ã—ã¦åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ã€ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã‚„æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’æ”¹å–„ã—ã€çŸ¥è­˜é›†ç´„å‹ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«è³ªå•å¿œç­”ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/9df9b0ce-1f95-4e48-a4c9-b4c6b87d0ac6" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Controllable.html">#Controllable</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/VideoGeneration/Understandings.html">#VideoGeneration/Understandings</a>
<span class="issue_date">Issue Date: 2023-05-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/677">Sketching the Future ï¼ˆSTFï¼‰: Applying Conditional Control Techniques to  Text-to-Video Models, Rohan Dhesikan+, arXiv'23</a>
<span class="snippet"><span>Summary</span>ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å‹•ç”»ç”Ÿæˆã‚’ControlNetã¨çµ„ã¿åˆã‚ã›ã€ã‚¹ã‚±ãƒƒãƒã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ã‚’åŸºã«å‹•ç”»ã‚’ç”Ÿæˆã™ã‚‹æ–°æ‰‹æ³•ã‚’ææ¡ˆã€‚ãƒ•ãƒ¬ãƒ¼ãƒ è£œé–“ã‚’è¡Œã„ã€Text-to-Video Zeroã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ´»ç”¨ã—ã¦é«˜å“è³ªã§ä¸€è²«æ€§ã®ã‚ã‚‹å‹•ç”»ã‚’ç”Ÿæˆã€‚ãƒ‡ãƒ¢å‹•ç”»ã‚„ãƒªã‚½ãƒ¼ã‚¹ã‚’æä¾›ã—ã€ã•ã‚‰ãªã‚‹ç ”ç©¶ã‚’ä¿ƒé€²ã€‚</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Embeddings.html">#Embeddings</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/RepresentationLearning.html">#RepresentationLearning</a>
<a class="button" href="articles/ContrastiveLearning.html">#ContrastiveLearning</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/Semi-Supervised.html">#Semi-Supervised</a>
<span class="issue_date">Issue Date: 2023-04-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/602">SemPPL: Predicting pseudo-labels for better contrastive representations, Matko BoÅ¡njak+, N_A, ICLR'23</a>
<span class="snippet"><span>Summary</span>æœ¬ç ”ç©¶ã§ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã«ãŠã‘ã‚‹åŠæ•™å¸«ã‚ã‚Šå­¦ç¿’ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€Semantic Positives via Pseudo-Labels (SemPPL)ã¨ã„ã†æ–°ã—ã„æ‰‹æ³•ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚ã“ã®æ‰‹æ³•ã¯ã€ãƒ©ãƒ™ãƒ«ä»˜ãã¨ãƒ©ãƒ™ãƒ«ãªã—ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ„ã¿åˆã‚ã›ã¦æƒ…å ±è±Šå¯Œãªè¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã€ResNet-$50$ã‚’ä½¿ç”¨ã—ã¦ImageNetã®$1\%$ãŠã‚ˆã³$10\%$ã®ãƒ©ãƒ™ãƒ«ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å ´åˆã€ç«¶åˆã™ã‚‹åŠæ•™å¸«ã‚ã‚Šå­¦ç¿’æ‰‹æ³•ã‚’ä¸Šå›ã‚‹æœ€é«˜æ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚SemPPLã¯ã€å¼·åŠ›ãªé ‘å¥æ€§ã€åˆ†å¸ƒå¤–ãŠã‚ˆã³è»¢ç§»æ€§èƒ½ã‚’ç¤ºã™ã“ã¨ãŒã§ãã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span>å¾Œã»ã©èª¬æ˜ã‚’è¿½è¨˜ã™ã‚‹

<br>

<img src="https://github.com/user-attachments/assets/4441dc6c-a7b2-4ec9-9748-b6558a96e1af" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/8a78a40e-f5c4-4742-9e5d-36cd1b8d0e60" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/04ded9aa-c875-4282-9e3b-7ce456a6cc44" alt="image" loading="lazy">é–¢é€£:

<br>

ãƒ»1975</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<span class="issue_date">Issue Date: 2023-04-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/563">Stable and low-precision training for large-scale vision-language models, Wortsman+, University of Washington, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>å¤§è¦æ¨¡ãªè¨€èª-è¦–è¦šãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’åŠ é€Ÿã—å®‰å®šã•ã›ã‚‹æ–°æ‰‹æ³•ã‚’ææ¡ˆã€‚SwitchBackã‚’ç”¨ã„ãŸint8é‡å­åŒ–ã§ã€CLIP ViT-Hugeã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã‚’13-25%å‘ä¸Šã•ã›ã€bfloat16ã¨åŒç­‰ã®æ€§èƒ½ã‚’ç¶­æŒã€‚float8ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚‚åŠ¹æœçš„ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã€åˆæœŸåŒ–æ–¹æ³•ãŒæˆåŠŸã«å¯„ä¸ã€‚æå¤±ã®ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’åˆ†æã—ã€AdamW-Adafactorãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚’æ¨å¥¨ã™ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®‰å®šæ€§ã‚’å‘ä¸Šã•ã›ãŸã€‚</span>
<span class="snippet"><span>Comment</span><img src="https://user-images.githubusercontent.com/12249301/235149432-1c818dc6-174c-4666-a26c-2ab9683b438b.png" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/ImageSegmentation.html">#ImageSegmentation</a>
<a class="button" href="articles/TechnicalReport.html">#TechnicalReport</a>
<span class="issue_date">Issue Date: 2023-04-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/535">Track Anything: Segment Anything Meets Videos, yang+, SUSTech VIP Lab, arXiv'23</a>
<span class="snippet"><span>Comment</span>Metaã®SAMã‚’ã€videoã«é©ç”¨ã—ã€videowå†…ã®segmentationã‚’è¿½åŠ å­¦ç¿’ãªã—ã§ã‚„ã‚Šã¾ã—ãŸã€ã¨ã„ã†è©±ã ã¨æ€ã‚ã‚Œã‚‹ã€‚</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/SIGGRAPH.html">#SIGGRAPH</a>
<span class="issue_date">Issue Date: 2022-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/496">Sketch-Guided Text-to-Image Diffusion Models, Andrey+, Google Research, SIGGRAPH'23</a>
<span class="snippet"><span>Summary</span>ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®ãƒ¢ãƒ‡ãƒ«ã¯é«˜å“è³ªãªç”»åƒåˆæˆã‚’å®Ÿç¾ã™ã‚‹ãŒã€ç©ºé–“çš„ç‰¹æ€§ã®åˆ¶å¾¡ãŒä¸è¶³ã—ã¦ã„ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€ã‚¹ã‚±ãƒƒãƒã‹ã‚‰ã®ç©ºé–“ãƒãƒƒãƒ—ã‚’ç”¨ã„ã¦äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å°ãæ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã€‚å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’å¿…è¦ã¨ã›ãšã€æ½œåœ¨ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹äºˆæ¸¬å™¨ï¼ˆLGPï¼‰ã‚’è¨“ç·´ã—ã€ç”»åƒã‚’ç©ºé–“ãƒãƒƒãƒ—ã«ä¸€è‡´ã•ã›ã‚‹ã€‚ãƒ”ã‚¯ã‚»ãƒ«ã”ã¨ã®è¨“ç·´ã«ã‚ˆã‚ŠæŸ”è»Ÿæ€§ã‚’æŒã¡ã€ã‚¹ã‚±ãƒƒãƒã‹ã‚‰ç”»åƒã¸ã®ç¿»è¨³ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦åŠ¹æœçš„ãªç”ŸæˆãŒå¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span>ã‚¹ã‚±ãƒƒãƒã¨promptã‚’å…¥åŠ›ã™ã‚‹ã“ã¨ã§ã€ã‚¹ã‚±ãƒƒãƒ biasedãªç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹æŠ€è¡“ã€‚ã™ã”ã„ã€‚

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/205189823-66052368-60a8-4f03-a4b6-37111bd1b361.png" alt="image" loading="lazy"></span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/MultitaskLearning.html">#MultitaskLearning</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/SpeechProcessing.html">#SpeechProcessing</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<span class="issue_date">Issue Date: 2025-07-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2183">[Paper Note] Perceiver IO: A General Architecture for Structured Inputs &amp; Outputs, Andrew Jaegle+, ICLR'22</a>
<span class="snippet"><span>Summary</span>æ±ç”¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£Perceiver IOã‚’ææ¡ˆã—ã€ä»»æ„ã®ãƒ‡ãƒ¼ã‚¿è¨­å®šã«å¯¾å¿œã—ã€å…¥åŠ›ã¨å‡ºåŠ›ã®ã‚µã‚¤ã‚ºã«å¯¾ã—ã¦ç·šå½¢ã«ã‚¹ã‚±ãƒ¼ãƒ«å¯èƒ½ã€‚æŸ”è»Ÿãªã‚¯ã‚¨ãƒªãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’è¿½åŠ ã—ã€ã‚¿ã‚¹ã‚¯ç‰¹æœ‰ã®è¨­è¨ˆã‚’ä¸è¦ã«ã€‚è‡ªç„¶è¨€èªã€è¦–è¦šç†è§£ã€ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ã§å¼·åŠ›ãªçµæœã‚’ç¤ºã—ã€GLUEãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§BERTã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’é”æˆã€‚</span>
<span class="snippet"><span>Comment</span>å½“æ™‚ç›¸å½“è©±é¡Œã¨ãªã£ãŸã•ã¾ã–ã¾ãªãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’çµ±ä¸€ã•ã‚ŒãŸæ çµ„ã¿ã§æ‰±ãˆã‚‹Perceiver IOè«–æ–‡

<br>

<img src="https://github.com/user-attachments/assets/d7893f14-d69c-4af8-8117-08c2a6095e8e" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/CLIP.html">#CLIP</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<span class="issue_date">Issue Date: 2025-05-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1928">LAION-5B: An open large-scale dataset for training next generation   image-text models, Christoph Schuhmann+, NeurIPS'22</a>
<span class="snippet"><span>Summary</span>LAION-5Bã¯ã€5.85å„„ã®CLIPãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ã‹ã‚‰æˆã‚‹å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€è‹±èªã®ãƒšã‚¢ãŒ2.32Bå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€CLIPã‚„GLIDEãªã©ã®ãƒ¢ãƒ‡ãƒ«ã®å†ç¾ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«åˆ©ç”¨ã•ã‚Œã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ç ”ç©¶ã‚’æ°‘ä¸»åŒ–ã—ã¾ã™ã€‚ã¾ãŸã€ãƒ‡ãƒ¼ã‚¿æ¢ç´¢ã‚„ã‚µãƒ–ã‚»ãƒƒãƒˆç”Ÿæˆã®ãŸã‚ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚„ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œå‡ºã®ãŸã‚ã®ã‚¹ã‚³ã‚¢ã‚‚æä¾›ã•ã‚Œã¾ã™ã€‚</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/CLIP.html">#CLIP</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/OOD.html">#OOD</a>
<span class="issue_date">Issue Date: 2023-05-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/681">Fine-Tuning can Distort Pretrained Features and Underperform   Out-of-Distribution, Ananya Kumar+, N_A, ICLR'22</a>
<span class="snippet"><span>Summary</span>äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¿ã‚¹ã‚¯ã«è»¢ç§»ã™ã‚‹éš›ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ç·šå½¢ãƒ—ãƒ­ãƒ¼ãƒ“ãƒ³ã‚°ã®2ã¤ã®æ–¹æ³•ãŒã‚ã‚‹ãŒã€æœ¬ç ”ç©¶ã§ã¯ã€åˆ†å¸ƒã®ã‚·ãƒ•ãƒˆãŒå¤§ãã„å ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒç·šå½¢ãƒ—ãƒ­ãƒ¼ãƒ“ãƒ³ã‚°ã‚ˆã‚Šã‚‚åˆ†å¸ƒå¤–ã§ç²¾åº¦ãŒä½ããªã‚‹ã“ã¨ã‚’ç™ºè¦‹ã—ãŸã€‚LP-FTã¨ã„ã†2æ®µéšæˆ¦ç•¥ã®ç·šå½¢ãƒ—ãƒ­ãƒ¼ãƒ“ãƒ³ã‚°å¾Œã®å…¨ä½“ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒã€ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ç·šå½¢ãƒ—ãƒ­ãƒ¼ãƒ“ãƒ³ã‚°ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span>äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’finetuningã™ã‚‹æ–¹æ³•ã¯å¤§ããåˆ†ã‘ã¦

<br>

1. linear layerã‚’ãƒ˜ãƒƒãƒ‰ã¨ã—ã¦concatã—ãƒ˜ãƒƒãƒ‰ã®ã¿ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å­¦ç¿’

<br>

2. äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å­¦ç¿’

<br>



<br>

ã®2ç¨®é¡ãŒã‚ã‚‹ã€‚

<br>

å‰è€…ã¯in-distributionãƒ‡ãƒ¼ã‚¿ã«å¼·ã„ãŒã€out-of-distributionã«å¼±ã„ã€‚å¾Œè€…ã¯é€†ã¨ã„ã†äº’ã„ãŒäº’ã„ã‚’è£œå®Œã—åˆã†é–¢ä¿‚ã«ã‚ã£ãŸã€‚

<br>

ãã“ã§ã€ã¾ãš1ã‚’å®Ÿæ–½ã—ã€ãã®å¾Œ2ã‚’å®Ÿæ–½ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã€‚in-distribution, out-of-distributionã®ä¸¡æ–¹ã§é«˜ã„æ€§èƒ½ã‚’å‡ºã™ã“ã¨ã‚’ç¤ºã—ãŸï¼ˆå®Ÿé¨“ã§ã¯ç”»åƒå‡¦ç†ç³»ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã¯ImageNet+CLIPã§äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ViTã‚’ç”¨ã„ã¦ã„ã‚‹)ã€‚

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/059d9056-bd3c-45f2-abd9-00c9f2a3d630" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<span class="issue_date">Issue Date: 2025-07-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2259">[Paper Note] Swin Transformer V2: Scaling Up Capacity and Resolution, Ze Liu+, arXiv'21</a>
<span class="snippet"><span>Summary</span>æœ¬è«–æ–‡ã§ã¯ã€å¤§è¦æ¨¡ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨å¿œç”¨ã«ãŠã‘ã‚‹èª²é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã®3ã¤ã®æŠ€è¡“ã‚’ææ¡ˆã€‚å…·ä½“çš„ã«ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®‰å®šæ€§å‘ä¸Šã®ãŸã‚ã®æ®‹å·®å¾Œæ­£è¦åŒ–æ³•ã€ä½è§£åƒåº¦ã‹ã‚‰é«˜è§£åƒåº¦ã¸ã®è»¢é€ã‚’å¯èƒ½ã«ã™ã‚‹ä½ç½®ãƒã‚¤ã‚¢ã‚¹æ³•ã€ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã®å¿…è¦æ€§ã‚’æ¸›å°‘ã•ã›ã‚‹è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’æ³•ã‚’ç”¨ã„ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€30å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®Swin Transformer V2ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€è¤‡æ•°ã®ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã§æ–°è¨˜éŒ²ã‚’æ¨¹ç«‹ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚‚å‘ä¸Šã—ã€ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã¨æ™‚é–“ã‚’å¤§å¹…ã«å‰Šæ¸›ã€‚</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<a class="button" href="articles/ICCV.html">#ICCV</a>
<span class="issue_date">Issue Date: 2025-07-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2258">[Paper Note] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, Ze Liu+, ICCV'21</a>
<span class="snippet"><span>Summary</span>Swin Transformerã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®æ–°ã—ã„ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹éšå±¤çš„ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ææ¡ˆã€‚ã‚·ãƒ•ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ–¹å¼ã«ã‚ˆã‚Šã€åŠ¹ç‡çš„ãªè‡ªå·±æ³¨æ„è¨ˆç®—ã‚’å®Ÿç¾ã—ã€ã•ã¾ã–ã¾ãªã‚¹ã‚±ãƒ¼ãƒ«ã§ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãŒå¯èƒ½ã€‚ç”»åƒåˆ†é¡ã‚„ç‰©ä½“æ¤œå‡ºã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãªã©ã§å¾“æ¥ã®æœ€å…ˆç«¯ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®ãƒ“ã‚¸ãƒ§ãƒ³ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã¦ã®å¯èƒ½æ€§ã‚’ç¤ºå”†ã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span>æ—¥æœ¬èªè§£èª¬:https://qiita.com/m_sugimura/items/139b182ee7c19c83e70aç”»åƒå‡¦ç†ã«ãŠã„ã¦ã€ç‰©ä½“ã®ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã‚„ã€è§£åƒåº¦ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€PatchMergeã¨å‘¼ã°ã‚Œã‚‹ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã®ã‚ˆã†ãªå‡¦ç†ã¨ã€å›ºå®šã‚µã‚¤ã‚ºã®ãƒ­ãƒ¼ã‚«ãƒ«ãªwindowã«åˆ†å‰²ã—ã¦Self-Attentionã‚’å®Ÿæ–½ã—ã€layerã”ã¨ã«é€šå¸¸ã®windowã¨ã‚·ãƒ•ãƒˆã•ã‚ŒãŸwindowã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§ã€windowé–“ã‚’è·¨ã„ã é–¢ä¿‚æ€§ã‚‚è€ƒæ…®ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹æ©Ÿæ§‹ã‚’å°å…¥ã—ãŸãƒ¢ãƒ‡ãƒ«ã€‚

<br>

<img src="https://github.com/user-attachments/assets/a2d5f78c-27ec-4f18-bd7d-5475085cfa7b" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/92fb10e1-614e-44ef-9e65-3920cd863d46" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/2b8a543a-069e-468a-bc3c-1f288cdcf577" alt="image" loading="lazy"></span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<span class="issue_date">Issue Date: 2023-08-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1009">ViLT: Vision-and-Language Transformer Without Convolution or Region   Supervision, Wonjae Kim+, N_A, ICML'21</a>
<span class="snippet"><span>Summary</span>VLPï¼ˆVision-and-Language Pre-trainingï¼‰ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ãƒ“ã‚¸ãƒ§ãƒ³ã¨è¨€èªã®ã‚¿ã‚¹ã‚¯ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã¦ã„ã‚‹ãŒã€ç¾åœ¨ã®æ–¹æ³•ã¯åŠ¹ç‡æ€§ã¨è¡¨ç¾åŠ›ã®é¢ã§å•é¡ŒãŒã‚ã‚‹ã€‚ãã“ã§ã€æœ¬ç ”ç©¶ã§ã¯ç•³ã¿è¾¼ã¿ãƒ•ãƒªãƒ¼ã®ãƒ“ã‚¸ãƒ§ãƒ³ã¨è¨€èªã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒï¼ˆViLTï¼‰ãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã™ã‚‹ã€‚ViLTã¯é«˜é€Ÿã§ã‚ã‚ŠãªãŒã‚‰ç«¶äº‰åŠ›ã®ã‚ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã€ã‚³ãƒ¼ãƒ‰ã¨äº‹å‰å­¦ç¿’æ¸ˆã¿ã®é‡ã¿ã¯GitHubã§åˆ©ç”¨å¯èƒ½ã§ã‚ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span>æ—¥æœ¬èªè§£èª¬:https://tech.fusic.co.jp/posts/2021-12-29-vilt/</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/ContrastiveLearning.html">#ContrastiveLearning</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<span class="issue_date">Issue Date: 2023-04-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/550">Learning Transferable Visual Models From Natural Language Supervision, Radford+, OpenAI, ICML'21</a>
<span class="snippet"><span>Comment</span>CLIPè«–æ–‡ã€‚å¤§é‡ã®ç”»åƒã¨ç”»åƒã«å¯¾å¿œã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ãƒšã‚¢ã‹ã‚‰ã€å¯¾è±¡å­¦ç¿’ã‚’è¡Œã„ã€ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆé–“ã®similarityã‚’ã¯ã‹ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸãƒ¢ãƒ‡ãƒ«

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/234729329-dfa5dc1e-c5fc-452c-8ead-76df7d1aeda4.png" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2022-09-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/487">Generating Racing Game Commentary from Vision, Language, and Structured Data, Tatsuya+, INLG'21</a>
<span class="snippet"><span>Comment</span>ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: https://kirt.airc.aist.go.jp/corpus/ja/RacingCommentary</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<span class="issue_date">Issue Date: 2021-11-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/431">ResNet strikes back: An improved training procedure in timm, Wightman+, NeurIPS'21 Workshop ImageNet PPF</a>
<span class="snippet"><span>Summary</span>æœ¬è«–æ–‡ã§ã¯ã€Residual Networksï¼ˆResNet-50ï¼‰ã®æ€§èƒ½ã‚’æ–°ãŸãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã‚’ç”¨ã„ã¦å†è©•ä¾¡ã—ã€ç«¶äº‰åŠ›ã®ã‚ã‚‹è¨­å®šã§80.4%ã®ãƒˆãƒƒãƒ—1ç²¾åº¦ã‚’é”æˆã—ãŸã“ã¨ã‚’å ±å‘Šã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å°†æ¥ã®ç ”ç©¶ã®ãŸã‚ã®ã‚ˆã‚Šè‰¯ã„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span>2015å¹´ä»¥å¾Œã€æ§˜ã€…ãªæœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€æ­£å‰‡åŒ–æ‰‹æ³•ã€ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãªã©ãŒææ¡ˆã•ã‚Œã‚‹ä¸­ã§ã€æœ€æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ãƒ¢ãƒ‡ãƒ«ã«ã¯ãã‚Œã‚‰ãŒé©ç”¨ã•ã‚Œã‚‹ä¸€æ–¹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ãªã‚‹ResNetã§ã¯ãã‚Œã‚‰ãŒé©ç”¨ã•ã‚Œãšã€è«–æ–‡ã®å€¤ã®ã¿ãŒå‚ç…§ã•ã‚Œã‚‹ç¾çŠ¶ã¯ãƒ•ã‚§ã‚¢ã§ã¯ãªã„ã®ã§ã€ResNetã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã‚ˆã†ãªè¨“ç·´æ‰‹æ³•ã‚’è¿½æ±‚ã—ãŸç ”ç©¶ã€‚

<br>



<br>



<br>



<br>

ResNetã«ãŠã‘ã‚‹æœ‰åŠ¹ãªè¨“ç·´æ‰‹æ³•ã¨ã—ã¦ä¸‹è¨˜ã‚’æ¨¡ç´¢ï¼š

<br>



<br>



<br>



<br>

æå¤±é–¢æ•°ã¨ã—ã¦ã€MixUpï¼ˆè¨“ç·´ç”»åƒã‚’é‡ã­åˆã‚ã›ã€çµ„ã¿åˆã‚ã›ãŸç”»åƒã®ãƒ©ãƒ™ãƒ«ã‚’ãƒŸãƒƒã‚¯ã‚¹ã—ã¦æ–°ã—ã„å­¦ç¿’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œã‚‹ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µæ‰‹æ³•ï¼‰ã¨ã€CutMixï¼ˆç”»åƒã‚’åˆ‡ã‚Šè²¼ã‚Šã—ã¦ã€åˆ‡ã‚Šè²¼ã‚Šéƒ¨åˆ†ã®é¢ç©ã«å¿œã˜ã¦ãƒ©ãƒ™ãƒ«ã®ã‚¹ã‚³ã‚¢ã‚’èª¿æ•´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µæ‰‹æ³•ï¼‰ã‚’é©ç”¨ã—ã€CutMixã«ã‚ˆã£ã¦å¤§å¹…ã«æ€§èƒ½ãŒæ”¹å–„ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚ã“ã®ã¨ãã€ãƒ©ãƒ™ãƒ«ã®ç¢ºç‡ã®å’ŒãŒ1ã¨ãªã‚‹å‰æã®å…ƒã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã§å­¦ç¿’ã™ã‚‹ã®ã§ã¯ãªãã€å…ƒç”»åƒã«å«ã¾ã‚Œã‚‹ç‰©ä½“ãŒä¸¡æ–¹å­˜åœ¨ã™ã‚‹ã¨ã„ã†å…¨ä½“ã®å…ƒBinaryCrossEntropyã‚’é©ç”¨ã—ãƒãƒ«ãƒãƒ©ãƒ™ãƒ«å•é¡Œã¨ã—ã¦å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€æ€§èƒ½ãŒå‘ä¸Šã€‚

<br>



<br>



<br>



<br>

ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µæ‰‹æ³•ã¨ã—ã¦ã€MixUp, CutMixã ã‘ã§ãªãã€é€šå¸¸ã®ãƒªã‚µã‚¤ã‚ºãƒ»åˆ‡ã‚ŠæŠœãã¨ã€æ°´å¹³æ–¹å‘ã®åè»¢ã‚’é©ç”¨ã—ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã™ã‚‹ã€‚åŠ ãˆã¦RandAugmentï¼ˆ14ç¨®é¡ã®ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µæ“ä½œã‹ã‚‰ã€Nå€‹ã‚µãƒ³ãƒ—ãƒ«ã—ã€å¼·ã•Mã§é †ç•ªã«é©ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µæ‰‹æ³•ã€‚N,Mã¯ãã‚Œãã‚Œ0ã€œ10ã®æ•´æ•°ãªã®ã§ã€10ã®äºŒä¹—ã‚ªãƒ¼ãƒ€ãƒ¼ã§ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã™ã‚Œã°ã€æœ€é©ãªN,Mã‚’å¾—ã‚‹ã€‚ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã™ã‚‹ã ã‘ã§ãŠæ‰‹è»½ã ãŒéå¸¸ã«å¼·åŠ›ï¼‰ã‚’é©ç”¨ã—ãŸã€‚

<br>



<br>



<br>



<br>

æ­£å‰‡åŒ–ã¨ã—ã¦ã€Weight Decayï¼ˆå­¦ç¿’éç¨‹ã§é‡ã¿ãŒå¤§ãããªã‚Šã™ããªã„ã‚ˆã†ã«ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’èª²ã—ã€éå­¦ç¿’ã‚’é˜²æ­¢ã™ã‚‹æ‰‹æ³•ã€‚L2æ­£å‰‡åŒ–ãªã©ã€‚ï¼‰ã¨ã€label smoothingï¼ˆæ­£è§£ãƒ©ãƒ™ãƒ«ãŒ1ã€ãã®ä»–ã¯0ã¨ãƒ©ãƒ™ãƒ«ä»˜ã‘ã™ã‚‹ã®ã§ã¯ãªãã€ãƒ©ãƒ™ãƒ«ã«ä¸€å®šã®ãƒã‚¤ã‚ºã‚’å…¥ã‚Œã€æ­£è§£ãƒ©ãƒ™ãƒ«ä»¥å¤–ã«ã‚‚é‡ã¿ãŒå…¥ã£ã¦ã„ã‚‹çŠ¶æ…‹ã«ã—ã€ãƒ©ãƒ™ãƒ«ä»˜ã‘ã®ãƒã‚¤ã‚ºã«ãƒ­ãƒã‚¹ãƒˆãªãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã€‚ãƒã‚¤ã‚ºã®å¼·ã•ã¯å®šæ•°ã§èª¿æ•´ã™ã‚‹ï¼‰ã€Repeated Augmentationï¼ˆåŒã˜ãƒãƒƒãƒå†…ã®ç”»åƒã«ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’é©ç”¨ã—ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤§ããã™ã‚‹ï¼‰ã€Stochastic Depthï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã§ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å‰Šé™¤ã—ã€ãã®é–“ã‚’æ’ç­‰é–¢æ•°ã§ç¹‹ãè¨“ç·´ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–èƒ½åŠ›ã¨è¨“ç·´æ™‚é–“ã‚’å‘ä¸Šã™ã‚‹ï¼‰ã‚’é©ç”¨ã€‚

<br>



<br>

Optimizerã¨ã—ã¦ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ResNetã§ã¯ã€SGDã‚„AdamWã§è¨“ç·´ã•ã‚Œã‚‹ã“ã¨ãŒå¤šã„ãŒã€Repeated Augmentationã¨ãƒã‚¤ãƒŠãƒªã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’çµ„ã¿åˆã‚ã›ãŸå ´åˆã¯LAMBãŒæœ‰åŠ¹ã§ã‚ã£ãŸã€‚ã¾ãŸã€å¾“æ¥ã‚ˆã‚Šã‚‚é•·ã„è¨“ç·´æ™‚é–“ï¼ˆ600epochã€æ§˜ã€…ãªæ­£å‰‡åŒ–æ‰‹æ³•ã‚’ä½¿ã£ã¦ã„ã‚‹ã®ã§éå­¦ç¿’ã—ã¥ã‚‰ã„ãŸã‚ï¼‰ã§å­¦ç¿’ã—ã€æœ€åˆã«ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚’ä½¿ã„å¾ã€…ã«å­¦ç¿’ç‡ã‚’ä¸Šã’ï¼ˆfinetuningã®å†èªè­˜ã“ã‚Œã¾ã§ã®weightã‚’ãªã‚‹ã¹ãå£Šã—ãŸããªã„ã‹ã‚‰å°ã•ã„å­¦ç¿’ç‡ã‹ã‚‰å§‹ã‚ã‚‹ã€ã‚ã‚‹ã„ã¯Momentumã‚„Adamã¨ã„ã£ãŸç§»å‹•å¹³å‡ã‚’ä½¿ã†æ‰‹æ³•ã§ã¯ç§»å‹•å¹³å‡ã‚’å–ã‚‹ãŸã‚ã®å£°å€ã®è“„ç©ãŒè¶³ã‚Šãªã„å ´åˆå­¦ç¿’ã®ä¿¡é ¼åº¦ãŒä½ã„ã®ã§æœ€åˆã®æ–¹ã¯å­¦ç¿’ç‡å°ã•ãã™ã‚‹ã¿ãŸã„ãªã€ã‚¤ãƒ¡ãƒ¼ã‚¸ï¼‰ãã®å¾Œã‚³ã‚µã‚¤ãƒ³é–¢æ•°ã«å¾“ã„å­¦ç¿’ç‡ã‚’æ¸›ã‚‰ã—ã¦ã„ãã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°æ³•ã§å­¦ç¿’ã€‚

<br>



<br>



<br>



<br>

è«–æ–‡ä¸­ã§ã¯ä¸Šè¨˜æ‰‹æ³•ã®3ç¨®é¡ã®çµ„ã¿åˆã‚ã›ï¼ˆA1,A2,A3ï¼‰ã‚’ææ¡ˆã—å®Ÿé¨“ã—ã¦ã„ã‚‹ã€‚

<br>



<br>

ResNet-50ã«å¯¾ã—ã¦A1,2,3ã‚’é©ç”¨ã—ãŸçµæœã€A1ã‚’é©ç”¨ã—ãŸå ´åˆã«ImageNetã®ãƒˆãƒƒãƒ—1ç²¾åº¦ãŒ80.4%ã§ã‚ã‚Šã€ã“ã‚Œã¯ResNet-50ã‚’ä½¿ã£ãŸå ´åˆã®SoTAã€‚å…ƒã®ResNetã®ç²¾åº¦ãŒ76%ç¨‹åº¦ã ã£ãŸã®ã§å¤§å¹…ã«å‘ä¸Šã—ãŸã€‚

<br>



<br>

åŒã˜å®Ÿé¨“è¨­å®šã‚’ä½¿ã£ãŸå ´åˆã®ä»–ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆViTã‚„EfficientNetãªã©ï¼‰ã¨æ¯”ã¹ã¦ã‚‚éœè‰²ã®ãªã„æ€§èƒ½ã‚’é”æˆã€‚

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/140302112-05392bbb-7014-4518-a001-55e91933a065.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

ã¾ãŸã€æœ¬è«–æ–‡ã§ææ¡ˆã•ã‚Œã¦ã„ã‚‹A2ã¨ã€DeiTã¨å‘¼ã°ã‚Œã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ææ¡ˆã•ã‚Œã¦ã„ã‚‹è¨“ç·´æ‰‹æ³•ï¼ˆT2ï¼‰ã‚’ãã‚Œãã‚Œã®ãƒ¢ãƒ‡ãƒ«ã«é©ç”¨ã—ãŸçµæœã€ResNetã§ã¯A2ã€DeiTã§ã¯T2ã®æ€§èƒ½ãŒè‰¯ã‹ã£ãŸã€‚ã¤ã¾ã‚Šã€ã€Œã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨è¨“ç·´æ–¹æ³•ã¯åŒæ™‚ã«æœ€é©åŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€ã¨ã„ã†ã“ã¨ã€‚ã“ã‚ŒãŒã“ã®è«–æ–‡ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è‚ã¨ã®ã“ã¨ã€‚

<br>



<br>



<br>



<br>

ï¼ˆã‚¹ãƒ†ãƒ¼ãƒˆã‚ªãƒ–AIã‚¬ã‚¤ãƒ‰ã®å†…å®¹ã‚’ä¸€éƒ¨è£œè¶³ã—ã¦è¨˜è¿°ã—ã¾ã—ãŸã€‚ã„ã¤ã‚‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ï¼‰

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/140302160-c31717ae-a225-47a4-ae33-f1cd081c419b.png" alt="image" loading="lazy">ç”»åƒç³»ã§ã©ã†ã„ã£ãŸè¨“ç·´æ‰‹æ³•ãŒåˆ©ç”¨ã•ã‚Œã‚‹ã‹è‰²ã€…æ›¸ã‹ã‚Œã¦ã„ãŸã®ã§å‹‰å¼·ã«ãªã£ãŸã€‚ç‰¹ã«ç”»åƒç³»ã®ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µæ‰‹æ³•ãªã‚“ã‹ã¯æ™®æ®µè§¦ã‚‰ãªã„ã®ã§å‹‰å¼·ã«ãªã‚‹ã€‚OpenReview:https://openreview.net/forum?id=NG6MJnVl6M5</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/DataAugmentation.html">#DataAugmentation</a>
<a class="button" href="articles/ContrastiveLearning.html">#ContrastiveLearning</a>
<a class="button" href="articles/Self-SupervisedLearning.html">#Self-SupervisedLearning</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2025-05-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1975">A Simple Framework for Contrastive Learning of Visual Representations, Ting Chen+, ICML'20</a>
<span class="snippet"><span>Summary</span>æœ¬è«–æ–‡ã§ã¯ã€è¦–è¦šè¡¨ç¾ã®å¯¾æ¯”å­¦ç¿’ã®ãŸã‚ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯SimCLRã‚’ææ¡ˆã—ã€ç‰¹åˆ¥ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚„ãƒ¡ãƒ¢ãƒªãƒãƒ³ã‚¯ãªã—ã§å¯¾æ¯”è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã‚’ç°¡ç´ åŒ–ã—ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®é‡è¦æ€§ã€å­¦ç¿’å¯èƒ½ãªéç·šå½¢å¤‰æ›ã®å°å…¥ã«ã‚ˆã‚‹è¡¨ç¾ã®è³ªå‘ä¸Šã€å¯¾æ¯”å­¦ç¿’ãŒå¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºã¨å¤šãã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰åˆ©ç›Šã‚’å¾—ã‚‹ã“ã¨ã‚’ç¤ºã—ã€ImageNetã§å¾“æ¥ã®æ‰‹æ³•ã‚’ä¸Šå›ã‚‹çµæœã‚’é”æˆã—ã¾ã—ãŸã€‚SimCLRã«ã‚ˆã‚‹è‡ªå·±æ•™å¸«ã‚ã‚Šè¡¨ç¾ã‚’ç”¨ã„ãŸç·šå½¢åˆ†é¡å™¨ã¯76.5%ã®ãƒˆãƒƒãƒ—1ç²¾åº¦ã‚’é”æˆã—ã€æ•™å¸«ã‚ã‚ŠResNet-50ã«åŒ¹æ•µã—ã¾ã™ã€‚ãƒ©ãƒ™ãƒ«ã®1%ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸå ´åˆã€85.8%ã®ãƒˆãƒƒãƒ—5ç²¾åº¦ã‚’é”æˆã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span>æ—¥æœ¬èªè§£èª¬:https://techblog.cccmkhd.co.jp/entry/2022/08/30/163625</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<a class="button" href="articles/KnowledgeEditing.html">#KnowledgeEditing</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-05-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1934">Editable Neural Networks, Anton Sinitsin+, ICLR'20</a>
<span class="snippet"><span>Summary</span>æ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®èª¤ã‚Šã‚’è¿…é€Ÿã«ä¿®æ­£ã™ã‚‹ãŸã‚ã«ã€Editable Trainingã¨ã„ã†ãƒ¢ãƒ‡ãƒ«éä¾å­˜ã®è¨“ç·´æ‰‹æ³•ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç‰¹å®šã®ã‚µãƒ³ãƒ—ãƒ«ã®èª¤ã‚Šã‚’åŠ¹ç‡çš„ã«ä¿®æ­£ã—ã€ä»–ã®ã‚µãƒ³ãƒ—ãƒ«ã¸ã®å½±éŸ¿ã‚’é¿ã‘ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚å¤§è¦æ¨¡ãªç”»åƒåˆ†é¡ã¨æ©Ÿæ¢°ç¿»è¨³ã‚¿ã‚¹ã‚¯ã§ãã®æœ‰åŠ¹æ€§ã‚’å®Ÿè¨¼ã€‚</span>
<span class="snippet"><span>Comment</span>ï¼ˆãŠãã‚‰ãï¼‰Knowledge Editingã‚’åˆã‚ã¦ææ¡ˆã—ãŸç ”ç©¶OpenReview:https://openreview.net/forum?id=HJedXaEtvS</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<a class="button" href="articles/Scaling%20Laws.html">#Scaling Laws</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2025-05-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1957">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, Mingxing Tan+, ICML'19</a>
<span class="snippet"><span>Summary</span>æœ¬è«–æ–‡ã§ã¯ã€ConvNetsã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’æ·±ã•ã€å¹…ã€è§£åƒåº¦ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®ã—ã¦ä½“ç³»çš„ã«ç ”ç©¶ã—ã€æ–°ã—ã„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ‰‹æ³•ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€MobileNetsã‚„ResNetã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¨¼ã—ã€EfficientNetsã¨ã„ã†æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’è¨­è¨ˆã€‚ç‰¹ã«EfficientNet-B7ã¯ã€ImageNetã§84.3%ã®ãƒˆãƒƒãƒ—1ç²¾åº¦ã‚’é”æˆã—ã€å¾“æ¥ã®ConvNetsã‚ˆã‚Šã‚‚å°å‹ã‹ã¤é«˜é€Ÿã§ã‚ã‚‹ã€‚CIFAR-100ã‚„Flowersãªã©ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚‚æœ€å…ˆç«¯ã®ç²¾åº¦ã‚’è¨˜éŒ²ã€‚ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span>å…ƒè«–æ–‡ã‚’ãƒ¡ãƒ¢ã£ã¦ãªã‹ã£ãŸã®ã§è¿½åŠ ã€‚

<br>

ãƒ»346

<br>



<br>

ã‚‚å‚ç…§ã®ã“ã¨ã€‚</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2021-06-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/388">On Empirical Comparisons of Optimizers for Deep Learning, Dami Choi+, N_A, arXiv'19</a>
<span class="snippet"><span>Summary</span>æ·±å±¤å­¦ç¿’ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®æ¯”è¼ƒã¯é‡è¦ã§ã‚ã‚Šã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¢ç´¢ç©ºé–“ãŒæ€§èƒ½ã«å½±éŸ¿ã™ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚Œã¦ã„ã‚‹ã€‚ç‰¹ã«ã€é©å¿œçš„å‹¾é…æ³•ã¯å¸¸ã«ä»–ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚ˆã‚Šã‚‚æ€§èƒ½ãŒä½ä¸‹ã—ãªã„ã“ã¨ãŒå®Ÿé¨“ã§ç¤ºã•ã‚Œã¦ãŠã‚Šã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«é–¢ã™ã‚‹å®Ÿç”¨çš„ãªãƒ’ãƒ³ãƒˆã‚‚æä¾›ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span>SGD, Momentum,RMSProp, Adam,NAdamç­‰ã®ä¸­ã‹ã‚‰ã€ã©ã®æœ€é©åŒ–æ‰‹æ³•(Optimizer)ãŒå„ªã‚Œã¦ã„ã‚‹ã‹ã‚’ç”»åƒåˆ†é¡ã¨è¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦æ¯”è¼ƒã—ãŸç ”ç©¶ï¼ˆä¸‹è¨˜æ—¥æœ¬èªè§£èª¬è¨˜äº‹ã‹ã‚‰å¼•ç”¨ï¼‰æ—¥æœ¬èªã§ã®è§£èª¬: https://akichan-f.medium.com/optimizerã¯ã©ã‚ŒãŒå„ªã‚Œã¦ã„ã‚‹ã‹-on-empirical-comparisons-of-optimizers-for-deep-learningã®ç´¹ä»‹-f843179e8a8dAdamãŒè‰¯ã„ã®ã ã‘ã©ã€å­¦ç¿’ç‡ä»¥å¤–ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãªã„ã¨æœ¬æ¥ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç™ºæ®ã•ã‚Œãªã„ã‹ã‚‚ã‚ˆã€ã¨ã„ã†æ„Ÿã˜ã£ã½ã„ICLR 2020 Open Review: https://openreview.net/forum?id=HygrAR4tPSOpenReview:https://openreview.net/forum?id=HygrAR4tPS</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Batch.html">#Batch</a>
<span class="issue_date">Issue Date: 2025-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2196">[Paper Note] Revisiting Small Batch Training for Deep Neural Networks, Dominic Masters+, arXiv'18</a>
<span class="snippet"><span>Summary</span>ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒæ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ€§èƒ½ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’å®Ÿé¨“çš„ã«æ¯”è¼ƒã€‚å¤§ããªãƒŸãƒ‹ãƒãƒƒãƒã¯è¨ˆç®—ã®ä¸¦åˆ—æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŒã€å°ã•ãªãƒŸãƒ‹ãƒãƒƒãƒã¯ä¸€èˆ¬åŒ–æ€§èƒ½ã‚’é«˜ã‚ã€å®‰å®šã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿç¾ã€‚æœ€è‰¯ã®æ€§èƒ½ã¯ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚º$m = 2$ã‹ã‚‰$m = 32$ã®ç¯„å›²ã§å¾—ã‚‰ã‚Œã€æ•°åƒã®ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¨å¥¨ã™ã‚‹ç ”ç©¶ã¨ã¯å¯¾ç…§çš„ã€‚</span>
<span class="snippet"><span>Comment</span>{Res, Reduced Alex}Netã«ãŠã„ã¦ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤§ããã™ã‚‹ã¨ã€å­¦ç¿’ãŒå®‰å®šã—ã‹ã¤é«˜ã„äºˆæ¸¬æ€§èƒ½ã‚’ç²å¾—ã§ãã‚‹å­¦ç¿’ç‡ã®rangeãŒå°ã•ããªã‚‹ã€‚ä¸€æ–¹ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå°ã•ã„ã¨æœ‰åŠ¹ãªå­¦ç¿’ç‡ã®rangeãŒåºƒã„ã€‚ã¾ãŸã€ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå°ã•ã„å ´åˆã¯ã€å‹¾é…è¨ˆç®—ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆãŒã‚ˆã‚Šé »ç¹ã«è¡Œã‚ã‚Œã‚‹ã€‚ã“ã®ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŒã‚ˆã‚Šé€²ã‚“ã çŠ¶æ…‹ã§å€‹ã€…ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å‹¾é…è¨ˆç®—ãŒè¡Œã‚ã‚Œã‚‹ãŸã‚ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå¤§ãã„å ´åˆã¨æ¯”ã¹ã‚‹ã¨ãƒ¢ãƒ‡ãƒ«ãŒã‚ˆã‚Šæ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹ã§å„ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å‹¾é…ãŒè¨ˆç®—ã•ã‚Œã‚‹ã“ã¨ã«ãªã‚‹ãŸã‚ã€å­¦ç¿’ãŒå®‰å®šã—è‰¯ã„æ±åŒ–æ€§èƒ½ã«ã¤ãªãŒã‚‹ã€ã¨ã„ã£ãŸè©±ã®æ¨¡æ§˜ã€‚

<br>



<br>

<img src="https://github.com/user-attachments/assets/f02f9016-6e9f-476d-a4c1-4f64bd51e9d5" alt="image" loading="lazy"></span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Normalization.html">#Normalization</a>
<span class="issue_date">Issue Date: 2025-04-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1856">Group Normalization, Yuxin Wu+, arXiv'18</a>
<span class="snippet"><span>Summary</span>ã‚°ãƒ«ãƒ¼ãƒ—æ­£è¦åŒ–ï¼ˆGNï¼‰ã¯ã€ãƒãƒƒãƒæ­£è¦åŒ–ï¼ˆBNï¼‰ã®ä»£æ›¿æ‰‹æ®µã¨ã—ã¦ææ¡ˆã•ã‚Œã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã«ä¾å­˜ã›ãšå®‰å®šã—ãŸç²¾åº¦ã‚’æä¾›ã—ã¾ã™ã€‚ç‰¹ã«ã€ãƒãƒƒãƒã‚µã‚¤ã‚º2ã®ResNet-50ã§ã¯ã€GNãŒBNã‚ˆã‚Šã‚‚10.6%ä½ã„èª¤å·®ã‚’ç¤ºã—ã€ä¸€èˆ¬çš„ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã‚‚åŒç­‰ã®æ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚GNã¯ç‰©ä½“æ¤œå‡ºã‚„ãƒ“ãƒ‡ã‚ªåˆ†é¡ãªã©ã®ã‚¿ã‚¹ã‚¯ã§BNã‚’ä¸Šå›ã‚‹çµæœã‚’ç¤ºã—ã€ç°¡å˜ã«å®Ÿè£…å¯èƒ½ã§ã™ã€‚</span>
<span class="snippet"><span>Comment</span>BatchNormalizationã¯ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå°ã•ã„ã¨ã†ã¾ãã„ã‹ãšã€ãƒ¡ãƒ¢ãƒªã®åˆ¶ç´„ã§å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºãŒè¨­å®šã§ããªã„å ´åˆã«å›°ã‚‹ã‹ã‚‰ãƒãƒƒãƒã‚µã‚¤ã‚ºã«ä¾å­˜ã—ãªã„normalizationã‚’è€ƒãˆãŸã‚ˆã€‚LayerNormã¨InstanceNormã‚‚ãƒãƒƒãƒã‚µã‚¤ã‚ºã«ä¾å­˜ã—ãªã„ã‘ã©ææ¡ˆæ‰‹æ³•ã®æ–¹ãŒç”»åƒç³»ã®ã‚¿ã‚¹ã‚¯ã ã¨æ€§èƒ½ãŒè‰¯ã„ã‚ˆã€ã¨ã„ã†è©±ã‚‰ã—ã„ã€‚

<br>



<br>

å„normalizationã¨ã®æ¯”è¼ƒã€‚åˆ†ã‹ã‚Šã‚„ã™ã„ã€‚

<br>

<img src="https://github.com/user-attachments/assets/128a6a2e-cac7-4d6a-9cf6-31119fb6b187" alt="image" loading="lazy"></span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Optimizer.html">#Optimizer</a>
<span class="issue_date">Issue Date: 2023-12-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1185">Large Batch Training of Convolutional Networks, Yang You+, N_A, arXiv'17</a>
<span class="snippet"><span>Summary</span>å¤§è¦æ¨¡ãªç•³ã¿è¾¼ã¿ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã«ã€æ–°ã—ã„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆã—ã¾ã—ãŸã€‚ã“ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€Layer-wise Adaptive Rate Scalingï¼ˆLARSï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ãªãŒã‚‰ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’æãªã‚ãšã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€Alexnetã‚’8Kã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã¾ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ã€Resnet-50ã‚’32Kã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã¾ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span>BatchSizeã‚’å¤§ããã™ã‚‹ã¨æ€§èƒ½ãŒè½ã¡ã¾ã™ã‚ˆã€ç³»ã®è©±ï¼ˆCNNï¼‰

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/deeb60b7-548c-4e50-94db-ce98eaf268e3" alt="image" loading="lazy">OpenReview:https://openreview.net/forum?id=rJ4uaX2aW

<br>



<br>

ICLR'18ã«rejectã•ã‚Œã¦ã„ã‚‹

<br>



<br>

å…ˆè¡Œç ”ç©¶ã§ææ¡ˆã‚ˆã‚Šã‚‚å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ‰±ãˆã‚‹synchronized SGDã¯å¼·ã¿ã ãŒã€è©•ä¾¡ãŒä¸€ã¤ã®ã‚¿ã‚¹ã‚¯ã®ã¿ãªã®ã§ã‚ˆã‚Šå¢—ã‚„ã—ãŸæ–¹ãŒconvincingã ã¨ã„ã†ã“ã¨ã€ææ¡ˆæ‰‹æ³•ã«è¿½åŠ ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¿…è¦ãªç‚¹ãŒæ‰‹æ³•ã‚’less appealingã«ã—ã¦ã—ã¾ã£ã¦ã„ã‚‹ã“ã¨ã€layer wise rate scailng (LARS)ã®ç†è«–çš„ãªjustificationãŒä½•ã‹æ¬²ã—ã„ã“ã¨ã€å…ˆè¡Œç ”ç©¶ã¨ã®æ¯”è¼ƒãŒã‚¯ãƒªã‚¢ã§ã¯ãªã„ã“ã¨ã€ãªã©ãŒç†ç”±ãªæ¨¡æ§˜ã€‚</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/CommentGeneration.html">#CommentGeneration</a>
<a class="button" href="articles/CVPR.html">#CVPR</a>
<span class="issue_date">Issue Date: 2019-09-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/327">Attend to You: Personalized Image Captioning with Context Sequence Memory Networks, Park+, CVPR'17</a>
<span class="snippet"><span>Comment</span>ç”»åƒãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã«ã€ãã®ç”»åƒã«å¯¾ã™ã‚‹Hashtag predictionã¨ã€personalizedãªpost generationã‚’è¡Œã†ã‚¿ã‚¹ã‚¯ã‚’ææ¡ˆã€‚

<br>



<br>

Instagramã®Postã®ç°¡æ˜“åŒ–ãªã©ã«å¿œç”¨ã§ãã‚‹ã€‚

<br>



<br>

Postã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ã¯ã€è‡ªèº«ã®è¨€è‘‰ã§ã€ç”»åƒã«ã¤ã„ã¦ã®èª¬æ˜ã‚„ã€contextã¨ã„ã£ãŸã“ã¨ã‚’èª¬æ˜ã—ãªã‘ã‚Œã°ãªã‚‰ãšã€image captioningã‚’ã™ã‚‹éš›ã«Personalization IssueãŒç”Ÿã˜ã‚‹ã“ã¨ã‚’æŒ‡æ‘˜ã€‚

<br>



<br>



<br>



<br>

</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/90">Multi-Task Video Captioning with Video and Entailment Generation, Pasunuru+, ACL'17</a>
<span class="snippet"><span>Comment</span>è§£èª¬ã‚¹ãƒ©ã‚¤ãƒ‰ï¼šhttps://www.slideshare.net/HangyoMasatsugu/hangyo-acl-paperreading2017multitask-video-captioning-with-video-and-entailment-generation/1multitask learningã§å‹•ç”»ï¼ˆã‹ãªã‚ŠçŸ­ã‚ï¼‰ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚’è¡Œãªã£ãŸè©±

<br>



<br>

(2025.05.12)

<br>

ä¸Šè¨˜è§£èª¬è³‡æ–™ä¸­ã®ã‚¹ã‚¯ã‚·ãƒ§ãŒã„ãã¤ã‹æ²è¼‰ã•ã‚Œã¦ã„ã¾ã—ãŸãŒå‰Šé™¤ã—ã¾ã—ãŸã€‚</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/GenerativeAdversarialNetwork.html">#GenerativeAdversarialNetwork</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/60">Generative Adversarial Networks: An Overview, Dumoulin+, IEEE-SPM'17</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Visual%20Words.html">#Visual Words</a>
<a class="button" href="articles/CVPR.html">#CVPR</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/63">Image Captioning with Semantic Attention, You+, CVPR'16.</a>
<span class="snippet"><span>Comment</span>ç”»åƒãã®ã‚‚ã®ã ã‘ã§ãªãã€ãƒ¢ãƒ‡ãƒ«ã¸ã®Inputã«Visual Wordsã‚’æ˜ç¤ºçš„ã«åŠ ãˆã‚‹ã“ã¨ã§ã€captioningã®ç²¾åº¦ãŒä¸ŠãŒã‚Šã¾ã—ãŸã¨ã„ã†è«–æ–‡</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Visual%20Words.html">#Visual Words</a>
<a class="button" href="articles/CVPR.html">#CVPR</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/62">What Value Do Explicit High Level Concepts Have in Vision to Language Problems?, Wu+, CVPR'16.</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/ECCV.html">#ECCV</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/61">Generating Visual Explanations, Hendrickks+, ECCV'16</a>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/ImageCaptioning.html">#ImageCaptioning</a>
<a class="button" href="articles/Reference-based.html">#Reference-based</a>
<span class="issue_date">Issue Date: 2023-05-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/670">CIDEr: Consensus-based Image Description Evaluation, Ramakrishna Vedantam+, N_A, CVPR'15</a>
<span class="snippet"><span>Summary</span>ç”»åƒã‚’æ–‡ç« ã§è‡ªå‹•çš„ã«èª¬æ˜ã™ã‚‹ã“ã¨ã¯ã€é•·å¹´ã®èª²é¡Œã§ã‚ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€äººé–“ã®åˆæ„ã‚’åˆ©ç”¨ã—ãŸç”»åƒèª¬æ˜ã®è©•ä¾¡ã®ãŸã‚ã®æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’ææ¡ˆã—ã€æ–°ã—ã„è‡ªå‹•è©•ä¾¡æŒ‡æ¨™ã¨2ã¤ã®æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å«ã‚€ã€‚ææ¡ˆæ‰‹æ³•ã¯ã€äººé–“ã®åˆ¤æ–­ã‚’ã‚ˆã‚Šæ­£ç¢ºã«æ‰ãˆã‚‹ã“ã¨ãŒã§ãã€5ã¤ã®æœ€å…ˆç«¯ã®ç”»åƒèª¬æ˜æ‰‹æ³•ã‚’è©•ä¾¡ã—ã€å°†æ¥ã®æ¯”è¼ƒã®ãŸã‚ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’æä¾›ã™ã‚‹ã€‚CIDEr-Dã¯ã€MS COCOè©•ä¾¡ã‚µãƒ¼ãƒãƒ¼ã®ä¸€éƒ¨ã¨ã—ã¦åˆ©ç”¨å¯èƒ½ã§ã‚ã‚Šã€ã‚·ã‚¹ãƒ†ãƒãƒ†ã‚£ãƒƒã‚¯ãªè©•ä¾¡ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<a class="button" href="articles/ImageClassification.html">#ImageClassification</a>
<span class="issue_date">Issue Date: 2025-05-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1958">ImageNet Classification with Deep Convolutional Neural Networks, Krizhevsky+, NIPS'12</a>
<span class="snippet"><span>Comment</span>ILSVRC 2012ã«ãŠã„ã¦åœ§å€’çš„ãªæ€§èƒ½ç¤ºã—ãŸã“ã¨ã§ç¾ä»£ã®DeepLearningã®ç«ä»˜ã‘å½¹ã¨ãªã£ãŸç ”ç©¶AlexNetã€‚ãƒ¡ãƒ¢ã£ã¦ãªã‹ã£ãŸã®ã§ä»Šæ›´ãªãŒã‚‰è¿½åŠ ã—ãŸã€‚AlexNetä»¥å‰ã®ç”»åƒèªè­˜æŠ€è¡“ã«ã¤ã„ã¦ã¯ç‰›ä¹…å…ˆç”ŸãŒã¾ã¨ã‚ã¦ãã ã•ã£ã¦ã„ã‚‹ï¼ˆå½“æ™‚ã®èª²é¡Œã¨ãã‚Œã«å¯¾ã™ã‚‹è§£æ±ºæ³•ã€ã—ã‹ã—ã¾ã èª²é¡ŒãŒâ€¦ã¨æ¬¡ã€…ã¨èª²é¡Œã«ç›´é¢ã—è§£æ±ºã—ã¦ã„ãæ§˜å­ãŒæã‹ã‚Œã¦ãŠã‚Šéå¸¸ã«èˆˆå‘³æ·±ã‹ã£ãŸ)ã€‚ç¾åœ¨ã§ã‚‚æ®‹ã£ã¦ã„ã‚‹æŠ€è¡“ã‚‚ç´¹ä»‹ã•ã‚Œã¦ã„ã‚‹ã€‚:

<br>

https://speakerdeck.com/yushiku/pre_alexnet

<br>



<br>

&gt; éå»ã®æŠ€è¡“ã ã‹ã‚‰ã¨ã„ã£ã¦èãæµã—ã¦ã„ã‚‹ã¨æ™‚ä»£èƒŒæ™¯ã®å¤‰åŒ–ã«ã‚ˆã£ã¦ãªã—å¾—ãŸã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é€ƒã™ã‹ã‚‚

<br>



<br>

ã“ã‚Œã¯è‚ã«éŠ˜ã˜ãŸã„ã€‚</span>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<a class="button" href="articles/ImageClassification.html">#ImageClassification</a>
<a class="button" href="articles/ObjectRecognition.html">#ObjectRecognition</a>
<a class="button" href="articles/ObjectLocalization.html">#ObjectLocalization</a>
<span class="issue_date">Issue Date: 2025-05-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1959">ImageNet: A Large-Scale Hierarchical Image Database, Deng+, CVPR'09</a>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/VideoGeneration/Understandings.html">#VideoGeneration/Understandings</a>
<span class="issue_date">Issue Date: 2025-07-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2312">Wan2.2, Alibaba Wan, 2025.07</a>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/alibaba_wan/status/1949827662416937443?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qåˆã®MoEã«ã‚ˆã‚‹Open WeightãªVideo generationãƒ¢ãƒ‡ãƒ«ã§ã€ç›´æ¥çš„ã«æ˜ã‚‹ã•ã‚„ã€ã‚«ãƒ©ãƒ¼ã€ã‚«ãƒ¡ãƒ©ã®å‹•ããªã©ã‚’åˆ¶å¾¡ã§ãã€text to video, image to video, unified video generationã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹æ¨¡æ§˜</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Document.html">#Document</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/parser.html">#parser</a>
<a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2294">LLM APIs Are Not Complete Document Parsers, Jerry Liu, 2025.07</a>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/jerryjliu0/status/1948475176062255504?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html">#MoE(Mixture-of-Experts)</a>
<span class="issue_date">Issue Date: 2025-06-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2115">ERNIE 4.5 Series, ERNIE TEAM, 2025.06</a>
<span class="snippet"><span>Comment</span>Tech Report:https://yiyan.baidu.com/blog/publication/ERNIE_Technical_Report.pdfå…ƒãƒã‚¹ãƒˆ:https://x.com/paddlepaddle/status/1939535276197744952?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qè§£èª¬ãƒã‚¹ãƒˆ:https://x.com/gm8xx8/status/1939576393098023188?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/Slide.html">#Slide</a>
<a class="button" href="articles/CVPR.html">#CVPR</a>
<span class="issue_date">Issue Date: 2025-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2098">CVPR 2025 é€Ÿå ±, Kataoka+, 2025.06</a>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/hirokatukataoka/status/1937815247923950079?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qã™ã”ã„ã¾ã¨ã‚ã â€¦</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<span class="issue_date">Issue Date: 2025-06-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2075">Kimi-VL-A3B-Thinking-2506, moonshotai, 2025.06</a>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/reach_vb/status/1937159672932286950?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qæ§˜ã€…ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§SoTA(gpt4o, Qwen2.5-VL-7B)ã‚’é”æˆã—ãŸReasoning VLMãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒšãƒ¼ãƒ‘ãƒ¼:

<br>

ãƒ»2200</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/FoundationModel.html">#FoundationModel</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<a class="button" href="articles/Video.html">#Video</a>
<span class="issue_date">Issue Date: 2025-06-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2034">V-JEPA 2, Meta, 2025.06</a>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/mervenoyann/status/1932814909722800196?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QPhysical Reasoning Leaderboardãªã‚‹ã‚‚ã®ã§ç¾åœ¨ãƒˆãƒƒãƒ—ãªæ¨¡æ§˜ã€‚

<br>



<br>

https://huggingface.co/spaces/facebook/physical_reasoning_leaderboard</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a>
<a class="button" href="articles/Slide.html">#Slide</a>
<span class="issue_date">Issue Date: 2025-05-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1987">ã€DLè¼ªèª­ä¼šã€‘ Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models, Deep Learning JP, 2025.05</a>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/kym384/status/1925852937835737569?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q1986 ã§Literatureã‚’ã–ã£ãã‚ŠæŠŠæ¡ã—ã¦ã‹ã‚‰ã“ã¡ã‚‰ã‚’èª­ã‚€ã®ãŒè‰¯ã•ãã†ã€‚</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a>
<a class="button" href="articles/Slide.html">#Slide</a>
<span class="issue_date">Issue Date: 2025-05-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1986">Masked Diffusion Modelã®é€²å±•, Deep Learning JP, 2025.03</a>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/kym384/status/1925852884656099572?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qã‚¹ãƒ©ã‚¤ãƒ‰ä¸­ã®ARã®ã‚ˆã†ã«KV CacheãŒä½¿ãˆãªã„å•é¡Œã«å¯¾å‡¦ã—ãŸç ”ç©¶ãŒ

<br>

ãƒ»1984

<br>



<br>

ã“ã®è¾ºã¯dLLMãŒæœ‰æœ›ã§ã‚ã‚Œã°ã€ã©ã‚“ã©ã‚“é€²åŒ–ã—ã¦ã„ãã®ã ã‚ã†ã€‚</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/AWS.html">#AWS</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<a class="button" href="articles/Japanese.html">#Japanese</a>
<span class="issue_date">Issue Date: 2025-05-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1976">Webã‚¹ã‚±ãƒ¼ãƒ«ã®æ—¥æœ¬èª-ç”»åƒã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ŒMOMIJIã€ã®æ§‹ç¯‰ _å·¨å¤§ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’AWSã§é«˜é€Ÿã«å‡¦ç†ã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³, Turing ï¼ˆstudio_graphï¼‰, 2025.05</a>
<span class="snippet"><span>Comment</span>è²´é‡ãªVLMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰ãƒã‚¦ãƒã‚¦é’å¡—ã‚Šã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã‚’å…·ä½“çš„ã«ã©ã†ã‚„ã£ã¦ã„ã‚‹ã®ã‹æ°—ã«ãªã‚‹</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LLMAgent.html">#LLMAgent</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<a class="button" href="articles/x-Use.html">#x-Use</a>
<span class="issue_date">Issue Date: 2025-04-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1896">Introducing UI-TARS-1.5, ByteDance, 2025.04</a>
<span class="snippet"><span>Summary</span>UI-TARSã¯ã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’å…¥åŠ›ã¨ã—ã¦äººé–“ã®ã‚ˆã†ã«ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¡Œã†ãƒã‚¤ãƒ†ã‚£ãƒ–GUIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€å¾“æ¥ã®å•†æ¥­ãƒ¢ãƒ‡ãƒ«ã«ä¾å­˜ã›ãšã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚å®Ÿé¨“ã§ã¯ã€10ä»¥ä¸Šã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§SOTAæ€§èƒ½ã‚’é”æˆã—ã€ç‰¹ã«OSWorldã‚„AndroidWorldã§ä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²ã—ã¾ã—ãŸã€‚UI-TARSã¯ã€å¼·åŒ–ã•ã‚ŒãŸçŸ¥è¦šã€çµ±ä¸€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€ã‚·ã‚¹ãƒ†ãƒ -2æ¨è«–ã€åå°„çš„ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒˆãƒ¬ãƒ¼ã‚¹ã«ã‚ˆã‚‹åå¾©ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãªã©ã®é©æ–°ã‚’å–ã‚Šå…¥ã‚Œã€æœ€å°é™ã®äººé–“ã®ä»‹å…¥ã§é©å¿œã—ç¶šã‘ã‚‹èƒ½åŠ›ã‚’æŒã£ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span>paper:https://arxiv.org/abs/2501.12326è‰²ã€…ã¨æ›¸ã„ã¦ã‚ã‚‹ãŒã€ã–ã£ãã‚Šè¨€ã†ã¨ByteDanceã«ã‚ˆã‚‹ã€Imageã¨Textã‚’inputã¨ã—ã¦å—ã‘å–ã‚Šã€Textã‚’outputã™ã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMã«ã‚ˆã‚‹Computer Use Agent (CUA)é–¢é€£

<br>

ãƒ»1794å…ƒãƒã‚¹ãƒˆ:https://x.com/_akhaliq/status/1912913195607663049?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<span class="issue_date">Issue Date: 2025-04-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1880">Large Vision Language Model ï¼ˆLVLMï¼‰ ã«é–¢ã™ã‚‹æœ€æ–°çŸ¥è¦‹ã¾ã¨ã‚ ï¼ˆPart 1ï¼‰, Daiki Shiono, 2024.11</a>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<span class="issue_date">Issue Date: 2025-04-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1863">Llama 4 Series, Meta, 2025.04</a>
<span class="snippet"><span>Comment</span>Downloads:https://www.llama.com/?utm_source=twitter&utm_medium=organic_social&utm_content=image&utm_campaign=llama4Huggingface:

<br>

https://huggingface.co/collections/meta-llama/llama-4-67f0c30d9fe03840bc9d0164è§£èª¬ãƒã‚¹ãƒˆ:https://x.com/iscienceluvr/status/1908601269004230763?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QArtificial Analysisã«ã‚ˆã‚‹æ€§èƒ½æ¤œè¨¼:https://x.com/artificialanlys/status/1908890796415414430?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>



<br>

MaverickãŒGPT4oã¨åŒç­‰ã€ScoutãŒGPT4o-miniã¨åŒç­‰

<br>



<br>

Update:https://x.com/artificialanlys/status/1909624239747182989?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qæ€§èƒ½ã«é–¢ã—ã¦ä¸å¯è§£ãªç‚¹ãŒå¤šãã†ãªã®ã§æ§˜å­è¦‹ã‚’ã—ã¦ã‚‚è‰¯ã„ã‹ã‚‚ã€‚æ€§èƒ½æ¤œè¨¼ï¼ˆMath-Perturb):https://x.com/kaixuanhuang1/status/1909387970773234088?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qæ—¥æœ¬èªã«ã‚ã¾ã‚Šå¼·ããªã„ã¨ã„ã†æƒ…å ±ã‚‚

<br>

å…ƒãƒã‚¹ãƒˆ:https://x.com/gosrum/status/1909626761098494060?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qã©ã†ã‚„ã‚‰vLLMã®Llama4ã®inferenceã«ãƒã‚°ãŒã‚ã£ãŸã‚„ã†ã§ã€vLLMã®Issue 16311ã«ã¦ã€Llama4ã®inferenceã«é–¢ã™ã‚‹ãƒã‚°ãŒä¿®æ­£ã•ã‚Œã€æ€§èƒ½ãŒå‘ä¸Šã—ãŸæ¨¡æ§˜ã€‚ã©ã®ãƒ™ãƒ³ãƒã‚’ä¿¡ã˜ãŸã‚‰è‰¯ã„ã‹ã¾ã‚‹ã§ã‚ã‹ã‚‰ã‚“ã€‚2025.0413ç¾åœ¨ã®chatbot arenaã®ãƒ©ãƒ³ã‚¯ã¯ã€32ä½ã¨ãªã‚Šï¼ˆchatbot arenaå‘ã‘ã«tuningã•ã‚Œã¦ã„ãŸã§ã‚ã‚ã†ãƒ¢ãƒ‡ãƒ«ã¯2ä½ã ã£ãŸï¼‰GPT-4oãŒ29ä½ã§ã‚ã‚‹ã“ã¨ã‚’è€ƒæ…®ã™ã‚‹ã¨ä¸Šè¨˜ã®Artificial Intelligenceã®è©•ä¾¡ã¨ã‚‚å¤§ä½“ä¸€è‡´ã—ã¦ã„ã‚‹ã€‚

<br>



<br>

https://lmarena.ai

<br>



<br>

é–¢é€£ãƒã‚¹ãƒˆ:https://x.com/tunguz/status/1911142310160855541?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<span class="issue_date">Issue Date: 2025-03-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1835">Qwen2.5-VL-32B-Instruct, Qwen Team, 2025.03</a>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/alibaba_qwen/status/1904227859616641534?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html">#SSM (StateSpaceModel)</a>
<span class="issue_date">Issue Date: 2025-03-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1830">Nemotron-H: A Family of Accurate, Efficient Hybrid Mamba-Transformer Models, Nvidia, 2025.03</a>
<span class="snippet"><span>Comment</span>é–¢é€£:

<br>

ãƒ»1820Transformerã®Self-attention Layerã‚’Mamba2 Layerã«ç½®æ›ã™ã‚‹ã“ã¨ã§ã€æ§˜ã€…ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§åŒç­‰ã®æ€§èƒ½ã€ã‚ã‚‹ã„ã¯ä¸Šå›ã‚‹æ€§èƒ½ã§3å€ç¨‹åº¦ã®Inference timeã®é«˜é€ŸåŒ–ã‚’ã—ã¦ã„ã‚‹ï¼ˆ65536 input, 1024 outputï¼‰ã€‚

<br>



<br>

56Bç¨‹åº¦ã®mediumã‚µã‚¤ã‚ºã®ãƒ¢ãƒ‡ãƒ«ã¨ã€8Bç¨‹åº¦ã®è»½é‡ãªãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦è¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹ã€‚ç‰¹ã«ã€8Bãƒ¢ãƒ‡ãƒ«ã§Mambaã¨Transformerã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã¨ã€é€šå¸¸ã®Transformerãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã—ã¦ã„ã‚‹ã€‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«15 Trillion Tokenã‚’åˆ©ç”¨ã—ã¦ãŠã‚Šã€ã“ã®ãƒ‡ãƒ¼ã‚¿é‡ã§ã®Apple to Appleã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£é–“ã®æ¯”è¼ƒã¯ã€ç¾çŠ¶ã§ã¯æœ€ã‚‚å¤§è¦æ¨¡ãªã‚‚ã®ã¨ã®ã“ã¨ã€‚æ€§èƒ½ã¯å¤šãã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã«ã—ã¦ã‚‚åŒç­‰ã€Commonsense Understandingã§ã¯ä¸Šå›ã£ã¦ã„ã‚‹ã€‚

<br>



<br>

ã¾ãŸã€å­¦ç¿’ã—ãŸNemotron-Hã‚’ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æŒã¤VLMã«ã¤ã„ã¦ã‚‚ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒè¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹ã€‚</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<span class="issue_date">Issue Date: 2025-03-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1809">SmolDocling-256M, IBM Research, 2025.03</a>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://www.linkedin.com/posts/andimarafioti_we-just-dropped-%F0%9D%97%A6%F0%9D%97%BA%F0%9D%97%BC%F0%9D%97%B9%F0%9D%97%97%F0%9D%97%BC%F0%9D%97%B0%F0%9D%97%B9%F0%9D%97%B6%F0%9D%97%BB%F0%9D%97%B4-activity-7307415358427013121-wS8m?utm_source=share&utm_medium=member_ios&rcm=ACoAACzQvjwB2FeLVE3yukDiUYtr5J4k-6nlNG4Apache-2.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã€‚è¨€èªã¯Englishã®ã¿ãªæ¨¡æ§˜ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªImage-To-Textãƒ¢ãƒ‡ãƒ«ã€‚ã‚µãƒ³ãƒ—ãƒ«ã¯ã“ã¡ã‚‰

<br>

<img src="https://github.com/user-attachments/assets/d16ce5a9-4336-4daa-ab6f-94d67ae77c41" alt="image" loading="lazy"></span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/ProprietaryLLM.html">#ProprietaryLLM</a>
<span class="issue_date">Issue Date: 2025-03-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1808">ERNIE4.5_X1, Baidu, 2025.03</a>
<span class="snippet"><span>Comment</span>è§£èª¬ãƒã‚¹ãƒˆ:https://x.com/ai_for_success/status/1901149459826045223?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qãƒ»ERNIE4.5ã¯GPT4.5ã‚’ã•ã¾ã–ã¾ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ä¸Šå›ã‚Šã€ä¾¡æ ¼ãŒãªã‚“ã¨GPT4.5ã®1%

<br>

ãƒ»X1ã¯ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªreasoningãƒ¢ãƒ‡ãƒ«ã§DeepSeek-R1ã¨åŒç­‰ã®æ€§èƒ½ã§åŠé¡

<br>



<br>

ã‚‰ã—ã„ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯6æœˆ30æ—¥ã«ã‚ªãƒ¼ãƒ—ãƒ³ï¼ˆã‚¦ã‚§ã‚¤ãƒˆï¼Ÿï¼‰ã«ãªã‚‹ã¨ã‚¹ãƒ¬ãƒƒãƒ‰ã§è¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹ã€‚</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<span class="issue_date">Issue Date: 2025-03-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1807">sarashina2-vision-{8b, 14b}, SB Intuitions, 2025.03</a>
<span class="snippet"><span>Comment</span>å…ƒãƒã‚¹ãƒˆ:https://x.com/sei_shinagawa/status/1901467733331701966?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QVLMã€‚Xã«æ•£è¦‹ã•ã‚Œã‚‹è©¦è¡Œä¾‹ã‚’è¦‹ã‚‹ã¨æ—¥æœ¬èªã®èª­ã¿å–ã‚Šæ€§èƒ½ã¯çµæ§‹é«˜ãã†ã«è¦‹ãˆã‚‹ã€‚ãƒ¢ãƒ‡ãƒ«æ§‹æˆã€å­¦ç¿’ã®è©³ç´°ã€ãŠã‚ˆã³è©•ä¾¡:https://x.com/sbintuitions/status/1901472307421278604?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QLLMï¼ˆsarashina2ï¼‰, Vision Encoderï¼ˆQwen2-VLï¼‰, Projectorã®3ã¤ã§æ§‹æˆã•ã‚Œã¦ãŠã‚Šã€3æ®µéšã®å­¦ç¿’ã‚’è¸ã‚“ã§ã„ã‚‹ã€‚

<br>

æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦Projectorã®ã¿ã‚’å­¦ç¿’ã—Vision Encoderã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’å¯¾å¿œã¥ã‘ã‚‹ã€‚ç¶šã„ã¦ã€æ—¥æœ¬èªã‚’å«ã‚€ç”»åƒã‚„æ—¥æœ¬ç‰¹æœ‰ã®é¢¨æ™¯ãªã©ã‚’ã†ã¾ãæ‰±ãˆã‚‹ã‚ˆã†ã«ã€ã“ã‚Œã‚‰ã‚’å¤šãæ´»ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ï¼ˆå†…è£½æ—¥æœ¬èªOCRãƒ‡ãƒ¼ã‚¿ã€å›³è¡¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’ç”¨ã„ã¦ã€Vision Encoderã¨Projectorã‚’å­¦ç¿’ã€‚æœ€å¾Œã«LLMã®Alignmentã‚’ã¨ã‚‹ãŸã‚ã«ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚¿ãƒ¼ã¨LLMã‚’å‰æ®µã®ãƒ‡ãƒ¼ã‚¿ã«åŠ ãˆã¦VQAãƒ‡ãƒ¼ã‚¿ï¼ˆå†…è£½åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ï¼‰ã‚„æ—¥æœ¬èªã®æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦å­¦ç¿’ã€‚Projectorã‚„MMLLMã‚’å…·ä½“çš„ã«ã©ã®ã‚ˆã†ã«å­¦ç¿’ã™ã‚‹ã‹ã¯

<br>

ãƒ»1225

<br>



<br>

ã‚’å‚ç…§ã®ã“ã¨ã€‚</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<span class="issue_date">Issue Date: 2025-01-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1737">Janus-Series: Unified Multimodal Understanding and Generation Models, DeepSeek, 2025.01</a>
<span class="snippet"><span>Comment</span>DeepSeekã«ã‚ˆã‚‹æ–°ãŸãªVLMã€Janus-ProãŒæœ¬æ—¥ãƒªãƒªãƒ¼ã‚¹ã€‚MIT LicenseJanus-Proã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€‚

<br>



<br>

githubä¸Šã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å›³è§£ã‹ã‚‰å¼•ç”¨ã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ï¼ˆãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒï¼‰ã®ç†è§£ã«é–¢ã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§LLaVAè¶…ãˆã€‚GenEval, DPG Benchã¨å‘¼ã°ã‚Œã‚‹ç”»åƒç”Ÿæˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§DALL-E 3è¶…ãˆã€‚

<br>

<img src="https://github.com/user-attachments/assets/39b51e99-723d-4105-a113-e4bfa847c69b" alt="image" loading="lazy">

<br>



<br>



<br>

ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆä¸­ã§ã®è©³ç´°ã‹ã‚‰å¼•ç”¨ã€‚ã©ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã‚‚åŸºæœ¬çš„ã«æœ€é«˜æ€§èƒ½ãªã‚ˆã†ã«è¦‹ãˆã‚‹ã€‚

<br>

<img src="https://github.com/user-attachments/assets/4c1bd071-966f-4d51-99f4-e60fa2f36b0a" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/a0b22d6e-debb-420a-bf8d-fe8833583d09" alt="image" loading="lazy">

<br>



<br>

ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆ: https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<span class="issue_date">Issue Date: 2025-01-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1730">Humanity's Last Exam, 2025.01</a>
<span class="snippet"><span>Summary</span>ã€Œäººé¡ã®æœ€å¾Œã®è©¦é¨“ã€ã¨ã„ã†æ–°ã—ã„ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å°å…¥ã—ã€100ä»¥ä¸Šã®ç§‘ç›®ã«ã‚ãŸã‚‹3,000ã®æŒ‘æˆ¦çš„ãªè³ªå•ã‚’æä¾›ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€LLMã®èƒ½åŠ›ã‚’æ­£ç¢ºã«æ¸¬å®šã—ã€éå­¦ç¿’ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã‚‚ä¿æŒã€‚</span>
<span class="snippet"><span>Comment</span>o1, DeepSeekR1ã®æ­£è§£ç‡ãŒ10%æœªæº€ã®æ–°ãŸãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<span class="issue_date">Issue Date: 2025-01-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1662">Killed by LLM, R0bk</a>
<span class="snippet"><span>Comment</span>Saturationã¨ãªã£ã¦ã„ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ã€æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’ã™ã§ã«æ¸¬å®šã§ããªããªã£ã¦ã—ã¾ã£ãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã®ã“ã¨ã€‚</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<a class="button" href="articles/ProprietaryLLM.html">#ProprietaryLLM</a>
<span class="issue_date">Issue Date: 2025-01-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1633">2024-ai-timeline, reach-vb, 2025.01</a>
<span class="snippet"><span>Comment</span>æœˆåˆ¥ã§2024å¹´ã«ãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸä¸»è¦ãªLLMï¼ˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªLLMã‚‚å«ã‚€ï¼‰ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãŒã¾ã¨ã‚ã‚‰ã‚Œã¦ã„ã‚‹ã€‚

<br>

API Onlyï¼ˆãƒ—ãƒ­ãƒ—ãƒ©ã‚¤ã‚¨ã‚¿ãƒªï¼‰ãªã®ã‹ã€OpenWeightãªã®ã‹ã‚‚ã‚¿ã‚°ä»˜ã‘ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/FoundationModel.html">#FoundationModel</a>
<a class="button" href="articles/MultiLingual.html">#MultiLingual</a>
<span class="issue_date">Issue Date: 2024-12-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1571">Introducing Amazon Nova, our new generation of foundation models, AWS, 2024.12</a>
<span class="snippet"><span>Comment</span>å‚è€ƒ:https://qiita.com/ysit/items/8433d149dbaab702d526ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆ: https://assets.amazon.science/9f/a3/ae41627f4ab2bde091f1ebc6b830/the-amazon-nova-family-of-models-technical-report-and-model-card.pdfå¾Œã§å€‹ã€…ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’ã¾ã¨ã‚ãŸã„ã€‚

<br>



<br>

ã¾ã‚ã§ã‚‚ã–ã£ãã‚Šè¨€ã†ã¨ã€ä»–ã®proprietaryãƒ¢ãƒ‡ãƒ«ã¨ã‚‚ãŠãŠã‚€ã­åŒç­‰ã®æ€§èƒ½ã§ã™ã€ã¨ã„ã†æ„Ÿã˜ã«è¦‹ãˆã‚‹ã€‚å€‹ã€…ã®ã‚¿ã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã§è¦‹ã‚‹ã¨ã€å¾—æ„ãªã‚‚ã®ã¨ä¸å¾—æ„ãªã‚‚ã®ã¯ã‚ã‚Šãã†ã§ã¯ã‚ã‚‹ã€‚

<br>



<br>

<img src="https://github.com/user-attachments/assets/c0c633d8-c64d-4a14-95cf-0d8b0d52a7f6" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/560f8c3e-65ff-4742-b7da-bc2b242dafcd" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/481a9635-128d-4931-a891-5f46d55b82bc" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/fc9b1bc0-b857-4a27-ad90-4940213c6ec6" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/a349b154-1844-41c2-84e3-7f981b1f6b72" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/a4381740-600c-402f-be0d-59ce60b7a562" alt="image" loading="lazy">

<br>



<br>

ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã¨ã‹ã‚‚ã€Proã¨GPT4oã‚’ãƒ‘ãƒƒã¨è¦‹ã§æ¯”è¼ƒã—ãŸæ„Ÿã˜ã€å„ªã‚Œã¦ã„ã‚‹ã‚ã‘ã§ã‚‚ãªã•ãã†ã€‚Liteã«å¯¾å¿œã™ã‚‹GPTã¯ãŠãã‚‰ãGPT4o-miniã ã¨æ€ã‚ã‚Œã‚‹ãŒã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã¯Liteã®æ–¹ãŒé«˜ãã†ã€‚

<br>

<img src="https://github.com/user-attachments/assets/734ee26f-2f16-46e4-a6e8-f5f2f0d65be3" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/fe1768e8-b417-4b89-a0c4-f6dffa99cf11" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/6334ee92-e426-49f5-8e1f-050e0b77fcf2" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/5c9ec797-ef7a-43e1-8540-42ccab265208" alt="image" loading="lazy">

<br>



<br>

ï¼ˆç”»åƒã¯è«–æ–‡ä¸­ã‹ã‚‰ã‚¹ã‚¯ã‚·ãƒ§ã—å¼•ç”¨ï¼‰ä¸‹è¨˜ãƒã‚¹ãƒˆã¯ç‹¬è‡ªã«è©•ä¾¡ã—ãŸçµæœã‚„ã€ã‚³ã‚¹ãƒˆã¨æ€§èƒ½ã®ãƒãƒ©ãƒ³ã‚¹ã«ã¤ã„ã¦è¨€åŠã—ã¦ã„ã‚‹ã€‚

<br>



<br>

ãƒ»Proã¯GPT4oã®ã‚³ã‚¹ãƒˆã®ç´„1/3

<br>

ãƒ»Pro, Lite, Flashã¯ã»ã‚Œãã‚Œã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«éå¸¸ã«å„ªã‚Œã¦ã„ã‚‹ï¼ˆQuality vs. Priceå‚ç…§ï¼‰

<br>



<br>

å…ƒãƒã‚¹ãƒˆ:https://x.com/artificialanlys/status/1864023052818030814?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html">#SSM (StateSpaceModel)</a>
<span class="issue_date">Issue Date: 2024-11-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1551">ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ï¼šMamba, Vision Mamba ï¼ˆVimï¼‰, Hironobu Fujiyoshi, 2024.11</a>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Library.html">#Library</a>
<a class="button" href="articles/Repository.html">#Repository</a>
<span class="issue_date">Issue Date: 2024-11-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1547">YomiToku, Kotaro Kinoshita, 2024.11</a>
<span class="snippet"><span>Comment</span>ã„ã‚ã‚†ã‚‹AI-OCRã§ã€ç¸¦æ›¸ãã®èªè­˜ã‚‚å¯èƒ½ã§ã€è¡¨ãªã©ã®æ§‹é€ åŒ–ã•ã‚ŒãŸæƒ…å ±ã‚‚èªè­˜å¯èƒ½ã¨ã®ã“ã¨ã€‚

<br>

æ‰‹æ›¸ãã¯èªè­˜ã§ãã‚‹ã®ã ã‚ã†ã‹?

<br>

CC BY-NC-SA 4.0 å…ƒãƒ„ã‚¤ãƒ¼ãƒˆ:https://x.com/kinocoai/status/1861386062175838303?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Slide.html">#Slide</a>
<span class="issue_date">Issue Date: 2024-11-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1528">Large Vision Language Model ï¼ˆLVLMï¼‰ã«é–¢ã™ã‚‹çŸ¥è¦‹ã¾ã¨ã‚, Daiki Shiono, 2024.11</a>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/GenerativeAI.html">#GenerativeAI</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<span class="issue_date">Issue Date: 2024-10-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1444">MovieGen, Meta, 2024.10</a>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Repository.html">#Repository</a>
<span class="issue_date">Issue Date: 2024-09-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1437">ECCV2024-Papers-with-Code, 2024.09</a>
<span class="snippet"><span>Comment</span>ECCV2024ã®å…¨ä½“åƒã‚’æ¦‚è¦³ã™ã‚‹ã®ã«æœ‰ç”¨ä»¥ä¸‹ã€Claude 3.5 Sonnetã«ç›®æ¬¡ã‚’å…¥åŠ›ã—ä¸€è¨€ã§å„é …ç›®ã‚’èª¬æ˜ã•ã›ãŸå†…å®¹ã€‚

<br>

hallucinationãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã®ã§å‚è€ƒç¨‹åº¦ã§ã€‚

<br>



<br>

--------------------

<br>

å„é …ç›®ã®æ¦‚è¦ã‚’ä¸€è¨€ã§èª¬æ˜ã„ãŸã—ã¾ã™ï¼š

<br>



<br>

1. 3DGS(Gaussian Splatting): 3Dç©ºé–“å†…ã®ã‚¬ã‚¦ã‚¹é–¢æ•°ã‚’ç”¨ã„ãŸæ–°ã—ã„3Dãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°æ‰‹æ³•ã€‚

<br>



<br>

2. Mamba / SSM: é•·æœŸä¾å­˜é–¢ä¿‚ã‚’åŠ¹ç‡çš„ã«å‡¦ç†ã™ã‚‹æ–°ã—ã„ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚

<br>



<br>

3. Avatars: ãƒ‡ã‚¸ã‚¿ãƒ«ç’°å¢ƒã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è¡¨ç¾ã™ã‚‹ä»®æƒ³ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã€‚

<br>



<br>

4. Backbone: ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®ä¸»è¦ãªç‰¹å¾´æŠ½å‡ºéƒ¨åˆ†ã€‚

<br>



<br>

5. CLIP: ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’åŒã˜ç©ºé–“ã«åŸ‹ã‚è¾¼ã‚€å¤§è¦æ¨¡ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã€‚

<br>



<br>

6. MAE: ç”»åƒã®ä¸€éƒ¨ã‚’éš ã—ã¦ã‹ã‚‰å†æ§‹ç¯‰ã™ã‚‹è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’æ‰‹æ³•ã€‚

<br>



<br>

7. Embodied AI: ç‰©ç†çš„ãªç’°å¢ƒã¨ç›¸äº’ä½œç”¨ã™ã‚‹ AI ã‚·ã‚¹ãƒ†ãƒ ã€‚

<br>



<br>

8. GAN: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¨è­˜åˆ¥ãƒ¢ãƒ‡ãƒ«ã‚’ç«¶äº‰ã•ã›ã¦å­¦ç¿’ã™ã‚‹ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã€‚

<br>



<br>

9. GNN: ã‚°ãƒ©ãƒ•æ§‹é€ ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã®ç¥çµŒãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€‚

<br>



<br>

10. å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM): ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ãªã©è¤‡æ•°ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’æ‰±ã†å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€‚

<br>



<br>

11. å¤§è¯­è¨€æ¨¡å‹(LLM): å¤§é‡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã•ã‚ŒãŸå¤§è¦æ¨¡ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã€‚

<br>



<br>

12. NAS: æœ€é©ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è‡ªå‹•æ¢ç´¢ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

13. OCR: ç”»åƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’èªè­˜ã—ã€ãƒ‡ã‚¸ã‚¿ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

14. NeRF: 3Dç©ºé–“ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¡¨ç¾ã™ã‚‹æ‰‹æ³•ã€‚

<br>



<br>

15. DETR: Transformerã‚’ç”¨ã„ãŸæ–°ã—ã„ç‰©ä½“æ¤œå‡ºã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚

<br>



<br>

16. Prompt: AIãƒ¢ãƒ‡ãƒ«ã«ä¸ãˆã‚‹æŒ‡ç¤ºã‚„æ–‡è„ˆã‚’è¨­å®šã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã€‚

<br>



<br>

17. æ‰©æ•£æ¨¡å‹(Diffusion Models): ãƒã‚¤ã‚ºã‚’å¾ã€…ã«é™¤å»ã—ã¦ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã€‚

<br>



<br>

18. ReID(é‡è¯†åˆ«): ç•°ãªã‚‹ç”»åƒã‚„æ˜ åƒé–“ã§åŒä¸€ã®äººç‰©ã‚„ç‰©ä½“ã‚’å†è­˜åˆ¥ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

19. é•¿å°¾åˆ†å¸ƒ(Long-Tail): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã§é »åº¦ã®ä½ã„ã‚¯ãƒ©ã‚¹ã‚„äº‹ä¾‹ã‚’æ‰±ã†å•é¡Œã€‚

<br>



<br>

20. Vision Transformer: ç”»åƒå‡¦ç†ã«Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’é©ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã€‚

<br>



<br>

21. è§†è§‰å’Œè¯­è¨€(Vision-Language): ç”»åƒã¨è¨€èªã‚’çµ„ã¿åˆã‚ã›ã¦å‡¦ç†ã™ã‚‹ã‚¿ã‚¹ã‚¯ã€‚

<br>



<br>

22. è‡ªç›‘ç£å­¦ä¹ (Self-supervised Learning): ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ‰ç”¨ãªè¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã€‚

<br>



<br>

23. æ•°æ®å¢å¼º(Data Augmentation): å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’äººå·¥çš„ã«å¢—ã‚„ã™æŠ€è¡“ã€‚

<br>



<br>

24. ç›®æ ‡æ£€æµ‹(Object Detection): ç”»åƒå†…ã®ç‰©ä½“ã®ä½ç½®ã¨ç¨®é¡ã‚’ç‰¹å®šã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

25. å¼‚å¸¸æ£€æµ‹(Anomaly Detection): é€šå¸¸ã¨ã¯ç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚„ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

26. ç›®æ ‡è·Ÿè¸ª(Visual Tracking): æ˜ åƒå†…ã®ç‰©ä½“ã®å‹•ãã‚’è¿½è·¡ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

27. è¯­ä¹‰åˆ†å‰²(Semantic Segmentation): ç”»åƒå†…ã®å„ãƒ”ã‚¯ã‚»ãƒ«ã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

28. å®ä¾‹åˆ†å‰²(Instance Segmentation): ç”»åƒå†…ã®å€‹ã€…ã®ç‰©ä½“ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆ†å‰²ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

29. å…¨æ™¯åˆ†å‰²(Panoptic Segmentation): æ„å‘³åˆ†å‰²ã¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åˆ†å‰²ã‚’çµ„ã¿åˆã‚ã›ãŸæŠ€è¡“ã€‚

<br>



<br>

30. åŒ»å­¦å›¾åƒ(Medical Image): åŒ»ç™‚ç›®çš„ã§æ’®å½±ã•ã‚ŒãŸç”»åƒã€‚

<br>



<br>

31. åŒ»å­¦å›¾åƒåˆ†å‰²(Medical Image Segmentation): åŒ»ç™‚ç”»åƒå†…ã®è‡“å™¨ã‚„ç—…å¤‰éƒ¨ä½ã‚’åˆ†å‰²ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

32. è§†é¢‘ç›®æ ‡åˆ†å‰²(Video Object Segmentation): å‹•ç”»å†…ã®ç‰©ä½“ã‚’è¿½è·¡ã—åˆ†å‰²ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

33. è§†é¢‘å®ä¾‹åˆ†å‰²(Video Instance Segmentation): å‹•ç”»å†…ã®å€‹ã€…ã®ç‰©ä½“ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆ†å‰²ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

34. å‚è€ƒå›¾åƒåˆ†å‰²(Referring Image Segmentation): è¨€èªè¨˜è¿°ã«åŸºã¥ã„ã¦ç”»åƒå†…ã®ç‰©ä½“ã‚’åˆ†å‰²ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

35. å›¾åƒæŠ å›¾(Image Matting): ç”»åƒã‹ã‚‰å‰æ™¯ã‚’ç²¾å¯†ã«æŠ½å‡ºã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

36. å›¾åƒç¼–è¾‘(Image Editing): ç”»åƒã®å†…å®¹ã‚’å¤‰æ›´ã¾ãŸã¯æ“ä½œã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

37. Low-level Vision: ç”»åƒã®ä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´ã‚„å‡¦ç†ã‚’æ‰±ã†åˆ†é‡ã€‚

<br>



<br>

38. è¶…åˆ†è¾¨ç‡(Super-Resolution): ä½è§£åƒåº¦ç”»åƒã‹ã‚‰é«˜è§£åƒåº¦ç”»åƒã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

39. å»å™ª(Denoising): ç”»åƒã‹ã‚‰ãƒã‚¤ã‚ºã‚’é™¤å»ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

40. å»æ¨¡ç³Š(Deblur): ã¼ã‘ãŸç”»åƒã‚’ã‚·ãƒ£ãƒ¼ãƒ—ã«ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

41. è‡ªåŠ¨é©¾é©¶(Autonomous Driving): äººé–“ã®æ“ä½œãªã—ã§è»Šä¸¡ã‚’åˆ¶å¾¡ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

42. 3Dç‚¹äº‘(3D Point Cloud): 3Dç©ºé–“å†…ã®ç‚¹ã®é›†åˆã§ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚„ç’°å¢ƒã‚’è¡¨ç¾ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å½¢å¼ã€‚

<br>



<br>

43. 3Dç›®æ ‡æ£€æµ‹(3D Object Detection): 3Dç©ºé–“å†…ã®ç‰©ä½“ã®ä½ç½®ã¨ç¨®é¡ã‚’ç‰¹å®šã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

44. 3Dè¯­ä¹‰åˆ†å‰²(3D Semantic Segmentation): 3Dãƒ‡ãƒ¼ã‚¿ã®å„ç‚¹ã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

45. 3Dç›®æ ‡è·Ÿè¸ª(3D Object Tracking): 3Dç©ºé–“å†…ã®ç‰©ä½“ã®å‹•ãã‚’è¿½è·¡ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

46. 3Dè¯­ä¹‰åœºæ™¯è¡¥å…¨(3D Semantic Scene Completion): éƒ¨åˆ†çš„ãª3Dãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Œå…¨ãª3Dã‚·ãƒ¼ãƒ³ã‚’æ¨å®šã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

47. 3Dé…å‡†(3D Registration): è¤‡æ•°ã®3Dãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ•´åˆ—ã•ã›ã‚‹æŠ€è¡“ã€‚

<br>



<br>

48. 3Däººä½“å§¿æ€ä¼°è®¡(3D Human Pose Estimation): 3Dç©ºé–“å†…ã®äººä½“ã®å§¿å‹¢ã‚’æ¨å®šã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

49. 3Däººä½“Meshä¼°è®¡(3D Human Mesh Estimation): 3Däººä½“ãƒ¡ãƒƒã‚·ãƒ¥ãƒ¢ãƒ‡ãƒ«ã‚’æ¨å®šã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

50. å›¾åƒç”Ÿæˆ(Image Generation): AIã‚’ç”¨ã„ã¦æ–°ã—ã„ç”»åƒã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

51. è§†é¢‘ç”Ÿæˆ(Video Generation): AIã‚’ç”¨ã„ã¦æ–°ã—ã„å‹•ç”»ã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

52. 3Dç”Ÿæˆ(3D Generation): AIã‚’ç”¨ã„ã¦æ–°ã—ã„3Dãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

53. è§†é¢‘ç†è§£(Video Understanding): å‹•ç”»ã®å†…å®¹ã‚’è§£æã—ç†è§£ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

54. è¡Œä¸ºè¯†åˆ«(Action Recognition): å‹•ç”»å†…ã®äººç‰©ã®è¡Œå‹•ã‚’è­˜åˆ¥ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

55. è¡Œä¸ºæ£€æµ‹(Action Detection): å‹•ç”»å†…ã®ç‰¹å®šã®è¡Œå‹•ã‚’æ¤œå‡ºã—ä½ç½®ç‰¹å®šã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

56. æ–‡æœ¬æ£€æµ‹(Text Detection): ç”»åƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã®ä½ç½®ã‚’æ¤œå‡ºã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

57. çŸ¥è¯†è’¸é¦(Knowledge Distillation): å¤§ããªãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã‚’å°ã•ãªãƒ¢ãƒ‡ãƒ«ã«è»¢ç§»ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

58. æ¨¡å‹å‰ªæ(Model Pruning): ãƒ¢ãƒ‡ãƒ«ã®é‡è¦ã§ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦è»½é‡åŒ–ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

59. å›¾åƒå‹ç¼©(Image Compression): ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’åŠ¹ç‡çš„ã«åœ§ç¸®ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

60. ä¸‰ç»´é‡å»º(3D Reconstruction): 2Dç”»åƒã‹ã‚‰3Dãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

61. æ·±åº¦ä¼°è®¡(Depth Estimation): 2Dç”»åƒã‹ã‚‰å¥¥è¡Œãæƒ…å ±ã‚’æ¨å®šã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

62. è½¨è¿¹é¢„æµ‹(Trajectory Prediction): ç‰©ä½“ã‚„äººã®å°†æ¥ã®å‹•ãã‚’äºˆæ¸¬ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

63. è½¦é“çº¿æ£€æµ‹(Lane Detection): é“è·¯ä¸Šã®è»Šç·šã‚’æ¤œå‡ºã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

64. å›¾åƒæè¿°(Image Captioning): ç”»åƒã®å†…å®¹ã‚’è‡ªç„¶è¨€èªã§èª¬æ˜ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

65. è§†è§‰é—®ç­”(Visual Question Answering): ç”»åƒã«é–¢ã™ã‚‹è³ªå•ã«ç­”ãˆã‚‹æŠ€è¡“ã€‚

<br>



<br>

66. æ‰‹è¯­è¯†åˆ«(Sign Language Recognition): æ‰‹è©±ã®ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã‚’èªè­˜ã—è§£é‡ˆã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

67. è§†é¢‘é¢„æµ‹(Video Prediction): å‹•ç”»ã®å°†æ¥ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’äºˆæ¸¬ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

68. æ–°è§†ç‚¹åˆæˆ(Novel View Synthesis): æ—¢å­˜ã®ç”»åƒã‹ã‚‰æ–°ã—ã„è¦–ç‚¹ã®ç”»åƒã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

69. Zero-Shot Learning(é›¶æ ·æœ¬å­¦ä¹ ): å­¦ç¿’æ™‚ã«è¦‹ãŸã“ã¨ã®ãªã„ã‚¯ãƒ©ã‚¹ã‚’èªè­˜ã™ã‚‹å­¦ç¿’æ‰‹æ³•ã€‚

<br>



<br>

70. ç«‹ä½“åŒ¹é…(Stereo Matching): ã‚¹ãƒ†ãƒ¬ã‚ªç”»åƒã‹ã‚‰æ·±åº¦æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

71. ç‰¹å¾åŒ¹é…(Feature Matching): ç•°ãªã‚‹ç”»åƒé–“ã§å¯¾å¿œã™ã‚‹ç‰¹å¾´ç‚¹ã‚’è¦‹ã¤ã‘ã‚‹æŠ€è¡“ã€‚

<br>



<br>

72. åœºæ™¯å›¾ç”Ÿæˆ(Scene Graph Generation): ç”»åƒå†…ã®ç‰©ä½“é–“ã®é–¢ä¿‚ã‚’æ§‹é€ åŒ–ã—ã¦è¡¨ç¾ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

73. è®¡æ•°(Counting): ç”»åƒå†…ã®ç‰¹å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®æ•°ã‚’æ•°ãˆã‚‹æŠ€è¡“ã€‚

<br>



<br>

74. éšå¼ç¥ç»è¡¨ç¤º(Implicit Neural Representations): é€£ç¶šé–¢æ•°ã¨ã—ã¦ã‚·ãƒ¼ãƒ³ã‚„å½¢çŠ¶ã‚’è¡¨ç¾ã™ã‚‹æ‰‹æ³•ã€‚

<br>



<br>

75. å›¾åƒè´¨é‡è¯„ä»·(Image Quality Assessment): ç”»åƒã®å“è³ªã‚’å®¢è¦³çš„ã«è©•ä¾¡ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

76. è§†é¢‘è´¨é‡è¯„ä»·(Video Quality Assessment): å‹•ç”»ã®å“è³ªã‚’å®¢è¦³çš„ã«è©•ä¾¡ã™ã‚‹æŠ€è¡“ã€‚

<br>



<br>

77. æ•°æ®é›†(Datasets): æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚„è©•ä¾¡ã«ä½¿ç”¨ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã®é›†åˆã€‚â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹ç¶šã„ã¦ã€å„é …ç›®ã‚’ã‚‚ã†å°‘ã—è©³ç´°ã«èª¬æ˜ã™ã‚‹ã‚ˆã†ã«ç”Ÿæˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆ

<br>



<br>

---------

<br>

ã¯ã„ã€å„é …ç›®ã«ã¤ã„ã¦ã‚ˆã‚Šè©³ç´°ãªèª¬æ˜ã‚’æä¾›ã„ãŸã—ã¾ã™ã€‚

<br>



<br>

1. 3DGS(Gaussian Splatting):

<br>

3Dç©ºé–“å†…ã«ã‚¬ã‚¦ã‚¹é–¢æ•°ã‚’é…ç½®ã—ã€ã“ã‚Œã‚‰ã‚’2Dç”»åƒã«æŠ•å½±ã™ã‚‹ã“ã¨ã§3Dã‚·ãƒ¼ãƒ³ã‚’è¡¨ç¾ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚å¾“æ¥ã®NeRFã‚ˆã‚Šã‚‚é«˜é€Ÿã§ã‚¯ã‚ªãƒªãƒ†ã‚£ã®é«˜ã„3Dãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

<br>



<br>

2. Mamba / SSM:

<br>

State Space Model (SSM)ã«åŸºã¥ãæ–°ã—ã„ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚Transformerã¨æ¯”è¼ƒã—ã¦é•·ã„ç³»åˆ—ã‚’åŠ¹ç‡çš„ã«å‡¦ç†ã§ãã€æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã§é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

<br>



<br>

3. Avatars:

<br>

ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç©ºé–“ã‚„ã‚²ãƒ¼ãƒ ã€VRç’°å¢ƒãªã©ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è¡¨ç¾ã™ã‚‹ä»®æƒ³ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã§ã™ã€‚æœ€è¿‘ã§ã¯ã€AIã‚’ç”¨ã„ã¦ã‚ˆã‚Šãƒªã‚¢ãƒ«ã§è‡ªç„¶ãªã‚¢ãƒã‚¿ãƒ¼ã®ç”Ÿæˆã‚„åˆ¶å¾¡ãŒå¯èƒ½ã«ãªã£ã¦ã„ã¾ã™ã€‚

<br>



<br>

4. Backbone:

<br>

æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ä¸»è¦ãªç‰¹å¾´æŠ½å‡ºéƒ¨åˆ†ã‚’æŒ‡ã—ã¾ã™ã€‚ResNetã€VGGã€EfficientNetãªã©ãŒä»£è¡¨çš„ã§ã€æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã®åŸºç¤ã¨ãªã‚‹é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚

<br>



<br>

5. CLIP:

<br>

OpenAIãŒé–‹ç™ºã—ãŸå¤§è¦æ¨¡ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã€ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’åŒã˜ç‰¹å¾´ç©ºé–“ã«åŸ‹ã‚è¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æŸ”è»Ÿãªç”»åƒæ¤œç´¢ã‚„åˆ†é¡ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

<br>



<br>

6. MAE (Masked Autoencoder):

<br>

ç”»åƒã®ä¸€éƒ¨ã‚’ãƒã‚¹ã‚¯ã—ã€ãã‚Œã‚’å†æ§‹ç¯‰ã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚’é€šã˜ã¦è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã‚’è¡Œã†æ‰‹æ³•ã§ã™ã€‚äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

<br>



<br>

7. Embodied AI:

<br>

ç‰©ç†çš„ãªç’°å¢ƒã¨ç›´æ¥ç›¸äº’ä½œç”¨ã™ã‚‹AIã‚·ã‚¹ãƒ†ãƒ ã‚’æŒ‡ã—ã¾ã™ã€‚ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã‚„è‡ªå‹•é‹è»¢ãªã©ã€å®Ÿä¸–ç•Œã§ã®ã‚¿ã‚¹ã‚¯é‚è¡Œã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚

<br>



<br>

8. GAN (Generative Adversarial Networks):

<br>

ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¨è­˜åˆ¥ãƒ¢ãƒ‡ãƒ«ã‚’ç«¶äº‰ã•ã›ã‚‹ã“ã¨ã§å­¦ç¿’ã‚’è¡Œã†ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚é«˜å“è³ªãªç”»åƒç”Ÿæˆãªã©ã€æ§˜ã€…ãªåˆ†é‡ã§å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

9. GNN (Graph Neural Networks):

<br>

ã‚°ãƒ©ãƒ•æ§‹é€ ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã®ç¥çµŒãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æã‚„åˆ†å­æ§‹é€ äºˆæ¸¬ãªã©ã€é–¢ä¿‚æ€§ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã«é©ã—ã¦ã„ã¾ã™ã€‚

<br>



<br>

10. å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM):

<br>

ãƒ†ã‚­ã‚¹ãƒˆã ã‘ã§ãªãã€ç”»åƒã€éŸ³å£°ã€å‹•ç”»ãªã©ã®è¤‡æ•°ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’ç†è§£ã—å‡¦ç†ã§ãã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ã‚ˆã‚Šè±Šã‹ãªã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚„ç†è§£ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

<br>



<br>

11. å¤§è¯­è¨€æ¨¡å‹(LLM):

<br>

GPT-3ã‚„LLaMAãªã©ã€å¤§é‡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã•ã‚ŒãŸå·¨å¤§ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è‡ªç„¶è¨€èªå‡¦ç†ã®å¤šãã®ã‚¿ã‚¹ã‚¯ã§é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

<br>



<br>

12. NAS (Neural Architecture Search):

<br>

æ©Ÿæ¢°å­¦ç¿’ã‚’ç”¨ã„ã¦æœ€é©ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹é€ ã‚’è‡ªå‹•çš„ã«æ¢ç´¢ã™ã‚‹æŠ€è¡“ã§ã™ã€‚äººæ‰‹ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆã®åŠ´åŠ›ã‚’è»½æ¸›ã—ã€ã‚ˆã‚ŠåŠ¹ç‡çš„ãªãƒ¢ãƒ‡ãƒ«ã®ç™ºè¦‹ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚

<br>



<br>

13. OCR (Optical Character Recognition):

<br>

ç”»åƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’èªè­˜ã—ã€æ©Ÿæ¢°å¯èª­ãªãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹æŠ€è¡“ã§ã™ã€‚æ–‡æ›¸ã®ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã‚„è‡ªå‹•ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ãªã©ã«åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

14. NeRF (Neural Radiance Fields):

<br>

3Dç©ºé–“ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¡¨ç¾ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚å°‘æ•°ã®2Dç”»åƒã‹ã‚‰é«˜å“è³ªãª3Dã‚·ãƒ¼ãƒ³ã®å†æ§‹ç¯‰ã¨æ–°è¦–ç‚¹ã®åˆæˆãŒå¯èƒ½ã§ã™ã€‚

<br>



<br>

15. DETR (DEtection TRansformer):

<br>

Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç‰©ä½“æ¤œå‡ºã‚¿ã‚¹ã‚¯ã«é©ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚å¾“æ¥ã®æ‰‹æ³•ã¨æ¯”ã¹ã¦ã‚·ãƒ³ãƒ—ãƒ«ã§ã‚ã‚ŠãªãŒã‚‰é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

<br>



<br>

16. Prompt:

<br>

AIãƒ¢ãƒ‡ãƒ«ã€ç‰¹ã«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«ä¸ãˆã‚‹æŒ‡ç¤ºã‚„æ–‡è„ˆã‚’è¨­å®šã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚é©åˆ‡ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’åˆ¶å¾¡ã—ã€æœ›ã¾ã—ã„çµæœã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

<br>



<br>

17. æ‰©æ•£æ¨¡å‹(Diffusion Models):

<br>

ãƒã‚¤ã‚ºã‚’å¾ã€…ã«é™¤å»ã—ãªãŒã‚‰ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚DALL-E 2ã‚„Stable Diffusionãªã©ã€é«˜å“è³ªãªç”»åƒç”Ÿæˆã§æ³¨ç›®ã‚’é›†ã‚ã¦ã„ã¾ã™ã€‚

<br>



<br>

18. ReID (é‡è¯†åˆ«):

<br>

ç•°ãªã‚‹ç”»åƒã‚„æ˜ åƒé–“ã§åŒä¸€ã®äººç‰©ã‚„ç‰©ä½“ã‚’å†è­˜åˆ¥ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ç›£è¦–ã‚«ãƒ¡ãƒ©ã‚·ã‚¹ãƒ†ãƒ ã‚„é¡§å®¢è¿½è·¡ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

19. é•¿å°¾åˆ†å¸ƒ(Long-Tail):

<br>

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã§é »åº¦ã®ä½ã„ã‚¯ãƒ©ã‚¹ã‚„äº‹ä¾‹ã‚’æ‰±ã†å•é¡Œã§ã™ã€‚ç¾å®Ÿä¸–ç•Œã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«å¯¾å¿œã™ã‚‹ãŸã‚ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å…¬å¹³æ€§ã¨æ±åŒ–æ€§èƒ½ã®å‘ä¸ŠãŒèª²é¡Œã¨ãªã£ã¦ã„ã¾ã™ã€‚

<br>



<br>

20. Vision Transformer:

<br>

è‡ªç„¶è¨€èªå‡¦ç†ã§æˆåŠŸã‚’åã‚ãŸTransformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç”»åƒå‡¦ç†ã«é©ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚CNNã¨æ¯”è¼ƒã—ã¦ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å­¦ç¿’æ™‚ã«é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

<br>



<br>

21. è§†è§‰å’Œè¯­è¨€(Vision-Language):

<br>

ç”»åƒã¨è¨€èªã‚’çµ„ã¿åˆã‚ã›ã¦å‡¦ç†ã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚„ç ”ç©¶åˆ†é‡ã§ã™ã€‚ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆã€è¦–è¦šçš„è³ªå•å¿œç­”ã€ç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ãªã©ãŒå«ã¾ã‚Œã¾ã™ã€‚

<br>



<br>

22. è‡ªç›‘ç£å­¦ä¹ (Self-supervised Learning):

<br>

å¤§é‡ã®ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ‰ç”¨ãªç‰¹å¾´è¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã«åºƒãä½¿ç”¨ã•ã‚Œã€å°‘é‡ã®ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã§ã® fine-tuning ã§é«˜ã„æ€§èƒ½ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

<br>



<br>

23. æ•°æ®å¢å¼º(Data Augmentation):

<br>

æ—¢å­˜ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å¤‰å½¢ã‚„å¤‰æ›´ã‚’åŠ ãˆã¦äººå·¥çš„ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ‹¡å¼µã™ã‚‹æŠ€è¡“ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½å‘ä¸Šã‚„ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã®æŠ‘åˆ¶ã«åŠ¹æœãŒã‚ã‚Šã¾ã™ã€‚

<br>



<br>

24. ç›®æ ‡æ£€æµ‹(Object Detection):

<br>

ç”»åƒå†…ã®ç‰©ä½“ã®ä½ç½®ã¨ç¨®é¡ã‚’ç‰¹å®šã™ã‚‹æŠ€è¡“ã§ã™ã€‚çŸ©å½¢ã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã§ç‰©ä½“ã®ä½ç½®ã‚’ç¤ºã—ã€å„ç‰©ä½“ã®ã‚¯ãƒ©ã‚¹ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚è‡ªå‹•é‹è»¢ã‚„ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ãªã©ã§åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

25. å¼‚å¸¸æ£€æµ‹(Anomaly Detection):

<br>

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®é€šå¸¸ã¨ã¯ç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’æ¤œå‡ºã™ã‚‹æŠ€è¡“ã§ã™ã€‚ä¸æ­£æ¤œçŸ¥ã€ç”£æ¥­ç”¨æ©Ÿå™¨ã®æ•…éšœäºˆæ¸¬ã€åŒ»ç™‚è¨ºæ–­ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

26. ç›®æ ‡è·Ÿè¸ª(Visual Tracking):

<br>

å‹•ç”»ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã§ç‰©ä½“ã®å‹•ãã‚’è¿½è·¡ã™ã‚‹æŠ€è¡“ã§ã™ã€‚è‡ªå‹•é‹è»¢ã€ã‚¹ãƒãƒ¼ãƒ„åˆ†æã€ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ãªã©ã€æ§˜ã€…ãªåˆ†é‡ã§æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

27. è¯­ä¹‰åˆ†å‰²(Semantic Segmentation):

<br>

ç”»åƒå†…ã®å„ãƒ”ã‚¯ã‚»ãƒ«ã‚’ã‚ã‚‰ã‹ã˜ã‚å®šç¾©ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã™ã‚‹æŠ€è¡“ã§ã™ã€‚è‡ªå‹•é‹è»¢ã«ãŠã‘ã‚‹é“è·¯ç’°å¢ƒã®ç†è§£ã‚„åŒ»ç™‚ç”»åƒè§£æãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

28. å®ä¾‹åˆ†å‰²(Instance Segmentation):

<br>

ç”»åƒå†…ã®å€‹ã€…ã®ç‰©ä½“ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆ†å‰²ã—ã€ãã‚Œãã‚Œã«å›ºæœ‰ã®ãƒ©ãƒ™ãƒ«ã‚’ä»˜ä¸ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ç‰©ä½“æ¤œå‡ºã¨æ„å‘³åˆ†å‰²ã‚’çµ„ã¿åˆã‚ã›ãŸã‚¿ã‚¹ã‚¯ã¨è¨€ãˆã¾ã™ã€‚

<br>



<br>

29. å…¨æ™¯åˆ†å‰²(Panoptic Segmentation):

<br>

æ„å‘³åˆ†å‰²ã¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åˆ†å‰²ã‚’çµ±åˆã—ãŸæŠ€è¡“ã§ã€ç”»åƒå†…ã®ã™ã¹ã¦ã®ãƒ”ã‚¯ã‚»ãƒ«ã«å¯¾ã—ã¦ã‚¯ãƒ©ã‚¹ã¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹IDã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚ã‚·ãƒ¼ãƒ³ã®å®Œå…¨ãªç†è§£ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚

<br>



<br>

30. åŒ»å­¦å›¾åƒ(Medical Image):

<br>

Xç·šã€CTã€MRIã€è¶…éŸ³æ³¢ãªã©ã®åŒ»ç™‚ç›®çš„ã§æ’®å½±ã•ã‚ŒãŸç”»åƒã‚’æŒ‡ã—ã¾ã™ã€‚è¨ºæ–­ã€æ²»ç™‚è¨ˆç”»ã€åŒ»å­¦ç ”ç©¶ãªã©ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

<br>



<br>

31. åŒ»å­¦å›¾åƒåˆ†å‰²(Medical Image Segmentation):

<br>

åŒ»ç™‚ç”»åƒå†…ã®è‡“å™¨ã€è…«ç˜ã€è¡€ç®¡ãªã©ã®ç‰¹å®šã®æ§‹é€ ã‚„ç—…å¤‰éƒ¨ä½ã‚’åˆ†å‰²ã™ã‚‹æŠ€è¡“ã§ã™ã€‚è¨ºæ–­æ”¯æ´ã‚„æ‰‹è¡“è¨ˆç”»ç«‹æ¡ˆã«é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚

<br>



<br>

32. è§†é¢‘ç›®æ ‡åˆ†å‰²(Video Object Segmentation):

<br>

å‹•ç”»ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®ç‰¹å®šã®ç‰©ä½“ã‚’è¿½è·¡ã—ã€ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«åˆ†å‰²ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ãƒ“ãƒ‡ã‚ªç·¨é›†ã‚„ã‚¢ã‚¦ã‚°ãƒ¡ãƒ³ãƒ†ãƒƒãƒ‰ãƒªã‚¢ãƒªãƒ†ã‚£ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

33. è§†é¢‘å®ä¾‹åˆ†å‰²(Video Instance Segmentation):

<br>

å‹•ç”»å†…ã®å€‹ã€…ã®ç‰©ä½“ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¿½è·¡ã—ã€ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«åˆ†å‰²ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚ãƒ“ãƒ‡ã‚ªè§£æã‚„è‡ªå‹•é‹è»¢ã‚·ã‚¹ãƒ†ãƒ ã§ã®ç’°å¢ƒç†è§£ã«å½¹ç«‹ã¡ã¾ã™ã€‚

<br>



<br>

34. å‚è€ƒå›¾åƒåˆ†å‰²(Referring Image Segmentation):

<br>

è‡ªç„¶è¨€èªã«ã‚ˆã‚‹è¨˜è¿°ã«åŸºã¥ã„ã¦ã€ç”»åƒå†…ã®ç‰¹å®šã®ç‰©ä½“ã‚„é ˜åŸŸã‚’åˆ†å‰²ã™ã‚‹æŠ€è¡“ã§ã™ã€‚äººé–“ã¨AIã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¿ƒé€²ã—ã¾ã™ã€‚

<br>



<br>

35. å›¾åƒæŠ å›¾(Image Matting):

<br>

ç”»åƒã‹ã‚‰å‰æ™¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç²¾å¯†ã«æŠ½å‡ºã™ã‚‹æŠ€è¡“ã§ã™ã€‚èƒŒæ™¯ç½®æ›ã‚„åˆæˆãªã©ã€ç”»åƒç·¨é›†ã‚¿ã‚¹ã‚¯ã§é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚

<br>



<br>

36. å›¾åƒç¼–è¾‘(Image Editing):

<br>

ç”»åƒã®å†…å®¹ã‚’å¤‰æ›´ã¾ãŸã¯æ“ä½œã™ã‚‹æŠ€è¡“ã®ç·ç§°ã§ã™ã€‚ç‰©ä½“ã®é™¤å»ãƒ»è¿½åŠ ã€ã‚¹ã‚¿ã‚¤ãƒ«å¤‰æ›ã€è‰²èª¿æ•´ãªã©ã€æ§˜ã€…ãªç·¨é›†æ“ä½œãŒå«ã¾ã‚Œã¾ã™ã€‚

<br>



<br>

37. Low-level Vision:

<br>

ç”»åƒã®ä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´ã‚„åŸºæœ¬çš„ãªå‡¦ç†ã‚’æ‰±ã†åˆ†é‡ã§ã™ã€‚ãƒã‚¤ã‚ºé™¤å»ã€è¶…è§£åƒã€ã‚¨ãƒƒã‚¸æ¤œå‡ºãªã©ã®åŸºç¤çš„ãªã‚¿ã‚¹ã‚¯ãŒå«ã¾ã‚Œã¾ã™ã€‚

<br>



<br>

38. è¶…åˆ†è¾¨ç‡(Super-Resolution):

<br>

ä½è§£åƒåº¦ã®ç”»åƒã‹ã‚‰é«˜è§£åƒåº¦ã®ç”»åƒã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚ç›£è¦–ã‚«ãƒ¡ãƒ©æ˜ åƒã®é®®æ˜åŒ–ã‚„å¤ã„å†™çœŸã®å¾©å…ƒãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

39. å»å™ª(Denoising):

<br>

ç”»åƒã‹ã‚‰ãƒã‚¤ã‚ºã‚’é™¤å»ã—ã€ã‚¯ãƒªã‚¢ãªç”»åƒã‚’å¾—ã‚‹æŠ€è¡“ã§ã™ã€‚ä½å…‰é‡æ’®å½±ã‚„åŒ»ç™‚ç”»åƒã®å“è³ªå‘ä¸Šãªã©ã€æ§˜ã€…ãªå ´é¢ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

40. å»æ¨¡ç³Š(Deblur):

<br>

ã¼ã‘ãŸç”»åƒã‚’ã‚·ãƒ£ãƒ¼ãƒ—ã«ã™ã‚‹æŠ€è¡“ã§ã™ã€‚æ‰‹ãƒ–ãƒ¬ã‚„è¢«å†™ä½“ãƒ–ãƒ¬ã®è£œæ­£ã€å¤ã„å†™çœŸã®å¾©å…ƒãªã©ã«æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

41. è‡ªåŠ¨é©¾é©¶(Autonomous Driving):

<br>

äººé–“ã®æ“ä½œãªã—ã§è»Šä¸¡ã‚’åˆ¶å¾¡ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã€ã‚»ãƒ³ã‚µãƒ¼èåˆã€æ±ºå®šã‚·ã‚¹ãƒ†ãƒ ãªã©ã€å¤šå²ã«ã‚ãŸã‚‹æŠ€è¡“ã®çµ±åˆãŒå¿…è¦ã§ã™ã€‚

<br>



<br>

42. 3Dç‚¹äº‘(3D Point Cloud):

<br>

3Dç©ºé–“å†…ã®ç‚¹ã®é›†åˆã§ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚„ç’°å¢ƒã‚’è¡¨ç¾ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å½¢å¼ã§ã™ã€‚LiDARãªã©ã®ã‚»ãƒ³ã‚µãƒ¼ã‹ã‚‰å–å¾—ã•ã‚Œã€3Dèªè­˜ã‚¿ã‚¹ã‚¯ã®åŸºç¤ã¨ãªã‚Šã¾ã™ã€‚

<br>



<br>

43. 3Dç›®æ ‡æ£€æµ‹(3D Object Detection):

<br>

3Dç©ºé–“å†…ã®ç‰©ä½“ã®ä½ç½®ã€ã‚µã‚¤ã‚ºã€å‘ãã‚’ç‰¹å®šã™ã‚‹æŠ€è¡“ã§ã™ã€‚è‡ªå‹•é‹è»¢ã‚„æ‹¡å¼µç¾å®Ÿãªã©ã®åˆ†é‡ã§é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚

<br>



<br>

44. 3Dè¯­ä¹‰åˆ†å‰²(3D Semantic Segmentation):

<br>

3Dãƒ‡ãƒ¼ã‚¿ã®å„ç‚¹ã‚„é ˜åŸŸã‚’ã‚ã‚‰ã‹ã˜ã‚å®šç¾©ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã™ã‚‹æŠ€è¡“ã§ã™ã€‚è‡ªå‹•é‹è»¢ã§ã®ç’°å¢ƒç†è§£ã‚„ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã§ã®ç‰©ä½“èªè­˜ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

45. 3Dç›®æ ‡è·Ÿè¸ª(3D Object Tracking):

<br>

æ™‚ç³»åˆ—ã®3Dãƒ‡ãƒ¼ã‚¿å†…ã§ç‰©ä½“ã®å‹•ãã‚’è¿½è·¡ã™ã‚‹æŠ€è¡“ã§ã™ã€‚è‡ªå‹•é‹è»¢ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹ä»–ã®è»Šä¸¡ã‚„æ­©è¡Œè€…ã®å‹•ãã®äºˆæ¸¬ãªã©ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

<br>



<br>

46. 3Dè¯­ä¹‰åœºæ™¯è¡¥å…¨(3D Semantic Scene Completion):

<br>

éƒ¨åˆ†çš„ãª3Dãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€ã‚ªã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³ã‚„æ¬ æã®ã‚ã‚‹é ˜åŸŸã‚’å«ã‚€å®Œå…¨ãª3Dã‚·ãƒ¼ãƒ³ã‚’æ¨å®šã™ã‚‹æŠ€è¡“ã§ã™ã€‚ãƒ­ãƒœãƒƒãƒˆãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚„æ‹¡å¼µç¾å®Ÿã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

47. 3Dé…å‡†(3D Registration):

<br>

è¤‡æ•°ã®3Dãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆç‚¹ç¾¤ã‚„è¡¨é¢ãƒ¢ãƒ‡ãƒ«ãªã©ï¼‰ã‚’æ­£ç¢ºã«æ•´åˆ—ã•ã›ã‚‹æŠ€è¡“ã§ã™ã€‚3Dã‚¹ã‚­ãƒ£ãƒ³ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆã‚„ä½ç½®åˆã‚ã›ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

<br>



<br>

48. 3Däººä½“å§¿æ€ä¼°è®¡(3D Human Pose Estimation):

<br>

2Dç”»åƒã‚„3Dãƒ‡ãƒ¼ã‚¿ã‹ã‚‰äººä½“ã®3æ¬¡å…ƒçš„ãªå§¿å‹¢ã‚’æ¨å®šã™ã‚‹æŠ€è¡“ã§ã™ã€‚ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚­ãƒ£ãƒ—ãƒãƒ£ã€ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã€ã‚¹ãƒãƒ¼ãƒ„åˆ†æãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

49. 3Däººä½“Meshä¼°è®¡(3D Human Mesh Estimation):

<br>

2Dç”»åƒã‚„3Dã‚¹ã‚­ãƒ£ãƒ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©³ç´°ãª3Däººä½“ãƒ¡ãƒƒã‚·ãƒ¥ãƒ¢ãƒ‡ãƒ«ã‚’æ¨å®šã™ã‚‹æŠ€è¡“ã§ã™ã€‚ãƒãƒ¼ãƒãƒ£ãƒ«ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚„ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³åˆ¶ä½œãªã©ã«æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

50. å›¾åƒç”Ÿæˆ(Image Generation):

<br>

AIã‚’ç”¨ã„ã¦æ–°ã—ã„ç”»åƒã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚GANã‚„Diffusion Modelãªã©ãŒä»£è¡¨çš„ã§ã€ã‚¢ãƒ¼ãƒˆå‰µä½œã‚„ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

51. è§†é¢‘ç”Ÿæˆ(Video Generation):

<br>

AIã‚’ç”¨ã„ã¦æ–°ã—ã„å‹•ç”»ã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚çŸ­ã„å…¥åŠ›ã‚¯ãƒªãƒƒãƒ—ã‹ã‚‰ã®å‹•ç”»ã®å»¶é•·ã‚„ã€ãƒ†ã‚­ã‚¹ãƒˆè¨˜è¿°ã‹ã‚‰ã®å‹•ç”»ç”Ÿæˆãªã©ãŒç ”ç©¶ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

52. 3Dç”Ÿæˆ(3D Generation):

<br>

AIã‚’ç”¨ã„ã¦æ–°ã—ã„3Dãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚è£½å“ãƒ‡ã‚¶ã‚¤ãƒ³ã€ã‚²ãƒ¼ãƒ é–‹ç™ºã€å»ºç¯‰è¨­è¨ˆãªã©ã®åˆ†é‡ã§æ³¨ç›®ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

53. è§†é¢‘ç†è§£(Video Understanding):

<br>

å‹•ç”»ã®å†…å®¹ã‚’è§£æã—ã€ã‚·ãƒ¼ãƒ³ã®æ§‹é€ ã€ç‰©ä½“ã®é–¢ä¿‚ã€ã‚¤ãƒ™ãƒ³ãƒˆã®é€²è¡Œãªã©ã‚’ç†è§£ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ãƒ“ãƒ‡ã‚ªæ¤œç´¢ã‚„è‡ªå‹•è¦ç´„ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

54. è¡Œä¸ºè¯†åˆ«(Action Recognition):

<br>

å‹•ç”»å†…ã®äººç‰©ã®è¡Œå‹•ã‚’è­˜åˆ¥ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã€ã‚¹ãƒãƒ¼ãƒ„åˆ†æã€ãƒ’ãƒ¥ãƒ¼ãƒãƒ³ãƒ»ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ»ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãªã©ã§æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

55. è¡Œä¸ºæ£€æµ‹(Action Detection):

<br>

å‹•ç”»å†…ã®ç‰¹å®šã®è¡Œå‹•ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ¤œå‡ºã—ã€ãã®æ™‚é–“çš„ãƒ»ç©ºé–“çš„ä½ç½®ã‚’ç‰¹å®šã™ã‚‹æŠ€è¡“ã§ã™ã€‚ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚·ã‚¹ãƒ†ãƒ ã‚„ç•°å¸¸è¡Œå‹•ã®æ¤œçŸ¥ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

ã¯ã„ã€ç¶šãã‚’èª¬æ˜ã„ãŸã—ã¾ã™ã€‚

<br>



<br>

56. æ–‡æœ¬æ£€æµ‹(Text Detection):

<br>

ç”»åƒã‚„å‹•ç”»å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã®ä½ç½®ã‚’æ¤œå‡ºã™ã‚‹æŠ€è¡“ã§ã™ã€‚OCRã‚·ã‚¹ãƒ†ãƒ ã®å‰å‡¦ç†ã¨ã—ã¦é‡è¦ã§ã€çœ‹æ¿ã®èªè­˜ã‚„æ–‡æ›¸åˆ†æãªã©ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

<br>



<br>

57. çŸ¥è¯†è’¸é¦(Knowledge Distillation):

<br>

å¤§è¦æ¨¡ã§è¤‡é›‘ãªã€Œæ•™å¸«ã€ãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã‚’ã€ã‚ˆã‚Šå°ã•ãªã€Œç”Ÿå¾’ã€ãƒ¢ãƒ‡ãƒ«ã«è»¢ç§»ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®è»½é‡åŒ–ã¨æ€§èƒ½ç¶­æŒã®ä¸¡ç«‹ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚

<br>



<br>

58. æ¨¡å‹å‰ªæ(Model Pruning):

<br>

å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰é‡è¦åº¦ã®ä½ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚„å±¤ã‚’å‰Šé™¤ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’è»½é‡åŒ–ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ãƒ¢ãƒã‚¤ãƒ«ãƒ‡ãƒã‚¤ã‚¹ã§ã®åŠ¹ç‡çš„ãªå®Ÿè¡Œãªã©ã«å½¹ç«‹ã¡ã¾ã™ã€‚

<br>



<br>

59. å›¾åƒå‹ç¼©(Image Compression):

<br>

ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’åŠ¹ç‡çš„ã«åœ§ç¸®ã—ã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¸¯åŸŸå¹…ã‚’ç¯€ç´„ã™ã‚‹æŠ€è¡“ã§ã™ã€‚æœ€è¿‘ã§ã¯æ©Ÿæ¢°å­¦ç¿’ã‚’ç”¨ã„ãŸæ–°ã—ã„åœ§ç¸®æ‰‹æ³•ã‚‚ç ”ç©¶ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

60. ä¸‰ç»´é‡å»º(3D Reconstruction):

<br>

2Dç”»åƒã‚„å‹•ç”»ã‹ã‚‰3Dãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹æŠ€è¡“ã§ã™ã€‚å»ºç¯‰ã€è€ƒå¤å­¦ã€æ˜ ç”»åˆ¶ä½œãªã©ã€æ§˜ã€…ãªåˆ†é‡ã§æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

61. æ·±åº¦ä¼°è®¡(Depth Estimation):

<br>

å˜çœ¼ã¾ãŸã¯è¤‡çœ¼ã®2Dç”»åƒã‹ã‚‰ã€ã‚·ãƒ¼ãƒ³ã®å¥¥è¡Œãæƒ…å ±ã‚’æ¨å®šã™ã‚‹æŠ€è¡“ã§ã™ã€‚3Då†æ§‹æˆã‚„æ‹¡å¼µç¾å®Ÿãªã©ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚

<br>



<br>

62. è½¨è¿¹é¢„æµ‹(Trajectory Prediction):

<br>

ç‰©ä½“ã‚„äººã®éå»ã®å‹•ãã«åŸºã¥ã„ã¦ã€å°†æ¥ã®å‹•ãã‚’äºˆæ¸¬ã™ã‚‹æŠ€è¡“ã§ã™ã€‚è‡ªå‹•é‹è»¢ã€ç¾¤è¡†è¡Œå‹•åˆ†æã€ã‚¹ãƒãƒ¼ãƒ„æˆ¦ç•¥ç«‹æ¡ˆãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

63. è½¦é“çº¿æ£€æµ‹(Lane Detection):

<br>

é“è·¯ä¸Šã®è»Šç·šã‚’æ¤œå‡ºã—è¿½è·¡ã™ã‚‹æŠ€è¡“ã§ã™ã€‚è‡ªå‹•é‹è»¢ã‚·ã‚¹ãƒ†ãƒ ã‚„å…ˆé€²é‹è»¢æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ ï¼ˆADASï¼‰ã«ãŠã„ã¦é‡è¦ãªè¦ç´ ã¨ãªã£ã¦ã„ã¾ã™ã€‚

<br>



<br>

64. å›¾åƒæè¿°(Image Captioning):

<br>

ç”»åƒã®å†…å®¹ã‚’è‡ªç„¶è¨€èªã§èª¬æ˜ã™ã‚‹æ–‡ç« ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚è¦–è¦šéšœå®³è€…æ”¯æ´ã‚„ç”»åƒæ¤œç´¢ã®é«˜åº¦åŒ–ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

65. è§†è§‰é—®ç­”(Visual Question Answering):

<br>

ç”»åƒã«é–¢ã™ã‚‹è‡ªç„¶è¨€èªã®è³ªå•ã«å¯¾ã—ã¦ã€é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚ç”»åƒç†è§£ã¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ä¸¡æ–¹ã®èƒ½åŠ›ãŒå¿…è¦ã¨ã•ã‚Œã¾ã™ã€‚

<br>



<br>

66. æ‰‹è¯­è¯†åˆ«(Sign Language Recognition):

<br>

æ‰‹è©±ã®ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã‚’èªè­˜ã—ã€ãã‚Œã‚’æ–‡å­—ã‚„éŸ³å£°ã«å¤‰æ›ã™ã‚‹æŠ€è¡“ã§ã™ã€‚è´è¦šéšœå®³è€…ã¨ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ”¯æ´ã«å½¹ç«‹ã¡ã¾ã™ã€‚

<br>



<br>

67. è§†é¢‘é¢„æµ‹(Video Prediction):

<br>

éå»ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã«åŸºã¥ã„ã¦ã€å‹•ç”»ã®å°†æ¥ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’äºˆæ¸¬ã™ã‚‹æŠ€è¡“ã§ã™ã€‚å‹•ç”»åœ§ç¸®ã€ç•°å¸¸æ¤œçŸ¥ã€è‡ªå‹•é‹è»¢ãªã©ã€æ§˜ã€…ãªå¿œç”¨ãŒè€ƒãˆã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

68. æ–°è§†ç‚¹åˆæˆ(Novel View Synthesis):

<br>

æ—¢å­˜ã®ç”»åƒã‚„é™ã‚‰ã‚ŒãŸè¦–ç‚¹ã®æƒ…å ±ã‹ã‚‰ã€æ–°ã—ã„è¦–ç‚¹ã®ç”»åƒã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚ä»®æƒ³ç¾å®Ÿã‚„è‡ªç”±è¦–ç‚¹æ˜ åƒãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

69. Zero-Shot Learning(é›¶æ ·æœ¬å­¦ä¹ ):

<br>

å­¦ç¿’æ™‚ã«è¦‹ãŸã“ã¨ã®ãªã„ã‚¯ãƒ©ã‚¹ã‚’èªè­˜ã™ã‚‹å­¦ç¿’æ‰‹æ³•ã§ã™ã€‚äº‹å‰ã«å­¦ç¿’ã—ã¦ã„ãªã„æ–°ã—ã„ã‚«ãƒ†ã‚´ãƒªã®ç‰©ä½“ã‚’è­˜åˆ¥ã™ã‚‹èƒ½åŠ›ã‚’æŒã¡ã¾ã™ã€‚

<br>



<br>

70. ç«‹ä½“åŒ¹é…(Stereo Matching):

<br>

ã‚¹ãƒ†ãƒ¬ã‚ªã‚«ãƒ¡ãƒ©ã§æ’®å½±ã•ã‚ŒãŸå·¦å³ã®ç”»åƒã‹ã‚‰å¯¾å¿œç‚¹ã‚’è¦‹ã¤ã‘ã€æ·±åº¦æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹æŠ€è¡“ã§ã™ã€‚3Då†æ§‹æˆã‚„æ·±åº¦æ¨å®šã®åŸºç¤ã¨ãªã‚Šã¾ã™ã€‚

<br>



<br>

71. ç‰¹å¾åŒ¹é…(Feature Matching):

<br>

ç•°ãªã‚‹ç”»åƒé–“ã§å¯¾å¿œã™ã‚‹ç‰¹å¾´ç‚¹ã‚’è¦‹ã¤ã‘ã‚‹æŠ€è¡“ã§ã™ã€‚ç”»åƒã®ã‚¹ãƒ†ã‚£ãƒƒãƒãƒ³ã‚°ã€ç‰©ä½“è¿½è·¡ã€SLAMãªã©ã€æ§˜ã€…ãªã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã®åŸºç¤ã¨ãªã‚Šã¾ã™ã€‚

<br>



<br>

72. åœºæ™¯å›¾ç”Ÿæˆ(Scene Graph Generation):

<br>

ç”»åƒå†…ã®ç‰©ä½“é–“ã®é–¢ä¿‚ã‚’æ§‹é€ åŒ–ã—ã¦è¡¨ç¾ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ã‚·ãƒ¼ãƒ³ã®æ„å‘³çš„ç†è§£ã‚„é«˜åº¦ãªç”»åƒæ¤œç´¢ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

73. è®¡æ•°(Counting):

<br>

ç”»åƒã‚„å‹•ç”»å†…ã®ç‰¹å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®æ•°ã‚’è‡ªå‹•çš„ã«æ•°ãˆã‚‹æŠ€è¡“ã§ã™ã€‚ç¾¤è¡†ã®æ¨å®šã€åœ¨åº«ç®¡ç†ã€ç”Ÿç‰©å­¦çš„ã‚µãƒ³ãƒ—ãƒ«ã®è¨ˆæ•°ãªã©ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

74. éšå¼ç¥ç»è¡¨ç¤º(Implicit Neural Representations):

<br>

3Dã‚·ãƒ¼ãƒ³ã‚„å½¢çŠ¶ã‚’é€£ç¶šé–¢æ•°ã¨ã—ã¦ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¡¨ç¾ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚NeRFã¯ã“ã®æ‰‹æ³•ã®ä¸€ä¾‹ã§ã€åŠ¹ç‡çš„ãªã‚·ãƒ¼ãƒ³è¡¨ç¾ãŒå¯èƒ½ã§ã™ã€‚

<br>



<br>

75. å›¾åƒè´¨é‡è¯„ä»·(Image Quality Assessment):

<br>

ç”»åƒã®å“è³ªã‚’å®¢è¦³çš„ã«è©•ä¾¡ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ç”»åƒå‡¦ç†ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ€§èƒ½è©•ä¾¡ã‚„ã€å“è³ªã«åŸºã¥ã„ãŸç”»åƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãªã©ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚

<br>



<br>

76. è§†é¢‘è´¨é‡è¯„ä»·(Video Quality Assessment):

<br>

å‹•ç”»ã®å“è³ªã‚’å®¢è¦³çš„ã«è©•ä¾¡ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã®å“è³ªç›£è¦–ã‚„ã€ãƒ“ãƒ‡ã‚ªåœ§ç¸®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æœ€é©åŒ–ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

<br>



<br>

77. æ•°æ®é›†(Datasets):

<br>

æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚„è©•ä¾¡ã«ä½¿ç”¨ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã®é›†åˆã§ã™ã€‚ImageNetã€COCOã€KITTI

<br>



<br>

ãªã©ã€å„ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒé–‹ç™ºã•ã‚Œã€ç ”ç©¶ã‚„æŠ€è¡“é–‹ç™ºã‚’åŠ é€Ÿã—ã¦ã„ã¾ã™ã€‚

<br>



<br>

ã“ã‚Œã‚‰ã®æŠ€è¡“ã¯ç›¸äº’ã«é–¢é€£ã—åˆã„ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã¨æ©Ÿæ¢°å­¦ç¿’ã®åˆ†é‡ã‚’å½¢ä½œã£ã¦ã„ã¾ã™ã€‚å¤šãã®å¿œç”¨åˆ†é‡ã§é©æ–°çš„ãªã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿã¿å‡ºã™ã¨ã¨ã‚‚ã«ã€æ–°ãŸãªèª²é¡Œã«ã‚‚ç›´é¢ã—ã¦ãŠã‚Šã€ä»Šå¾Œã•ã‚‰ãªã‚‹ç™ºå±•ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<span class="issue_date">Issue Date: 2024-09-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1426">Molmo, AI2, 2024.09</a>
<span class="snippet"><span>Summary</span>Molmoã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ãŸæœ€å…ˆç«¯ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ç‰¹ã«å°å‹ãƒ¢ãƒ‡ãƒ«ãŒå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã€‚Molmoã¯ã€ç‰©ç†çš„ãŠã‚ˆã³ä»®æƒ³çš„ãªä¸–ç•Œã¨ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¯èƒ½ã«ã—ã€éŸ³å£°ãƒ™ãƒ¼ã‚¹ã®èª¬æ˜ã‚’ç”¨ã„ãŸæ–°ã—ã„ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å°å…¥ã€‚ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã€éè¨€èªçš„æ‰‹ãŒã‹ã‚Šã‚’æ´»ç”¨ã—ã¦è³ªå•ã«ç­”ãˆã‚‹èƒ½åŠ›ã‚’æŒã¤ã€‚Molmoãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚¦ã‚§ã‚¤ãƒˆã§ãƒ—ãƒ­ãƒ—ãƒ©ã‚¤ã‚¨ã‚¿ãƒªã‚·ã‚¹ãƒ†ãƒ ã«å¯¾æŠ—ã™ã‚‹æ€§èƒ½ã‚’ç™ºæ®ã—ã€ä»Šå¾Œã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚§ã‚¤ãƒˆã‚„ãƒ‡ãƒ¼ã‚¿ã‚’å…¬é–‹äºˆå®šã€‚</span>
<span class="snippet"><span>Comment</span>ä»¥ä¸‹ãŒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœï¼ˆVLMã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼‰ã€‚11 benchmarksã¨æ›¸ã‹ã‚Œã¦ã„ã‚‹ã®ã¯ã€VLMã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã‚ã‚‹ç‚¹ã«æ³¨æ„ã€‚

<br>



<br>



<br>



<br>

<img width="981" alt="image" src="https://github.com/user-attachments/assets/510204e5-4cfb-4ba3-a6db-fff717a637bc">

<br>



<br>

<img width="940" alt="image" src="https://github.com/user-attachments/assets/a4a77006-fcde-4c33-b6df-54dc5d8cbdfa">

<br>



<br>

</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<span class="issue_date">Issue Date: 2024-09-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1422">Llama 3.2: Revolutionizing edge AI and vision with open, customizable models, Meta, 2024.09</a>
<span class="snippet"><span>Comment</span>11Bã¨90Bã®VLMã¨ã€ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹å‘ã‘ã®1B, 3Bã®SLMã‚’ç™ºè¡¨ã€‚

<br>

<img src="https://github.com/user-attachments/assets/13c4af37-19bd-4de7-b501-eb48f955af0c" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/d6b75b15-88cb-4d9e-9838-0da24308ccda" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/7475b30d-4619-4117-a911-d308291f86cb" alt="image" loading="lazy">Llama3.2ã®VLMã§ã¯ã€äº‹å‰å­¦ç¿’ã•ã‚ŒãŸimage encoderã‚’äº‹å‰å­¦ç¿’ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦çµ„ã¿åˆã‚ã›ã‚‹ãŸã‚ã®Adapterã‚’è¤‡æ•°å­¦ç¿’ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦å®Ÿç¾ã€‚

<br>



<br>

å…·ä½“çš„ã«ã¯ã€Llama 3.1ï¼ˆtext only modelï¼‰ã«å¯¾ã—ã¦ã€image encoderã¨Adapterã‚’è¿½åŠ ã—ã€å¤§è¦æ¨¡ã§ãƒã‚¤ã‚¸ãƒ¼ãªï¼ˆimage,textï¼‰ãƒšã‚¢ã§äº‹å‰å­¦ç¿’ã€‚ç¶šã„ã¦ã€ä¸­è¦æ¨¡ã®ã‚µã‚¤ã‚ºã®é«˜å“è³ªãªin-domainï¼ˆi.e. æ§˜ã€…ãªãƒ‰ãƒ¡ã‚¤ãƒ³ã®ï¼‰ã®çŸ¥è­˜ã‚’é«˜ã‚ã‚‹ã‚ˆã†ãªï¼ˆimage,textï¼‰ãƒšã‚¢ã§å­¦ç¿’ã—ãŸã€‚

<br>



<br>

äº‹å¾Œå­¦ç¿’ã§ã¯ã€Llama3.1ã¨åŒæ§˜ã«SFT, Rejection Sampling, DPOã®ãƒ©ã‚¦ãƒ³ãƒ‰ã‚’è¤‡æ•°å›ç¹°ã‚Šè¿”ã—ãŸã€‚Llama3.1ã‚’ç”¨ã„ã¦ã€in-domainã®ç”»åƒã«å¯¾ã™ã‚‹QAã‚’Data Augmentationã—ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã€‚ã•ã‚‰ã«å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ã¦å…¨ã¦ã®å›ç­”å€™è£œã‚’ãƒ©ãƒ³ã‚¯ã¥ã‘ã—ã¦é«˜å“è³ªãªSFTãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã€‚ã¾ãŸã€ãƒ¢ãƒ‡ãƒ«ã®å®‰å…¨æ€§ãŒé«˜ã¾ã‚‹ã‚ˆã†ãªãƒ‡ãƒ¼ã‚¿ã‚‚è¿½åŠ ã—ãŸã€‚

<br>



<br>

Llama3.1ã®äº‹å¾Œå­¦ç¿’ã®ãƒ—ãƒ­ã‚»ã‚¹ã«ã¤ã„ã¦ã¯ 1359 ã‚‚å‚ç…§ã®ã“ã¨ã€‚</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Repository.html">#Repository</a>
<span class="issue_date">Issue Date: 2024-09-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1379">ml-engineering</a>
<span class="snippet"><span>Comment</span>LLMã‚„VLMã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã‚„ãƒã‚¦ãƒã‚¦ãŒã¾ã¨ã‚ã‚‰ã‚ŒãŸãƒªãƒã‚¸ãƒˆãƒª</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<span class="issue_date">Issue Date: 2024-04-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1281">Grok-1.5 Vision Preview, 2024</a>
<span class="snippet"><span>Comment</span><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/88dd70ce-5874-4786-8e66-7484984c7a72" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Library.html">#Library</a>
<a class="button" href="articles/Alignment.html">#Alignment</a>
<a class="button" href="articles/TextualInversion.html">#TextualInversion</a>
<span class="issue_date">Issue Date: 2024-03-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1258">repeng</a>
<span class="snippet"><span>Comment</span>LLMã®å‡ºåŠ›ã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æ•°ç™¾å€‹ã®äº‹ä¾‹ã ã‘ã§å­¦ç¿’ã—ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚promptã§æŒ‡å®šã™ã‚‹ã®ã¨ã¯ç•°ãªã‚Šã€æ•°å€¤ã§ã‚¹ã‚¿ã‚¤ãƒ«ã®å¼·ã•ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ãŒå¯èƒ½ã‚‰ã—ã„ï¼ˆå…ƒãƒ„ã‚¤ãƒ¼ãƒˆï¼‰ã€‚ç”»åƒç”Ÿæˆåˆ†é‡ã«ãŠã‘ã‚‹Textual Inversionã¨åŒã˜æŠ€è¡“ã¨ã®ã“ã¨ã€‚

<br>



<br>

Textual Inversionã¨ã¯ã€å°‘é‡ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”¨ã„ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€éƒ¨åˆ†ã«æ–°ãŸãªã€Œå˜èªã€ã‚’è¿½åŠ ã—ã€å˜èªã¨å¯¾å¿œã™ã‚‹ç”»åƒã‚’ç”¨ã„ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹ã“ã¨ã§ã€promptä¸­ã§ã€Œå˜èªã€ã‚’åˆ©ç”¨ã—ãŸå ´åˆã«å­¦ç¿’ã—ãŸç”»åƒã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ãªãã¦ã‚‚å¯ï¼‰ã‚’ç”Ÿæˆã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹æŠ€è¡“ã€ã‚‰ã—ã„ã€‚

<br>



<br>

Huggiegface: https://huggingface.co/docs/diffusers/training/text_inversion

<br>

ï¼ˆå‚è€ƒï¼‰GPTã«è³ªå•ã—ãŸéš›ã®ãƒ­ã‚°: https://chat.openai.com/share/e4558c44-ce09-417f-9c77-6f3855e583fa

<br>

å…ƒãƒ„ã‚¤ãƒ¼ãƒˆ: https://x.com/webbigdata/status/1770272397184389211?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Prompting.html">#Prompting</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/AutomaticPromptEngineering.html">#AutomaticPromptEngineering</a>
<span class="issue_date">Issue Date: 2023-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1171">multimodal-maestro</a>
<span class="snippet"><span>Comment</span>Large Multimodal Model (LMM)ã«ãŠã„ã¦ã€é›‘ãªpromptã‚’ä¸ãˆã‚‹ã¦ã‚‚è‡ªå‹•çš„ã«è‰¯ã„æ„Ÿã˜outputã‚’ç”Ÿæˆã—ã¦ãã‚Œã‚‹ã£ã½ã„ï¼Ÿ

<br>



<br>



<br>



<br>

ä»¥ä¸‹ã®ä¾‹ã¯ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰ã®å¼•ç”¨ã§ã‚ã‚‹ãŒã€ã“ã®ä¾‹ã§ã¯ã€"Find dog." ã¨ã„ã†é›‘ãªpromptã‹ã‚‰ã€ç”»åƒä¸­å¤®ã«ä½ç½®ã™ã‚‹çŠ¬ã«[9]ã¨ã„ã†ãƒ©ãƒ™ãƒ«ã‚’ä¸ãˆã¾ã—ãŸã€ã¨ã„ã†responseã‚’å¾—ã‚‰ã‚Œã¦ã„ã‚‹ã€‚pipelineã¨ã—ã¦ã¯ã€Visual Promptã«å¯¾ã—ã¦ã¾ãšSAMã‚’ç”¨ã„ã¦ã‚¤ãƒ¡ãƒ¼ã‚¸ã®segmentationã‚’è¡Œã„ã€å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«ãƒ©ãƒ™ãƒ«ã‚’æŒ¯ã‚‹ã€‚ã“ã®ãƒ©ãƒ™ãƒ«ãŒæŒ¯ã‚‰ã‚ŒãŸç”»åƒã¨ã€"Find dog." ã¨ã„ã†é›‘ãªpromptã‚’ä¸ãˆã‚‹ã ã‘ã§è‰¯ã„æ„Ÿã˜ã«å‡¦ç†ã‚’ã—ã¦ãã‚Œã‚‹ã‚ˆã†ã ã€‚    

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/5220e62f-93f1-4eb9-b365-a9caaf933778" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/GenerativeAI.html">#GenerativeAI</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<span class="issue_date">Issue Date: 2023-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1170">LaVie: Text-to-Video generation, demo</a>
<span class="snippet"><span>Comment</span>ãƒ‡ãƒ¢ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§è©¦ã—ã¦ã¿ãŸã‚‰ã€3ç§’ã»ã©ã®prompté€šã‚Šã®å‹•ç”»ãŒç”Ÿæˆã•ã‚ŒãŸã€‚

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/4343fa52-698c-4a59-bad0-758fcd30d3ac" alt="image" loading="lazy">

<br>



<br>

FF14ã®èµ¤é­”å°å£«ã«å¤‰ãˆãŸã‚‰ã€ãã‚Œã£ã½ã„ã®å‡ºã¦ããŸ

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/07b6def8-01f2-4baf-9ba3-ab1ccc40c90e" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/TabularData.html">#TabularData</a>
<span class="issue_date">Issue Date: 2023-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1167">Table Transformer Demo</a>
<span class="snippet"><span>Comment</span>PDFä¸­ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ãã®æ§‹é€ ï¼ˆè¡Œåˆ—ã‚»ãƒ«ï¼‰ã‚’detectã™ã‚‹ãƒ¢ãƒ‡ãƒ«

<br>



<br>

Exampleã¯ä»¥ä¸‹ã®ã‚ˆã†ãªæ„Ÿã˜ï¼ˆæ—¥æœ¬èªã ã¨ã©ã‚Œãã‚‰ã„ã§ãã‚‹ã®ã‹ãª...ï¼‰

<br>



<br>



<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/7f62e16b-1ff8-46ad-b6df-7792981f8f58" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2023-11-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1156">ML Papers Explained</a>
<span class="snippet"><span>Comment</span>ä»¥ä¸‹ã®åˆ†é‡ã®ä»£è¡¨çš„ãªè«–æ–‡ãŒã¾ã¨ã‚ã‚‰ã‚Œã¦ã„ã‚‹ï¼ˆåŸºæœ¬çš„ã«ã¯Transformerç™»å ´å¾Œã®ã‚‚ã®ãŒå¤šã„ï¼‰

<br>



<br>

ãƒ»è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆTransformer, Elmoãªã©ï¼‰

<br>

ãƒ»Visionãƒ¢ãƒ‡ãƒ«ï¼ˆViTãªã©ï¼‰

<br>

ãƒ»CNNï¼ˆAlexNetãªã©ï¼‰

<br>

ãƒ»Single Stage Object Detectors

<br>

ãƒ»Region-based Convolutional Neural Networks

<br>

ãƒ»DocumentAIï¼ˆTableNetãªã©ï¼‰

<br>

ãƒ»Layout Transformers

<br>

ãƒ»Tabular Deeplearning</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ImageCaptioning.html">#ImageCaptioning</a>
<a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2023-11-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1114">Zero-shot Learningç¶²ç¾…çš„ã‚µãƒ¼ãƒ™ã‚¤: CLIPãŒåˆ‡ã‚Šé–‹ã„ãŸVision &amp; Languageã®æ–°ã—ã„ä¸–ç•Œ</a>
<span class="snippet"><span>Comment</span>ã“ã‚Œã¯ã™ã”ã„ã¾ã¨ã‚â€¦ã€‚ã¾ã é€”ä¸­ã¾ã§ã—ã‹èª­ã‚ã¦ã„ãªã„ã€‚CLIPã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã—ã¦CLIPã‚’å¼•ç”¨ã—ã¦ã„ã‚‹è«–æ–‡ã‹ã‚‰é‡è¦ãªã‚‚ã®ã‚’æ¦‚è¦ä»˜ãã§ã¾ã¨ã‚ã¦ã„ã‚‹ã€‚</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<span class="issue_date">Issue Date: 2023-10-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1107">StableDiffusion, LLMã®GPUãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã®ã‚ã‚Œã“ã‚Œ</a>
<span class="snippet"><span>Comment</span>Gradient Accumulation, Gradient Checkpointingã®èª¬æ˜ãŒä¸å¯§ã§ã‚ã‹ã‚Šã‚„ã™ã‹ã£ãŸã€‚</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ChatGPT.html">#ChatGPT</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<span class="issue_date">Issue Date: 2023-09-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1052">GPT-4V</a>
<span class="snippet"><span>Comment</span>ãŠã†â€¦ã‚„ã¹ãˆãªâ€¦

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/3ee7dc96-af6f-47f9-98c0-c6be5d9384f1" alt="image" loading="lazy"></span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<span class="issue_date">Issue Date: 2023-08-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1003">èµ°è¡Œå‹•ç”»ã‚’èª¬æ˜ã™ã‚‹LLMã‚’ä½œæˆã—ã€80å°ã®GPUã§åˆ†æ•£ä¸¦åˆ—å­¦ç¿’ã•ã›ãŸè©±</a>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/FoundationModel.html">#FoundationModel</a>
<span class="issue_date">Issue Date: 2023-07-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/897">Introducing CM3leon, a more efficient, state-of-the-art generative model for text and images, 2023</a>
<span class="snippet"><span>Summary</span>æœ€è¿‘ã®è‡ªç„¶è¨€èªå‡¦ç†ã®é€²æ­©ã«ã‚ˆã‚Šã€ç”Ÿæˆå‹AIãƒ¢ãƒ‡ãƒ«ã¸ã®é–¢å¿ƒã¨ç ”ç©¶ãŒåŠ é€Ÿã—ã¦ã„ã¾ã™ã€‚CM3leonã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®ç”Ÿæˆã¨ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã¸ã®ç”Ÿæˆã‚’è¡Œã†å˜ä¸€ã®åŸºç¤ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/ImageCaptioning.html">#ImageCaptioning</a>
<span class="issue_date">Issue Date: 2023-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/871">Comparing captioning models</a>
<span class="snippet"><span>Comment</span>SoTAã®vision languageãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¢ã€‚BLIP, BLIP2,GIT,InstructBLIPã‚’è©¦ã›ã‚‹</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/FoundationModel.html">#FoundationModel</a>
<a class="button" href="articles/InductiveBias.html">#InductiveBias</a>
<span class="issue_date">Issue Date: 2023-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/809">Objaverse-XL: A Universe of 10M+ 3D Objects</a>
<span class="snippet"><span>Comment</span>10Mã‚’è¶…ãˆã‚‹3D objectã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å…¬é–‹ã—ã€3D Modelã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦Zero123-XLã‚’è¨“ç·´ã€‚

<br>

å…ƒãƒ„ã‚¤ãƒ¼ãƒˆã®GifãŒã‚ã‹ã‚Šã‚„ã™ã„ã€‚

<br>

https://twitter.com/mattdeitke/status/1678855859089326080?s=46&t=8VBxVyng2U93usaVloHk7w

<br>



<br>

ãŸã¨ãˆã°inputã•ã‚ŒãŸã‚¤ãƒ¡ãƒ¼ã‚¸ã«å¯¾ã—ã¦ã€è‡ªç”±ã«ã‚«ãƒ¡ãƒ©ã®è¦–ç‚¹ã‚’è¨­å®šã—ã€ãã®è¦–ç‚¹ã‹ã‚‰ã®ç‰©ä½“ã®ç”»åƒã‚’å‡ºåŠ›ã§ãã‚‹ã€‚</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/SpeechProcessing.html">#SpeechProcessing</a>
<span class="issue_date">Issue Date: 2023-07-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/784">Awesome Multimodal LLMs</a>
<span class="snippet"><span>Comment</span>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªLLMã®ãƒªã‚¹ãƒˆãŒã¾ã¨ã‚ã‚‰ã‚Œã¦ã„ã‚‹</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Library.html">#Library</a>
<a class="button" href="articles/Explanation.html">#Explanation</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<span class="issue_date">Issue Date: 2022-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/499">Transformers Interpret, 2022</a>
<span class="snippet"><span>Comment</span>transformersã®ãƒ¢ãƒ‡ãƒ«ã‚’ãŸã£ãŸ2è¡Œè¿½åŠ ã™ã‚‹ã ã‘ã§ã€explainableã«ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª

<br>



<br>

åŸºæœ¬çš„ã«textã¨visionã®classificationã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹æ¨¡æ§˜

<br>

text classificationã®å ´åˆã€ãŸã¨ãˆã°input tokenã®å„ãƒˆãƒ¼ã‚¯ãƒ³ã®åˆ†é¡ã«å¯¾ã™ã‚‹å¯„ä¸åº¦ã‚’outputã—ã¦ãã‚Œã‚‹ã€‚</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<span class="issue_date">Issue Date: 2022-10-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/489">CNN vs. ViT, ç‰›ä¹…å…ˆç”Ÿ</a>
<span class="snippet"><span>Comment</span>ãƒ»Swin Transformer, Depth-wise conv, ConvNeXt, ViTã¨CNNã®ãƒ­ãƒã‚¹ãƒˆæ€§ã®é•ã„ã®è©±ãŒã‚ã‚Šå‹‰å¼·ã«ãªã‚‹

<br>



<br>

ãƒ»æœ€çµ‚çš„ãªçµè«–ãŒã€CNNã‚‚Transformerã‚‚å¤‰ã‚ã‚‰ãªã„ï¼ˆæ˜ç¢ºãªå‹è€…ã¯ã„ãªã„; ä»Šã®ã¨ã“ã‚å¼•ãåˆ†ã‘ï¼‰ã¨ã„ã†ã®ã¯ãŠã‚‚ã—ã‚ã‹ã£ãŸdepth-wise conv, point-wise convã®è§£èª¬è¨˜äº‹ï¼šhttps://agirobots.com/depthwise-pointwise-convolution/

<br>



<br>



<br>



<br>

é€šå¸¸ã®CNNã®ãƒ•ã‚£ãƒ«ã‚¿ã«ã‚ˆã‚‹feature mapè¨ˆç®—ã‚’ã€ç©ºé–“æ–¹å‘ï¼ˆdepth-wise convï¼‰ã¨ãƒãƒ£ãƒãƒ«æ–¹å‘ï¼ˆpoint-wise conv; 1x1 convï¼‰ã«åˆ†è§£ã™ã‚‹ã“ã¨ã§å¤§å¹…ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°å‰Šæ¸›</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/CVPR.html">#CVPR</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2021-11-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/430">Deep Residual Learning for Image Recognition, He+, Microsoft Research, CVPRâ€™16</a>
<span class="snippet"><span>Comment</span>ResNetè«–æ–‡

<br>



<br>

ResNetã§ã¯ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¨ˆç®—ã™ã‚‹é–¢æ•°ã‚’ã€æ®‹å·®F(x)ã¨æ’ç­‰é–¢æ•°xã®å’Œã¨ã—ã¦å®šç¾©ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒå…¥åŠ›ã¨ã®å·®åˆ†ã ã‘ã‚’å­¦ç¿’ã™ã‚Œã°è‰¯ããªã‚Šã€ãƒ¢ãƒ‡ãƒ«ã‚’æ·±ãã—ã¦ã‚‚æœ€é©åŒ–ãŒã—ã‚„ã™ããªã‚‹åŠ¹æœãã‚ã‚‹ã€‚æ•°ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«Residual Connectionã‚’å°å…¥ã—ã€æ’ç­‰é–¢æ•°ã«ã‚ˆã‚‹ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆãŒã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã„ã‚‹ã€‚

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/140301726-1d2e89e1-1d69-43d9-8d2b-0adb272e577a.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

ResNetãŒææ¡ˆã•ã‚Œã‚‹ä»¥å‰ã€ãƒ¢ãƒ‡ãƒ«ã‚’æ·±ãã™ã‚Œã°è¡¨ç¾åŠ›ãŒä¸ŠãŒã‚‹ã¯ãšãªã®ã«ã€å®Ÿéš›ã«ã¯ç²¾åº¦ãŒä¸‹ãŒã£ã¦ã—ã¾ã†ã“ã¨ã‹ã‚‰ã€ç†è«–ä¸Šãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒæ’ç­‰é–¢æ•°ã¨ãªã‚‹ã‚ˆã†ã«åˆæœŸåŒ–ã™ã‚Œã°ã€æ·±ã„ãƒ¢ãƒ‡ãƒ«ã§ã‚‚æµ…ã„ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã®è¡¨ç¾ãŒç²å¾—ã§ãã‚‹ã€ã¨è¨€ã†è€ƒãˆæ–¹ã‚’ç™ºå±•ã•ã›ãŸã€‚

<br>



<br>



<br>



<br>

ï¼ˆã‚¹ãƒ†ãƒ¼ãƒˆã‚ªãƒ–AIã‚¬ã‚¤ãƒ‰ã«åŸºã¥ãï¼‰åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã§ã‚ˆã‚Šå±¤ã‚’æ·±ãã§ãã‚‹ï¼ˆPlainãªæ§‹é€ ã¨æ¯”ã¹ã‚‹ã¨å±¤ãŒ1ã¤å¢—ãˆã‚‹ï¼‰Bottleneckã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚‚ææ¡ˆã—ã¦ã„ã‚‹ã€‚

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/140302452-649b0ea7-cce4-44c1-9e7d-b509ef8bca52.png" alt="image" loading="lazy">

<br>



<br>

ä»Šã‚„å½“ãŸã‚Šå‰ã®ã‚ˆã†ã«ä½¿ã‚ã‚Œã¦ã„ã‚‹Residual Connectionã¯ã€å±¤ã®æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«å¿…é ˆã®æŠ€è¡“ãªã®ã ã¨å†èªè­˜ã€‚</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<a class="button" href="articles/ImageClassification.html">#ImageClassification</a>
<span class="issue_date">Issue Date: 2021-05-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/346">EfficientNetè§£èª¬, omiita ï¼ˆã‚ªãƒŸãƒ¼ã‚¿ï¼‰, 2019</a>
<span class="snippet"><span>Comment</span>æ—¢å­˜ç”»åƒèªè­˜ãƒ¢ãƒ‡ãƒ«ã®æ§‹é€ ã¯å¤‰åŒ–ã•ã›ãšã€åºƒã•ã€æ·±ã•ã€è§£åƒåº¦ã‚’è¤‡åˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€å¾“æ¥ã‚ˆã‚Šã‚‚å°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€ã‹ã¤å­¦ç¿’é€Ÿåº¦ã§SoTAã‚’é”æˆã€‚åºƒã•ã€æ·±ã•ã€è§£åƒåº¦ã¯ãã‚Œãã‚Œæ€§èƒ½ã«äº’ã„ã«å½±éŸ¿ã—ã‚ã£ã¦ãŠã‚Šã€å¾“æ¥ã®ã‚ˆã†ã«åˆ¥ã€…ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã®ã§ã¯ãªãã€3ã¤ã®ãƒãƒ©ãƒ³ã‚¹ã‚’ã¨ã‚ŠãªãŒã‚‰ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã€‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹éš›ã¯ã€çµæœçš„ã«ã¯ãã‚Œãã‚Œã‚’ã‚ã‚‹å€¤ã§å®šæ•°å€ã™ã‚Œã°è‰¯ãã€ãã®ã‚ã‚‹å€¤ã¯æœ€å¤§ãƒ¡ãƒ¢ãƒªã‚„æœ€å¤§FLOPSæ•°ä»¥ä¸‹ï¼ˆãŠã‚ˆã³FLOPSãŒ2ã®Î¦ä¹—ã§å¢—åŠ ã™ã‚‹ã‚ˆã†ãªï¼‰ã¨ã„ã£ãŸåˆ¶ç´„ä¸‹ã§AccuracyãŒæœ€å¤§åŒ–ã•ã‚Œã‚‹å€¤ã‚’ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã§è¦‹ã¤ã‘ã‚‹ï¼ˆã‚‰ã—ã„ã€‚ã–ã£ãã‚Šã¨ã—ãŸç†è§£ï¼‰ã€‚

<br>

è»¢ç§»å­¦ç¿’ã—ã¦ã‚‚å¤šãã®ã‚¿ã‚¹ã‚¯ã§SoTAé”æˆã—ãŸã€‚</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2021-05-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/344">MLP-like Architecture</a>
<span class="snippet"><span>Comment</span>gMLP:å¤§è¦æ¨¡ãªself-attentionãŒç„¡ã„Spatial Gating Unitã‚’æ­è¼‰ã—ãŸã‚·ãƒ³ãƒ—ãƒ«ãªMLPã§ã‚‚ã€Transformerã®æ€§èƒ½ã«è¿‘ã¥ã‘ãŸã‚ˆï¼ˆç‰¹ã«CVï¼‰ã€‚ã¤ã¾ã‚Šã€self-attentionã¯essentialã¨ã„ã†ã‚ã‘ã§ã¯ãªã•ãã†ã ã‚ˆã€‚

<br>



<br>

NLPã®å ´åˆã¯gMLPã ã¨Transformerã¨perplexityã§comparableã€ä¸€éƒ¨downstreamã‚¿ã‚¹ã‚¯ã ã¨å‹ã¦ãªã‹ã£ãŸã‘ã©ã€single headã®tiny attentionã‚’è¿½åŠ ã—ãŸã‚‰ã€Transformerã‚’perplexityã¨GLUEã®ä¸€éƒ¨ã‚¿ã‚¹ã‚¯ã§outperformã—ãŸã‚ˆã€‚

<br>

ã¤ã¾ã‚Šã€Transformerã¿ãŸã„ã«å¤§è¦æ¨¡ãªself-attentionã¯å¿…é ˆã§ã¯ãªãã€å°è¦æ¨¡ã®attentionã§ï¼ˆcross sentenceã®é–¢ä¿‚æ€§ã‚’æ‰ãˆã‚‹ã«ã¯ï¼‰ååˆ†ã ã‚ˆã€‚

<br>

ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚‚Transformerã‚’ä¸Šå›ã£ãŸã‚ˆã€‚

<br>



<br>

ã£ã¦æ„Ÿã˜ï¼Ÿ

<br>



<br>

ã‚“ãƒ¼Transformerã«å‹ã£ãŸã¿ãŸã„ãªè¨€ã„æ–¹ã‚’SNSã ã¨è¦‹ã‹ã‘ã‚‹ã‘ã©ã€è©•ä¾¡ã—ã¦ã‚‹ã‚¿ã‚¹ã‚¯ãŒå°‘ãªã„ã—ã€ã©ã¡ã‚‰ã‹ã¨ã„ã†ã¨comparableãªdownstreamã‚¿ã‚¹ã‚¯ãŒå¤šã„ã—ã€ãã‚Œã¯è¨€ã„éãã§ã¯ï¼Ÿ

<br>

ã“ã®è«–æ–‡ãŒè¨€ã„ãŸã„ã®ã¯ã€å¤§è¦æ¨¡ãªself-attentionãŒæ€§èƒ½ã‚’å‡ºã™ä¸Šã§essentialãªã‚ã‘ã§ã¯ãªã„ã‚ˆã€ã£ã¦ã“ã¨ã§ã‚ã‚Šã€

<br>



<br>

ãƒ»CVã®å ´åˆã¯self-attentionã¯å¿…é ˆã§ã¯ãªã„

<br>

ãƒ»NLPã§ã¯ã€tiny attentionã§ã‚‚ååˆ†

<br>



<br>

ã¨ã„ã†æ„Ÿã˜ãªã®ã§ã¯ã€‚

<br>

ã¾ã‚ã§ã‚‚Transformerã¨comparableãªã‚‰ã€Transformerä¸€å¼·ã§ã¯ç„¡ããªã£ãŸã‚ˆã­Spatial Gating Unitï¼ˆSGUï¼‰ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®é–¢ä¿‚æ€§ã‚’æ‰ãˆã‚‹ãŸã‚ã®ã‚²ãƒ¼ãƒˆã§ã€SGUãŒç„¡ã„ã¨gMLPãƒ–ãƒ­ãƒƒã‚¯ã¯ãŸã ã®äºŒå±¤ã®FFNã¨ãªã‚‹ã€‚

<br>



<br>

SGUã¯ã€å…¥åŠ›ã‚’spatial dimensionã«å¯¾ã—ã¦ç·šå½¢å¤‰æ›ã—ãŸå€¤ã¨ã€å…ƒã®å…¥åŠ›ã®element-wiseãªç©ã§è¡¨ç¾ã™ã‚‹ã€‚ã“ã®ç·šå½¢å¤‰æ›ã‚’ã™ã‚‹éš›ã¯ã€Wã®å€¤ã‚’0ã®è¿‘å‚ã§åˆæœŸåŒ–ã—ã€ãƒã‚¤ã‚¢ã‚¹é …ã‚’1ã«åˆæœŸåŒ–ã™ã‚‹ã“ã¨ãŒã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã ã£ãŸã€‚ã“ã‚Œã¯ã€å­¦ç¿’ã®åˆã‚ã§ã¯ç·šå½¢å¤‰æ›ã¯identical mappingã«è¿‘ã„ã‚‚ã®ã¨ãªã‚‹ãŸã‚ã€gMLPãƒ–ãƒ­ãƒƒã‚¯ã¯FFNã«è¿‘ã„ã‚‚ã®ã¨ãªã‚‹ã€‚ã“ã‚ŒãŒå­¦ç¿’ãŒé€²ã‚€ã«ã¤ã‚ŒWã®é‡ã¿ãŒèª¿æ•´ã•ã‚Œã€cross tokenã®é–¢ä¿‚æ€§ã‚’æ‰ãˆãŸãƒ–ãƒ­ãƒƒã‚¯ã¸ã¨å¾ã€…ã«å¤‰åŒ–ã—ã¦ã„ãã“ã¨ã«ãªã‚‹ã€‚

<br>

ã¾ãŸã€SGUã¸ã®å…¥åŠ›ã¯GLUã®ã‚ˆã†ã«channel dimensionã«äºŒåˆ†å‰²ã—ã€ç‰‡æ–¹ã‚’element-wiseç©ã«ã€ã‚‚ã†ä¸€æ–¹ã‚’spatialãªç·šå½¢å¤‰æ›ã«åˆ©ç”¨ã™ã‚‹ï¼ˆ4ç¨®é¡è©¦ã—ãŸä¸­ã§ä¸€ç•ªæ€§èƒ½ãŒè‰¯ã‹ã£ãŸï¼‰ã€‚</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/CommentGeneration.html">#CommentGeneration</a>
<span class="issue_date">Issue Date: 2019-09-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/326">Cross-domain personalized image captioning, Long+, 2019</a>
<button onclick="hideContent(0)" style="display: none;">hide</button>
</div>


    </div>

</article>
<div class="post-nav">
<a class="previous" href="/paper_notes/articles/Composition.html" title="Compositionã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">Compositionã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§</a><a class="next" href="/paper_notes/articles/ConceptErasure.html" title="ConceptErasureã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">ConceptErasureã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§</a>
</div>
<div class="post-related">
      <div>Related Articles</div>
      <ul>
        <li class="">
          <a class="post-link" href="/paper_notes/articles/PEFT(Adaptor_LoRA).html" title="PEFT(Adaptor/LoRA)ã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            PEFT(Adaptor/LoRA)ã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/Generalization.html" title="Generalizationã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            Generalizationã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/MLOps.html" title="MLOpsã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            MLOpsã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/ComputerVision.html" title="ComputerVisionã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            ComputerVisionã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
</ul>
    </div>
<div class="post-comments"></div></section>
</div>


  </section>
  <section class="sidebar" style="margin-left: 15px;">
    <!-- Get sidebar items --><style type="text/css" media="screen">
.post-menu ul {
  list-style: none;
  padding: 0;
  margin: 0;
}
</style>

<div class="post-menu">
  <div class="post-menu-title">TOC</div>
  <div class="post-menu-content"></div>
</div>

<script>
  function generateContent() {
    var menu = document.querySelector(".post-menu");
    var menuContent =  menu.querySelector(".post-menu-content");
    var headings = document.querySelector(".post-content").querySelectorAll("h2, h3, h4, h5, h6");

    // Hide menu when no headings
    if (headings.length === 0) {
      return menu.style.display = "none";
    }

    // Generate post menu
    var menuHTML = '';
    for (var i = 0; i < headings.length; i++) {
      var h = headings[i];
      menuHTML += (
        '<li class="h-' + h.tagName.toLowerCase() + '">'
        + '<a href="#h-' + h.getAttribute('id') + '">' + h.textContent + '</a></li>');
    }

    menuContent.innerHTML = '<ul>' + menuHTML + '</ul>';

    // The header element
    var header = document.querySelector('header.site-header');

    function doMenuCollapse(index, over_items) {
      var items = menuContent.firstChild.children;

      if (over_items == undefined) {
        over_items = 20;
      }

      if (items.length < over_items) {
        return;
      }

      var activeItem = items[index];
      var beginItem = activeItem
      var endItem = activeItem
      var beginIndex = index;
      var endIndex = index + 1;
      while (beginIndex >= 0
        && !items[beginIndex].classList.contains('h-h2')) {
        beginIndex -= 1;
      }
      while (endIndex < items.length
        && !items[endIndex].classList.contains('h-h2')) {
        endIndex += 1;
      }
      for (var i = 0; i < beginIndex; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
      for (var i = beginIndex + 1; i < endIndex; i++) {
        item = items[i]
        // if (!item.classList.contains('h-h2')) {
          item.style.display = '';
        // }
      }
      for (var i = endIndex; i < items.length; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
    }

    // Init menu collapsed
    doMenuCollapse(-1);

    // Active the menu item
    window.addEventListener('scroll', function (event) {
      var lastActive = menuContent.querySelector('.active');
      var changed = true;
      var activeIndex = -1;
      for (var i = headings.length - 1; i >= 0; i--) {
        var h = headings[i];
        var headingRect = h.getBoundingClientRect();
        var headerRect = header.getBoundingClientRect();
        var headerTop = Math.floor(headerRect.top);
        var headerHeight = Math.floor(headerRect.height);
        var headerHeight = headerTop + headerHeight + 20;
        if (headingRect.top <= headerHeight) {
          var id = 'h-' + h.getAttribute('id');
          var a = menuContent.querySelector('a[href="#' + id  + '"]');
          var curActive = a.parentNode;
          if (curActive) {
            curActive.classList.add('active');
            activeIndex = i;
          }
          if (lastActive == curActive) {
            changed = false;
          }
          break;
        }
      }
      if (changed) {
        if (lastActive) {
          lastActive.classList.remove('active');
        }
        doMenuCollapse(activeIndex);
      }
      event.preventDefault();
    });
  }
  generateContent();
</script>
</section>
</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/paper_notes/"></data>

  <div class="wrapper">
    <div class="site-footer-inner">
<div>Copyright Â© 2023-current AkihikoWATANABE. The header images and any thumbnail images for the posts were generated by ChatGPT's DALL-E3.</div>
      <div>Powered by <a title="Jekyll is a simple, blog-aware, static site
      generator." href="https://jekyllrb.com/">Jekyll</a> &amp; <a title="Yat, yet
      another theme." href="https://github.com/jeffreytse/jekyll-theme-yat">Yat Theme</a>.</div>
      <div class="footer-col rss-subscribe">Subscribe <a href="/paper_notes/feed.xml">via RSS</a>
</div>
    </div>
  </div>
</footer>
</body>
</html>
