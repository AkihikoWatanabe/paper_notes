<!DOCTYPE html>
<html lang="ja">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-translate-customization" content="108d9124921d80c3-80e20d618ff053c8-g4f02ec6f3dba68b7-c">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>ComputerVisionã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§ | ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ</title>
<meta name="generator" content="Jekyll v3.10.0">
<meta property="og:title" content="ComputerVisionã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
<meta name="author" content="AkihikoWATANABE">
<meta property="og:locale" content="ja">
<meta name="description" content="ComputerVision #NeuralNetwork #Analysis #Supervised #RepresentationLearning #Self-SupervisedLearning #CLIP #One-Line Notes">
<meta property="og:description" content="ComputerVision #NeuralNetwork #Analysis #Supervised #RepresentationLearning #Self-SupervisedLearning #CLIP #One-Line Notes">
<link rel="canonical" href="http://akihikowatanabe.github.io/paper_notes/articles/ComputerVision.html">
<meta property="og:url" content="http://akihikowatanabe.github.io/paper_notes/articles/ComputerVision.html">
<meta property="og:site_name" content="ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-11-06T00:47:52+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="ComputerVisionã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"AkihikoWATANABE"},"dateModified":"2025-11-06T00:47:52+00:00","datePublished":"2025-11-06T00:47:52+00:00","description":"ComputerVision #NeuralNetwork #Analysis #Supervised #RepresentationLearning #Self-SupervisedLearning #CLIP #One-Line Notes","headline":"ComputerVisionã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§","mainEntityOfPage":{"@type":"WebPage","@id":"http://akihikowatanabe.github.io/paper_notes/articles/ComputerVision.html"},"url":"http://akihikowatanabe.github.io/paper_notes/articles/ComputerVision.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="icon" href="">
  <link rel="canonical" href="http://akihikowatanabe.github.io">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/main.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/custom_button.css">
  <script src="/paper_notes/assets/js/main.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script><link type="application/atom+xml" rel="alternate" href="http://akihikowatanabe.github.io/paper_notes/feed.xml" title="ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ">
<script>
  function showMore(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "block";
          
      // ã“ã®ãƒœã‚¿ãƒ³ã®å‚ç…§ã‚’å–å¾—ã—ã¦éè¡¨ç¤ºã«ã—ã¾ã™
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "none";
      }
          
      // hideãƒœã‚¿ãƒ³ã®å‚ç…§ã‚’å–å¾—ã—ã¦è¡¨ç¤ºã—ã¾ã™
      const hideButton = contentDiv.querySelector('button[onclick^="hideContent"]');
      if (hideButton) {
        hideButton.style.display = "block";
      }
    }
  }

  function hideContent(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "none";
  
      // moreãƒœã‚¿ãƒ³ã®å‚ç…§ã‚’å–å¾—ã—ã¦è¡¨ç¤ºã—ã¾ã™
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "block";
      }
  
      // ã“ã®ãƒœã‚¿ãƒ³ã‚’éš ã—ã¾ã™
      const hideButtons = document.querySelectorAll('button[onclick^="hideContent"]');
      const hideButton = hideButtons[index];
      if (hideButton) {
        hideButton.style.display = "none";
      }
    }
  }
</script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script charset="UTF-8" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/go.min.js" async></script>



















<script>
// Init highlight js
document.addEventListener('DOMContentLoaded', function(event) {
  var els = document.querySelectorAll('pre code')

  function addLangData(block) {
    var outer = block.parentElement.parentElement.parentElement;
    var lang = block.getAttribute('data-lang');
    for (var i = 0; i < outer.classList.length; i++) {
      var cls = outer.classList[i];
      if (cls.startsWith('language-')) {
        lang = cls;
        break;
      }
    }
    if (!lang) {
      cls = block.getAttribute('class');
      lang = cls ? cls.replace('hljs ', '') : '';
    }
    if (lang.startsWith('language-')) {
      lang = lang.substr(9);
    }
    block.setAttribute('class', 'hljs ' + lang);
    block.parentNode.setAttribute('data-lang', lang);
  }

  function addBadge(block) {
    var enabled = ('true' || 'true').toLowerCase();
    if (enabled == 'true') {
      var pre = block.parentElement;
      pre.classList.add('badge');
    }
  }

  function handle(block) {
    addLangData(block);
    addBadge(block)
    hljs.highlightBlock(block);
  }

  for (var i = 0; i < els.length; i++) {
    var el = els[i];
    handle(el);
  }
});
</script>

<style>
  /* code language badge */
  pre.badge::before {
    content: attr(data-lang);
    color: #fff;
    background-color: #ff4e00;
    padding: 0 .5em;
    border-radius: 0 2px;
    text-transform: uppercase;
    text-align: center;
    min-width: 32px;
    display: inline-block;
    position: absolute;
    right: 0;
  }

  /* fix wrong badge display for firefox browser */
  code > table pre::before {
    display: none;
  }
</style>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe-lightbox.umd.min.js" async></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe.umd.min.js" async></script>
<link href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/photoswipe.min.css" rel="stylesheet">
<style>
  .pswp .pswp__container .pswp__img {
    background-color: white;
  }
</style>

<script>
  function initPhotoSwipe() {
    let customOptions = {};

    try {
      const data = `{"gallery"=>"section.main", "children"=>"a.photo-swipe", "bgOpacity"=>0.8, "padding"=>{"top"=>20, "bottom"=>40, "left"=>100, "right"=>100}}`.replaceAll("=>", ":");
      customOptions = JSON.parse(data);
    } catch (e) {
      console.info("Invalid custom photo previewer options! " + e.message);
    }

    // Define object and gallery options
    const options = Object.assign(
      {
        gallery: "section.main",
        children: "a.photo-swipe",
        photo_scale: 2,
        // dynamic import is not supported in UMD version
        pswpModule: PhotoSwipe,
      },
      customOptions
    );

    const galleryEl = document.querySelector(options.gallery);
    if (!galleryEl) {
      return;
    }

    const imgEls = [];
    const els = galleryEl.querySelectorAll("img:not(.emoji)");
    els.forEach((el) => {
      if (el.src.trim() == "") {
        return;
      }
      if (!imgEls.includes(el)) {
        imgEls.push(el);
      }
    });

    if (imgEls.length === 0) {
      return;
    }

    imgEls.forEach((imgEl) => {
      imgEl.outerHTML = `
      <a class="photo-swipe"
        href="${imgEl.src}"
        data-pswp-width="${
          Math.max(imgEl.naturalWidth, imgEl.width) * options.photo_scale
        }"
        data-pswp-height="${
          Math.max(imgEl.naturalHeight, imgEl.height) * options.photo_scale
        }"
        data-pswp-caption="${imgEl.getAttribute("caption") || imgEl.alt}"
        target="_blank">
        ${imgEl.outerHTML}
      </a>`;
    });

    // Initialize PhotoSwipe 5
    var lightbox = new PhotoSwipeLightbox(options);

    lightbox.init();
  }

  window.addEventListener("load", initPhotoSwipe);
</script>
<meta name="google-site-verification" content="u_DTTPcCZ806iq51zgirHyWq3556HUKGq8AQfH91iFI">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-P70KSB88WH"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-P70KSB88WH');
  </script>

<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>





































































































































<header class="site-header site-header-transparent" role="banner">

  <div class="wrapper">
    <div class="site-header-inner">
<span class="site-brand"><a class="site-brand-inner" rel="author" href="/paper_notes/">
  <img class="site-favicon" title="ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ" src="" onerror="this.style.display='none'">
  ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ
</a>
</span><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger">
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewbox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
              </svg>
            </span>
          </label>

          <div class="trigger">









<span class="page-link">



<div id="google_translate_element" style="display: none;">
</div>

<span class="ct-language">
  <ul class="list-unstyled ct-language-dropdown">
    
      <li>
        <a href="#" class="lang-select" data-lang="en">
          
          <img src="https://cdn.countryflags.com/thumbs/united-states-of-america/flag-400.png" title="English">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="fr">
          
          <img src="https://cdn.countryflags.com/thumbs/france/flag-400.png" title="French">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="zh-CN">
          
          <img src="https://cdn.countryflags.com/thumbs/china/flag-400.png" title="Chinese(Simple)">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ja">
          
          <img src="https://cdn.countryflags.com/thumbs/japan/flag-400.png" title="Japanese">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ko">
          
          <img src="https://cdn.countryflags.com/thumbs/south-korea/flag-400.png" title="Korean">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ru">
          
          <img src="https://cdn.countryflags.com/thumbs/russia/flag-400.png" title="Russian">
          
        </a>
      </li>
    
  </ul>
</span>

<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({
    pageLanguage: 'ja',
    autoDisplay: false,
    layout: google.translate.TranslateElement.InlineLayout.VERTICAL
  }, 'google_translate_element');

  // Links to cross-origin destinations are unsafe
  var gll = document.getElementsByClassName('goog-logo-link')[0];
  if (gll) {
    gll.setAttribute('rel', 'noopener');
  }

  function restoreLang() {
    var iframe = document.getElementsByClassName('goog-te-banner-frame')[0];
    if (!iframe) return;

    var innerDoc = iframe.contentDocument || iframe.contentWindow.document;
    var restore_el = innerDoc.getElementsByTagName("button");

    for (var i = 0; i < restore_el.length; i++) {
      if (restore_el[i].id.indexOf("restore") >= 0) {
        restore_el[i].click();
        var close_el = innerDoc.getElementsByClassName("goog-close-link");
        close_el[0].click();
        return;
      }
    }
  }

  function triggerHtmlEvent(element, eventName) {
    var event;
    if (document.createEvent) {
      event = document.createEvent('HTMLEvents');
      event.initEvent(eventName, true, true);
      element.dispatchEvent(event);
    } else {
      event = document.createEventObject();
      event.eventType = eventName;
      element.fireEvent('on' + event.eventType, event);
    }
  }

  var googleCombo = document.querySelector("select.goog-te-combo");
  var langSelect = document.querySelector('.ct-language');
  langSelect.addEventListener('click', function(event) {
    if (!event.target) {
      return;
    }

    var selected = document.querySelector('.ct-language .ct-language-selected');
    if (selected) {
      selected.classList.remove('ct-language-selected');
    }

    var target = event.target;
    while (target && target !== langSelect ) {
      if (target.matches('.lang-select')) {
        break;
      }
      target = target.parentElement;
    }

    if (target && target.matches('.lang-select')) {
      var lang = target.getAttribute('data-lang');
      if (googleCombo.value == lang) {
        restoreLang();
      } else {
        target.parentElement.classList.add('ct-language-selected');
        googleCombo.value = lang;
        triggerHtmlEvent(googleCombo, 'change');
      }
    }

    event.preventDefault();
  });
}
</script>

<script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit" async></script>
</span>
</div>
        </nav>
</div>
  </div>
</header>

<script>
  function initHeader() {
    var lastScrollY = getScrollPos().y;
    var documentElement = document.documentElement;

    function storeScrollData() {
      var y = getScrollPos().y;documentElement.setAttribute("data-header-transparent", "");var scrollStatus = "";

      if (y <= 0) {
        scrollStatus = "top";
      } else if ((window.innerHeight + y) >= document.body.offsetHeight) {
        scrollStatus = "bottom";
      } else {
        var isScrollDown = (y - lastScrollY > 0) ? true : false;
        scrollStatus = isScrollDown ? "down" : "up";
      }

      lastScrollY = y;
      documentElement.setAttribute("data-scroll-status", scrollStatus);
    }

    window.addEventListener('scroll', function(e) {
      storeScrollData();
    });

    storeScrollData();
  }
  document.addEventListener('DOMContentLoaded', initHeader);
</script>


























































































































































<style>
    html .page-banner .page-banner-img > *:first-child {
      opacity: 0.4;
    }

    html[data-theme="dark"] .page-banner .page-banner-img > *:first-child {
      opacity: 0.2872;
    }
  </style>
<section class="page-banner">
    <div class="page-banner-img">
<div style="background-image: url(/paper_notes/assets/images/banner.png)"></div>
        <img class="img-placeholder" src="/paper_notes/assets/images/banner.png">
</div>
    <div class="wrapper">
      <div class="page-banner-inner">
<header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ</h1>
  <h2 class="post-subtitle">å‹‰å¼·ã—ãŸè«–æ–‡ã‚„æŠ€è¡“ç­‰ã®æƒ…å ±ã‚’Githubã®Issueã«ãƒ¡ãƒ¢ã£ã¦ã„ã‚‹ã²ã¨ã®ãƒ–ãƒ­ã‚°ã€‚
ãã‚Œãªã‚Šã«ãƒ¡ãƒ¢ã®é‡ãŒè“„ç©ã•ã‚Œã¦ããŸã®ã§ã€ä¸€åº¦æ•´ç†ã—ãŸã„ãªã¨æ€ã„ãƒ–ãƒ­ã‚°ã¯ã˜ã‚ã¦ã¿ã¾ã—ãŸï¼
è‡ªç„¶è¨€èªå‡¦ç†(NLP), æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ (RecommenderSystem), Educational Data Mining (EDM), Learning Analytics (LA)ãªã©ã®åˆ†é‡ã®ãƒ¡ãƒ¢ãŒå¤šã„ã¨æ€ã„ã¾ã™ã€‚
æœ€è¿‘ã¯ç‰¹ã«LLMã®å‹‰å¼·ãŒå¤šã‚ã§ã™ :)</h2>

  <div class="post-meta">
    <time class="dt-published" datetime="2025-11-06T00:47:52+00:00" itemprop="datePublished"><i class="fa fa-calendar"></i> Nov 6, 2025
    </time><span class="post-author left-vsplit"><i class="fa fa-pencil"></i> AkihikoWATANABE</span>
    
































    <span class="post-reading-time left-vsplit"><i class="fa fa-clock-o"></i> About 9 hours 39 mins</span>
  </div></header>
</div>
    </div>
  </section><script>
  function hashLocate(hashValue) {
    hashValue = hashValue.replace(/^.*#h-/, '');
    hashValue = decodeURIComponent(hashValue);
    var element = document.getElementById(hashValue);

    if (!element) {
      return;
    }

    var header = document.querySelector('header.site-header');
    var headerRect = header.getBoundingClientRect();
    var headerTop = Math.floor(headerRect.top);
    var headerHeight = Math.floor(headerRect.height);
    var scrollPos = getScrollPos();
    var offsetY = element.offsetTop - (headerTop + headerHeight + 20);

    if (offsetY == scrollPos.y) {
      return;
    }

    if (headerTop == 0  && offsetY > scrollPos.y) {
      offsetY += headerHeight + 2;
    } else if (headerTop < 0  && offsetY < scrollPos.y) {
      offsetY -= headerHeight - 2;
    }

    smoothScrollTo(offsetY);
  }

  // The first event occurred
  window.addEventListener('load', function(event) {
    if (window.location.hash) {
      hashLocate(window.location.hash);
    }
  });

  // The first event occurred
  window.addEventListener('click', function(event) {
    if (event.target.tagName.toLowerCase() == 'a') {
      hashLocate(event.target.getAttribute('href'));
    }
  });
</script>
<div class="theme-toggle">
  <input type="checkbox" id="theme-switch">
  <label for="theme-switch">
    <div class="toggle"></div>
    <div class="names">
      <p class="light">Light</p>
      <p class="dark">Dark</p>
    </div>
  </label>
</div>




<script>
  (function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var nightModeOption = ('off' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
      storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
      var data = storage.getItem('theme');
      try {
        data = JSON.parse(data ? data : '');
      } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
      }
      return data;
    }

    function handleThemeToggle(nightShift) {
      themeData.nightShift = nightShift;
      saveThemeData(themeData);
      html.dataset.theme = nightShift ? 'dark' : 'light';
      setTimeout(function() {
        sw.checked = nightShift ? true : false;
      }, 50);
    }

    function autoThemeToggle() {
      // Next time point of theme toggle
      var now = new Date();
      var toggleAt = new Date();
      var hours = now.getHours();
      var nightShift = hours >= 19 || hours <=7;

      if (nightShift) {
        if (hours > 7) {
          toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
      } else {
        toggleAt.setHours(19);
      }

      toggleAt.setMinutes(0);
      toggleAt.setSeconds(0);
      toggleAt.setMilliseconds(0)

      var delay = toggleAt.getTime() - now.getTime();

      // auto toggle theme mode
      setTimeout(function() {
        handleThemeToggle(!nightShift);
      }, delay);

      return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
      };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
      handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
      var data = autoThemeToggle();

      // Toggle theme by local setting
      if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
      } else {
        handleThemeToggle(themeData.nightShift);
      }
    } else if (nightModeOption == 'manual') {
      handleThemeToggle(themeData.nightShift);
    } else {
      var nightShift = themeData.nightShift;
      if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
      }
      handleThemeToggle(nightShift);
    }
  })();
</script>
<div id="click-to-top" class="click-to-top">
  <i class="fa fa-arrow-up"></i>
</div>
<script>
  (function () {
    const clickToTop = document.getElementById('click-to-top');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 100) {
        clickToTop.classList.add('show')
      }else {
        clickToTop.classList.remove('show')
      }
    });
    clickToTop.addEventListener('click', () => {
      window.smoothScrollTo(0);
    });
  })();
</script>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="framework">
  <section class="main">

     <div class="post">
  <section>









<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="post-content e-content" itemprop="articleBody">

      <h2 id="ComputerVision"> ComputerVision</h2>
<div class="visible-content">
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Supervised.html" target="_blank" rel="noopener noreferrer">#Supervised</a>
<a class="button" href="articles/RepresentationLearning.html" target="_blank" rel="noopener noreferrer">#RepresentationLearning</a>
<a class="button" href="articles/Self-SupervisedLearning.html" target="_blank" rel="noopener noreferrer">#Self-SupervisedLearning</a>
<a class="button" href="articles/CLIP.html" target="_blank" rel="noopener noreferrer">#CLIP</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>


<br>


<span class="issue_date">Issue Date: 2025-10-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3524" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Notes] Investigating fine- and coarse-grained structural correspondences between deep neural networks and human object image similarity judgments using unsupervised alignment, Takahashi+, Neural Networks'26, 2026.03</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/oizumim/status/1983800844933066931?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>CLIP, è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’, æ•™å¸«ã‚ã‚Šå­¦ç¿’ã‚’æ¯”è¼ƒã—ãŸã¨ãã«ã€CLIPãŒäººé–“ãŒç²å¾—ã™ã‚‹objectã®representationã¨ã‚‚ã£ã¨ã‚‚alignã—ã¦ã„ã‚‹ä¸€æ–¹ã§ã€è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã¯ã»ã¨ã‚“ã©å¶ç„¶ãƒ¬ãƒ™ãƒ«ã§ã—ã‹alignã—ãªã„ï¼ˆãŸã ã—ã€ç²—ã„ãƒ¬ãƒ™ãƒ«ã§è¦‹ã‚‹ã¨äººé–“ã§è¨€ã†ã¨ã“ã‚ã®ã‚«ãƒ†ã‚´ãƒªãƒ¬ãƒ™ãƒ«ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’å½¢æˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼‰ã€‚ã“ã®ãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã§ã®å­¦ç¿’ãŒäººé–“ãŒç²å¾—ã™ã‚‹è¡¨ç¾ã¨fine-grainedãªãƒ¬ãƒ™ãƒ«ã§alignã™ã‚‹ãŸã‚ã«éå¸¸ã«é‡è¦ã§ã‚ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚Œã‚‹ã€ã¨ã„ã†æ„Ÿã˜ã‚‰ã—ã„</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/Speech.html" target="_blank" rel="noopener noreferrer">#Speech</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/2D%20(Image).html" target="_blank" rel="noopener noreferrer">#2D (Image)</a>
<a class="button" href="articles/TTS.html" target="_blank" rel="noopener noreferrer">#TTS</a>
<a class="button" href="articles/AudioLanguageModel.html" target="_blank" rel="noopener noreferrer">#AudioLanguageModel</a>


<br>


<span class="issue_date">Issue Date: 2025-11-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3584" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction, Chaoyou Fu+, NeurIPS'25, 2025.01</a>
<span class="snippet"><span>GPT Summary</span>- éŸ³å£°ã®å½¹å‰²ã‚’é‡è¦–ã—ãŸãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆMLLMï¼‰ã®è¨“ç·´æ‰‹æ³•ã‚’ææ¡ˆã€‚è¦–è¦šã¨éŸ³å£°ã®ç›¸äº’ä½œç”¨ã‚’å¼·åŒ–ã—ã€ASRã‚„TTSãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãªã—ã§åŠ¹ç‡çš„ãªéŸ³å£°å¯¾è©±ã‚’å®Ÿç¾ã€‚ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯æ‰‹æ³•ã¨æ¯”è¼ƒã—ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®è¦–è¦šã¨éŸ³å£°ã®ç›¸äº’ä½œç”¨ãŒå¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/prakashkagitha/status/1985824703169044572?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>image/video, speechã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘ã¨ã‚Šãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«éŸ³å£°ã‚’å‡ºåŠ›ã™ã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã€‚<br><img src="https://github.com/user-attachments/assets/e3334f97-9fad-43de-9eb8-571cda31a107" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ObjectDetection.html" target="_blank" rel="noopener noreferrer">#ObjectDetection</a>


<br>


<span class="issue_date">Issue Date: 2025-11-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3583" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] YOLOv12: Attention-Centric Real-Time Object Detectors, Yunjie Tian+, NeurIPS'25, 2025.02</a>
<span class="snippet"><span>GPT Summary</span>- YOLOv12ã¯ã€æ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æ´»ç”¨ã—ãŸæ–°ã—ã„YOLOãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã€CNNãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã®é€Ÿåº¦ã‚’ç¶­æŒã—ã¤ã¤ã€ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ç‰¹ã«ã€YOLOv12-Nã¯T4 GPUä¸Šã§1.64 msã®æ¨è«–é…å»¶ã§40.6%ã®mAPã‚’é”æˆã—ã€YOLOv10-NãŠã‚ˆã³YOLOv11-Nã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã€‚ã¾ãŸã€YOLOv12ã¯RT-DETRã‚„RT-DETRv2ã‚ˆã‚Šã‚‚å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã—ã€è¨ˆç®—é‡ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ãªãŒã‚‰ã‚‚é«˜é€Ÿãªå®Ÿè¡Œã‚’å®Ÿç¾ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/prakashkagitha/status/1985824676216783193?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
</div>
<p><button onclick="showMore(0)">more</button></p>
<div class="hidden-content">
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/SelfCorrection.html" target="_blank" rel="noopener noreferrer">#SelfCorrection</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-11-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3582" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models   with Reinforcement Learning, Haozhe Wang+, NeurIPS'25, 2025.04</a>
<span class="snippet"><span>GPT Summary</span>- ã‚¹ãƒ­ãƒ¼ã‚¹ãƒ­ãƒ¼ã‚¹æ€è€ƒã‚·ã‚¹ãƒ†ãƒ ã¯ã€æ˜ç¤ºçš„ãªåçœã‚’é€šã˜ã¦é›£ã—ã„å•é¡Œã‚’è§£æ±ºã™ã‚‹å¯èƒ½æ€§ã‚’ç¤ºã—ã¦ã„ã‚‹ãŒã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–èƒ½åŠ›ã¯ãƒ•ã‚¡ã‚¹ãƒˆã‚¹ãƒ­ãƒ¼ã‚¹æ€è€ƒãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã§ã‚ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€å¼·åŒ–å­¦ç¿’ã‚’ç”¨ã„ã¦è¦–è¦šã¨è¨€èªã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ãƒ­ãƒ¼ã‚¹ãƒ­ãƒ¼ã‚¹æ€è€ƒèƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã€é¸æŠçš„ã‚µãƒ³ãƒ—ãƒ«ãƒªãƒ—ãƒ¬ã‚¤ï¼ˆSSRï¼‰ã¨å¼·åˆ¶çš„å†è€ƒã‚’å°å…¥ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«VL-Rethinkerã¯MathVistaã€MathVerseã§ãã‚Œãã‚Œ80.4%ã€63.5%ã®æœ€å…ˆç«¯ã‚¹ã‚³ã‚¢ã‚’é”æˆã—ã€ä»–ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã‚‚å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/prakashkagitha/status/1985824700644348091?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/2D%20(Image).html" target="_blank" rel="noopener noreferrer">#2D (Image)</a>
<a class="button" href="articles/4D%20(Video).html" target="_blank" rel="noopener noreferrer">#4D (Video)</a>
<a class="button" href="articles/Omni.html" target="_blank" rel="noopener noreferrer">#Omni</a>
<a class="button" href="articles/text.html" target="_blank" rel="noopener noreferrer">#text</a>
<span class="issue_date">Issue Date: 2025-11-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3581" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] UNO-Bench: A Unified Benchmark for Exploring the Compositional Law  Between Uni-modal and Omni-modal in Omni Models, Chen Chen+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯UNO-Benchã‚’ææ¡ˆã—ã€ãƒ¦ãƒ‹ãƒ¢ãƒ¼ãƒ€ãƒ«ã¨ã‚ªãƒ ãƒ‹ãƒ¢ãƒ¼ãƒ€ãƒ«ã®èƒ½åŠ›ã‚’44ã®ã‚¿ã‚¹ã‚¯ã¨5ã¤ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã§è©•ä¾¡ã€‚äººé–“ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã¨è‡ªå‹•åœ§ç¸®ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã€è¤‡é›‘ãªæ¨è«–ã‚’è©•ä¾¡ã™ã‚‹å¤šæ®µéšã‚ªãƒ¼ãƒ—ãƒ³ã‚¨ãƒ³ãƒ‰è³ªå•å½¢å¼ã‚’å°å…¥ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ã‚ªãƒ ãƒ‹ãƒ¢ãƒ¼ãƒ€ãƒ«ã®èƒ½åŠ›ãŒãƒ¢ãƒ‡ãƒ«ã®å¼·ã•ã«å¿œã˜ã¦ç•°ãªã‚‹å½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://meituan-longcat.github.io/UNO-Bench/" target="_blank" rel="noopener noreferrer">https://meituan-longcat.github.io/UNO-Bench/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1985907110786253019?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<a class="button" href="articles/Short.html" target="_blank" rel="noopener noreferrer">#Short</a>
<span class="issue_date">Issue Date: 2025-11-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3562" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint, Heekyung Lee+, EMNLP'25, 2025.05</a>
<span class="snippet"><span>GPT Summary</span>- ãƒªãƒã‚¹ãƒ‘ã‚ºãƒ«ã¯è¦–è¦šçš„ãªè¬ã§ã‚ã‚Šã€VLMã«ç‰¹æœ‰ã®æŒ‘æˆ¦ã‚’ã‚‚ãŸã‚‰ã™ã€‚å¾“æ¥ã®ã‚¿ã‚¹ã‚¯ã¨ã¯ç•°ãªã‚Šã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªæŠ½è±¡åŒ–ã‚„è±¡å¾´çš„æ¨è«–ãŒå¿…è¦ã€‚æœ¬ç ”ç©¶ã§ã¯ã€è‹±èªã®ãƒªãƒã‚¹ãƒ‘ã‚ºãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã—ã€VLMã®è§£é‡ˆèƒ½åŠ›ã‚’èª¿æŸ»ã€‚çµæœã€VLMã¯ã‚·ãƒ³ãƒ—ãƒ«ãªè¦–è¦šçš„æ‰‹ãŒã‹ã‚Šã«ã¯å¼·ã„ãŒã€æŠ½è±¡çš„æ¨è«–ã‚„è¦–è¦šçš„ãƒ¡ã‚¿ãƒ•ã‚¡ãƒ¼ã®ç†è§£ã«ã¯è‹¦åŠ´ã™ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/annele222/status/1984993238160425380?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Rebus Puzzleã®ä¾‹ã€‚ãŸã¨ãˆã°ä¸Šã®ä¾‹ã¯long time no seeãŒç­”ãˆã ãŒã€Timeã‚’èªè­˜ã—ã¦CãŒæŠœã‘ã¦ãŠã‚Šã€ã‹ã¤seeã¨Cã®éŸ³éŸ»ãŒä¼¼ã¦ã„ã‚‹ã¨ã„ã£ãŸè§£é‡ˆã‚’ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚Waterfallã®ä¾‹ã§ã¯ã€Waterã¨ã„ã†æ–‡å­—åˆ—ãŒæ»ã®ã‚ˆã†ã«ä¸‹ã«å‘ã‹ã£ã¦ã„ã‚‹æ§˜å­ã‹ã‚‰é¡æ¨ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚ãŠã‚‚ã—ã‚ã„ã€‚<br><img src="https://github.com/user-attachments/assets/53038e07-fafb-42fe-94f2-2ba6901c544d" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LLMAgent.html" target="_blank" rel="noopener noreferrer">#LLMAgent</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/Safety.html" target="_blank" rel="noopener noreferrer">#Safety</a>
<a class="button" href="articles/ComputerUse.html" target="_blank" rel="noopener noreferrer">#ComputerUse</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/Live.html" target="_blank" rel="noopener noreferrer">#Live</a>
<a class="button" href="articles/Safeguard.html" target="_blank" rel="noopener noreferrer">#Safeguard</a>
<span class="issue_date">Issue Date: 2025-11-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3558" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] OS-Sentinel: Towards Safety-Enhanced Mobile GUI Agents via Hybrid  Validation in Realistic Workflows, Qiushi Sun+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ¢ãƒã‚¤ãƒ«ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®‰å…¨æ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã€MobileRisk-Liveã¨ã„ã†å‹•çš„ã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹ç’°å¢ƒã‚’å°å…¥ã—ã€OS-Sentinelã¨ã„ã†æ–°ã—ã„ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å®‰å…¨æ€§æ¤œå‡ºãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã€‚OS-Sentinelã¯ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«ã®é•åæ¤œå‡ºã¨æ–‡è„ˆãƒªã‚¹ã‚¯è©•ä¾¡ã‚’çµ±åˆã—ã€å®Ÿé¨“ã§æ—¢å­˜æ‰‹æ³•ã«å¯¾ã—ã¦10%-30%ã®æ€§èƒ½å‘ä¸Šã‚’é”æˆã€‚è‡ªå¾‹å‹ãƒ¢ãƒã‚¤ãƒ«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä¿¡é ¼æ€§å‘ä¸Šã«å¯„ä¸ã™ã‚‹é‡è¦ãªæ´å¯Ÿã‚’æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span><p>dataset:


<a href="https://huggingface.co/datasets/OS-Copilot/MobileRisk" target="_blank" rel="noopener noreferrer">https://huggingface.co/datasets/OS-Copilot/MobileRisk</a>


<br>pj page:


<a href="https://qiushisun.github.io/OS-Sentinel-Home/" target="_blank" rel="noopener noreferrer">https://qiushisun.github.io/OS-Sentinel-Home/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1985319506512843220?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/3D%20(Scene).html" target="_blank" rel="noopener noreferrer">#3D (Scene)</a>
<a class="button" href="articles/Robotics.html" target="_blank" rel="noopener noreferrer">#Robotics</a>
<a class="button" href="articles/VisionLanguageActionModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageActionModel</a>
<a class="button" href="articles/SpatialUnderstanding.html" target="_blank" rel="noopener noreferrer">#SpatialUnderstanding</a>
<span class="issue_date">Issue Date: 2025-11-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3557" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] From Spatial to Actions: Grounding Vision-Language-Action Model in  Spatial Foundation Priors, Zhengshen Zhang+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- FALCONï¼ˆFrom Spatial to Actionï¼‰ã¯ã€è¦–è¦š-è¨€èª-è¡Œå‹•ï¼ˆVLAï¼‰ãƒ¢ãƒ‡ãƒ«ã®ç©ºé–“çš„æ¨è«–ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’è§£æ¶ˆã™ã‚‹æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§ã€3Dç©ºé–“ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¡Œå‹•ãƒ˜ãƒƒãƒ‰ã«æ³¨å…¥ã—ã¾ã™ã€‚RGBã‹ã‚‰å¹¾ä½•å­¦çš„æƒ…å ±ã‚’æä¾›ã—ã€æ·±åº¦ã‚„ãƒãƒ¼ã‚ºã‚’èåˆã•ã›ã‚‹ã“ã¨ã§é«˜ã„å¿ å®Ÿåº¦ã‚’å®Ÿç¾ã—ã€å†è¨“ç·´ã‚„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å¤‰æ›´ã¯ä¸è¦ã§ã™ã€‚FALCONã¯ã€ç©ºé–“è¡¨ç¾ã‚„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®è»¢é€å¯èƒ½æ€§ã‚’å‘ä¸Šã•ã›ã€11ã®ç¾å®Ÿä¸–ç•Œã®ã‚¿ã‚¹ã‚¯ã§æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://falcon-vla.github.io/" target="_blank" rel="noopener noreferrer">https://falcon-vla.github.io/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/travis_laflamee/status/1985327139475112336?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Self-SupervisedLearning.html" target="_blank" rel="noopener noreferrer">#Self-SupervisedLearning</a>
<a class="button" href="articles/RLVR.html" target="_blank" rel="noopener noreferrer">#RLVR</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/2D%20(Image).html" target="_blank" rel="noopener noreferrer">#2D (Image)</a>
<a class="button" href="articles/3D%20(Scene).html" target="_blank" rel="noopener noreferrer">#3D (Scene)</a>
<a class="button" href="articles/SpatialUnderstanding.html" target="_blank" rel="noopener noreferrer">#SpatialUnderstanding</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<a class="button" href="articles/Pixel-based.html" target="_blank" rel="noopener noreferrer">#Pixel-based</a>
<span class="issue_date">Issue Date: 2025-11-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3555" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised  Reinforcement Learning, Yuhong Liu+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- ç©ºé–“ç†è§£ã«ãŠã‘ã‚‹LVLMã®å¼±ç‚¹ã‚’å…‹æœã™ã‚‹ãŸã‚ã€è‡ªå·±æ•™å¸«ã‚ã‚Šå¼·åŒ–å­¦ç¿’ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ Spatial-SSRLã‚’ææ¡ˆã€‚5ã¤ã®å‰æã‚¿ã‚¹ã‚¯ã‚’è‡ªå‹•å®šå¼åŒ–ã—ã€æ¤œè¨¼ãŒå®¹æ˜“ãªä¿¡å·ã‚’å°å‡ºã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç©ºé–“æ¨è«–ãŒå¤§å¹…ã«æ”¹å–„ã•ã‚Œã€7ã¤ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§Qwen2.5-VLãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«å¯¾ã—ã¦å¹³å‡ç²¾åº¦ãŒ4.63%ï¼ˆ3Bï¼‰ãŠã‚ˆã³3.89%ï¼ˆ7Bï¼‰å‘ä¸Šã€‚ã‚·ãƒ³ãƒ—ãƒ«ãªç›£è¦–ãŒRLVRã‚’å¯èƒ½ã«ã—ã€LVLMã®ç©ºé–“çŸ¥èƒ½å‘ä¸Šã«å¯„ä¸ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1985177625564684602?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>RGB/RGB-D imageãŒgivenãªã¨ãã«ã€<br>- cropped patch inpainting<br>- flipped patch recognition<br>- shuffled patch reordering<br>- regional depth ordering<br>- relative 3D position prediction<br><br>ã®5ã¤ã®verifiableãªã‚¿ã‚¹ã‚¯ã‚’å®šç¾©ã—self supervisedãªmannerã§RLã™ã‚‹ã“ã¨ã§Spatial Understandingèƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹è©±ã‚‰ã—ã„<br><img src="https://github.com/user-attachments/assets/5ecd7925-6674-4c15-8d67-ec5a59133de4" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2025-10-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3500" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] The Principles of Diffusion Models, Chieh-Hsin Lai+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- ã“ã®ãƒ¢ãƒã‚°ãƒ©ãƒ•ã§ã¯ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®æ ¸å¿ƒåŸå‰‡ã¨ãã®å¤šæ§˜ãªå®šå¼åŒ–ã®èµ·æºã‚’æ¢ã‚Šã¾ã™ã€‚æ‹¡æ•£ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚¤ã‚ºã«è…æ•—ã•ã›ã‚‹å‰æ–¹ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰å§‹ã¾ã‚Šã€é€†ãƒ—ãƒ­ã‚»ã‚¹ã‚’å­¦ç¿’ã—ã¦ãƒã‚¤ã‚ºã‚’ãƒ‡ãƒ¼ã‚¿ã«æˆ»ã™ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚ä¸‰ã¤ã®è¦–ç‚¹ï¼ˆå¤‰åˆ†çš„ã€ã‚¹ã‚³ã‚¢ãƒ™ãƒ¼ã‚¹ã€ãƒ•ãƒ­ãƒ¼ãƒ™ãƒ¼ã‚¹ï¼‰ã‚’é€šã˜ã¦ã€ãƒã‚¤ã‚ºé™¤å»ã‚„ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã®æ–¹æ³•ã‚’èª¬æ˜ã—ã€å…±é€šã®åŸºç›¤ã¨ã—ã¦æ™‚é–“ä¾å­˜ã®é€Ÿåº¦å ´ã‚’ææ¡ˆã—ã¾ã™ã€‚ã•ã‚‰ã«ã€åˆ¶å¾¡å¯èƒ½ãªç”Ÿæˆã‚„åŠ¹ç‡çš„ãªæ•°å€¤ã‚½ãƒ«ãƒãƒ¼ã«ã¤ã„ã¦ã‚‚è­°è«–ã—ã€æ·±å±¤å­¦ç¿’ã®çŸ¥è­˜ã‚’æŒã¤èª­è€…ã«æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ç†è§£ã‚’æä¾›ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jcjesselai/status/1983325172909433002?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/NormalizingFlow.html" target="_blank" rel="noopener noreferrer">#NormalizingFlow</a>
<a class="button" href="articles/Compression.html" target="_blank" rel="noopener noreferrer">#Compression</a>
<span class="issue_date">Issue Date: 2025-10-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3488" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] FARMER: Flow AutoRegressive Transformer over Pixels, Guangting Zheng+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- FARMERã¨ã„ã†æ–°ã—ã„ç”Ÿæˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€æ­£è¦åŒ–ãƒ•ãƒ­ãƒ¼ã¨è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ã¦é«˜å“è³ªãªç”»åƒåˆæˆã¨å°¤åº¦æ¨å®šã‚’å®Ÿç¾ã€‚æ½œåœ¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¸ã®å¤‰æ›ã‚„è‡ªå·±æ•™å¸«ã‚ã‚Šæ¬¡å…ƒå‰Šæ¸›ã«ã‚ˆã‚Šã€ARãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®åŠ¹ç‡ã‚’å‘ä¸Šã€‚æ¨è«–é€Ÿåº¦ã‚’åŠ é€Ÿã™ã‚‹è’¸ç•™ã‚¹ã‚­ãƒ¼ãƒ ã¨ç”»åƒç”Ÿæˆå“è³ªã‚’å‘ä¸Šã•ã›ã‚‹åˆ†é¡å™¨ãƒ•ãƒªãƒ¼ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚’å°å…¥ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€FARMERã¯æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦ç«¶äº‰åŠ›ã®ã‚ã‚‹æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1983015650139222334?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1983034143580795131?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã“ã‚Œã¯...ğŸ‘€ğŸ‘€ğŸ‘€</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/Normalization.html" target="_blank" rel="noopener noreferrer">#Normalization</a>
<span class="issue_date">Issue Date: 2025-10-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3485" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] SeeDNorm: Self-Rescaled Dynamic Normalization, Wenrui Cai+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- SeeDNormã¯ã€å…¥åŠ›ã«åŸºã¥ã„ã¦å‹•çš„ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ã‚’èª¿æ•´ã™ã‚‹æ–°ã—ã„æ­£è¦åŒ–å±¤ã§ã‚ã‚Šã€RMSNormã®é™ç•Œã‚’å…‹æœã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å…¥åŠ›ã®ãƒãƒ«ãƒ æƒ…å ±ã‚’ä¿æŒã—ã€ãƒ‡ãƒ¼ã‚¿ä¾å­˜ã®è‡ªå·±å†ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å®Ÿç¾ã€‚å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚„ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã§ã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼ã—ã€å¾“æ¥ã®æ­£è¦åŒ–æ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ã¾ã—ãŸã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<a class="button" href="articles/3D%20Reconstruction.html" target="_blank" rel="noopener noreferrer">#3D Reconstruction</a>
<a class="button" href="articles/3D%20(Scene).html" target="_blank" rel="noopener noreferrer">#3D (Scene)</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<a class="button" href="articles/SpatialUnderstanding.html" target="_blank" rel="noopener noreferrer">#SpatialUnderstanding</a>
<span class="issue_date">Issue Date: 2025-10-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3484" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] IGGT: Instance-Grounded Geometry Transformer for Semantic 3D  Reconstruction, Hao Li+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- äººé–“ã®3Dã‚·ãƒ¼ãƒ³ç†è§£ã‚’æ¨¡å€£ã™ã‚‹ãŸã‚ã€ç©ºé–“å†æ§‹ç¯‰ã¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç†è§£ã‚’çµ±åˆã—ãŸInstanceGrounded Geometry Transformerï¼ˆIGGTï¼‰ã‚’ææ¡ˆã€‚IGGTã¯2Dè¦–è¦šå…¥åŠ›ã‚’ç”¨ã„ã¦å¹¾ä½•å­¦çš„æ§‹é€ ã¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’çµ±ä¸€çš„ã«è¡¨ç¾ã—ã€3Dã‚·ãƒ¼ãƒ³ã®ä¸€è²«æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚æ–°ãŸã«æ§‹ç¯‰ã—ãŸInsScene-15Kãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ã€3Dä¸€è²«æ€§ã®ã‚ã‚‹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ¬ãƒ™ãƒ«ã®ãƒã‚¹ã‚¯æ³¨é‡ˆã‚’æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://lifuguan.github.io/IGGT_official/" target="_blank" rel="noopener noreferrer">https://lifuguan.github.io/IGGT_official/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/10027lifuguan/status/1983104396503527512?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1984790549615034788?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ImageSegmentation.html" target="_blank" rel="noopener noreferrer">#ImageSegmentation</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<span class="issue_date">Issue Date: 2025-10-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3469" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of  Images and Videos, Haobo Yuan+, arXiv'25, 2025.01</a>
<span class="snippet"><span>GPT Summary</span>- Sa2VAã¯ã€ç”»åƒã¨å‹•ç”»ã®åŸºç›¤ç†è§£ã®ãŸã‚ã®çµ±ä¸€ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€æœ€å°é™ã®ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆæŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§å¤šæ§˜ãªã‚¿ã‚¹ã‚¯ã‚’ã‚µãƒãƒ¼ãƒˆã€‚SAM-2ã¨LLaVAã‚’çµ„ã¿åˆã‚ã›ã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€å‹•ç”»ã‚’çµ±åˆã€‚æ–°ãŸã«å°å…¥ã—ãŸRef-SAVãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã‚ˆã‚Šã€è¤‡é›‘ãªå‹•ç”»ã‚·ãƒ¼ãƒ³ã§ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆè¡¨ç¾ã‚’å¼·åŒ–ã€‚å®Ÿé¨“çµæœã¯ã€ç‰¹ã«å‚ç…§å‹•ç”»ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã§æœ€å…ˆç«¯ã®æˆæœã‚’ç¤ºã—ã€å®Ÿä¸–ç•Œã®å¿œç”¨ãŒæœŸå¾…ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>HF:


<a href="https://huggingface.co/collections/ByteDance/sa2va-model-zoo" target="_blank" rel="noopener noreferrer">https://huggingface.co/collections/ByteDance/sa2va-model-zoo</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/xtl994/status/1982746306406908309?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1876864159339426037?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Zero/Few/ManyShotPrompting.html" target="_blank" rel="noopener noreferrer">#Zero/Few/ManyShotPrompting</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-10-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3461" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Kaputt: A Large-Scale Dataset for Visual Defect Detection, Sebastian HÃ¶fer+, ICCV'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æ–°ã—ã„å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ææ¡ˆã—ã€å°å£²ç‰©æµã«ãŠã‘ã‚‹æ¬ é™¥æ¤œå‡ºã®èª²é¡Œã«å¯¾å¿œã€‚230,000æšã®ç”»åƒã¨29,000ä»¥ä¸Šã®æ¬ é™¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å«ã¿ã€MVTec-ADã®40å€ã®è¦æ¨¡ã€‚æ—¢å­˜æ‰‹æ³•ã®é™ç•Œã‚’ç¤ºã—ã€56.96%ã®AUROCã‚’è¶…ãˆãªã„çµæœã‚’å¾—ãŸã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ä»Šå¾Œã®ç ”ç©¶ã‚’ä¿ƒé€²ã™ã‚‹ãŸã‚ã«åˆ©ç”¨å¯èƒ½ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gabriberton/status/1979565856897331212?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Zero/Few/ManyShotPrompting.html" target="_blank" rel="noopener noreferrer">#Zero/Few/ManyShotPrompting</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-10-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3460" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Roboflow100-VL: A Multi-Domain Object Detection Benchmark for   Vision-Language Models, Peter Robicheaux+, NeurIPS'25, 2025.05</a>
<span class="snippet"><span>GPT Summary</span>- è¦–è¦šã¨è¨€èªã®ãƒ¢ãƒ‡ãƒ«ï¼ˆVLMsï¼‰ã¯ã€ä¸€èˆ¬çš„ãªç‰©ä½“ã«å¯¾ã—ã¦å„ªã‚ŒãŸã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ¤œå‡ºæ€§èƒ½ã‚’ç¤ºã™ãŒã€åˆ†å¸ƒå¤–ã®ã‚¯ãƒ©ã‚¹ã‚„ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦ã¯ä¸€èˆ¬åŒ–ãŒé›£ã—ã„ã€‚ãã“ã§ã€å°‘æ•°ã®è¦–è¦šä¾‹ã¨è±Šå¯Œãªãƒ†ã‚­ã‚¹ãƒˆè¨˜è¿°ã‚’ç”¨ã„ã¦VLMã‚’æ–°ã—ã„æ¦‚å¿µã«æ•´åˆã•ã›ã‚‹å¿…è¦ãŒã‚ã‚‹ã¨ææ¡ˆã€‚Roboflow100-VLã¨ã„ã†å¤šæ§˜ãªæ¦‚å¿µã‚’æŒã¤100ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ç‰©ä½“æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å°å…¥ã—ã€æœ€å…ˆç«¯ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚’è¡Œã£ãŸã€‚ç‰¹ã«ã€é›£ã—ã„åŒ»ç™‚ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆç²¾åº¦ãŒä½ãã€å°‘æ•°ã‚·ãƒ§ãƒƒãƒˆã®æ¦‚å¿µæ•´åˆãŒæ±‚ã‚ã‚‰ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gabriberton/status/1982270700883951875?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªLLMã§ã¯ICLãŒã†ã¾ãã„ã‹ãªã„ã‚‰ã—ã„ã€ã¨ã„ã†è©±ã‚‰ã—ã„ã€‚<br><br>ãŒã€<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3467" target="_blank" rel="noopener noreferrer">[Paper Note] Many-Shot In-Context Learning in Multimodal Foundation Models, Yixing Jiang+, arXiv'24, 2024.05</a>
<br><br>ã§ã®çŸ¥è¦‹ã¨ç•°ãªã‚‹ã€‚å·®ç•°ã¯ãªã‚“ã ã‚ã†ã‹ï¼Ÿ<br><br>ä»¥ä¸‹ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã§è­°è«–ãŒã•ã‚Œã¦ã„ã‚‹:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/marchoelle/status/1982589731260203110?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>pj page:


<a href="https://rf100-vl.org" target="_blank" rel="noopener noreferrer">https://rf100-vl.org</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/Robotics.html" target="_blank" rel="noopener noreferrer">#Robotics</a>
<a class="button" href="articles/memory.html" target="_blank" rel="noopener noreferrer">#memory</a>
<a class="button" href="articles/VisionLanguageActionModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageActionModel</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<a class="button" href="articles/LongHorizon.html" target="_blank" rel="noopener noreferrer">#LongHorizon</a>
<span class="issue_date">Issue Date: 2025-10-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3459" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] MemER: Scaling Up Memory for Robot Control via Experience Retrieval, Ajay Sridhar+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒ­ãƒœãƒƒãƒˆãƒãƒªã‚·ãƒ¼ã«äººé–“ã®ã‚ˆã†ãªè¨˜æ†¶èƒ½åŠ›ã‚’ä¸ãˆã‚‹ãŸã‚ã®éšå±¤çš„ãƒãƒªã‚·ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã€‚é«˜ãƒ¬ãƒ™ãƒ«ãƒãƒªã‚·ãƒ¼ãŒé–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’é¸æŠã—ã€ä½ãƒ¬ãƒ™ãƒ«ãƒãƒªã‚·ãƒ¼ã«æŒ‡ç¤ºã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã€é•·æœŸçš„ãªä¾å­˜é–¢ä¿‚ã‚’åŠ¹ç‡çš„ã«æ¨è«–ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ææ¡ˆæ‰‹æ³•MemERãŒå¾“æ¥ã®æ–¹æ³•ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/ajaysridhar0/status/1982239143431393432?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>pj page:


<a href="https://jen-pan.github.io/memer/" target="_blank" rel="noopener noreferrer">https://jen-pan.github.io/memer/</a>


</p>
<p>å‹•ç”»ã‚¹ãƒˆãƒªãƒ¼ãƒ å…¨ã¦ã‚’å¸¸ã«inputã™ã‚‹ã®ã§ã¯ãªãã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ã¯é™ã‚‰ã‚Œã¦ã„ã‚‹ã®ã§ã€VLMã«ã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒ¡ãƒ¢ãƒªä¸Šã§ç®¡ç†ã™ã‚‹ã‚ˆã†ãªå½¹å‰²ã‚’ä¸ãˆã€instructionã¨å®Ÿç¾ã™ã‚‹ãŸã‚ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦å‹•çš„ã«å¿…è¦ãªæƒ…å ±ã®ã¿ã‚’VLAã«ä¸ãˆã‚‹ã“ã¨ã§long horizonã§ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’æ”¹å–„ã™ã‚‹ã€ã¿ãŸã„ãªè©±ã‚‰ã—ã„</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-10-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3441" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Positional Encoding Field, Yunpeng Bai+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- Diffusion Transformersï¼ˆDiTsï¼‰ã¯ã€è¦–è¦šç”Ÿæˆã«ãŠã„ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚ã‚Šã€ãƒ‘ãƒƒãƒãƒˆãƒ¼ã‚¯ãƒ³ã¨ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆPEï¼‰ã‚’ç”¨ã„ã¦ã„ã¾ã™ã€‚æœ¬ç ”ç©¶ã§ã¯ã€DiTsãŒã©ã®ã‚ˆã†ã«è¦–è¦šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ•´ç†ã™ã‚‹ã‹ã‚’å†è€ƒã—ã€PEã®æ‘‚å‹•ã«å¯¾ã—ã¦ã‚‚ä¸€è²«ã—ãŸå‡ºåŠ›ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚ã“ã‚Œã«åŸºã¥ãã€ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’3Dãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«æ‹¡å¼µã—ãŸPE-Fieldã‚’ææ¡ˆã—ã€ãƒœãƒªãƒ¥ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¨è«–ã¨éšå±¤çš„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’çµ„ã¿è¾¼ã¿ã¾ã—ãŸã€‚å¼·åŒ–ã•ã‚ŒãŸDiTã¯ã€æ–°ã—ã„è¦–ç‚¹åˆæˆã¨ç©ºé–“ç”»åƒç·¨é›†ã«ãŠã„ã¦æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’é”æˆã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://yunpeng1998.github.io/PE-Field-HomePage/" target="_blank" rel="noopener noreferrer">https://yunpeng1998.github.io/PE-Field-HomePage/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/byp215bai/status/1981809736535208219?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<span class="issue_date">Issue Date: 2025-10-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3383" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Beyond Isolated Words: Diffusion Brush for Handwritten Text-Line   Generation, Gang Dai+, ICCV'25, 2025.08</a>
<span class="snippet"><span>GPT Summary</span>- æ‰‹æ›¸ããƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã«ãŠã„ã¦ã€DiffBrushã¨ã„ã†æ–°ã—ã„æ‹¡æ•£ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã€‚ã‚¹ã‚¿ã‚¤ãƒ«ã¨å†…å®¹ã®æ­£ç¢ºæ€§ã‚’ä¸¡ç«‹ã•ã›ã‚‹ãŸã‚ã€ã‚¹ã‚¿ã‚¤ãƒ«å­¦ç¿’ã‚’å†…å®¹ã‹ã‚‰åˆ‡ã‚Šé›¢ã—ã€ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã§å†…å®¹ã‚’å­¦ç¿’ã™ã‚‹æˆ¦ç•¥ã‚’æ¡ç”¨ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€é«˜å“è³ªãªãƒ†ã‚­ã‚¹ãƒˆãƒ©ã‚¤ãƒ³ç”ŸæˆãŒç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1980918946963534003?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰‹æ›¸ãæ–‡å­—ç”Ÿæˆ</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Pixel-based.html" target="_blank" rel="noopener noreferrer">#Pixel-based</a>
<span class="issue_date">Issue Date: 2025-10-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3375" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text  Inputs in Multimodal LLMs, Yanhong Li+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”»åƒã¨ã—ã¦æä¾›ã™ã‚‹ã“ã¨ã§ã€LLMã®ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’å‰Šæ¸›ã—ã¤ã¤æ€§èƒ½ã‚’ç¶­æŒã§ãã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”»åƒã«ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã—ã€ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã«ç›´æ¥å…¥åŠ›ã™ã‚‹ã“ã¨ã§ã€å¿…è¦ãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å¤§å¹…ã«æ¸›å°‘ã•ã›ã‚‹ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€RULERã¨CNN/DailyMailã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æ€§èƒ½ã‚’æãªã†ã“ã¨ãªãã€ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¯€ç´„ãŒå®Ÿç¾ã§ãã‚‹ã“ã¨ã‚’ç¢ºèªã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1980942325573648703?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Controllable.html" target="_blank" rel="noopener noreferrer">#Controllable</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/VariationalAutoEncoder.html" target="_blank" rel="noopener noreferrer">#VariationalAutoEncoder</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-10-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3373" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] OminiControl: Minimal and Universal Control for Diffusion Transformer, Zhenxiong Tan+, ICCV'25 Highlight, 2024.11</a>
<span class="snippet"><span>GPT Summary</span>- OminiControlã¯ã€Diffusion Transformerï¼ˆDiTï¼‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ãŠã‘ã‚‹ç”»åƒæ¡ä»¶ä»˜ã‘ã®æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’æœ€å°é™ã«æŠ‘ãˆã¤ã¤ã€æŸ”è»Ÿãªãƒˆãƒ¼ã‚¯ãƒ³ç›¸äº’ä½œç”¨ã¨å‹•çš„ãªä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Ÿç¾ã€‚åºƒç¯„ãªå®Ÿé¨“ã«ã‚ˆã‚Šã€è¤‡æ•°ã®æ¡ä»¶ä»˜ã‘ã‚¿ã‚¹ã‚¯ã§å°‚é–€çš„æ‰‹æ³•ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€åˆæˆã•ã‚ŒãŸç”»åƒãƒšã‚¢ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ŒSubjects200Kã€ã‚’å°å…¥ã€‚åŠ¹ç‡çš„ã§å¤šæ§˜ãªç”»åƒç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã®å¯èƒ½æ€§ã‚’ç¤ºå”†ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/yxy2168/status/1980244155667476923?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>DiTã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ï¼ˆMMAä»¥å¤–ã¯ï¼‰å¤‰æ›´ã›ãšã«ã€Condition Image C_Iã‚’VAEã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãŸnoisy inputã‚’DiTã®inputã«concatã—é †ä¼æ’­ã•ã›ã‚‹ã“ã¨ã§ã€DiTã‚’unified conditioningãƒ¢ãƒ‡ãƒ«ï¼ˆï¼C_Iã®ç‰¹å¾´é‡ã‚’ä»–ã®inputã¨åŒã˜latent spaceã§å­¦ç¿’ã•ã›çµ±åˆçš„ã«æ‰±ã†ï¼‰ã¨ã—ã¦å­¦ç¿’ã™ã‚‹[^1]ã€‚<br><br>[^1]: æ—¢å­˜ç ”ç©¶ã¯åˆ¥ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‹ã‚‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãŸfeatureãŒåŠ ç®—ã•ã‚Œã¦ã„ã¦ï¼ˆå¼3ï¼‰ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€éƒ¨åˆ†ã«åˆ¥é€”ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¿…è¦ã ã£ãŸã ã‘ã§ãªãã€åŠ ç®—ã¯ç©ºé–“çš„ãªå¯¾å¿œé–¢ä¿‚ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã†ã¾ãå¯¾å‡¦ã§ããšï¼ˆfeatureã®æ¬¡å…ƒãŒç©ºé–“çš„ãªæƒ…å ±ã«å¯¾å¿œã—ã¦ã„ã‚‹ãŸã‚ï¼‰ã€conditional tokenã¨imageã®äº¤äº’ä½œç”¨ã‚’å¦¨ã’ã¦ã„ãŸã€‚<br><br>ã¾ãŸã€positional encodingã®indexã‚’conditional tokenã¨noisy image tokensã¨å…±æœ‰ã™ã‚‹ã¨ã€ç©ºé–“çš„ãªå¯¾å¿œé–¢ä¿‚ãŒå­˜åœ¨ã™ã‚‹ã‚¿ã‚¹ã‚¯ï¼ˆedge guided generationç­‰ï¼‰ã¯ã†ã¾ãã„ã£ãŸãŒã€è¢«å†™ä½“ã‚’æŒ‡å®šã™ã‚‹ç”Ÿæˆï¼ˆsubject driven generation)ã®ã‚ˆã†ãªå¯¾å¿œé–¢ä¿‚ãŒå­˜åœ¨ã—ãªã„ã‚¿ã‚¹ã‚¯ï¼ˆnon-aligned task)ã®å ´åˆã¯ã†ã¾ãã„ã‹ãªã‹ã£ãŸã€‚ã—ã‹ã—ã€non-aligned taskã®å ´åˆã¯ã€indexã«ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’åŠ ãˆã‚·ãƒ•ãƒˆã•ã›ã‚‹ï¼ˆå¼4ï¼‰ã“ã¨ã§ã€conditional text/image tokené–“ã§ç©ºé–“çš„ã«overlapã—ãªã„ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã§æ€§èƒ½ãŒå¤§å¹…ã«æ”¹å–„ã—ãŸã€‚<br><br>æ—¢å­˜ç ”ç©¶ã§ã¯ã€C_Iã®å¼·ã•ã‚’ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã™ã‚‹ãŸã‚ã«ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦å®šæ•°ã‚’å°å…¥ã—ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸfeatureã‚’åŠ ç®—ã™ã‚‹éš›ã®å¼·ã•ã‚’èª¿æ•´ã—ã¦ã„ãŸãŒï¼ˆ3.2.3ç¯€ï¼‰ã€æœ¬æ‰‹æ³•ã§ã¯concatã‚’ã™ã‚‹ãŸã‚ã“ã®ã‚ˆã†ãªæ–¹æ³•ã¯ä½¿ãˆãªã„ã€‚ãã®ãŸã‚ã€Multi-Modal Attention(MMA)ã«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã£ã¦å¼·ã•ã‚’èª¿æ•´å¯èƒ½ãªbias matrixã‚’å°å…¥ã—ã€C_Iã¨Xã®attentionã®äº¤äº’ä½œç”¨ã®å¼·ã•ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§å¯¾å¿œã—ãŸï¼ˆå¼5,6ï¼‰ã€‚</p></span><br><br>
<a class="button" href="articles/Multi.html" target="_blank" rel="noopener noreferrer">#Multi</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/QuestionAnswering.html" target="_blank" rel="noopener noreferrer">#QuestionAnswering</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Conversation.html" target="_blank" rel="noopener noreferrer">#Conversation</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/2D%20(Image).html" target="_blank" rel="noopener noreferrer">#2D (Image)</a>
<span class="issue_date">Issue Date: 2025-10-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3371" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] FineVision: Open Data Is All You Need, Luis Wiedmann+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€è¦–è¦šã¨è¨€èªã®ãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰ã®ãŸã‚ã«ã€24ç™¾ä¸‡ã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰ãªã‚‹çµ±ä¸€ã‚³ãƒ¼ãƒ‘ã‚¹ã€ŒFineVisionã€ã‚’ç´¹ä»‹ã€‚ã“ã‚Œã¯200ä»¥ä¸Šã®ã‚½ãƒ¼ã‚¹ã‚’çµ±åˆã—ã€åŠè‡ªå‹•åŒ–ã•ã‚ŒãŸãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚Œã¦ã„ã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿ã®è¡›ç”Ÿã¨é‡è¤‡æ’é™¤ãŒè¡Œã‚ã‚Œã€66ã®å…¬çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«å¯¾ã™ã‚‹æ±šæŸ“é™¤å»ã‚‚é©ç”¨ã€‚FineVisionã§è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã€æ—¢å­˜ã®ã‚ªãƒ¼ãƒ—ãƒ³ãƒŸãƒƒã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€ãƒ‡ãƒ¼ã‚¿ä¸­å¿ƒã®VLMç ”ç©¶ã®åŠ é€Ÿã‚’ç›®æŒ‡ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://huggingface.co/spaces/HuggingFaceM4/FineVision" target="_blank" rel="noopener noreferrer">https://huggingface.co/spaces/HuggingFaceM4/FineVision</a>


</p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2413" target="_blank" rel="noopener noreferrer">[Paper Note] Grounding Multilingual Multimodal LLMs With Cultural Knowledge, Jean de Dieu Nyandwi+, EMNLP'25</a>
</p>
<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1981093262912819418?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è‘—è€…ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/lusxvr/status/1963609337546293448?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/4D%20(Video).html" target="_blank" rel="noopener noreferrer">#4D (Video)</a>
<span class="issue_date">Issue Date: 2025-10-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3364" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Self-Forcing++: Towards Minute-Scale High-Quality Video Generation, Justin Cui+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€é•·ã„å‹•ç”»ç”Ÿæˆã«ãŠã‘ã‚‹å“è³ªåŠ£åŒ–ã‚’è»½æ¸›ã™ã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¾ã™ã€‚æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã‚’æ´»ç”¨ã—ã€è‡ªå·±ç”Ÿæˆã—ãŸé•·ã„å‹•ç”»ã‹ã‚‰æŠ½å‡ºã—ãŸã‚µãƒ³ãƒ—ãƒ«ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’é€šã˜ã¦å­¦ç”Ÿãƒ¢ãƒ‡ãƒ«ã«ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã€é•·ã•ã‚’æœ€å¤§20å€ã«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã—ã¤ã¤æ™‚é–“çš„ä¸€è²«æ€§ã‚’ç¶­æŒã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æœ€å¤§4åˆ†15ç§’ã®å‹•ç”»ã‚’ç”Ÿæˆå¯èƒ½ã§ã€å¾“æ¥ã®æ‰‹æ³•ã‚ˆã‚Šã‚‚å¿ å®Ÿåº¦ã¨ä¸€è²«æ€§ã§å¤§å¹…ã«å„ªã‚ŒãŸçµæœã‚’ç¤ºã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1980711685686997412?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãŠãƒ¼ã€ã‚‚ã†++ãŒå‡ºã¦ããŸã€‚ã™ã”ã„ã‚¹ãƒ”ãƒ¼ãƒ‰æ„Ÿã ã€‚</p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2870" target="_blank" rel="noopener noreferrer">[Paper Note] Self Forcing: Bridging the Train-Test Gap in Autoregressive Video   Diffusion, Xun Huang+, NeurIPS'25</a>
</p>
<p>Self Forcingã¨æ¯”è¼ƒã—ã¦50sä»¥ä¸Šã§ã®ç”Ÿæˆã®æ€§èƒ½ãŒå‘ä¸Šã—ã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã‚‹</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ContextWindow.html" target="_blank" rel="noopener noreferrer">#ContextWindow</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-10-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3357" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Glyph: Scaling Context Windows via Visual-Text Compression, Jiale Cheng+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒã¤å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®å®Ÿç”¨æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã€Glyphã¨ã„ã†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”»åƒã«å¤‰æ›ã—ã¦è¦–è¦šã¨è¨€èªã®ãƒ¢ãƒ‡ãƒ«ï¼ˆVLMsï¼‰ã§å‡¦ç†ã—ã¾ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚Šã€3-4å€ã®ãƒˆãƒ¼ã‚¯ãƒ³åœ§ç¸®ã‚’å®Ÿç¾ã—ã€ç²¾åº¦ã‚’ç¶­æŒã—ã¤ã¤å‡¦ç†é€Ÿåº¦ã‚’ç´„4å€å‘ä¸Šã•ã›ã¾ã™ã€‚ã•ã‚‰ã«ã€128Kã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®VLMãŒ1Mãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†å¯èƒ½ã«ãªã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1980474912168112471?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/arankomatsuzaki/status/1980722682246398069?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”»åƒã«ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã—ã¦VLMã«å…¥åŠ›ã™ã‚‹ã“ã¨ã§textã¨æ¯”è¼ƒã—ã¦3.2å€KV Cache (context)ã‚’åœ§ç¸®ã—ã€prefillingã¨ãƒ‡ã‚³ãƒ¼ãƒ‰é€Ÿåº¦ã‚‚4.8, 4.4å€é«˜é€ŸåŒ–ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚‰ã—ã„<br><br><img src="https://github.com/user-attachments/assets/e65f880d-0d04-434f-9a51-accc84d44a6f" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Temporal.html" target="_blank" rel="noopener noreferrer">#Temporal</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/2D%20(Image).html" target="_blank" rel="noopener noreferrer">#2D (Image)</a>
<a class="button" href="articles/TTS.html" target="_blank" rel="noopener noreferrer">#TTS</a>
<a class="button" href="articles/4D%20(Video).html" target="_blank" rel="noopener noreferrer">#4D (Video)</a>
<a class="button" href="articles/Omni.html" target="_blank" rel="noopener noreferrer">#Omni</a>
<a class="button" href="articles/audio.html" target="_blank" rel="noopener noreferrer">#audio</a>
<a class="button" href="articles/text.html" target="_blank" rel="noopener noreferrer">#text</a>
<span class="issue_date">Issue Date: 2025-10-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3354" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding  LLM, Hanrong Ye+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- OmniVinciã¯ã€è¦–è¦šã¨éŸ³å£°ã‚’çµ±åˆã—ãŸã‚ªãƒ ãƒ‹ãƒ¢ãƒ¼ãƒ€ãƒ«LLMã‚’æ§‹ç¯‰ã™ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã‚ã‚Šã€3ã¤ã®é©æ–°ï¼ˆOmniAlignNetã€Temporal Embedding Groupingã€Constrained Rotary Time Embeddingï¼‰ã‚’ææ¡ˆã€‚2400ä¸‡ã®ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ã®ç›¸äº’å¼·åŒ–ã‚’å®Ÿç¾ã€‚DailyOmniã€MMARã€Video-MMEã§ã®æ€§èƒ½å‘ä¸Šã‚’é”æˆã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½¿ç”¨é‡ã‚’å¤§å¹…ã«å‰Šæ¸›ã€‚ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã‚„åŒ»ç™‚AIãªã©ã®å¿œç”¨ã«ãŠã‘ã‚‹ã‚ªãƒ ãƒ‹ãƒ¢ãƒ¼ãƒ€ãƒ«ã®åˆ©ç‚¹ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://nvlabs.github.io/OmniVinci/" target="_blank" rel="noopener noreferrer">https://nvlabs.github.io/OmniVinci/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_akhaliq/status/1980396179356844292?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>image, video, ãƒ†ã‚­ã‚¹ãƒˆ, éŸ³å£°ã‚’ç†è§£ã—ãƒ†ã‚­ã‚¹ãƒˆã‚’å‡ºåŠ›ï¼ˆTTSã‚‚å¯ï¼‰ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã«é–¢ã™ã‚‹æ–°ãŸãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ææ¡ˆã—ã¦ã„ã‚‹æ¨¡æ§˜</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<span class="issue_date">Issue Date: 2025-10-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3350" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] End-to-End Multi-Modal Diffusion Mamba, Chunhao Lu+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- MDMï¼ˆMulti-modal Diffusion Mambaï¼‰ã¨ã„ã†æ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ææ¡ˆã—ã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å‡¦ç†ã‚’çµ±ä¸€ã€‚Mambaãƒ™ãƒ¼ã‚¹ã®é¸æŠæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹æœ‰ã®æƒ…å ±ã‚’æ®µéšçš„ã«ç”Ÿæˆã€‚é«˜è§£åƒåº¦ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’åŒæ™‚ã«ç”Ÿæˆã—ã€æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã€‚è¨ˆç®—åŠ¹ç‡ã‚’ä¿ã¡ãªãŒã‚‰ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ±ä¸€ã™ã‚‹æ–°ãŸãªæ–¹å‘æ€§ã‚’ç¢ºç«‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1980161738084602036?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/2D%20(Image).html" target="_blank" rel="noopener noreferrer">#2D (Image)</a>
<span class="issue_date">Issue Date: 2025-10-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3336" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] VChain: Chain-of-Visual-Thought for Reasoning in Video Generation, Ziqi Huang+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- VChainã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®è¦–è¦šçš„æ¨è«–ã‚’å‹•ç”»ç”Ÿæˆã«æ´»ç”¨ã™ã‚‹æ–°ã—ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã€é‡è¦ãªã‚­ãƒ¼ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç”Ÿæˆã—ã€å‹•ç”»ç”Ÿæˆå™¨ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’åŠ¹ç‡çš„ã«ã‚¬ã‚¤ãƒ‰ã—ã¾ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚Šã€è¤‡é›‘ãªã‚·ãƒŠãƒªã‚ªã«ãŠã„ã¦ç”Ÿæˆå‹•ç”»ã®å“è³ªãŒå¤§å¹…ã«å‘ä¸Šã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://eyeline-labs.github.io/VChain/" target="_blank" rel="noopener noreferrer">https://eyeline-labs.github.io/VChain/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/realningyu/status/1980064375844331889?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Chain-of-Visual-Thoughts</p>
<p>keyframeã‚’chain-of-thoughtsã«å«ã‚ã‚‹ã“ã¨ã§ã€æ™‚é–“ç™ºå±•ã‚’ã‚ˆã‚Šæ­£ç¢ºã«ã—ã‚ˆã†ã¨ã„ã†è©¦ã¿ã«è¦‹ãˆã‚‹ã€‚è¿½åŠ ã®å­¦ç¿’ãªã—ã§å®Ÿæ–½ã§ãã‚‹ã¨ã®ã“ã¨ã€‚<br>&lt;img width="943" height="635" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/a7283398-2a61-45be-b7a4-eb7452656e06"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/a7283398-2a61-45be-b7a4-eb7452656e06"&lt;/a&gt;


/&gt;</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/InstructionTuning.html" target="_blank" rel="noopener noreferrer">#InstructionTuning</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<a class="button" href="articles/ImageSynthesis.html" target="_blank" rel="noopener noreferrer">#ImageSynthesis</a>
<span class="issue_date">Issue Date: 2025-10-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3335" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] MetaMorph: Multimodal Understanding and Generation via Instruction   Tuning, Shengbang Tong+, ICCV'25, 2024.12</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€è¦–è¦šçš„æŒ‡ç¤ºèª¿æ•´ã®æ–°æ‰‹æ³•VPiTã‚’ææ¡ˆã—ã€LLMãŒãƒ†ã‚­ã‚¹ãƒˆã¨è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚VPiTã¯ã€ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚ŒãŸç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹èƒ½åŠ›ã‚’LLMã«æ•™ãˆã€è¦–è¦šç”Ÿæˆèƒ½åŠ›ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã—ãŸã€‚ç‰¹ã«ã€ç†è§£ãƒ‡ãƒ¼ã‚¿ãŒç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šã‚‚åŠ¹æœçš„ã«ä¸¡æ–¹ã®èƒ½åŠ›ã«å¯„ä¸ã™ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã¾ã—ãŸã€‚MetaMorphãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€è¦–è¦šç†è§£ã¨ç”Ÿæˆã§ç«¶äº‰åŠ›ã®ã‚ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã€LLMã®äº‹å‰å­¦ç¿’ã‹ã‚‰å¾—ãŸçŸ¥è­˜ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€è¦–è¦šç”Ÿæˆã«ãŠã‘ã‚‹ä¸€èˆ¬çš„ãªå¤±æ•—ã‚’å…‹æœã—ã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€LLMãŒè¦–è¦šç†è§£ã¨ç”Ÿæˆã«é©å¿œã§ãã‚‹å¯èƒ½æ€§ãŒç¤ºå”†ã•ã‚Œã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/davidjfan/status/1979994285379641487?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Self-SupervisedLearning.html" target="_blank" rel="noopener noreferrer">#Self-SupervisedLearning</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<a class="button" href="articles/Scalability.html" target="_blank" rel="noopener noreferrer">#Scalability</a>
<span class="issue_date">Issue Date: 2025-10-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3334" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Scaling Language-Free Visual Representation Learning, David Fan+, ICCV'25, 2025.04</a>
<span class="snippet"><span>GPT Summary</span>- è¦–è¦šçš„è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼ˆSSLï¼‰ã¯ã€CLIPã«æ¯”ã¹ã¦è¦–è¦šçš„è³ªå•å¿œç­”ï¼ˆVQAï¼‰ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒåŠ£ã‚‹ãŒã€åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¨“ç·´ã™ã‚‹ã“ã¨ã§ã€è¦–è¦šçš„SSLãƒ¢ãƒ‡ãƒ«ãŒCLIPãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚ã‚¹ã‚±ãƒ¼ãƒ«ãŒè‰¯ã„ã“ã¨ã‚’ç¤ºã—ãŸã€‚è¦–è¦šçš„SSLã¯ã€VQAã‚„å¾“æ¥ã®è¦–è¦šãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§CLIPãƒ¬ãƒ™ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¦–è¦šä¸­å¿ƒã®è¡¨ç¾å­¦ç¿’ã«æ–°ãŸãªæ©Ÿä¼šãŒé–‹ã‹ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://davidfan.io/webssl/" target="_blank" rel="noopener noreferrer">https://davidfan.io/webssl/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/davidjfan/status/1979994285379641487?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Self-SupervisedLearning.html" target="_blank" rel="noopener noreferrer">#Self-SupervisedLearning</a>
<span class="issue_date">Issue Date: 2025-10-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3331" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Advancing End-to-End Pixel Space Generative Modeling via Self-supervised  Pre-training, Jiachen Lei+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æ–°ã—ã„äºŒæ®µéšãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã¨åŠ¹ç‡ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’åŸ‹ã‚ã‚‹ã€‚ç¬¬ä¸€æ®µéšã§æ„å‘³ã®ã‚ã‚‹ã‚»ãƒãƒ³ãƒ†ã‚£ã‚¯ã‚¹ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£ã—ã€ç¬¬äºŒæ®µéšã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¨ãƒ‡ã‚³ãƒ¼ãƒ€ã‚’çµ±åˆã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€‚ImageNetãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ã€ç‰¹ã«æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¯å¾“æ¥æ‰‹æ³•ã‚’å¤§ããä¸Šå›ã‚Šã€ä¸€è²«æ€§ãƒ¢ãƒ‡ãƒ«ã¯é«˜è§£åƒåº¦ç”»åƒã§ã®ç›´æ¥ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«æˆåŠŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1979911820401086613?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/InstructionTuning.html" target="_blank" rel="noopener noreferrer">#InstructionTuning</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<a class="button" href="articles/SpatialUnderstanding.html" target="_blank" rel="noopener noreferrer">#SpatialUnderstanding</a>
<span class="issue_date">Issue Date: 2025-10-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3330" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Thinking with Camera: A Unified Multimodal Model for Camera-Centric  Understanding and Generation, Kang Liao+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- ã‚«ãƒ¡ãƒ©ä¸­å¿ƒã®ç†è§£ã¨ç”Ÿæˆã‚’çµ±åˆã—ãŸãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã€ŒPuffinã€ã‚’ææ¡ˆã€‚Puffinã¯ã€è¨€èªå›å¸°ã¨æ‹¡æ•£ç”Ÿæˆã‚’çµ„ã¿åˆã‚ã›ã€ã‚«ãƒ¡ãƒ©ã‚’è¨€èªã¨ã—ã¦æ‰±ã†æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¡ç”¨ã€‚400ä¸‡ã®è¦–è¦š-è¨€èª-ã‚«ãƒ¡ãƒ©ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ŒPuffin-4Mã€ã§è¨“ç·´ã•ã‚Œã€ç©ºé–“çš„ãªè¦–è¦šçš„æ‰‹ãŒã‹ã‚Šã‚’è€ƒæ…®ã—ãŸæ¨è«–ã‚’å®Ÿç¾ã€‚å®Ÿé¨“çµæœã§ã¯ã€å°‚é–€ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚Šå¤šæ§˜ãªã‚¿ã‚¹ã‚¯ã«å¯¾å¿œå¯èƒ½ã€‚ç ”ç©¶æˆæœã¯ã‚³ãƒ¼ãƒ‰ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨å…±ã«å…¬é–‹äºˆå®šã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1979911820401086613?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>pj page: 


<a href="https://kangliao929.github.io/projects/puffin/" target="_blank" rel="noopener noreferrer">https://kangliao929.github.io/projects/puffin/</a>


</p></span><br><br>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/WorldModels.html" target="_blank" rel="noopener noreferrer">#WorldModels</a>
<span class="issue_date">Issue Date: 2025-10-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3321" target="_blank" rel="noopener noreferrer" class="title-link">VAGEN Reinforcing World Model Reasoning for Multi-Turn VLM Agents, Wang+, NeurIPS'25</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wzihanw/status/1979240555646214199?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Controllable.html" target="_blank" rel="noopener noreferrer">#Controllable</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/ComputerUse.html" target="_blank" rel="noopener noreferrer">#ComputerUse</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/4D%20(Video).html" target="_blank" rel="noopener noreferrer">#4D (Video)</a>
<span class="issue_date">Issue Date: 2025-10-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3318" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Ctrl-VI: Controllable Video Synthesis via Variational Inference, Haoyi Duan+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ“ãƒ‡ã‚ªç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®åˆ¶ç´„ã‚’å…‹æœã™ã‚‹ãŸã‚ã«ã€Ctrl-VIã¨ã„ã†æ–°ã—ã„ãƒ“ãƒ‡ã‚ªåˆæˆæ‰‹æ³•ã‚’ææ¡ˆã€‚æŒ‡å®šè¦ç´ ã«å¯¾ã—ã¦é«˜ã„åˆ¶å¾¡æ€§ã‚’æŒã¡ã€éæŒ‡å®šè¦ç´ ã«ã¯å¤šæ§˜æ€§ã‚’ç¶­æŒã€‚å¤‰åˆ†æ¨è«–ã‚’ç”¨ã„ã¦è¤‡æ•°ã®ãƒ“ãƒ‡ã‚ªç”Ÿæˆãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã§åˆæˆåˆ†å¸ƒã‚’è¿‘ä¼¼ã—ã€KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã®æœ€å°åŒ–ã‚’æ®µéšçš„ã«è¡Œã†ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€åˆ¶å¾¡æ€§ã€å¤šæ§˜æ€§ã€3Dã®ä¸€è²«æ€§ãŒå‘ä¸Šã—ãŸã“ã¨ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/du_yilun/status/1979001983701770272?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<a class="button" href="articles/Scalability.html" target="_blank" rel="noopener noreferrer">#Scalability</a>
<span class="issue_date">Issue Date: 2025-10-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3316" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] From Pixels to Words -- Towards Native Vision-Language Primitives at  Scale, Haiwen Diao+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- ãƒã‚¤ãƒ†ã‚£ãƒ–ãªãƒ“ã‚¸ãƒ§ãƒ³ãƒ»ãƒ©ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰ã®èª²é¡Œã‚’æ˜ç¢ºã«ã—ã€åŠ¹æœçš„ãªæ§‹ç¯‰æŒ‡é‡ã‚’ç¤ºã™ã€‚å…·ä½“çš„ã«ã¯ã€ãƒ”ã‚¯ã‚»ãƒ«ã¨å˜èªã®æ•´åˆã€ãƒ“ã‚¸ãƒ§ãƒ³ã¨ãƒ©ãƒ³ã‚²ãƒ¼ã‚¸ã®çµ±åˆã€ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«ç‰¹æ€§ã®å…·ç¾åŒ–ã‚’é‡è¦–ã€‚æ–°ãŸã«é–‹ç™ºã—ãŸNEOã¯ã€390Mã®ç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆä¾‹ã§è¦–è¦šçš„çŸ¥è¦šã‚’åŠ¹ç‡çš„ã«ç™ºå±•ã•ã›ã€ã‚³ã‚¹ãƒˆåŠ¹ç‡ã®é«˜ã„ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã‚’æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/liuziwei7/status/1979548018044031370?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/Robotics.html" target="_blank" rel="noopener noreferrer">#Robotics</a>
<a class="button" href="articles/VisionLanguageActionModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageActionModel</a>
<span class="issue_date">Issue Date: 2025-10-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3312" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning  and Online Reinforcement Learning, Hanyang Chen+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- Embodied Reasoning Agent (ERA)ã¯ã€äº‹å‰çŸ¥è­˜å­¦ç¿’ã¨ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ã‚’çµ±åˆã—ãŸäºŒæ®µéšã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã€è¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½å‘ä¸Šã‚’ç›®æŒ‡ã™ã€‚ç¬¬ä¸€æ®µéšã§ã¯ã€è»Œé“æ‹¡å¼µã€ç’°å¢ƒå›ºå®šã€å¤–éƒ¨çŸ¥è­˜ã‹ã‚‰åŸºç¤çŸ¥è­˜ã‚’æŠ½å‡ºã—ã€ç¬¬äºŒæ®µéšã§ã‚ªãƒ³ãƒ©ã‚¤ãƒ³RLã‚’ç”¨ã„ã¦ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚è‡ªå·±è¦ç´„ã€å¯†ãªå ±é…¬å½¢æˆã€ã‚¿ãƒ¼ãƒ³ãƒ¬ãƒ™ãƒ«ã®ãƒãƒªã‚·ãƒ¼æœ€é©åŒ–ã‚’å°å…¥ã—ã€EB-ALFREDã¨EB-Manipulationã‚¿ã‚¹ã‚¯ã§å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹æˆæœã‚’ç¤ºã—ãŸã€‚ERAã¯å…·ç¾åŒ–çŸ¥èƒ½ã®å®Ÿç”¨çš„ãªé“ã‚’æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://embodied-reasoning-agent.github.io" target="_blank" rel="noopener noreferrer">https://embodied-reasoning-agent.github.io</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/ainativef/status/1978627172152807730?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/2D%20(Image).html" target="_blank" rel="noopener noreferrer">#2D (Image)</a>
<a class="button" href="articles/Editing.html" target="_blank" rel="noopener noreferrer">#Editing</a>
<a class="button" href="articles/ImageSynthesis.html" target="_blank" rel="noopener noreferrer">#ImageSynthesis</a>
<span class="issue_date">Issue Date: 2025-10-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3310" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Learning an Image Editing Model without Image Editing Pairs, Nupur Kumari+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒšã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã›ãšã«ç”»åƒç·¨é›†ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’ææ¡ˆã€‚æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’å±•é–‹ã—ã€è¦–è¦š-è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æ´»ç”¨ã—ã¦ç›´æ¥æœ€é©åŒ–ã‚’è¡Œã†ã€‚ç”Ÿæˆç”»åƒã®è¦–è¦šçš„å¿ å®Ÿæ€§ã‚’ä¿ã¤ãŸã‚ã«åˆ†å¸ƒãƒãƒƒãƒãƒ³ã‚°æå¤±ï¼ˆDMDï¼‰ã‚’å°å…¥ã€‚æ¨™æº–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§è©•ä¾¡ã—ãŸçµæœã€å¾“æ¥ã®æ•™å¸«ã‚ã‚Šãƒšã‚¢ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã®æ€§èƒ½ã‚’é”æˆã—ã€RLãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã‚’ã‚‚ä¸Šå›ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1979095863881257441?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<span class="issue_date">Issue Date: 2025-10-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3304" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Frequency-Dynamic Attention Modulation for Dense Prediction, Linwei Chen+, ICCV'25, 2025.07</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€Vision Transformersï¼ˆViTsï¼‰ã®å‘¨æ³¢æ•°å¿œç­”ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«ã€Frequency-Dynamic Attention Modulationï¼ˆFDAMï¼‰ã‚’ææ¡ˆã€‚FDAMã¯ã€æ³¨æ„è¡Œåˆ—ã®ãƒ­ãƒ¼ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ã‚’åè»¢ã•ã›ã‚‹Attention Inversionï¼ˆAttInvï¼‰ã¨ã€ç•°ãªã‚‹å‘¨æ³¢æ•°æˆåˆ†ã«é‡ã¿ä»˜ã‘ã‚’è¡Œã†Frequency Dynamic Scalingï¼ˆFreqScaleï¼‰ã‹ã‚‰æˆã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¡¨ç¾ã®å´©å£Šã‚’å›é¿ã—ã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚„ç‰©ä½“æ¤œå‡ºãªã©ã®ã‚¿ã‚¹ã‚¯ã§ä¸€è²«ã—ãŸæ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ã€‚ãƒªãƒ¢ãƒ¼ãƒˆã‚»ãƒ³ã‚·ãƒ³ã‚°æ¤œå‡ºã§ã‚‚æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1979109880830267606?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/AttentionSinks.html" target="_blank" rel="noopener noreferrer">#AttentionSinks</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/interactive.html" target="_blank" rel="noopener noreferrer">#interactive</a>
<span class="issue_date">Issue Date: 2025-10-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3291" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] LongLive: Real-time Interactive Long Video Generation, Shuai Yang+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- LongLiveã¯ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªé•·ç·¨å‹•ç”»ç”Ÿæˆã®ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¬ãƒ™ãƒ«ã®è‡ªå·±å›å¸°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã€‚å› æœçš„æ³¨æ„ARãƒ¢ãƒ‡ãƒ«ã‚’æ¡ç”¨ã—ã€KVå†ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’çµ±åˆã™ã‚‹ã“ã¨ã§ã€è¦–è¦šçš„ä¸€è²«æ€§ã¨æ„å‘³çš„æ•´åˆæ€§ã‚’ä¿ã¡ãªãŒã‚‰åŠ¹ç‡çš„ãªç”Ÿæˆã‚’å®Ÿç¾ã€‚1.3Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’32 GPUæ—¥ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€å˜ä¸€ã®NVIDIA H100ã§20.7 FPSã‚’ç¶­æŒã€‚æœ€å¤§240ç§’ã®å‹•ç”»ç”Ÿæˆã‚’ã‚µãƒãƒ¼ãƒˆã—ã€INT8é‡å­åŒ–æ¨è«–ã‚‚å¯¾å¿œã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/yukangchen_/status/1978653384539341287?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2870" target="_blank" rel="noopener noreferrer">[Paper Note] Self Forcing: Bridging the Train-Test Gap in Autoregressive Video   Diffusion, Xun Huang+, NeurIPS'25</a>
</p>
<p>pj page: 


<a href="https://nvlabs.github.io/LongLive/" target="_blank" rel="noopener noreferrer">https://nvlabs.github.io/LongLive/</a>


</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/AttentionSinks.html" target="_blank" rel="noopener noreferrer">#AttentionSinks</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-10-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3270" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] StreamingVLM: Real-Time Understanding for Infinite Video Streams, Ruyi Xu+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- StreamingVLMã¯ã€ç„¡é™ã®ãƒ“ãƒ‡ã‚ªã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ç†è§£ã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã‚’çµ±ä¸€ã—ãŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æ¡ç”¨ã€‚ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚·ãƒ³ã‚¯ã®çŠ¶æ…‹ã‚’å†åˆ©ç”¨ã—ã€çŸ­ã„ãƒ“ã‚¸ãƒ§ãƒ³ãƒˆãƒ¼ã‚¯ãƒ³ã¨é•·ã„ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ä¿æŒã™ã‚‹ã“ã¨ã§ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’æŠ‘ãˆã¤ã¤é«˜ã„æ€§èƒ½ã‚’å®Ÿç¾ã€‚æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯Inf-Streams-Evalã§66.18%ã®å‹ç‡ã‚’é”æˆã—ã€ä¸€èˆ¬çš„ãªVQAèƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã«æˆåŠŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/teortaxestex/status/1978324546370343088?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã“ã‚Œã¯èˆˆå‘³æ·±ã„</p>
<p>ä¿æŒã™ã‚‹KV Cacheã®ä¸Šé™ã‚’æ±ºã‚ã€Sink Token[^1]ã¯ä¿æŒã—[^2]ï¼ˆ512ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã€textual tokenã¯é•·è·é›¢ã§ä¿æŒã€visual tokenã¯çŸ­è·é›¢ã§ä¿æŒã€ã¾ãŸpositional encodingã¨ã—ã¦ã¯RoPEã‚’æ¡ç”¨ã™ã‚‹ãŒã€å›ºå®šã•ã‚ŒãŸãƒ¬ãƒ³ã‚¸ã®ä¸­ã§å‹•çš„ã«indexã‚’æ›´æ–°ã™ã‚‹ã“ã¨ã§ã€ä½ç›¸ã‚’å­¦ç¿’æ™‚ã®rangeã«åã‚OODã«ãªã‚‰ãªã„ã‚ˆã†ãªå·¥å¤«ã‚’ã™ã‚‹ã“ã¨ã§ã€memoryã¨è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’ä¸€å®šã«ä¿ã¡ãªãŒã‚‰long contextã§ã®ä¸€è²«æ€§ã¨ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®latencyã‚’å®Ÿç¾ã™ã‚‹ã€ã¨ã„ã£ãŸè©±ã«ã¿ãˆã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/4d063c90-e10a-4d07-9095-f87ee85c33fb" alt="image" loading="lazy"><br><br>å­¦ç¿’æ™‚ã¯ãƒ•ãƒ¬ãƒ¼ãƒ ãŒoverlapã—ãŸè¤‡æ•°ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†ã‘ã¦ã€ãã‚Œãã‚Œã‚’full attentionã§å­¦ç¿’ã™ã‚‹ï¼ˆSink Tokenã¯ä¿æŒã™ã‚‹ï¼‰ã€‚ã“ã‚Œã¯ä¸Šè¿°ã®inferenceæ™‚ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨æ•´åˆã—ã¦ãŠã‚Šå­¦ç¿’æ™‚ã¨inferenceæ™‚ã®gapãŒæœ€å°é™ã«ãªã‚‹ã€‚ã¾ãŸã€ã‚ã–ã‚ã–long videoã§å­¦ç¿’ã™ã‚‹å¿…è¦ãŒãªã„ã€‚ï¼ˆç¾ã—ã„è§£æ±ºæ–¹æ³•ï¼‰<br><img src="https://github.com/user-attachments/assets/98b50d1b-b9c4-427a-93f5-d385b2bc35a1" alt="image" loading="lazy"><br><br>[^1]: decoder-only transformerã®ä½™å‰°ãªattention scoreã®æ¨ã¦å ´ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹sequenceå†’é ­ã®æ•°ãƒˆãƒ¼ã‚¯ãƒ³(3--4ãƒˆãƒ¼ã‚¯ãƒ³ç¨‹åº¦ï¼‰ã®ã“ã¨ã€‚æœ¬è«–æ–‡ã§ã¯512ãƒˆãƒ¼ã‚¯ãƒ³ã¨å¤§ãã‚ã®Sink Tokenã‚’ä¿æŒã—ã¦ã„ã‚‹ã€‚<br>[^2]: Attention Sinksã«ã‚ˆã£ã¦ã€long contextã®æ€§èƒ½ãŒæ”¹å–„ã•ã‚Œ <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1860" target="_blank" rel="noopener noreferrer">Why do LLMs attend to the first token?, Federico Barbero+, COLM'25</a>
 decoder-only transformerã®å±¤ãŒæ·±ã„éƒ¨åˆ†ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®è¡¨ç¾ãŒå‡ä¸€åŒ–ã•ã‚Œã¦ã—ã¾ã†over-mixingã‚’æŠ‘åˆ¶ã™ã‚‹ <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1861" target="_blank" rel="noopener noreferrer">Efficient Streaming Language Models with Attention Sinks, Guangxuan Xiao+, ICLR'24</a>
 ã“ã¨ãŒå ±å‘Šã•ã‚Œã¦ã„ã‚‹</p>
<p>AttentionSinké–¢é€£ãƒªãƒ³ã‚¯:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1861" target="_blank" rel="noopener noreferrer">Efficient Streaming Language Models with Attention Sinks, Guangxuan Xiao+, ICLR'24</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1860" target="_blank" rel="noopener noreferrer">Why do LLMs attend to the first token?, Federico Barbero+, COLM'25</a>
</p>
<p>â†‘ã“ã‚Œã¯å…ƒãƒã‚¹ãƒˆã‚’èª­ã‚“ã§ï¼ˆã¨è«–æ–‡æ–œã‚èª­ã¿ï¼‰ã®æ„Ÿæƒ³ã®ã‚ˆã†ãªã‚‚ã®ãªã®ã§ã€è©³ç´°ã¯å¾Œã§å…ƒè«–æ–‡ã‚’èª­ã‚€ã€‚</p>
<p>é–¢é€£:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/yukangchen_/status/1978653384539341287?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/DataMixture.html" target="_blank" rel="noopener noreferrer">#DataMixture</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-10-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3267" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Learning to See Before Seeing: Demystifying LLM Visual Priors from  Language Pre-training, Junlin Han+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã§è¨“ç·´ã•ã‚ŒãªãŒã‚‰ã‚‚è¦–è¦šçš„å…ˆå…¥è¦³ã‚’ç™ºå±•ã•ã›ã€å°‘é‡ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ãƒ¼ã‚¿ã§è¦–è¦šã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œå¯èƒ½ã«ã™ã‚‹ã€‚è¦–è¦šçš„å…ˆå…¥è¦³ã¯ã€è¨€èªã®äº‹å‰è¨“ç·´ä¸­ã«ç²å¾—ã•ã‚ŒãŸçŸ¥è­˜ã§ã‚ã‚Šã€æ¨è«–ä¸­å¿ƒã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç™ºå±•ã™ã‚‹ã€‚çŸ¥è¦šã®å…ˆå…¥è¦³ã¯åºƒç¯„ãªã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰å¾—ã‚‰ã‚Œã€è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã«æ•æ„Ÿã§ã‚ã‚‹ã€‚è¦–è¦šã‚’æ„è­˜ã—ãŸLLMã®äº‹å‰è¨“ç·´ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ä¸­å¿ƒã®ãƒ¬ã‚·ãƒ”ã‚’ææ¡ˆã—ã€500,000 GPUæ™‚é–“ã‚’ã‹ã‘ãŸå®Ÿé¨“ã«åŸºã¥ãå®Œå…¨ãªMLLMæ§‹ç¯‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ç¤ºã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¦–è¦šçš„å…ˆå…¥è¦³ã‚’è‚²æˆã™ã‚‹æ–°ã—ã„æ–¹æ³•ã‚’æä¾›ã—ã€æ¬¡ä¸–ä»£ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMã®ç™ºå±•ã«å¯„ä¸ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1977982648531476607?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>MLE Bench (Multi-Level Existence Bench)</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-10-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3258" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Diffusion Transformers with Representation Autoencoders, Boyang Zheng+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€å¾“æ¥ã®VAEã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’äº‹å‰å­¦ç¿’ã•ã‚ŒãŸè¡¨ç¾ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã«ç½®ãæ›ãˆãŸRepresentation Autoencodersï¼ˆRAEï¼‰ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é«˜å“è³ªãªå†æ§‹æˆã¨è±Šã‹ãªæ½œåœ¨ç©ºé–“ã‚’å®Ÿç¾ã—ã€æ‹¡æ•£ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®æ€§èƒ½å‘ä¸Šã‚’å›³ã‚‹ã€‚RAEã¯ã€è£œåŠ©çš„ãªè¡¨ç¾æ•´åˆæå¤±ãªã—ã§æ—©ã„åæŸã‚’é”æˆã—ã€ImageNetã§å„ªã‚ŒãŸç”»åƒç”Ÿæˆçµæœã‚’ç¤ºã—ãŸã€‚RAEã¯ã€æ‹¡æ•£ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®æ–°ã—ã„ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã—ã¦ã®åˆ©ç‚¹ã‚’æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://rae-dit.github.io" target="_blank" rel="noopener noreferrer">https://rae-dit.github.io</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1978001535717216751?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>U-Netã‚’Backboneã¨ã—ãŸVAEã®ä»£ã‚ã‚Šã«ViTã«åŸºã¥ãï¼ˆdown, up- scalingç„¡ã—ã®ï¼‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šå°‘ãªã„è¨ˆç®—é‡ã§é«˜ã„æ€§èƒ½ã‚’é”æˆã—ã¾ã—ãŸã€ã¨ã„ã£ãŸè©±ã«è¦‹ãˆã‚‹ã€‚</p>
<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1977967098736549990?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1978018195953848384?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Prompting.html" target="_blank" rel="noopener noreferrer">#Prompting</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/AutomaticPromptEngineering.html" target="_blank" rel="noopener noreferrer">#AutomaticPromptEngineering</a>
<span class="issue_date">Issue Date: 2025-10-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3250" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for  MLLMs, Yumin Choi+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ï¼ˆMPOï¼‰ã‚’ææ¡ˆã—ã€ãƒ†ã‚­ã‚¹ãƒˆã¨éãƒ†ã‚­ã‚¹ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…±åŒæœ€é©åŒ–ã™ã‚‹æ–°ãŸãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç¤ºã™ã€‚MPOã¯ã€ãƒ™ã‚¤ã‚ºã«åŸºã¥ãé¸æŠæˆ¦ç•¥ã‚’ç”¨ã„ã¦å€™è£œãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é¸å®šã—ã€ç”»åƒã‚„å‹•ç”»ãªã©å¤šæ§˜ãªãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«ãŠã„ã¦ãƒ†ã‚­ã‚¹ãƒˆå°‚ç”¨æ‰‹æ³•ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç™ºæ®ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€MLLMsã®æ½œåœ¨èƒ½åŠ›ã‚’æœ€å¤§é™ã«å¼•ãå‡ºã™é‡è¦ãªã‚¹ãƒ†ãƒƒãƒ—ã‚’ç¢ºç«‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dongkikim95/status/1977751622613684654?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/SelfImprovement.html" target="_blank" rel="noopener noreferrer">#SelfImprovement</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/Label-free.html" target="_blank" rel="noopener noreferrer">#Label-free</a>
<span class="issue_date">Issue Date: 2025-10-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3241" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified  Self-Play, Qinsi Wang+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- Vision-Zeroã¯ã€è¦–è¦šã¨è¨€èªã®ãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰ã®è‡ªå·±æ”¹å–„ã‚’ä¿ƒé€²ã™ã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³éä¾å­˜ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚Šã€ä»»æ„ã®ç”»åƒãƒšã‚¢ã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸç«¶äº‰çš„ãªè¦–è¦šã‚²ãƒ¼ãƒ ã‚’é€šã˜ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†ã€‚ä¸»ãªç‰¹å¾´ã¯ã€æˆ¦ç•¥çš„è‡ªå·±å¯¾æˆ¦ã«ã‚ˆã‚‹è‡ªå¾‹çš„ãªãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã€ä»»æ„ã®ç”»åƒã‹ã‚‰ã®ã‚²ãƒ¼ãƒ ç”Ÿæˆã«ã‚ˆã‚‹å¤šæ§˜ãªãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã®æ¨è«–èƒ½åŠ›å‘ä¸Šã€ãã—ã¦åå¾©è‡ªå·±å¯¾æˆ¦ãƒãƒªã‚·ãƒ¼æœ€é©åŒ–ï¼ˆIterative-SPOï¼‰ã«ã‚ˆã‚‹æŒç¶šçš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã§ã‚ã‚‹ã€‚Vision-Zeroã¯ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã€ä»–ã®æ³¨é‡ˆãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã‚’ä¸Šå›ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://github.com/wangqinsi1/Vision-Zero" target="_blank" rel="noopener noreferrer">https://github.com/wangqinsi1/Vision-Zero</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1977623963603005554?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã¨ã¦ã‚‚è‰¯ã•ãã†</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<a class="button" href="articles/FlowMatching.html" target="_blank" rel="noopener noreferrer">#FlowMatching</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3203" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Flow-GRPO: Training Flow Matching Models via Online RL, Jie Liu+, NeurIPS'25, 2025.05</a>
<span class="snippet"><span>GPT Summary</span>- Flow-GRPOã¯ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ã‚’ãƒ•ãƒ­ãƒ¼ãƒãƒƒãƒãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã«çµ±åˆã—ãŸæ–°ã—ã„æ‰‹æ³•ã§ã€ODEã‚’SDEã«å¤‰æ›ã™ã‚‹ã“ã¨ã§RLæ¢ç´¢ã®ãŸã‚ã®çµ±è¨ˆçš„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’å®Ÿç¾ã—ã€ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—ã‚’å‰Šæ¸›ã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚å®Ÿé¨“çµæœã§ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®ã‚¿ã‚¹ã‚¯ã§æ€§èƒ½ãŒå¤§å¹…ã«å‘ä¸Šã—ã€GenEvalã®ç²¾åº¦ãŒ63%ã‹ã‚‰95%ã«ã€è¦–è¦šçš„ãƒ†ã‚­ã‚¹ãƒˆãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã®ç²¾åº¦ãŒ59%ã‹ã‚‰92%ã«æ”¹å–„ã•ã‚ŒãŸã€‚ã¾ãŸã€å ±é…¬ãƒãƒƒã‚­ãƒ³ã‚°ãŒã»ã¨ã‚“ã©ç™ºç”Ÿã›ãšã€ç”»åƒã®è³ªã‚„å¤šæ§˜æ€§ã‚’æãªã†ã“ã¨ãªãå ±é…¬ãŒå¢—åŠ ã—ãŸã€‚</span>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/FlowMatching.html" target="_blank" rel="noopener noreferrer">#FlowMatching</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3201" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] DiffusionNFT: Online Diffusion Reinforcement with Forward Process, Kaiwen Zheng+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- Diffusion Negative-aware FineTuningï¼ˆDiffusionNFTï¼‰ã¯ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ã‚’ç”¨ã„ã¦æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’æœ€é©åŒ–ã™ã‚‹æ–°ã—ã„æ‰‹æ³•ã§ã€ãƒã‚¸ãƒ†ã‚£ãƒ–ã¨ãƒã‚¬ãƒ†ã‚£ãƒ–ãªç”Ÿæˆã‚’å¯¾æ¯”ã•ã›ã‚‹ã“ã¨ã§å¼·åŒ–ä¿¡å·ã‚’çµ„ã¿è¾¼ã¿ã¾ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚Šã€å°¤åº¦æ¨å®šãŒä¸è¦ã«ãªã‚Šã€ã‚¯ãƒªãƒ¼ãƒ³ãªç”»åƒã®ã¿ã§ãƒãƒªã‚·ãƒ¼æœ€é©åŒ–ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚DiffusionNFTã¯ã€FlowGRPOã‚ˆã‚Šã‚‚æœ€å¤§25å€åŠ¹ç‡çš„ã§ã€GenEvalã‚¹ã‚³ã‚¢ã‚’çŸ­æœŸé–“ã§å¤§å¹…ã«æ”¹å–„ã—ã€è¤‡æ•°ã®å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§SD3.5-Mediumã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1976497448022610334?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3202" target="_blank" rel="noopener noreferrer">Introducing Stable Diffusion 3.5, StabilityAI, 2024.10</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3203" target="_blank" rel="noopener noreferrer">[Paper Note] Flow-GRPO: Training Flow Matching Models via Online RL, Jie Liu+, NeurIPS'25, 2025.05</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3204" target="_blank" rel="noopener noreferrer">[Paper Note] Classifier-Free Diffusion Guidance, Jonathan Ho+, arXiv'22, 2022.07</a>
</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Decoder.html" target="_blank" rel="noopener noreferrer">#Decoder</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3192" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Heptapod: Language Modeling on Visual Signals, Yongxin Zhu+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- Heptapodã¯ã€å› æœæ³¨æ„ã‚’ç”¨ã„ãŸç”»åƒè‡ªå‹•å›å¸°ãƒ¢ãƒ‡ãƒ«ã§ã€CFGã¸ã®ä¾å­˜ã‚’æ’é™¤ã—ã€æ„å‘³ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’é¿ã‘ã‚‹ã€‚ä¸»ãªé©æ–°ã¯ã€2Dåˆ†å¸ƒäºˆæ¸¬ã‚’è¡Œã†å› æœTransformerã§ã€ç”»åƒã®2Dç©ºé–“å…¨ä½“ã«ã‚ãŸã‚‹åˆ†å¸ƒã‚’å­¦ç¿’ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç”Ÿæˆçš„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é€šã˜ã¦ç”»åƒã®æ„å‘³ã‚’æ‰ãˆã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚‹ã€‚ImageNetç”Ÿæˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§FIDå€¤2.70ã‚’é”æˆã—ã€å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä¸Šå›ã‚‹æˆæœã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1976233895092945107?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-10-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3184" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density, Randall Balestriero+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- JEPAã¯ã€æ½œåœ¨ç©ºé–“äºˆæ¸¬ã¨ååæŸã‚’çµ„ã¿åˆã‚ã›ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã€ãƒ‡ãƒ¼ã‚¿å¯†åº¦ã‚’æ¨å®šã™ã‚‹èƒ½åŠ›ã‚’æŒã¤ã€‚æˆåŠŸè£ã«è¨“ç·´ã•ã‚ŒãŸJEPAã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚„å¤–ã‚Œå€¤æ¤œå‡ºã«åˆ©ç”¨å¯èƒ½ã§ã€ã‚µãƒ³ãƒ—ãƒ«ã®ç¢ºç‡ã‚’åŠ¹ç‡çš„ã«è¨ˆç®—ã§ãã‚‹ã€‚JEPA-SCOREã¨å‘¼ã°ã‚Œã‚‹æ‰‹æ³•ã‚’ç”¨ã„ã¦ã€ã•ã¾ã–ã¾ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚„è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’æ‰‹æ³•ã§ãã®åŠ¹æœãŒå®Ÿè¨¼ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/randall_balestr/status/1975913453211791836?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1975838782231617950?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Tokenizer.html" target="_blank" rel="noopener noreferrer">#Tokenizer</a>
<a class="button" href="articles/Decoder.html" target="_blank" rel="noopener noreferrer">#Decoder</a>
<span class="issue_date">Issue Date: 2025-10-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3177" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization, ThÃ©ophane Vallaeys+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æ–°ã—ã„ãƒ”ã‚¯ã‚»ãƒ«æ‹¡æ•£ãƒ‡ã‚³ãƒ¼ãƒ€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆSSDDï¼‰ã‚’ææ¡ˆã—ã€KL-VAEã«ä¾å­˜ã›ãšã«é«˜å“è³ªãªç”»åƒå†æ§‹æˆã‚’å®Ÿç¾ã€‚SSDDã¯æ•µå¯¾çš„æå¤±ãªã—ã§è¨“ç·´ã•ã‚Œã€å†æ§‹æˆFIDã‚’æ”¹å–„ã—ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é€Ÿåº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€KL-VAEã®ä»£æ›¿ã¨ã—ã¦è¿…é€Ÿã‹ã¤é«˜å“è³ªãªç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ãŒå¯èƒ½ã¨ãªã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1975484440475505039?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/COLM.html" target="_blank" rel="noopener noreferrer">#COLM</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/Geometric.html" target="_blank" rel="noopener noreferrer">#Geometric</a>
<span class="issue_date">Issue Date: 2025-10-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3133" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] VisOnlyQA: Large Vision Language Models Still Struggle with Visual   Perception of Geometric Information, Ryo Kamoi+, COLM'25, 2024.12</a>
<span class="snippet"><span>GPT Summary</span>- LVLMsã®å¹¾ä½•å­¦çš„èªè­˜ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ŒVisOnlyQAã€ã‚’å°å…¥ã—ã€LVLMsãŒç”»åƒå†…ã®å¹¾ä½•å­¦çš„æƒ…å ±ã‚’æ­£ç¢ºã«èªè­˜ã§ããªã„ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã—ãŸã€‚23ã®LVLMsï¼ˆGPT-4oã‚„Gemini 2.5 Proã‚’å«ã‚€ï¼‰ã¯ã€VisOnlyQAã§ã®æ€§èƒ½ãŒä½ãã€è¿½åŠ ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã§ã¯æ”¹å–„ã•ã‚Œãªã„ã€‚ã‚ˆã‚Šå¼·åŠ›ãªLLMã‚’ä½¿ç”¨ã™ã‚‹LVLMsã¯å¹¾ä½•å­¦çš„èªè­˜ãŒå‘ä¸Šã™ã‚‹ãŒã€è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰ã®æƒ…å ±å‡¦ç†ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã§ã‚ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=PYHwlyu2fa#discussion" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=PYHwlyu2fa#discussion</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/ryokamoi/status/1974547359842656388?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/OOD.html" target="_blank" rel="noopener noreferrer">#OOD</a>
<a class="button" href="articles/Generalization.html" target="_blank" rel="noopener noreferrer">#Generalization</a>
<span class="issue_date">Issue Date: 2025-10-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3121" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Visual Instruction Bottleneck Tuning, Changdae Oh+, NeurIPS'25, 2025.05</a>
<span class="snippet"><span>GPT Summary</span>- MLLMã¯æœªçŸ¥ã®ã‚¯ã‚¨ãƒªã«å¯¾ã—ã¦æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹ãŒã€æ—¢å­˜ã®æ”¹å–„ç­–ã¯å¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚„è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’è¦ã™ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€æƒ…å ±ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åŸç†ã«åŸºã¥ãã€MLLMã®å …ç‰¢æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®Vittleã‚’ææ¡ˆã€‚45ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å®Ÿè¨¼å®Ÿé¨“ã«ã‚ˆã‚Šã€VittleãŒMLLMã®å …ç‰¢æ€§ã‚’ä¸€è²«ã—ã¦æ”¹å–„ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/sharonyixuanli/status/1974150056501535207?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/Sparse.html" target="_blank" rel="noopener noreferrer">#Sparse</a>
<span class="issue_date">Issue Date: 2025-10-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3109" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] VideoNSA: Native Sparse Attention Scales Video Understanding, Enxin Song+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- VideoNSAã¯ã€ãƒ“ãƒ‡ã‚ªç†è§£ã®ãŸã‚ã«Native Sparse Attentionã‚’é©ç”¨ã—ã€é•·ã„æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®ä¸€è²«æ€§ã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã€‚216Kã®ãƒ“ãƒ‡ã‚ªæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§Qwen2.5-VLã‚’ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã«ã¯å¯†ãªæ³¨æ„ã€ãƒ“ãƒ‡ã‚ªã«ã¯NSAã‚’ä½¿ç”¨ã€‚ãƒˆãƒ¼ã‚¯ãƒ³åœ§ç¸®ã‚„å¾“æ¥ã®ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒã—ã¦ã€é•·ã„ãƒ“ãƒ‡ã‚ªç†è§£ã‚„æ™‚é–“çš„æ¨è«–ã§æ€§èƒ½ãŒå‘ä¸Šã€‚ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æã«ã‚ˆã‚Šã€ä¿¡é ¼æ€§ã®ã‚ã‚‹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚„æ³¨æ„ã®æœ€é©é…åˆ†ãªã©ã®é‡è¦ãªç™ºè¦‹ãŒå¾—ã‚‰ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenhaocha1/status/1974164887090684316?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/OpenSource.html" target="_blank" rel="noopener noreferrer">#OpenSource</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-10-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3105" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] LLaVA-OneVision-1.5: Fully Open Framework for Democratized Multimodal  Training, Xiang An+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- LLaVA-OneVision-1.5ã¯ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã¨è²¡æ”¿ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã—ã¤ã¤æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®Ÿç¾ã™ã‚‹æ–°ã—ã„å¤§è¦æ¨¡ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ã‚ªãƒ¼ãƒ—ãƒ³ã§åŠ¹ç‡çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æä¾›ã—ã€85Mã®äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨26Mã®æŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å«ã‚€å¤§è¦æ¨¡ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚åŠ¹ç‡çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã‚ˆã‚Šã€é™ã‚‰ã‚ŒãŸäºˆç®—å†…ã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¯èƒ½ã¨ãªã‚Šã€å¹…åºƒã„ä¸‹æµã‚¿ã‚¹ã‚¯ã§ç«¶äº‰åŠ›ã®ã‚ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ç‰¹ã«ã€LLaVA-OneVision-1.5-8Bã¯18ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§Qwen2.5-VL-7Bã‚’ä¸Šå›ã‚Šã€4Bãƒ¢ãƒ‡ãƒ«ã¯å…¨ã¦ã®27ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§Qwen2.5-VL-3Bã‚’è¶…ãˆã¦ã„ã¾ã™ã€‚ä»Šå¾Œã€LLaVA-OneVision-1.5-RLã®ãƒªãƒªãƒ¼ã‚¹ã‚‚äºˆå®šã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/xiyaowang10/status/1973887115781140598?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å„ç¨®ãƒ™ãƒ³ãƒã§Qwen2.5-VLè¶…ãˆ</p>
<p>pj page:


<a href="https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5" target="_blank" rel="noopener noreferrer">https://github.com/EvolvingLMMs-Lab/LLaVA-OneVision-1.5</a>


</p>
<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1974632456348385583?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/Medical.html" target="_blank" rel="noopener noreferrer">#Medical</a>
<span class="issue_date">Issue Date: 2025-10-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3097" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Radiology's Last Exam ï¼ˆRadLEï¼‰: Benchmarking Frontier Multimodal AI  Against Human Experts and a Taxonomy of Visual Reasoning Errors in Radiology, Suvrankar Datta+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- åŒ»ç™‚ç”»åƒã®è§£é‡ˆã«ãŠã‘ã‚‹AIãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã€50ã®å°‚é–€çš„ãªã€Œã‚¹ãƒãƒƒãƒˆè¨ºæ–­ã€ã‚±ãƒ¼ã‚¹ã‚’ç”¨ã„ãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’é–‹ç™ºã€‚5ã¤ã®æœ€å‰ç·šAIãƒ¢ãƒ‡ãƒ«ï¼ˆGPT-5ã€o3ã€Gemini 2.5 Proã€Grok-4ã€Claude Opus 4.1ï¼‰ã‚’ãƒ†ã‚¹ãƒˆã—ãŸçµæœã€ãƒœãƒ¼ãƒ‰èªå®šæ”¾å°„ç·šåŒ»ãŒæœ€é«˜ã®è¨ºæ–­ç²¾åº¦ï¼ˆ83%ï¼‰ã‚’é”æˆã—ã€AIãƒ¢ãƒ‡ãƒ«ã¯æœ€è‰¯ã®GPT-5ã§ã‚‚30%ã«ç•™ã¾ã£ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€AIãƒ¢ãƒ‡ãƒ«ãŒé›£ã—ã„è¨ºæ–­ã‚±ãƒ¼ã‚¹ã«ãŠã„ã¦æ”¾å°„ç·šåŒ»ã«ã¯åŠã°ãªã„ã“ã¨ãŒç¤ºã•ã‚Œã€åŒ»ç™‚ç”»åƒã«ãŠã‘ã‚‹AIã®é™ç•Œã¨ç„¡ç›£è¦–ä½¿ç”¨ã¸ã®è­¦å‘ŠãŒå¼·èª¿ã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/drdatta_aiims/status/1973373655251038701?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/kimmonismus/status/1974594801598418963?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/InformationRetrieval.html" target="_blank" rel="noopener noreferrer">#InformationRetrieval</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<span class="issue_date">Issue Date: 2025-10-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3085" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] ModernVBERT: Towards Smaller Visual Document Retrievers, Paul Teiletche+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã¯æ–‡æ›¸æ¤œç´¢ã«ãŠã„ã¦åŠ¹ç‡çš„ãªä»£æ›¿æ‰‹æ®µã¨ã—ã¦æ™®åŠã—ã¦ã„ã‚‹ãŒã€å†åˆ©ç”¨ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒæ¤œç´¢æ€§èƒ½ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¨ãªã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€è¦–è¦šæ–‡æ›¸æ¤œç´¢ãƒ¢ãƒ‡ãƒ«ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã®åŸå‰‡çš„ãªãƒ¬ã‚·ãƒ”ã‚’ç¢ºç«‹ã—ã€æ³¨æ„ãƒã‚¹ã‚­ãƒ³ã‚°ã‚„ç”»åƒè§£åƒåº¦ãªã©ãŒæ€§èƒ½ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹è¦å› ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚ã“ã‚Œã«åŸºã¥ãã€250Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãªè¦–è¦š-è¨€èªã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ModernVBERTã‚’é–‹ç™ºã—ã€æ–‡æ›¸æ¤œç´¢ã‚¿ã‚¹ã‚¯ã§å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’é”æˆã—ãŸã€‚ãƒ¢ãƒ‡ãƒ«ã¨ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jobergum/status/1973830118637551626?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>MIT Licence<br>HF: 


<a href="https://huggingface.co/ModernVBERT" target="_blank" rel="noopener noreferrer">https://huggingface.co/ModernVBERT</a>


</p>
<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1974047955301626065?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Off-Policy.html" target="_blank" rel="noopener noreferrer">#Off-Policy</a>
<a class="button" href="articles/WorldModels.html" target="_blank" rel="noopener noreferrer">#WorldModels</a>
<span class="issue_date">Issue Date: 2025-10-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3069" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Training Agents Inside of Scalable World Models, Danijar Hafner+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- ã€ŒDreamer 4ã€ã¯ã€ãƒ“ãƒ‡ã‚ªã‚²ãƒ¼ãƒ ã€ŒMinecraftã€ã«ãŠã„ã¦ç‰©ä½“ã®ç›¸äº’ä½œç”¨ã‚’æ­£ç¢ºã«äºˆæ¸¬ã—ã€å¼·åŒ–å­¦ç¿’ã‚’ç”¨ã„ã¦åˆ¶å¾¡ã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã™ã‚‹ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã™ã€‚ã“ã®ãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆå¼·åˆ¶ç›®çš„ã¨åŠ¹ç‡çš„ãªãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ´»ç”¨ã—ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–æ¨è«–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚ã•ã‚‰ã«ã€å°‘é‡ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¸€èˆ¬çš„ãªè¡Œå‹•ã‚’å­¦ç¿’ã—ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ã¿ã§ãƒ€ã‚¤ãƒ¤ãƒ¢ãƒ³ãƒ‰ã‚’å–å¾—ã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚’æˆåŠŸã•ã›ã¾ã—ãŸã€‚Dreamer 4ã¯ã€ç’°å¢ƒã¨ã®ç›¸äº’ä½œç”¨ãªã—ã«å­¦ã¶èƒ½åŠ›ã‚’æŒã¤åˆã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã‚ã‚Šã€çŸ¥èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¸ã®æ–°ãŸãªé“ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dair_ai/status/1974852100355231853?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/ImageCaptioning.html" target="_blank" rel="noopener noreferrer">#ImageCaptioning</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/LLM-as-a-Judge.html" target="_blank" rel="noopener noreferrer">#LLM-as-a-Judge</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/MultiDimensional.html" target="_blank" rel="noopener noreferrer">#MultiDimensional</a>
<span class="issue_date">Issue Date: 2025-10-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3059" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] VELA: An LLM-Hybrid-as-a-Judge Approach for Evaluating Long Image   Captions, Kazuki Matsuda+, EMNLP'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€é•·ã„ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®è‡ªå‹•è©•ä¾¡ã«ç‰¹åŒ–ã—ãŸæ–°ã—ã„æŒ‡æ¨™VELAã‚’ææ¡ˆã—ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆMLLMsï¼‰ã‚’æ´»ç”¨ã—ãŸè©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã€‚ã•ã‚‰ã«ã€è©•ä¾¡æŒ‡æ¨™ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã®LongCap-Arenaãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å°å…¥ã—ã€7,805æšã®ç”»åƒã¨32,246ä»¶ã®äººé–“ã®åˆ¤æ–­ã‚’ç”¨ã„ã¦ã€VELAãŒæ—¢å­˜ã®æŒ‡æ¨™ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/yuigawada/status/1973309026546098355?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ImageCaptioning.html" target="_blank" rel="noopener noreferrer">#ImageCaptioning</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-09-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3023" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement  Learning, Long Xing+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦ã€å¾“æ¥ã®ç›£è¦–å‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆSFTï¼‰ã®é™ç•Œã‚’å…‹æœã™ã‚‹ãŸã‚ã€æ¤œè¨¼å¯èƒ½ãªå ±é…¬ã‚’ç”¨ã„ãŸå¼·åŒ–å­¦ç¿’ï¼ˆRLVRï¼‰ã‚’ææ¡ˆã€‚æ–°ã—ã„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€Œã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ‹ãƒ³ã‚°å¼·åŒ–å­¦ç¿’ï¼ˆCapRLï¼‰ã€ã‚’å°å…¥ã—ã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®è³ªã‚’ãã®æœ‰ç”¨æ€§ã§å†å®šç¾©ã€‚CapRLã¯ã€è¦–è¦šéä¾å­˜ã®LLMã®ç²¾åº¦ã«åŸºã¥ãå®¢è¦³çš„ãªå ±é…¬ã‚’å¾—ã‚‹äºŒæ®µéšã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ¡ç”¨ã€‚CapRL-3Bã«ã‚ˆã‚‹äº‹å‰å­¦ç¿’ã¯ã€12ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å¤§å¹…ãªæ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ã—ã€Qwen2.5-VL-72Bã¨åŒç­‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1972455938939322805?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>HF:


<a href="https://huggingface.co/collections/long-xing1/caprl-68d64ac32ded31596c36e189" target="_blank" rel="noopener noreferrer">https://huggingface.co/collections/long-xing1/caprl-68d64ac32ded31596c36e189</a>


</p>
<p>å…¬å¼ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/intern_lm/status/1983468239993917446?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/Sparse.html" target="_blank" rel="noopener noreferrer">#Sparse</a>
<span class="issue_date">Issue Date: 2025-09-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3003" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via   Semantic-Aware Permutation, Shuo Yang+, NeurIPS'25 Spotlight, 2025.05</a>
<span class="snippet"><span>GPT Summary</span>- Diffusion Transformersï¼ˆDiTsï¼‰ã®å‹•ç”»ç”Ÿæˆã«ãŠã‘ã‚‹ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã€é‡è¦ãƒˆãƒ¼ã‚¯ãƒ³ã®ç‰¹å®šç²¾åº¦ã‚’æœ€å¤§åŒ–ã—è¨ˆç®—ã®ç„¡é§„ã‚’æœ€å°åŒ–ã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸è¦ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯SVG2ã‚’ææ¡ˆã€‚SVG2ã¯æ„å‘³ã«åŸºã¥ããƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¨å†é…ç½®ã‚’è¡Œã„ã€è¨ˆç®—åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€HunyuanVideoãŠã‚ˆã³Wan 2.1ã§ãã‚Œãã‚Œæœ€å¤§2.30å€ãŠã‚ˆã³1.89å€ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’é”æˆã—ã€PSNRã‚’ç¶­æŒã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/chenfeng_x/status/1971343654662046184?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>pj page:


<a href="https://svg-project.github.io/v2/" target="_blank" rel="noopener noreferrer">https://svg-project.github.io/v2/</a>


</p>
<p>Q, Kãã‚Œãã‚Œã«ã¤ã„ã¦ç‹¬ç«‹ã—ã¦kmeansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿæ–½ã—ã€æ„å‘³çš„ã«é¡ä¼¼ã—ãŸQ, Kã‚’ã‚¯ãƒ©ã‚¹ã‚¿åŒ–ã—ã€mapä¸Šã§æ•£ã‚‰ã°ã£ã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®é…ç½®ã‚’æ•´é “ã—ã¦è¨ˆç®—æ©Ÿä¸Šã§åŠ¹ç‡çš„ã«æ‰±ãˆã‚‹ã‚ˆã†ã«ã—ã€å„ã‚¯ãƒ©ã‚¹ã‚¿ã®centroidã‚’attention scoreã®è¨ˆç®—ã«ç”¨ã„ã¦ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¹ã‚³ã‚¢ã‚’è¿‘ä¼¼ã™ã‚‹ã“ã¨ã§è¨ˆç®—ã‚’åŠ¹ç‡åŒ–ã—ã¾ã™ã€ã¨ã„ã£ãŸè©±ãªæ¨¡æ§˜ã€‚ã¾ãŸã€ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªã‚¯ãƒ©ã‚¹ã‚¿ã¨ãã†ã§ã¯ç„¡ã„ã‚‚ã®ãŒã‚ã‚‹ã®ã§ã€på€‹ã®ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªã‚¯ãƒ©ã‚¹ã‚¿ã‚’é¸æŠã—ã•ã‚‰ã«åŠ¹ç‡åŒ–ã‚’ã™ã‚‹æ¨¡æ§˜ã€‚<br><img src="https://github.com/user-attachments/assets/862cf5c8-5583-4f94-8b67-59177c444176" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-09-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2983" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Video models are zero-shot learners and reasoners, ThaddÃ¤us Wiedemer+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆèƒ½åŠ›ãŒè‡ªç„¶è¨€èªå‡¦ç†ã‚’å¤‰é©ã—ãŸã‚ˆã†ã«ã€ç”Ÿæˆãƒ“ãƒ‡ã‚ªãƒ¢ãƒ‡ãƒ«ã‚‚ä¸€èˆ¬ç›®çš„ã®è¦–è¦šç†è§£ã«å‘ã‹ã†å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚Veo 3ã¯ã€ç‰©ä½“ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚„ã‚¨ãƒƒã‚¸æ¤œå‡ºãªã©ã€è¨“ç·´ã•ã‚Œã¦ã„ãªã„å¹…åºƒã„ã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã€è¦–è¦šæ¨è«–ã®åˆæœŸå½¢æ…‹ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚Veoã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆèƒ½åŠ›ã¯ã€ãƒ“ãƒ‡ã‚ªãƒ¢ãƒ‡ãƒ«ãŒä¸€èˆ¬çš„ãªè¦–è¦šåŸºç›¤ãƒ¢ãƒ‡ãƒ«ã«ãªã‚‹é“ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://video-zero-shot.github.io" target="_blank" rel="noopener noreferrer">https://video-zero-shot.github.io</a>


</p>
<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1971157183384723628?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/scaling01/status/1972010222853243097?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1973164588595290374?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/GRPO.html" target="_blank" rel="noopener noreferrer">#GRPO</a>
<span class="issue_date">Issue Date: 2025-09-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2940" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] BranchGRPO: Stable and Efficient GRPO with Structured Branching in  Diffusion Models, Yuming Li+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- BranchGRPOã‚’ææ¡ˆã—ã€ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆãƒ—ãƒ­ã‚»ã‚¹ã‚’åˆ†å²ãƒ„ãƒªãƒ¼ã«å†æ§‹ç¯‰ã™ã‚‹ã“ã¨ã§ã€ç”»åƒãŠã‚ˆã³å‹•ç”»ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡ã‚’å‘ä¸Šã€‚å…±æœ‰ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’ç”¨ã„ã¦ã‚³ã‚¹ãƒˆã‚’åˆ†æ•£ã—ã€ã‚¹ãƒ‘ãƒ¼ã‚¹ãªå ±é…¬ã‚’å¯†ãªä¿¡å·ã«å¤‰æ›ã€‚HPDv2.1ã§æœ€å¤§16%ã®æ•´åˆæ€§å‘ä¸Šã¨55%ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“çŸ­ç¸®ã‚’å®Ÿç¾ã€‚BranchGRPO-Mixã¯DanceGRPOã‚ˆã‚Š4.7å€é€Ÿããƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€‚WanXå‹•ç”»ç”Ÿæˆã§ã‚‚é«˜ã„Video-Alignã‚¹ã‚³ã‚¢ã‚’é”æˆã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://fredreic1849.github.io/BranchGRPO-Webpage/" target="_blank" rel="noopener noreferrer">https://fredreic1849.github.io/BranchGRPO-Webpage/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1970326241384505761?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<span class="issue_date">Issue Date: 2025-09-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2930" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Transfusion: Predict the Next Token and Diffuse Images with One   Multi-Modal Model, Chunting Zhou+, ICLR'25, 2024.08</a>
<span class="snippet"><span>GPT Summary</span>- Transfusionã¯ã€é›¢æ•£ãƒ‡ãƒ¼ã‚¿ã¨é€£ç¶šãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹æ‰‹æ³•ã§ã€è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®æå¤±é–¢æ•°ã¨æ‹¡æ•£ã‚’çµ„ã¿åˆã‚ã›ã¦å˜ä¸€ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’è¨“ç·´ã—ã¾ã™ã€‚æœ€å¤§7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’äº‹å‰è¨“ç·´ã—ã€ãƒ¦ãƒ‹ãƒ¢ãƒ¼ãƒ€ãƒ«ãŠã‚ˆã³ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å„ªã‚ŒãŸã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’ç¤ºã—ã¾ã—ãŸã€‚ãƒ¢ãƒ€ãƒªãƒ†ã‚£ç‰¹æœ‰ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å±¤ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã€7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«ã§ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã§ãã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=SI2hI0frk6" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=SI2hI0frk6</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<span class="issue_date">Issue Date: 2025-09-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2925" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid  Vision Tokenizer, Yanghao Li+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- Manzanoã¯ã€è¦–è¦šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç†è§£ã¨ç”Ÿæˆã‚’çµ±ä¸€çš„ã«è¡Œã†ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã§ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç”»åƒãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¬ã‚·ãƒ”ã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è»½æ¸›ã—ã¾ã™ã€‚å˜ä¸€ã®ãƒ“ã‚¸ãƒ§ãƒ³ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãŒç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã¸ã®åŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã—ã€è‡ªå·±å›å¸°å‹LLMãŒãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒãƒˆãƒ¼ã‚¯ãƒ³ã®é«˜ãƒ¬ãƒ™ãƒ«ã®æ„å‘³ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚ã“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚Šã€ä¸¡æ–¹ã®èƒ½åŠ›ã®å…±åŒå­¦ç¿’ãŒå¯èƒ½ã¨ãªã‚Šã€æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/arankomatsuzaki/status/1969974676802990478?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1969974517024923936?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>DocVQAã®ã‚ªãƒ©ã‚¯ãƒ«ã¯ãƒ©ãƒ™ãƒ«ãƒã‚¤ã‚ºã¨æ›–æ˜§æ€§ã®è¦³ç‚¹ã‹ã‚‰94--95ã¨ã„ã†ä¸»å¼µ:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/vikhyatk/status/1970585801600967009?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<a class="button" href="articles/SpatialUnderstanding.html" target="_blank" rel="noopener noreferrer">#SpatialUnderstanding</a>
<span class="issue_date">Issue Date: 2025-09-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2921" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Perception Encoder: The best visual embeddings are not at the output of   the network, Daniel Bolya+, NeurIPS'25, 2025.04</a>
<span class="snippet"><span>GPT Summary</span>- Perception Encoderï¼ˆPEï¼‰ã¯ã€ç”»åƒã¨å‹•ç”»ç†è§£ã®ãŸã‚ã®æ–°ã—ã„ãƒ“ã‚¸ãƒ§ãƒ³ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§ã€ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ“ã‚¸ãƒ§ãƒ³ã¨è¨€èªã®å­¦ç¿’ã‚’é€šã˜ã¦è¨“ç·´ã•ã‚Œã¦ã„ã¾ã™ã€‚å¾“æ¥ã®ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã«ä¾å­˜ã›ãšã€å¯¾ç…§çš„ãªãƒ“ã‚¸ãƒ§ãƒ³ã¨è¨€èªã®è¨“ç·´ã ã‘ã§å¼·åŠ›ãªåŸ‹ã‚è¾¼ã¿ã‚’ç”Ÿæˆã—ã¾ã™ã€‚åŸ‹ã‚è¾¼ã¿ã‚’å¼•ãå‡ºã™ãŸã‚ã«ã€è¨€èªã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã¨ç©ºé–“ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆã®2ã¤ã®æ‰‹æ³•ã‚’å°å…¥ã€‚PEãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆç”»åƒãƒ»å‹•ç”»åˆ†é¡ã§é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ã€Q&amp;Aã‚¿ã‚¹ã‚¯ã‚„ç©ºé–“ã‚¿ã‚¹ã‚¯ã§ã‚‚æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã—ã¾ã—ãŸã€‚ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/andreamadotto/status/1969529427064471619?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1983642930515734898?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/Findings.html" target="_blank" rel="noopener noreferrer">#Findings</a>
<span class="issue_date">Issue Date: 2025-09-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2920" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Lost in Embeddings: Information Loss in Vision-Language Models, Wenyan Li+, EMNLP'25 Findings, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- è¦–è¦šã¨è¨€èªã®ãƒ¢ãƒ‡ãƒ«ï¼ˆVLMsï¼‰ã®æŠ•å½±ã‚¹ãƒ†ãƒƒãƒ—ã«ã‚ˆã‚‹æƒ…å ±æå¤±ã‚’åˆ†æã™ã‚‹ãŸã‚ã€2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã€‚1ã¤ç›®ã¯ã€æŠ•å½±å‰å¾Œã®ç”»åƒè¡¨ç¾ã®kè¿‘å‚é–¢ä¿‚ã®å¤‰åŒ–ã‚’è©•ä¾¡ã—ã€2ã¤ç›®ã¯è¦–è¦šåŸ‹ã‚è¾¼ã¿ã®å†æ§‹ç¯‰ã«ã‚ˆã£ã¦æƒ…å ±æå¤±ã‚’æ¸¬å®šã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ã‚³ãƒã‚¯ã‚¿ãŒè¦–è¦šè¡¨ç¾ã®å¹¾ä½•å­¦ã‚’æ­ªã‚ã€kè¿‘å‚ãŒ40ï½60%ä¹–é›¢ã™ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã€ã“ã‚Œã¯æ¤œç´¢æ€§èƒ½ã®ä½ä¸‹ã¨é–¢é€£ã€‚ãƒ‘ãƒƒãƒãƒ¬ãƒ™ãƒ«ã®å†æ§‹ç¯‰ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®æŒ™å‹•ã«å¯¾ã™ã‚‹æ´å¯Ÿã‚’æä¾›ã—ã€é«˜ã„æƒ…å ±æå¤±ãŒãƒ¢ãƒ‡ãƒ«ã®è‹¦æ‰‹ãªäº‹ä¾‹ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenyan62/status/1969298016684163195?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1969557366245933068?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/VariationalAutoEncoder.html" target="_blank" rel="noopener noreferrer">#VariationalAutoEncoder</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-09-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2870" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Self Forcing: Bridging the Train-Test Gap in Autoregressive Video   Diffusion, Xun Huang+, NeurIPS'25</a>
<span class="snippet"><span>GPT Summary</span>- Self Forcingã¯ã€è‡ªå‹•å›å¸°å‹ãƒ“ãƒ‡ã‚ªæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®æ–°ã—ã„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã§ã€ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãƒã‚¤ã‚¢ã‚¹ã®å•é¡Œã«å¯¾å‡¦ã—ã¾ã™ã€‚å¾“æ¥ã®æ‰‹æ³•ãŒçœŸã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ãã®ã«å¯¾ã—ã€Self Forcingã¯è‡ªå·±ç”Ÿæˆã—ãŸå‡ºåŠ›ã«åŸºã¥ã„ã¦ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç”Ÿæˆã—ã€å…¨ä½“ã®å“è³ªã‚’è©•ä¾¡ã™ã‚‹ãƒ›ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ãªæå¤±ã‚’ç”¨ã„ã¾ã™ã€‚è¨ˆç®—ã‚³ã‚¹ãƒˆã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ãŸã‚ã«ã€å°‘æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¨ç¢ºç‡çš„å‹¾é…åˆ‡æ–­ã‚’æ¡ç”¨ã—ã€ãƒ­ãƒ¼ãƒ«ã‚¤ãƒ³ã‚°KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’å°å…¥ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ“ãƒ‡ã‚ªç”ŸæˆãŒå¯èƒ½ã§ã€éå› æœçš„æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ç”Ÿæˆå“è³ªã«åŒ¹æ•µã¾ãŸã¯ãã‚Œã‚’ä¸Šå›ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://self-forcing.github.io" target="_blank" rel="noopener noreferrer">https://self-forcing.github.io</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/xunhuang1995/status/1968797718593098087?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è‡ªå·±å›å¸°çš„ãªå‹•ç”»ç”Ÿæˆï¼ˆã‚’ã™ã‚‹ï¼‰ãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€å­¦ç¿’æ™‚ã¯ground-truchã®contextãŒåˆ©ç”¨ã—ã¦å­¦ç¿’ã•ã‚Œã‚‹ãŒã€æ¨è«–æ™‚ã¯è‡ªèº«ãŒç”Ÿæˆçµæœãã®ã‚‚ã®ã‚’contextã¨ã—ã¦åˆ©ç”¨ã™ã‚‹ãŸã‚ã€å­¦ç¿’-æ¨è«–æ™‚ã«gapãŒç”Ÿã˜ã€ï¼ˆå¾ã€…ã«èª¤å·®ãŒè“„ç©ã™ã‚‹ã“ã¨ã§ï¼‰å“è³ªãŒåŠ£åŒ–ã™ã‚‹ã¨ã„ã†å•é¡Œï¼ˆexposure biasï¼‰ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€å­¦ç¿’æ™‚ã‹ã‚‰è‡ªèº«ãŒç”Ÿæˆã—ãŸå‡ºåŠ›ã‚’contextã¨ã—ã¦ä¸ãˆã¦ç”Ÿæˆã‚’è¡Œã„ï¼ˆãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆï¼‰ã€å‹•ç”»å…¨ä½“ã«å¯¾ã—ã¦åˆ†å¸ƒã®æ•´åˆæ€§ã‚’æ¸¬ã‚‹lossã‚’å°å…¥ï¼ˆ=ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ã®èª¤å·®ã‚’æœ€å°åŒ–ã«ã™ã‚‹ã®ã§ã¯ãªãã€å‹•ç”»å…¨ä½“ã«å¯¾ã—ã¦ï¼ˆåˆ†å¸ƒã®ï¼‰èª¤å·®ã‚’æœ€é©åŒ–ã™ã‚‹ï¼‰ã™ã‚‹ã“ã¨ã§ã€exposure biasã‚’è»½æ¸›ã™ã‚‹ã€ã¨ã„ã†è©±ãªæ¨¡æ§˜ã€‚</p>
<p>çµæœçš„ã«ã€å˜ä¸€ã®RTX4090ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ“ãƒ‡ã‚ªç”ŸæˆãŒé«˜å“è³ªã«ç”Ÿæˆå¯èƒ½ã¨ãªã£ãŸï¼ˆã‹ã‚‚ã—ã‚Œãªã„ï¼‰:<br>


<a href="https://note.com/ngc_shj/n/n505b2f7cdfe4" target="_blank" rel="noopener noreferrer">https://note.com/ngc_shj/n/n505b2f7cdfe4</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<span class="issue_date">Issue Date: 2025-09-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2863" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] LMFusion: Adapting Pretrained Language Models for Multimodal Generation, Weijia Shi+, NeurIPS'25</a>
<span class="snippet"><span>GPT Summary</span>- LMFusionã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®LLMã«ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ç”Ÿæˆèƒ½åŠ›ã‚’ä»˜ä¸ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®ç†è§£ãƒ»ç”Ÿæˆã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚æ—¢å­˜ã®Llama-3ã®é‡ã¿ã‚’æ´»ç”¨ã—ã€ç”»åƒå‡¦ç†ã®ãŸã‚ã®ä¸¦åˆ—ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¿½åŠ ã€‚å„ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã¯ç‹¬ç«‹ã—ã¦å‡¦ç†ã•ã‚Œã€ç›¸äº’ä½œç”¨ãŒå¯èƒ½ã§ã™ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€LMFusionã¯ç”»åƒç†è§£ã‚’20%ã€ç”Ÿæˆã‚’3.6%å‘ä¸Šã•ã›ã€Llama-3ã®è¨€èªèƒ½åŠ›ã‚’ç¶­æŒã—ã¤ã¤ã€åŠ¹ç‡çš„ã«ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’é–‹ç™ºã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/weijiashi2/status/1870107645677568248?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å…ˆè¡Œç ”ç©¶:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2930" target="_blank" rel="noopener noreferrer">[Paper Note] Transfusion: Predict the Next Token and Diffuse Images with One   Multi-Modal Model, Chunting Zhou+, ICLR'25, 2024.08</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2931" target="_blank" rel="noopener noreferrer">[Paper Note] U-Net: Convolutional Networks for Biomedical Image Segmentation, Olaf Ronneberger+, MICCAI'15, 2015.05</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/ContextEngineering.html" target="_blank" rel="noopener noreferrer">#ContextEngineering</a>
<span class="issue_date">Issue Date: 2025-09-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2850" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] VisionZip: Longer is Better but Not Necessary in Vision Language Models, Senqiao Yang+, CVPR'25</a>
<span class="snippet"><span>GPT Summary</span>- VisionZipã¯ã€è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã®å†—é•·æ€§ã‚’å‰Šæ¸›ã—ã€åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®æ–°ã—ã„æ‰‹æ³•ã§ã‚ã‚Šã€ç”»åƒã‚„å‹•ç”»ã®ç†è§£ã‚¿ã‚¹ã‚¯ã«é©ç”¨å¯èƒ½ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€å¾“æ¥ã®æ‰‹æ³•ã‚ˆã‚Šã‚‚5%ä»¥ä¸Šã®æ€§èƒ½å‘ä¸Šã‚’é”æˆã—ã€æ¨è«–é€Ÿåº¦ã‚‚å¤§å¹…ã«æ”¹å–„ã€‚ãƒˆãƒ¼ã‚¯ãƒ³ã®é•·ã•ã‚’å¢—ã‚„ã™ã®ã§ã¯ãªãã€ã‚ˆã‚Šè‰¯ã„è¦–è¦šç‰¹å¾´ã®æŠ½å‡ºã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ã“ã¨ã‚’ææ¡ˆã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dl_hacks/status/1968528535041229209?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/LLMAgent.html" target="_blank" rel="noopener noreferrer">#LLMAgent</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-09-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2826" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] SWE-bench Multimodal: Do AI Systems Generalize to Visual Software   Domains?, John Yang+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- è‡ªå¾‹ã‚·ã‚¹ãƒ†ãƒ ã®ãƒã‚°ä¿®æ­£èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã€SWE-bench Mã‚’ææ¡ˆã€‚ã“ã‚Œã¯è¦–è¦šè¦ç´ ã‚’å«ã‚€JavaScriptã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã®ã‚¿ã‚¹ã‚¯ã‚’å¯¾è±¡ã¨ã—ã€617ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åé›†ã€‚å¾“æ¥ã®SWE-benchã‚·ã‚¹ãƒ†ãƒ ãŒè¦–è¦šçš„å•é¡Œè§£æ±ºã«è‹¦åŠ´ã™ã‚‹ä¸­ã€SWE-agentã¯ä»–ã®ã‚·ã‚¹ãƒ†ãƒ ã‚’å¤§ããä¸Šå›ã‚Šã€12%ã®ã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=riTiq3i21b" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=riTiq3i21b</a>


</p>
<p>pj page:


<a href="https://www.swebench.com/multimodal.html" target="_blank" rel="noopener noreferrer">https://www.swebench.com/multimodal.html</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<a class="button" href="articles/4D%20(Video).html" target="_blank" rel="noopener noreferrer">#4D (Video)</a>
<span class="issue_date">Issue Date: 2025-09-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2823" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] 4DNeX: Feed-Forward 4D Generative Modeling Made Easy, Zhaoxi Chen+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- 4DNeXã¯ã€å˜ä¸€ã®ç”»åƒã‹ã‚‰å‹•çš„3Dã‚·ãƒ¼ãƒ³ã‚’ç”Ÿæˆã™ã‚‹åˆã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚Šã€äº‹å‰å­¦ç¿’ã•ã‚ŒãŸãƒ“ãƒ‡ã‚ªæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§åŠ¹ç‡çš„ãª4Dç”Ÿæˆã‚’å®Ÿç¾ã€‚å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ4DNeX-10Mã‚’æ§‹ç¯‰ã—ã€RGBã¨XYZã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’çµ±ä¸€çš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€4DNeXã¯æ—¢å­˜æ‰‹æ³•ã‚’ä¸Šå›ã‚‹åŠ¹ç‡æ€§ã¨ä¸€èˆ¬åŒ–èƒ½åŠ›ã‚’ç¤ºã—ã€å‹•çš„ã‚·ãƒ¼ãƒ³ã®ç”Ÿæˆçš„4Dãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ¢ãƒ‡ãƒ«ã®åŸºç›¤ã‚’æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://4dnex.github.io" target="_blank" rel="noopener noreferrer">https://4dnex.github.io</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/liuziwei7/status/1967604322591789418?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/OpenSource.html" target="_blank" rel="noopener noreferrer">#OpenSource</a>
<a class="button" href="articles/Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-09-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2820" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] OpenVision 2: A Family of Generative Pretrained Visual Encoders for  Multimodal Learning, Yanqing Liu+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€OpenVisionã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç°¡ç´ åŒ–ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹æ–¹æ³•ã‚’ææ¡ˆã€‚ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¨å¯¾ç…§æå¤±ã‚’å‰Šé™¤ã—ã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ‹ãƒ³ã‚°æå¤±ã®ã¿ã‚’ä½¿ç”¨ã—ãŸOpenVision 2ã‚’å°å…¥ã€‚åˆæœŸçµæœã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“ã‚’ç´„1.5å€çŸ­ç¸®ã—ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç´„1.8å€å‰Šæ¸›ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã€10å„„ä»¥ä¸Šã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’å¼·èª¿ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1967865921399296286?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>äº‹å‰å­¦ç¿’æ™‚ã«text, image encoderã®contrastive lossã§å­¦ç¿’ã—ã¦ã„ãŸãŒã€text encoderã‚’ç„¡ãã—image encoderã«å…¥åŠ›ã•ã‚ŒãŸimageã‹ã‚‰captionã‚’ç”Ÿæˆã™ã‚‹caption lossã®ã¿ã«ã™ã‚‹ã“ã¨ã§æ€§èƒ½ã‚’è½ã¨ã™ã“ã¨ãªãåŠ¹ç‡ã‚’æ”¹å–„</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2025-09-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2819" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human  Preference, Xiangwei Shen+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Direct-Alignæ‰‹æ³•ã‚’ç”¨ã„ã¦ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã—ã€å…ƒã®ç”»åƒã‚’åŠ¹æœçš„ã«å¾©å…ƒã€‚ã•ã‚‰ã«ã€SRPOã‚’å°å…¥ã—ã€å ±é…¬ã‚’ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§èª¿æ•´ã™ã‚‹ã“ã¨ã§ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ä¾å­˜ã‚’æ¸›å°‘ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€FLUXãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¢ãƒªã‚ºãƒ ã¨ç¾çš„å“è³ªã‚’3å€ä»¥ä¸Šå‘ä¸Šã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://tencent.github.io/srpo-project-page/" target="_blank" rel="noopener noreferrer">https://tencent.github.io/srpo-project-page/</a>


</p>
<p>SRPO (Semantic Relative Preference Optimization)<br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2341" target="_blank" rel="noopener noreferrer">[Paper Note] SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement
  Learning on LLM, Xiaojiang Zhang+, arXiv'25</a>
<br><br>ã¨åç§°ãŒé‡è¤‡ã—ã¦ã„ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<span class="issue_date">Issue Date: 2025-09-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2812" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] K-LoRA: Unlocking Training-Free Fusion of Any Subject and Style LoRAs, Ziheng Ouyang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- K-LoRAã¯ã€ç•°ãªã‚‹LoRAã‚’åŠ¹æœçš„ã«èåˆã—ã€ä¸»é¡Œã¨ã‚¹ã‚¿ã‚¤ãƒ«ã‚’åŒæ™‚ã«ä¿æŒã™ã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã€‚å„ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã§Top-Kè¦ç´ ã‚’æ¯”è¼ƒã—ã€æœ€é©ãªLoRAã‚’é¸æŠã™ã‚‹ã“ã¨ã§ã€ä¸»é¡Œã¨ã‚¹ã‚¿ã‚¤ãƒ«ã®ç‰¹å¾´ã‚’ãƒãƒ©ãƒ³ã‚¹ã‚ˆãçµ±åˆã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ææ¡ˆæ‰‹æ³•ãŒå¾“æ¥ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/nagasaiabhinay/status/1967410633584087358?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å…ˆè¡Œç ”ç©¶:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2813" target="_blank" rel="noopener noreferrer">[Paper Note] Implicit Style-Content Separation using B-LoRA, Yarden Frenkel+, ECCV'24</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1159" target="_blank" rel="noopener noreferrer">[Paper Note] ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs, Viraj Shah+, N/A, ECCV'24</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/3D%20(Scene).html" target="_blank" rel="noopener noreferrer">#3D (Scene)</a>
<span class="issue_date">Issue Date: 2025-09-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2810" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] SpatialVID: A Large-Scale Video Dataset with Spatial Annotations, Jiahao Wang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- SpatialVIDãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€21,000æ™‚é–“ä»¥ä¸Šã®ç”Ÿå‹•ç”»ã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸ2.7ç™¾ä¸‡ã®ã‚¯ãƒªãƒƒãƒ—ã‚’å«ã¿ã€ã‚«ãƒ¡ãƒ©ãƒãƒ¼ã‚ºã€æ·±åº¦ã€å‹•çš„ãƒã‚¹ã‚¯ãªã©ã®è©³ç´°ãª3Dæ³¨é‡ˆã‚’æä¾›ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç©ºé–“çŸ¥èƒ½ã®ãƒ¢ãƒ‡ãƒ«ã®ä¸€èˆ¬åŒ–ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã‚’ä¿ƒé€²ã—ã€ãƒ“ãƒ‡ã‚ªãŠã‚ˆã³3Dãƒ“ã‚¸ãƒ§ãƒ³ç ”ç©¶ã«ãŠã„ã¦é‡è¦ãªè³‡ç”£ã¨ãªã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://nju-3dv.github.io/projects/SpatialVID/" target="_blank" rel="noopener noreferrer">https://nju-3dv.github.io/projects/SpatialVID/</a>


<br>dataset:


<a href="https://huggingface.co/datasets/SpatialVID/SpatialVID-HQ" target="_blank" rel="noopener noreferrer">https://huggingface.co/datasets/SpatialVID/SpatialVID-HQ</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1967260292569845885?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>CC-BY-NC-SA 4.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹</p></span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/SpatialUnderstanding.html" target="_blank" rel="noopener noreferrer">#SpatialUnderstanding</a>
<span class="issue_date">Issue Date: 2025-09-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2774" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis  from Data to Architecture, Wanyue Zhang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ç©ºé–“ç†è§£ã¯MLLMsã«ã¨ã£ã¦é‡è¦ã ãŒã€ä¾ç„¶ã¨ã—ã¦èª²é¡ŒãŒå¤šã„ã€‚æœ¬ç ”ç©¶ã§ã¯ã€å˜ä¸€è¦–ç‚¹ã€å¤šè¦–ç‚¹ã€ãƒ“ãƒ‡ã‚ªã®3ã¤ã®ã‚·ãƒŠãƒªã‚ªã«ãŠã‘ã‚‹ç©ºé–“ç†è§£ã‚’ä½“ç³»çš„ã«åˆ†æã—ã€MulSeTã¨ã„ã†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ææ¡ˆã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®å¢—åŠ ã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã«å¯„ä¸ã™ã‚‹ãŒã€é™ç•ŒãŒã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ã¾ãŸã€ç©ºé–“ç†è§£ã¯è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«ä¾å­˜ã—ã¦ãŠã‚Šã€æ¨è«–ã®æ³¨å…¥ã‚’é€šã˜ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ”¹å–„ã®å¯èƒ½æ€§ã‚’æ¢ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€MLLMsã®é™ç•Œã‚’æ˜ã‚‰ã‹ã«ã—ã€ç©ºé–“æ¨è«–èƒ½åŠ›å‘ä¸Šã®æ–°ãŸãªæ–¹å‘æ€§ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/askalphaxiv/status/1965822971261718549?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/3D%20(Scene).html" target="_blank" rel="noopener noreferrer">#3D (Scene)</a>
<a class="button" href="articles/WorldModels.html" target="_blank" rel="noopener noreferrer">#WorldModels</a>
<a class="button" href="articles/4D%20(Video).html" target="_blank" rel="noopener noreferrer">#4D (Video)</a>
<span class="issue_date">Issue Date: 2025-09-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2773" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] 3D and 4D World Modeling: A Survey, Lingdong Kong+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬èª¿æŸ»ã¯ã€3DãŠã‚ˆã³4Dã®ä¸–ç•Œãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã¨ç”Ÿæˆã«ç‰¹åŒ–ã—ãŸåˆã®åŒ…æ‹¬çš„ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æä¾›ã—ã€æ­£ç¢ºãªå®šç¾©ã¨æ§‹é€ åŒ–ã•ã‚ŒãŸåˆ†é¡æ³•ã‚’å°å…¥ã€‚å‹•ç”»ãƒ™ãƒ¼ã‚¹ã€å æœ‰ãƒ™ãƒ¼ã‚¹ã€LiDARãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç¶²ç¾…ã—ã€ç‰¹åŒ–ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨è©•ä¾¡æŒ‡æ¨™ã‚’è¦ç´„ã€‚å®Ÿç”¨çš„ãªå¿œç”¨ã‚„æœªè§£æ±ºã®èª²é¡Œã‚’è­°è«–ã—ã€ä»Šå¾Œã®ç ”ç©¶æ–¹å‘ã‚’ç¤ºã™ã“ã¨ã§ã€ã“ã®åˆ†é‡ã®é€²å±•ã®åŸºç›¤ã‚’æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/zhenjun_zhao/status/1966137312515092501?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Alignment.html" target="_blank" rel="noopener noreferrer">#Alignment</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<span class="issue_date">Issue Date: 2025-09-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2768" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Reconstruction Alignment Improves Unified Multimodal Models, Ji Xie+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- çµ±ä¸€å¤šãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆUMMsï¼‰ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€ã‚¹ãƒ‘ãƒ¼ã‚¹ãªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã«ä¾å­˜ã—ã¦ãŠã‚Šã€è¦–è¦šçš„è©³ç´°ã‚’è¦‹é€ƒã™ã“ã¨ãŒå¤šã„ã€‚ãã“ã§ã€å†æ§‹æˆã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆï¼ˆRecAï¼‰ã‚’å°å…¥ã—ã€è¦–è¦šç†è§£ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®åŸ‹ã‚è¾¼ã¿ã‚’ç”¨ã„ã¦ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãªã—ã§è±Šå¯Œãªç›£è¦–ã‚’æä¾›ã€‚RecAã¯UMMã‚’è¦–è¦šç†è§£åŸ‹ã‚è¾¼ã¿ã«æ¡ä»¶ä»˜ã‘ã€è‡ªå·±ç›£è¦–å‹ã®å†æ§‹æˆæå¤±ã§æœ€é©åŒ–ã—ã€ç”Ÿæˆã¨ç·¨é›†ã®å¿ å®Ÿåº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚27 GPUæ™‚é–“ã§ã€ç”»åƒç”Ÿæˆæ€§èƒ½ã‚„ç·¨é›†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã€åŠ¹ç‡çš„ãªãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã¨ã—ã¦ã®åœ°ä½ã‚’ç¢ºç«‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://reconstruction-alignment.github.io" target="_blank" rel="noopener noreferrer">https://reconstruction-alignment.github.io</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/xdwang101/status/1965908302581420204?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2769" target="_blank" rel="noopener noreferrer">[Paper Note] GenEval: An Object-Focused Framework for Evaluating Text-to-Image   Alignment, Dhruba Ghosh+, NeurIPS'23</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2770" target="_blank" rel="noopener noreferrer">[Paper Note] ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment, Xiwei Hu+, arXiv'24</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/GRPO.html" target="_blank" rel="noopener noreferrer">#GRPO</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-09-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2749" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual  Search, Xin Lai+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Mini-o3ã‚·ã‚¹ãƒ†ãƒ ã¯ã€æ•°åã‚¹ãƒ†ãƒƒãƒ—ã®æ·±ã„ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³æ¨è«–ã‚’å®Ÿç¾ã—ã€è¦–è¦šæ¤œç´¢ã‚¿ã‚¹ã‚¯ã§æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’é”æˆã€‚Visual Probe Datasetã‚’æ§‹ç¯‰ã—ã€å¤šæ§˜ãªæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¤ºã™ãƒ‡ãƒ¼ã‚¿åé›†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é–‹ç™ºã€‚ã‚ªãƒ¼ãƒãƒ¼ã‚¿ãƒ¼ãƒ³ãƒã‚¹ã‚­ãƒ³ã‚°æˆ¦ç•¥ã«ã‚ˆã‚Šã€ã‚¿ãƒ¼ãƒ³æ•°ãŒå¢—ãˆã‚‹ã»ã©ç²¾åº¦ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã€‚</span>
<span class="snippet"><span>Comment</span><p>HF:


<a href="https://huggingface.co/Mini-o3" target="_blank" rel="noopener noreferrer">https://huggingface.co/Mini-o3</a>


</p>
<p>pj page:


<a href="https://mini-o3.github.io" target="_blank" rel="noopener noreferrer">https://mini-o3.github.io</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1965616579024228527?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ—¢å­˜ã®ã‚ªãƒ¼ãƒ—ãƒ³ãªVLMã¯ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã®ã‚¿ãƒ¼ãƒ³æ•°ã‚’å¢—ã‚„ã›ãªã„ã¨ã„ã†èª²é¡ŒãŒã‚ã£ãŸãŒãã‚Œã‚’å…‹æœã™ã‚‹ãƒ¬ã‚·ãƒ”ã«é–¢ã™ã‚‹ç ”ç©¶ãªæ¨¡æ§˜ã€‚å…ƒãƒã‚¹ãƒˆã«ã‚ˆã‚‹ã¨6ã‚¿ãƒ¼ãƒ³ã¾ã§ã®ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã§å­¦ç¿’ã—ã¦ã‚‚ã€inferenceæ™‚ã«ã¯32ã‚¿ãƒ¼ãƒ³ã¾ã§ã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹ã¨ã‹ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/LLMAgent.html" target="_blank" rel="noopener noreferrer">#LLMAgent</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/ComputerUse.html" target="_blank" rel="noopener noreferrer">#ComputerUse</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-09-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2696" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn  Reinforcement Learning, Haoming Wang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- UI-TARS-2ã¯ã€GUIç”¨è‡ªå¾‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã§ã€ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã€å®‰å®šåŒ–ã•ã‚ŒãŸãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³RLã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰GUIç’°å¢ƒã‚’çµ±åˆã€‚å®Ÿè¨¼è©•ä¾¡ã§ã¯ã€å‰ãƒ¢ãƒ‡ãƒ«ã‚’å¤§å¹…ã«ä¸Šå›ã‚Šã€è¤‡æ•°ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§é«˜ã„ã‚¹ã‚³ã‚¢ã‚’é”æˆã€‚ç´„60%ã®äººé–“ãƒ¬ãƒ™ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã€é•·æœŸçš„ãªæƒ…å ±æ¢ç´¢ã‚¿ã‚¹ã‚¯ã«ã‚‚é©å¿œå¯èƒ½ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã®åˆ†æãŒå®‰å®šæ€§ã¨åŠ¹ç‡å‘ä¸Šã®æ´å¯Ÿã‚’æä¾›ã—ã€å®Ÿä¸–ç•Œã®ã‚·ãƒŠãƒªã‚ªã¸ã®ä¸€èˆ¬åŒ–èƒ½åŠ›ã‚’å¼·èª¿ã€‚</span>
<span class="snippet"><span>Comment</span><p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1896" target="_blank" rel="noopener noreferrer">Introducing UI-TARS-1.5, ByteDance, 2025.04</a>
</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1963783886565183913?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>1.5ã‚’ãƒªãƒªãƒ¼ã‚¹ã—ã¦ã‹ã‚‰5ãƒ¶æœˆã§å¤§å¹…ã«æ€§èƒ½ã‚’å‘ä¸Šã—ãŸæ¨¡æ§˜</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/SelfCorrection.html" target="_blank" rel="noopener noreferrer">#SelfCorrection</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/Critic.html" target="_blank" rel="noopener noreferrer">#Critic</a>
<span class="issue_date">Issue Date: 2025-09-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2683" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model, Xiyao Wang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€è¦–è¦šã¨è¨€èªã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ãŠã„ã¦ã€æ‰¹è©•ãƒ¢ãƒ‡ãƒ«ã‚’å¼·åŒ–å­¦ç¿’ã‚’ç”¨ã„ã¦å†ç·¨æˆã—ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«ç›´æ¥é©ç”¨ã™ã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ‰¹è©•ãƒ¢ãƒ‡ãƒ«LLaVA-Critic-R1ã‚’ç”Ÿæˆã—ã€è¦–è¦šçš„æ¨è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ã¾ã—ãŸã€‚ã•ã‚‰ã«ã€è‡ªå·±æ‰¹è©•ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€è¿½åŠ ã®è¨“ç·´ãªã—ã«æ¨è«–ã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã—ãŸã€‚ã“ã®çµæœã¯ã€è©•ä¾¡ã¨ç”Ÿæˆã®ä¸¡æ–¹ã«å„ªã‚ŒãŸçµ±ä¸€ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿç¾ã™ã‚‹å¯èƒ½æ€§ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_akhaliq/status/1963248881027748265?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>HF:


<a href="https://huggingface.co/collections/lmms-lab/llava-critic-r1-68922484e5822b89fab4aca1" target="_blank" rel="noopener noreferrer">https://huggingface.co/collections/lmms-lab/llava-critic-r1-68922484e5822b89fab4aca1</a>


</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/GRPO.html" target="_blank" rel="noopener noreferrer">#GRPO</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-09-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2645" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs  via Bi-Mode Annealing and Reinforce Learning, Jie Jiang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- R-4Bã¯ã€å•é¡Œã®è¤‡é›‘ã•ã«å¿œã˜ã¦æ€è€ƒã‚’è¡Œã†ã‹ã©ã†ã‹ã‚’é©å¿œçš„ã«åˆ¤æ–­ã™ã‚‹è‡ªå‹•æ€è€ƒå‹ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆMLLMï¼‰ã§ã‚ã‚‹ã€‚æ€è€ƒèƒ½åŠ›ã¨éæ€è€ƒèƒ½åŠ›ã‚’æŒãŸã›ã€ãƒã‚¤ãƒ¢ãƒ¼ãƒ‰ãƒãƒªã‚·ãƒ¼æœ€é©åŒ–ï¼ˆBPOï¼‰ã‚’ç”¨ã„ã¦æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã®èµ·å‹•ã‚’ç²¾åº¦è‰¯ãåˆ¤æ–­ã™ã‚‹ã€‚è¨“ç·´ã«ã¯å¤šæ§˜ãªãƒˆãƒ”ãƒƒã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã€å®Ÿé¨“çµæœã¯R-4BãŒ25ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã€ç‰¹ã«æ¨è«–é›†ç´„å‹ã‚¿ã‚¹ã‚¯ã§ä½ã‚³ã‚¹ãƒˆã§é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ãŸã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1962445854654288036?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>VLMã«thinking, non-thinkingã‚’å…¥åŠ›ã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘ã•ã›ã‚‹æ‰‹æ³•</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<span class="issue_date">Issue Date: 2025-08-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2599" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Mixture of Contexts for Long Video Generation, Shengqu Cai+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- é•·å‹•ç”»ç”Ÿæˆã«ãŠã‘ã‚‹é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒ¢ãƒªã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã€ã‚¹ãƒ‘ãƒ¼ã‚¹æ³¨æ„ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€ŒMixture of Contextsï¼ˆMoCï¼‰ã€ã‚’ææ¡ˆã€‚MoCã¯ã€å‹•çš„ã«æƒ…å ±é‡ã®å¤šã„ãƒãƒ£ãƒ³ã‚¯ã¨å¿…é ˆã®ã‚¢ãƒ³ã‚«ãƒ¼ã‚’é¸æŠã—ã€å› æœãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’ç”¨ã„ã¦æ³¨æ„ã‚’å‘ã‘ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é‡è¦ãªå±¥æ­´ã«è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’å‰²ã‚Šå½“ã¦ã€æ•°åˆ†é–“ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«ã‚ãŸã£ã¦ã‚¢ã‚¤ãƒ‡ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚„ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¿æŒã™ã‚‹ã€‚åŠ¹ç‡æ€§ãŒå‘ä¸Šã—ã€å®Ÿç”¨çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨åˆæˆãŒå¯èƒ½ã«ãªã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://primecai.github.io/moc/" target="_blank" rel="noopener noreferrer">https://primecai.github.io/moc/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1961361528244113749?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Controllable.html" target="_blank" rel="noopener noreferrer">#Controllable</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2025-08-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2591" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] OmniHuman-1.5: Instilling an Active Mind in Avatars via Cognitive  Simulation, Jianwen Jiang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ã€ŒOmniHuman-1.5ã€ã¯ã€ç‰©ç†çš„å¦¥å½“æ€§ã¨æ„å‘³çš„ä¸€è²«æ€§ã‚’å…¼ã­å‚™ãˆãŸã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚‹ã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ã€éŸ³å£°ã€ç”»åƒã€ãƒ†ã‚­ã‚¹ãƒˆã®å…±åŒæ„å‘³ã‚’è§£é‡ˆã™ã‚‹ã“ã¨ã§ã€æ„Ÿæƒ…ã‚„æ„å›³ã«åŸºã¥ã„ãŸå‹•ä½œã‚’ç”Ÿæˆã€‚æ–°ã—ã„ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«DiTã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚Šã€ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ã®å¯¾ç«‹ã‚’è»½æ¸›ã—ã€ãƒªãƒƒãƒ—ã‚·ãƒ³ã‚¯ç²¾åº¦ã‚„å‹•ä½œã®è‡ªç„¶ã•ã§å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã€‚è¤‡é›‘ãªã‚·ãƒŠãƒªã‚ªã¸ã®æ‹¡å¼µæ€§ã‚‚ç¤ºã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://omnihuman-lab.github.io/v1_5/" target="_blank" rel="noopener noreferrer">https://omnihuman-lab.github.io/v1_5/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/aicia_solid/status/1960957629465026882?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>promptã«ã‚ˆã£ã¦çŠ¶æ³ã‚„æ„Ÿæƒ…ãªã©ã®è¡¨ç¾ã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ãŒå¯èƒ½ã‚‰ã—ã„</p>
<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1963895614728778187?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/WorldModels.html" target="_blank" rel="noopener noreferrer">#WorldModels</a>
<a class="button" href="articles/Game.html" target="_blank" rel="noopener noreferrer">#Game</a>
<span class="issue_date">Issue Date: 2025-08-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2581" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive  World Model, Xianglong He+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Matrix-Game 2.0ã‚’ææ¡ˆã—ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªä¸–ç•Œãƒ¢ãƒ‡ãƒ«ãŒãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§é•·ã„ãƒ“ãƒ‡ã‚ªã‚’ç”Ÿæˆã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚ä¸»ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªæ¡ä»¶ã‚’å¯èƒ½ã«ã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ³¨å…¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆã®ãŸã‚ã®æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®è’¸ç•™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€25 FPSã§é«˜å“è³ªãª1åˆ†é–“ã®ãƒ“ãƒ‡ã‚ªã‚’ç”Ÿæˆå¯èƒ½ã€‚ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¨ã‚³ãƒ¼ãƒ‰ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1960840603224433155?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>pj page:


<a href="https://matrix-game-v2.github.io" target="_blank" rel="noopener noreferrer">https://matrix-game-v2.github.io</a>


</p>
<p>å…¬å¼:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/skywork_ai/status/1961271333003956461?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/CurriculumLearning.html" target="_blank" rel="noopener noreferrer">#CurriculumLearning</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2580" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Ovis2.5 Technical Report, Shiyin Lu+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Ovis2.5ã¯ã€ãƒã‚¤ãƒ†ã‚£ãƒ–è§£åƒåº¦ã®è¦–è¦šèªè­˜ã¨ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–ã‚’å¼·åŒ–ã™ã‚‹ãŸã‚ã«è¨­è¨ˆã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã€ç”»åƒã‚’å¯å¤‰è§£åƒåº¦ã§å‡¦ç†ã—ã€è¤‡é›‘ãªè¦–è¦šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è©³ç´°ã‚’ä¿æŒã—ã¾ã™ã€‚æ¨è«–æ™‚ã«ã¯åçœã‚’è¡Œã†ã€Œæ€è€ƒãƒ¢ãƒ¼ãƒ‰ã€ã‚’æä¾›ã—ã€ç²¾åº¦å‘ä¸Šã‚’å›³ã‚Šã¾ã™ã€‚5æ®µéšã®ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã§è¨“ç·´ã•ã‚Œã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ãƒ¼ã‚¿ã®åŠ¹ç‡çš„ãªå‡¦ç†ã‚’å®Ÿç¾ã€‚Ovis2.5-9Bã¯OpenCompassã§å¹³å‡78.3ã‚’è¨˜éŒ²ã—ã€Ovis2-8Bã«å¯¾ã—ã¦å¤§å¹…ãªæ”¹å–„ã‚’ç¤ºã—ã¾ã—ãŸã€‚Ovis2.5-2Bã‚‚73.9ã‚’é”æˆã—ã€ãƒªã‚½ãƒ¼ã‚¹åˆ¶ç´„ã®ã‚ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã«æœ€é©ã§ã™ã€‚STEMãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚„è¤‡é›‘ãªãƒãƒ£ãƒ¼ãƒˆåˆ†æã«ãŠã„ã¦ã‚‚å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1960840587168637183?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>HF:


<a href="https://huggingface.co/AIDC-AI/Ovis2.5-9B" target="_blank" rel="noopener noreferrer">https://huggingface.co/AIDC-AI/Ovis2.5-9B</a>


<br><br>Apache2.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹<br><br>GLM-4.1V-9B-Thinkingã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ãªæ¨¡æ§˜ã€‚<br><img src="https://github.com/user-attachments/assets/becc30fe-db20-40c1-a94c-143487ffd9ff" alt="image" loading="lazy"><br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2128" target="_blank" rel="noopener noreferrer">[Paper Note] GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable  Reinforcement Learning, GLM-V Team+, arXiv'25</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Hallucination.html" target="_blank" rel="noopener noreferrer">#Hallucination</a>
<a class="button" href="articles/SelfImprovement.html" target="_blank" rel="noopener noreferrer">#SelfImprovement</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2577" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Self-Rewarding Vision-Language Model via Reasoning Decomposition, Zongxia Li+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Vision-Language Models (VLMs)ã¯è¦–è¦šçš„å¹»è¦šã‚„è¨€èªçš„ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã«æ‚©ã¾ã•ã‚Œã‚‹ã“ã¨ãŒå¤šã„ã€‚ã“ã‚Œã‚‰ã®å•é¡Œã¯ã€ãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ãŒä¸­é–“ã®è¦–è¦šçš„æ¨è«–ã«å¯¾ã™ã‚‹æŒ‡å°ã‚’æ¬ ã„ã¦ã„ã‚‹ãŸã‚ã«ç”Ÿã˜ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€å¤–éƒ¨ã®è¦–è¦šçš„ç›£è¦–ã«ä¾å­˜ã›ãšã«è¦–è¦šçš„æ¨è«–ã‚’æ”¹å–„ã™ã‚‹è‡ªå·±å ±é…¬æ³•Vision-SR1ã‚’ææ¡ˆã€‚ãƒ¢ãƒ‡ãƒ«ã¯è¦–è¦šçš„çŸ¥è¦šã¨è¨€èªçš„æ¨è«–ã‚’2æ®µéšã«åˆ†è§£ã—ã€è‡ªå·±å®Œçµå‹ã®è¦–è¦šçš„çŸ¥è¦šã‚’ç”Ÿæˆã—ã€ãã®å¾Œã«è¨€èªçš„æ¨è«–ã‚’è¡Œã†ã“ã¨ã§å ±é…¬ã‚’è¨ˆç®—ã™ã‚‹ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€Vision-SR1ãŒè¦–è¦šçš„æ¨è«–ã‚’æ”¹å–„ã—ã€å¹»è¦šã‚’è»½æ¸›ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_akhaliq/status/1960888293031088273?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/aicia_solid/status/1960962793265578174?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2553" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,  Reasoning, and Efficiency, Weiyun Wang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- InternVL 3.5ã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®æ–°ã—ã„ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã§ã€Cascade Reinforcement Learningã‚’ç”¨ã„ã¦æ¨è«–èƒ½åŠ›ã¨åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ç²—ã‹ã‚‰ç´°ã¸ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã«ã‚ˆã‚Šã€MMMã‚„MathVistaãªã©ã®ã‚¿ã‚¹ã‚¯ã§å¤§å¹…ãªæ”¹å–„ã‚’å®Ÿç¾ã€‚Visual Resolution Routerã‚’å°å…¥ã—ã€è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã®è§£åƒåº¦ã‚’å‹•çš„ã«èª¿æ•´ã€‚Decoupled Vision-Language Deploymentæˆ¦ç•¥ã«ã‚ˆã‚Šã€è¨ˆç®—è² è·ã‚’ãƒãƒ©ãƒ³ã‚¹ã•ã›ã€æ¨è«–æ€§èƒ½ã‚’æœ€å¤§16.0%å‘ä¸Šã•ã›ã€é€Ÿåº¦ã‚’4.05å€å‘ä¸Šã€‚æœ€å¤§ãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®MLLMã§æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã—ã€å•†æ¥­ãƒ¢ãƒ‡ãƒ«ã¨ã®æ€§èƒ½ã‚®ãƒ£ãƒƒãƒ—ã‚’ç¸®å°ã€‚å…¨ã¦ã®ãƒ¢ãƒ‡ãƒ«ã¨ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1960076908088922147?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/zhihufrontier/status/1972502056209662441?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Multi.html" target="_blank" rel="noopener noreferrer">#Multi</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/LLMAgent.html" target="_blank" rel="noopener noreferrer">#LLMAgent</a>
<a class="button" href="articles/SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2531" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] ToolVQA: A Dataset for Multi-step Reasoning VQA with External Tools, Shaofeng Yin+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€å®Ÿä¸–ç•Œã®ãƒ„ãƒ¼ãƒ«ä½¿ç”¨èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€23Kã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‹ã‚‰ãªã‚‹å¤§è¦æ¨¡ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ŒToolVQAã€ã‚’ææ¡ˆã€‚ToolVQAã¯ã€å®Ÿéš›ã®è¦–è¦šçš„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨å¤šæ®µéšæ¨è«–ã‚¿ã‚¹ã‚¯ã‚’ç‰¹å¾´ã¨ã—ã€ToolEngineã‚’ç”¨ã„ã¦äººé–“ã®ã‚ˆã†ãªãƒ„ãƒ¼ãƒ«ä½¿ç”¨æ¨è«–ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã€‚7B LFMã‚’å¾®èª¿æ•´ã—ãŸçµæœã€ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã€GPT-3.5-turboã‚’ä¸Šå›ã‚‹ä¸€èˆ¬åŒ–èƒ½åŠ›ã‚’æŒã¤ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>äººé–“ã«ã‚ˆã‚‹å°è¦æ¨¡ãªã‚µãƒ³ãƒ—ãƒ«ï¼ˆã‚¤ãƒ¡ãƒ¼ã‚¸ã‚·ãƒŠãƒªã‚ªã€ãƒ„ãƒ¼ãƒ«ã‚»ãƒƒãƒˆã€ã‚¯ã‚¨ãƒªã€å›ç­”ã€tool use trajectory)ã‚’ç”¨ã„ã¦Foundation Modelã«äº‹å‰çŸ¥è­˜ã¨ã—ã¦ä¸ãˆã‚‹ã“ã¨ã§ã€ã‚ˆã‚ŠrealisticãªscenarioãŒåˆæˆã•ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸä¸Šã§æ–°ãŸãªVQAã‚’4kç¨‹åº¦åˆæˆã€‚ãã®å¾Œ10äººã®ã‚¢ãƒãƒ†ãƒ¼ã‚¿ã«ã‚ˆã£ã¦é«˜å“è³ªãªã‚µãƒ³ãƒ—ãƒ«ã«ã®ã¿Filteringã™ã‚‹ã“ã¨ã§ä½œæˆã•ã‚ŒãŸã€å¾“æ¥ã‚ˆã‚Šã‚‚å®Ÿä¸–ç•Œã®è¨­å®šã«è¿‘ãã€reasoningã®è¤‡é›‘ã•ãŒé«˜ã„VQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãªæ¨¡æ§˜ã€‚<br><br><img src="https://github.com/user-attachments/assets/8759244c-89f9-47d7-9c72-81744ef68db1" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/4bc22c9d-79f3-4c16-b5a4-3c054895b416" alt="image" loading="lazy"><br><br>å…·ä½“çš„ã«ã¯ã€image contextxãŒä¸ãˆã‚‰ã‚ŒãŸæ™‚ã«ã€ChatGPT-4oã‚’ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã¨ã—ã¦ã€å‰å›ã®ãƒ„ãƒ¼ãƒ«ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®é¸æŠã‚’givenã«ã—ã€äººé–“ãŒä½œæˆã—ãŸãƒ—ãƒ¼ãƒ«ã«å«ã¾ã‚Œã‚‹ã‚µãƒ³ãƒ—ãƒ«ã®ä¸­ã‹ã‚‰Longest Common Subsequence (LCS) ã«ã‚ˆã‚‹ä¸€è‡´åº¦åˆã„ã«åŸºã¥ã„ã¦äººæ‰‹ã«ã‚ˆã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚’é¸æŠã—ã€å‹•çš„ã«contextã«å«ã‚ã‚‹ã“ã¨ã§å¤šæ§˜ãªã§å®Ÿä¸–ç•Œã«ã‚ˆã‚Šè¿‘ã—ã„multi step tooluseãªtrajectoryã‚’åˆæˆã™ã‚‹ã€ã¨ã„ã£ãŸæ‰‹æ³•ã«è¦‹ãˆã‚‹ã€‚pp.4--5ã«æ•°å¼ã‚„å›³ã«ã‚ˆã‚‹ç›´æ„Ÿçš„ãªèª¬æ˜ãŒã‚ã‚‹ã€‚ãªãŠã€LCSã‚’å…·ä½“çš„ã«ã©ã®ã‚ˆã†ãªæ–‡å­—åˆ—ã«å¯¾ã—ã¦ã€ã©ã®ã‚ˆã†ãªå‰å‡¦ç†ã‚’ã—ãŸä¸Šã§é©ç”¨ã—ã¦ã„ã‚‹ã®ã‹ã¾ã§ã¯è¿½ãˆã¦ã„ãªã„ã€‚<br><img src="https://github.com/user-attachments/assets/9915c3d5-e984-4611-94d4-999ad08dc49d" alt="image" loading="lazy"></p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1959125184285483090?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LLMAgent.html" target="_blank" rel="noopener noreferrer">#LLMAgent</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/Factuality.html" target="_blank" rel="noopener noreferrer">#Factuality</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-08-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2518" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents, Shilong Li+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- MM-BrowseCompã¯ã€AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¤œç´¢ãŠã‚ˆã³æ¨è«–èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã€224ã®æ‰‹ä½œã‚Šã®è³ªå•ã‚’å«ã‚€ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç”»åƒã‚„å‹•ç”»ã‚’å«ã‚€æƒ…å ±ã®é‡è¦æ€§ã‚’è€ƒæ…®ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®æ‰‹æ³•ã®é™ç•Œã‚’ç¤ºã™ã€‚æœ€å…ˆç«¯ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã§ã¯ã€OpenAI o3ãªã©ã®ãƒˆãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ã§ã‚‚29.02%ã®ç²¾åº¦ã«ã¨ã©ã¾ã‚Šã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«èƒ½åŠ›ã®æœ€é©åŒ–ä¸è¶³ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gezhang86038849/status/1958381269617955165?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/QuestionAnswering.html" target="_blank" rel="noopener noreferrer">#QuestionAnswering</a>
<a class="button" href="articles/SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2502" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] VisualWebInstruct: Scaling up Multimodal Instruction Data through Web   Search, Yiming Jia+, EMNLP'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€æ¨è«–ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä¸è¶³ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€VisualWebInstructã¨ã„ã†æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã€‚30,000ã®ã‚·ãƒ¼ãƒ‰ç”»åƒã‹ã‚‰Googleç”»åƒæ¤œç´¢ã‚’ç”¨ã„ã¦700Kä»¥ä¸Šã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªURLã‚’åé›†ã—ã€ç´„900Kã®QAãƒšã‚¢ã‚’æ§‹ç¯‰ã€‚ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã€Llava-OVã§10-20ãƒã‚¤ãƒ³ãƒˆã€MAmmoTH-VLã§5ãƒã‚¤ãƒ³ãƒˆã®æ€§èƒ½å‘ä¸Šã‚’ç¤ºã—ã€æœ€è‰¯ãƒ¢ãƒ‡ãƒ«MAmmoTH-VL2ã¯è¤‡æ•°ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’é”æˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€Vision-Language Modelsã®æ¨è«–èƒ½åŠ›å‘ä¸Šã«å¯„ä¸ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenhuchen/status/1958317145349075446?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>pj page:


<a href="https://tiger-ai-lab.github.io/VisualWebInstruct/" target="_blank" rel="noopener noreferrer">https://tiger-ai-lab.github.io/VisualWebInstruct/</a>


</p>
<p>verified versionãŒå…¬é–‹:<br>


<a href="https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct_Verified" target="_blank" rel="noopener noreferrer">https://huggingface.co/datasets/TIGER-Lab/VisualWebInstruct_Verified</a>


<br><br>ãƒã‚¹ãƒˆ:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenhuchen/status/1981750996469449012?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/NormalizingFlow.html" target="_blank" rel="noopener noreferrer">#NormalizingFlow</a>
<span class="issue_date">Issue Date: 2025-08-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2456" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] JetFormer: An Autoregressive Generative Model of Raw Images and Text, Michael Tschannen+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- JetFormerã¯ã€ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®å…±åŒç”Ÿæˆã‚’åŠ¹ç‡åŒ–ã™ã‚‹è‡ªå·±å›å¸°å‹ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼å°‚ç”¨ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã§ã‚ã‚Šã€åˆ¥ã€…ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«ä¾å­˜ã›ãšã€ä¸¡ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’ç†è§£ãƒ»ç”Ÿæˆå¯èƒ½ã€‚æ­£è¦åŒ–ãƒ•ãƒ­ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®ç”Ÿæˆå“è³ªã§æ—¢å­˜ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ç«¶åˆã—ã¤ã¤ã€å …ç‰¢ãªç”»åƒç†è§£èƒ½åŠ›ã‚’ç¤ºã™ã€‚JetFormerã¯é«˜å¿ å®Ÿåº¦ã®ç”»åƒç”Ÿæˆã¨å¼·åŠ›ãªå¯¾æ•°å°¤åº¦å¢ƒç•Œã‚’å®Ÿç¾ã™ã‚‹åˆã®ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=sgAp2qG86e" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=sgAp2qG86e</a>


</p>
<p>ç”»åƒã‚’normalizing flowã§ã‚½ãƒ•ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›ã—ã€transformerã§ã‚½ãƒ•ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã•ã›ã‚‹ã‚ˆã†ã«å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã‚’åŒã˜ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§å­¦ç¿’ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸã€ã¿ãŸã„ãªè©±ã£ã½ã„ï¼ŸãŠã‚‚ã—ã‚ãã†<br><img src="https://github.com/user-attachments/assets/d8615d39-40bc-4470-8a20-4de574ab78ff" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/ComputerUse.html" target="_blank" rel="noopener noreferrer">#ComputerUse</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2447" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] UI-Venus Technical Report: Building High-performance UI Agents with RFT, Zhangxuan Gu+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- UI-Venusã¯ã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«åŸºã¥ããƒã‚¤ãƒ†ã‚£ãƒ–UIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã€UIã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã§æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’é”æˆã€‚7BãŠã‚ˆã³72Bãƒãƒªã‚¢ãƒ³ãƒˆã¯ã€Screenspot-V2 / Proãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§é«˜ã„æˆåŠŸç‡ã‚’è¨˜éŒ²ã—ã€æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã€‚å ±é…¬é–¢æ•°ã‚„ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã‚’å°å…¥ã—ã€ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®æ–°ã—ã„è‡ªå·±é€²åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚‚ææ¡ˆã€‚ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®UIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å…¬é–‹ã—ã€ã•ã‚‰ãªã‚‹ç ”ç©¶ã‚’ä¿ƒé€²ã€‚ã‚³ãƒ¼ãƒ‰ã¯GitHubã§å…¥æ‰‹å¯èƒ½ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_akhaliq/status/1956344636831662567?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1957262667493826891?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>HF:


<a href="https://huggingface.co/collections/inclusionAI/ui-venus-689f2fb01a4234cbce91c56a" target="_blank" rel="noopener noreferrer">https://huggingface.co/collections/inclusionAI/ui-venus-689f2fb01a4234cbce91c56a</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/interactive.html" target="_blank" rel="noopener noreferrer">#interactive</a>
<a class="button" href="articles/Game.html" target="_blank" rel="noopener noreferrer">#Game</a>
<span class="issue_date">Issue Date: 2025-08-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2429" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Hunyuan-GameCraft: High-dynamic Interactive Game Video Generation with  Hybrid History Condition, Jiaqi Li+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ã€ŒHunyuan-GameCraftã€ã¨ã„ã†æ–°ã—ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€ã‚²ãƒ¼ãƒ ç’°å¢ƒã«ãŠã‘ã‚‹é«˜ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–å‹•ç”»ç”Ÿæˆã‚’å®Ÿç¾ã€‚ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ã¨ãƒã‚¦ã‚¹ã®å…¥åŠ›ã‚’çµ±åˆã—ã€å‹•ç”»ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’è‡ªå·±å›å¸°çš„ã«æ‹¡å¼µã™ã‚‹ã“ã¨ã§ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³åˆ¶å¾¡ã¨ä¸€è²«æ€§ã‚’å‘ä¸Šã€‚å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€è¦–è¦šçš„å¿ å®Ÿæ€§ã¨ãƒªã‚¢ãƒªã‚ºãƒ ã‚’å¼·åŒ–ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/tencenthunyuan/status/1955839140173631656?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å˜ä½“ã®ç”»åƒã¨ã€promptã€ãƒã‚¦ã‚¹ãƒ»ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰å…¥åŠ›ã«åŸºã¥ã„ã¦interactiveã«å‹•ç”»ã‚’åˆæˆã™ã‚‹ã€‚è»½é‡ãªGPUã§ã‚‚å‹•ä½œã™ã‚‹ã‚ˆã†ã«ã€é«˜å“è³ªãªåˆæˆãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã£ã¦ãƒ¢ãƒ‡ãƒ«ã‚’è’¸ç•™ã—è»½é‡ãªãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã—ãŸã‚Šã‚‚ã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚ãã®ã†ã¡å®¶åº­ã®ã‚²ãƒ¼ãƒŸãƒ³ã‚°PCã§ã“ã†ã„ã£ãŸãƒ¢ãƒ‡ãƒ«ã§ã‚²ãƒ¼ãƒ ã‚’ã™ã‚‹æ—¥ãŒæ¥ã‚‹ã®ã ã‚ã†ã‹ã€‚<br><img src="https://github.com/user-attachments/assets/c301284d-1003-4dd0-a5cf-89dd44fc8b56" alt="image" loading="lazy"></p>
<p>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ä½¿ã‚ã‚Œã¦ã„ã‚‹æŠ€è¡“:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2526" target="_blank" rel="noopener noreferrer">[Paper Note] DiT: Self-supervised Pre-training for Document Image Transformer, Junlong Li+, ACMMM'22</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/550" target="_blank" rel="noopener noreferrer">Learning Transferable Visual Models From Natural Language Supervision, Radford+, OpenAI, ICML'21</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/LLMAgent.html" target="_blank" rel="noopener noreferrer">#LLMAgent</a>
<a class="button" href="articles/SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/DeepResearch.html" target="_blank" rel="noopener noreferrer">#DeepResearch</a>
<span class="issue_date">Issue Date: 2025-08-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2424" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent, Xinyu Geng+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- WebWatcherã¯ã€è¦–è¦šã¨è¨€èªã®æ¨è«–èƒ½åŠ›ã‚’å¼·åŒ–ã—ãŸãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã‚ã‚Šã€æƒ…å ±æ¢ç´¢ã®å›°é›£ã•ã«å¯¾å‡¦ã™ã‚‹ã€‚åˆæˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è»Œè·¡ã‚’ç”¨ã„ãŸåŠ¹ç‡çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚Šã€æ·±ã„æ¨è«–èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚æ–°ãŸã«ææ¡ˆã•ã‚ŒãŸBrowseComp-VLãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®å®Ÿé¨“ã«ã‚ˆã‚Šã€WebWatcherã¯è¤‡é›‘ãªVQAã‚¿ã‚¹ã‚¯ã§ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å¤§å¹…ã«ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/richardxp888/status/1955645614685077796?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å…¬å¼:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/ali_tongyilab/status/1961348492506665289?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/Cultural.html" target="_blank" rel="noopener noreferrer">#Cultural</a>
<span class="issue_date">Issue Date: 2025-08-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2413" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Grounding Multilingual Multimodal LLMs With Cultural Knowledge, Jean de Dieu Nyandwi+, EMNLP'25</a>
<span class="snippet"><span>GPT Summary</span>- MLLMsã¯é«˜ãƒªã‚½ãƒ¼ã‚¹ç’°å¢ƒã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ãŒã€ä½ãƒªã‚½ãƒ¼ã‚¹è¨€èªã‚„æ–‡åŒ–çš„ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã«å¯¾ã—ã¦ã¯èª²é¡ŒãŒã‚ã‚‹ã€‚ã“ã‚Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€Wikidataã‚’æ´»ç”¨ã—ã€æ–‡åŒ–çš„ã«é‡è¦ãªã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’è¡¨ã™ç”»åƒã‚’ç”¨ã„ãŸå¤šè¨€èªè¦–è¦šè³ªå•å¿œç­”ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ŒCulturalGroundã€ã‚’ç”Ÿæˆã€‚CulturalPangeaã¨ã„ã†ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®MLLMã‚’è¨“ç·´ã—ã€æ–‡åŒ–ã«åŸºã¥ã„ãŸã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒMLLMsã®æ–‡åŒ–çš„ã‚®ãƒ£ãƒƒãƒ—ã‚’ç¸®å°ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚CulturalPangeaã¯ã€å¾“æ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚’å¹³å‡5.0ãƒã‚¤ãƒ³ãƒˆä¸Šå›ã‚‹æ€§èƒ½ã‚’é”æˆã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gneubig/status/1955308632305782957?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>pj page:


<a href="https://neulab.github.io/CulturalGround/" target="_blank" rel="noopener noreferrer">https://neulab.github.io/CulturalGround/</a>


<br><br>VQAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸­ã®æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã¯3.1%ç¨‹åº¦ã§ã€&lt;image, Question, answer&gt;ã®3ã¤çµ„ã§æ§‹æˆã•ã‚Œã‚‹ã€‚wikidataã‹ã‚‰ç‰¹å®šã®æ–‡åŒ–ã¨ç´ã¥ã„ãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ï¼ˆ42ã‚«å›½; äºº,å ´æ‰€,çµ„ç¹”,ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ï¼‰ã‚’æŠ½å‡ºã—ã€é–¢é€£ã™ã‚‹image dataã‚’1--3å€‹ç¨‹åº¦wikimediaã‹ã‚‰åé›†ã€‚76ç¨®é¡ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”¨ã„ã¦ã€draftã®QAã‚’ç”Ÿæˆã—ã€LLMã‚’ç”¨ã„ã¦æ´—ç·´ï¼ˆæ–‡åŒ–çš„ãªè‡ªç„¶ã•ã€æµæš¢ã•ï¼‰ã•ã›ã‚‹ã€‚æœ€çµ‚çš„ã«VLM(Qwen2.5-VL-32B/72B or Gemma-3-12B/72B-Instructã‚’æ–‡åŒ–ã”ã¨ã«å¼·ã„æ–¹ã‚’é¸æŠã—ã¦åˆ©ç”¨)ã‚’ç”¨ã„ã¦irrelevantãªimage, question, answerã®ä¸‰ã¤çµ„ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆrelevanceã®ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã¨äº‹å®Ÿæƒ…å ±ã®verification)ã™ã‚‹ã€‚<br><br>ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2470" target="_blank" rel="noopener noreferrer">[Paper Note] Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages, Xiang Yue+, arXiv'24</a>
<br><br>ã‚’åˆ©ç”¨(Qwen2-7Bã«å¯¾ã—ã¦CLIPãƒ™ãƒ¼ã‚¹ã®vision encoderã‚’åˆ©ç”¨ã—ãŸVLM)ã—ã€Vision Encoderã¯frozenã—ã€LLMã¨connectorï¼ˆãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®æ©‹æ¸¡ã—ã‚’ã™ã‚‹ï¼ˆå¤§æŠµã¯ï¼‰MLP)ã®ã¿ã‚’finetuningã—ãŸã€‚catastrophic forgettingã‚’é˜²ããŸã‚ã«äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨ã‚’è£œå®Œã—finetuningã§ã‚‚åˆ©ç”¨ã—ã€ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®èªè­˜åŠ›ã‚’é«˜ã‚ã‚‹ãŸã‚ã«M3LSãƒ‡ãƒ¼ã‚¿ãªã‚‹ã‚‚ã®ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦è¿½åŠ ã—ã¦ã„ã‚‹ã€‚<br><br>Finetuningã®çµæœã€æ–‡åŒ–çš„ãªå¤šæ§˜æ€§ã‚’æŒã¤è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ï¼ˆe.g., <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2471" target="_blank" rel="noopener noreferrer">[Paper Note] CVQA: Culturally-diverse Multilingual Visual Question Answering
  Benchmark, David Romero+, arXiv'24</a>
 Figure1ã®Japaneseã®ã‚µãƒ³ãƒ—ãƒ«ã‚’è¦‹ã‚‹ã¨ä¸€ç›®ã§ã©ã®ã‚ˆã†ãªãƒ™ãƒ³ãƒã‹åˆ†ã‹ã‚‹ï¼‰ã¨ä¸€èˆ¬çš„ãªãƒãƒ«ãƒãƒªãƒ³ã‚¬ãƒ«ãªè©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®åŒæ–¹ã§gainãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã€‚<br><img src="https://github.com/user-attachments/assets/61b33047-4c7c-4785-99f7-bcaa131bcfbf" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/8088e61f-ef46-4bcd-bc94-8d6f6318ca0e" alt="image" loading="lazy"><br><br>VQAã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§åˆ©ç”¨ã•ã‚ŒãŸpromptã¯ä¸‹è¨˜<br><img src="https://github.com/user-attachments/assets/a9c5b463-a3e3-4565-b2f2-95268252179d" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/GRPO.html" target="_blank" rel="noopener noreferrer">#GRPO</a>
<a class="button" href="articles/On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<span class="issue_date">Issue Date: 2025-08-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2410" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] AR-GRPO: Training Autoregressive Image Generation Models via  Reinforcement Learning, Shihao Yuan+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- AR-GRPOã¯ã€è‡ªå·±å›å¸°ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ã‚’çµ±åˆã—ãŸæ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã€ç”Ÿæˆç”»åƒã®å“è³ªã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«GRPOã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é©ç”¨ã€‚ã‚¯ãƒ©ã‚¹æ¡ä»¶ãŠã‚ˆã³ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ã®ç”»åƒç”Ÿæˆã‚¿ã‚¹ã‚¯ã§å®Ÿé¨“ã‚’è¡Œã„ã€æ¨™æº–ã®ARãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦å“è³ªã¨äººé–“ã®å¥½ã¿ã‚’å¤§å¹…ã«æ”¹å–„ã—ãŸã€‚çµæœã¯ã€ARç”»åƒç”Ÿæˆã«ãŠã‘ã‚‹å¼·åŒ–å­¦ç¿’ã®æœ‰åŠ¹æ€§ã‚’ç¤ºã—ã€é«˜å“è³ªãªç”»åƒåˆæˆã®æ–°ãŸãªå¯èƒ½æ€§ã‚’é–‹ãã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1955234358136373421?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2456" target="_blank" rel="noopener noreferrer">[Paper Note] JetFormer: An Autoregressive Generative Model of Raw Images and Text, Michael Tschannen+, ICLR'25</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/VisionLanguageActionModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageActionModel</a>
<span class="issue_date">Issue Date: 2025-08-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2407" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] MolmoAct: Action Reasoning Models that can Reason in Space, Jason Lee+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ¨è«–ãƒ¢ãƒ‡ãƒ«ï¼ˆARMsï¼‰ã§ã‚ã‚‹MolmoActã¯ã€çŸ¥è¦šã€è¨ˆç”»ã€åˆ¶å¾¡ã‚’ä¸‰æ®µéšã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§çµ±åˆã—ã€èª¬æ˜å¯èƒ½ã§æ“ä½œå¯èƒ½ãªè¡Œå‹•ã‚’å®Ÿç¾ã€‚ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨å®Ÿä¸–ç•Œã§é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã€ç‰¹ã«SimplerEnv Visual Matchingã‚¿ã‚¹ã‚¯ã§70.5%ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆç²¾åº¦ã‚’é”æˆã€‚MolmoAct Datasetã‚’å…¬é–‹ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚Šãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¹³å‡5.5%å‘ä¸Šã€‚å…¨ã¦ã®ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å…¬é–‹ã—ã€ARMsã®æ§‹ç¯‰ã«å‘ã‘ãŸã‚ªãƒ¼ãƒ—ãƒ³ãªè¨­è¨ˆå›³ã‚’æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span><p>`Action Reasoning Models (ARMs)`<br><br>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1955168414294589844?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br>blog: 


<a href="https://allenai.org/blog/molmoact" target="_blank" rel="noopener noreferrer">https://allenai.org/blog/molmoact</a>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1426" target="_blank" rel="noopener noreferrer">Molmo, AI2, 2024.09</a>
</p>
<p>models:<br>- 


<a href="https://huggingface.co/allenai/MolmoAct-7B-D-Pretrain-0812" target="_blank" rel="noopener noreferrer">https://huggingface.co/allenai/MolmoAct-7B-D-Pretrain-0812</a>


<br>- 


<a href="https://huggingface.co/allenai/MolmoAct-7B-D-0812" target="_blank" rel="noopener noreferrer">https://huggingface.co/allenai/MolmoAct-7B-D-0812</a>


<br><br>datasets:<br>- 


<a href="https://huggingface.co/datasets/allenai/MolmoAct-Dataset" target="_blank" rel="noopener noreferrer">https://huggingface.co/datasets/allenai/MolmoAct-Dataset</a>


<br>- 


<a href="https://huggingface.co/datasets/allenai/MolmoAct-Pretraining-Mixture" target="_blank" rel="noopener noreferrer">https://huggingface.co/datasets/allenai/MolmoAct-Pretraining-Mixture</a>


<br>- 


<a href="https://huggingface.co/datasets/allenai/MolmoAct-Midtraining-Mixture" target="_blank" rel="noopener noreferrer">https://huggingface.co/datasets/allenai/MolmoAct-Midtraining-Mixture</a>


</p>
<p>ãƒ‡ãƒ¼ã‚¿ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ãŒã€ã‚³ãƒ¼ãƒ‰ãŒè¦‹å½“ãŸã‚‰ãªã„ï¼Ÿ</p>
<p>ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ã‚³ãƒ¼ãƒ‰ã‚‚å…¬é–‹ã•ã‚ŒãŸæ¨¡æ§˜:<br>- 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/djiafei/status/1964319001053372455?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br>- 


<a href="https://github.com/allenai/MolmoAct" target="_blank" rel="noopener noreferrer">https://github.com/allenai/MolmoAct</a>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/RLVR.html" target="_blank" rel="noopener noreferrer">#RLVR</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2392" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable  Reward Models, Xiangxiang Zhang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- StructVRMã¯ã€è¤‡é›‘ãªå¤šè³ªå•æ¨è«–ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€éƒ¨åˆ†çš„ãªæ­£ç¢ºæ€§ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®æ§‹é€ åŒ–ã•ã‚ŒãŸæ¤œè¨¼å¯èƒ½ãªå ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’å°å…¥ã€‚ã‚µãƒ–è³ªå•ãƒ¬ãƒ™ãƒ«ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã—ã€å¾®å¦™ãªéƒ¨åˆ†çš„ãªã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€Seed-StructVRMãŒ12ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®ã†ã¡6ã¤ã§æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ãŸã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ã“ã‚Œã¯ã€è¤‡é›‘ãªæ¨è«–ã«ãŠã‘ã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›å‘ä¸Šã«å¯„ä¸ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1954315513397760130?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è¤‡æ•°ã®sub-questionãŒå­˜åœ¨ã™ã‚‹ã‚ˆã†ãªè¤‡é›‘ãªå•é¡Œã«å¯¾ã—ã¦ã€æ—¢å­˜ã®RLVRã«ãŠã‘ã‚‹å…¨ä½“ã«å¯¾ã—ã¦binary rewardã‚’é©ç”¨ã™ã‚‹æ–¹æ³•ã¯å ±é…¬ãŒè’ã™ãã‚‹ãŸã‚ã€ã‚ˆã‚Šfine-grainedãªverifiableãªå ±é…¬ã‚’è¨­è¨ˆã™ã‚‹ã“ã¨ã§ã€å­¦ç¿’ã‚’å®‰å®šåŒ–ã—æ€§èƒ½ã‚‚å‘ä¸Š<br><img src="https://github.com/user-attachments/assets/e3bf6ca8-6873-4d42-83c8-4e9148c16d1d" alt="image" loading="lazy"><br><br>ä»¥ä¸‹ãŒverifierã®ã‚µãƒ³ãƒ—ãƒ«<br><img src="https://github.com/user-attachments/assets/c02274a4-5979-402c-a9c8-145cb1b284bf" alt="image" loading="lazy"></p>
<p>general purposeãªreal worldã«å¯¾ã™ã‚‹multimodal reasoningã‚·ã‚¹ãƒ†ãƒ ã‚’ä½œæˆã™ã‚‹ã«ã¯é«˜å“è³ªã§å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ãªã®ã§ã€ä»¥ä¸‹ã®ã‚ˆã†ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ç”¨ã„ã¦ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’åˆæˆã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚å¾Œã§èª­ã‚€ã€‚ã‚µãƒãƒªãŒå…ƒãƒã‚¹ãƒˆã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã®ã§å…¨ä½“åƒã‚’ã–ã£ãã‚ŠçŸ¥ã‚ŠãŸã„å ´åˆã¯å‚ç…§ã®ã“ã¨ã€‚<br><img src="https://github.com/user-attachments/assets/6f3b7503-cd3a-4d32-9080-51b875901c23" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/ContrastiveLearning.html" target="_blank" rel="noopener noreferrer">#ContrastiveLearning</a>
<a class="button" href="articles/Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<span class="issue_date">Issue Date: 2025-08-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2370" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Scaling Vision Pre-Training to 4K Resolution, Baifeng Shi+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- PS3ã‚’ç”¨ã„ã¦CLIPã‚¹ã‚¿ã‚¤ãƒ«ã®è¦–è¦šäº‹å‰å­¦ç¿’ã‚’4Kè§£åƒåº¦ã«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã—ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’æŠ‘ãˆã¤ã¤é«˜è§£åƒåº¦ã®è¦–è¦šèªè­˜ã‚’æ”¹å–„ã€‚VILA-HDãƒ¢ãƒ‡ãƒ«ã¯ã€ä½è§£åƒåº¦ã§ã®ã‚°ãƒ­ãƒ¼ãƒãƒ«ç”»åƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã‚’è¡Œã„ã€å±€æ‰€çš„ãªé«˜è§£åƒåº¦é ˜åŸŸã‚’é¸æŠçš„ã«å‡¦ç†ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å¾“æ¥ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒã—ã¦é«˜ã„æ€§èƒ½ã‚’ç™ºæ®ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’æœ€å¤§4.3å€å‰Šæ¸›ã€‚PS3ã¯è§£åƒåº¦ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç‰¹æ€§ã‚’æŒã¡ã€è¤‡æ•°ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å„ªã‚ŒãŸåŠ¹ç‡ã‚’é”æˆã€‚æ–°ãŸã«ææ¡ˆã•ã‚ŒãŸ4KProãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã¯ã€VILA-HDãŒä»–ã®MLLMã‚’ä¸Šå›ã‚‹çµæœã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/baifeng_shi/status/1952898951662977199?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å•†ç”¨åˆ©ç”¨ã¯ä¸å¯ãªæ¨¡æ§˜</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<span class="issue_date">Issue Date: 2025-08-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2344" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] BUFFER-X: Towards Zero-Shot Point Cloud Registration in Diverse Scenes, Minkyun Seo+, ICCV'25</a>
<span class="snippet"><span>GPT Summary</span>- BUFFER-Xã¨ã„ã†ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆç™»éŒ²ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ææ¡ˆã—ã€ç’°å¢ƒç‰¹æœ‰ã®ãƒœã‚¯ã‚»ãƒ«ã‚µã‚¤ã‚ºã‚„æ¢ç´¢åŠå¾„ã¸ã®ä¾å­˜ã€ãƒ‰ãƒ¡ã‚¤ãƒ³å¤–ãƒ­ãƒã‚¹ãƒˆæ€§ã®ä½ã•ã€ã‚¹ã‚±ãƒ¼ãƒ«ä¸ä¸€è‡´ã®å•é¡Œã«å¯¾å‡¦ã€‚ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã®ãƒ‘ãƒƒãƒãƒ™ãƒ¼ã‚¹ã®è¨˜è¿°å­ç”Ÿæˆã¨éšå±¤çš„ã‚¤ãƒ³ãƒ©ã‚¤ã‚¢æ¤œç´¢ã‚’ç”¨ã„ã¦ã€ã•ã¾ã–ã¾ãªã‚·ãƒ¼ãƒ³ã§ã®ãƒ­ãƒã‚¹ãƒˆæ€§ã‚’å‘ä¸Šã€‚æ–°ã—ã„ä¸€èˆ¬åŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ç”¨ã„ã¦ã€BUFFER-XãŒæ‰‹å‹•èª¿æ•´ãªã—ã§å¤§å¹…ãªä¸€èˆ¬åŒ–ã‚’é”æˆã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rsasaki0109/status/1951478059002966159?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã“ã®è¾ºã®åˆ†é‡ã±ã£ã¨è¦‹ã§å…¨ç„¶ã‚ã‹ã‚‰ãªã„â€¦</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/MultiLingual.html" target="_blank" rel="noopener noreferrer">#MultiLingual</a>
<a class="button" href="articles/CLIP.html" target="_blank" rel="noopener noreferrer">#CLIP</a>
<span class="issue_date">Issue Date: 2025-07-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2320" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] MetaCLIP 2: A Worldwide Scaling Recipe, Yung-Sung Chuang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- MetaCLIP 2ã‚’ææ¡ˆã—ã€CLIPã‚’ã‚¼ãƒ­ã‹ã‚‰è¨“ç·´ã™ã‚‹ãŸã‚ã®æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç¤ºã™ã€‚è‹±èªã¨éè‹±èªãƒ‡ãƒ¼ã‚¿ã®ç›¸äº’åˆ©ç›Šã‚’å¾—ã‚‹ãŸã‚ã®æœ€å°é™ã®å¤‰æ›´ã‚’åŠ ãˆã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã®ImageNetåˆ†é¡ã§è‹±èªå°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’é”æˆã€‚å¤šè¨€èªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã‚‚æ–°ãŸãªæœ€å…ˆç«¯ã‚’è¨˜éŒ²ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jaseweston/status/1950366185742016935?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<span class="issue_date">Issue Date: 2025-07-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2300" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Ming-Omni: A Unified Multimodal Model for Perception and Generation, Inclusion AI+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Ming-Omniã¯ã€ç”»åƒã€ãƒ†ã‚­ã‚¹ãƒˆã€éŸ³å£°ã€å‹•ç”»ã‚’å‡¦ç†ã§ãã‚‹çµ±ä¸€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã€éŸ³å£°ç”Ÿæˆã¨ç”»åƒç”Ÿæˆã«ãŠã„ã¦å„ªã‚ŒãŸèƒ½åŠ›ã‚’ç¤ºã™ã€‚å°‚ç”¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’ç”¨ã„ã¦ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‹ã‚‰ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŠ½å‡ºã—ã€MoEã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§å‡¦ç†ã™ã‚‹ã“ã¨ã§ã€åŠ¹ç‡çš„ã«ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ›ã‚’èåˆã€‚éŸ³å£°ãƒ‡ã‚³ãƒ¼ãƒ€ã¨é«˜å“è³ªãªç”»åƒç”Ÿæˆã‚’çµ±åˆã—ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¿œã˜ãŸãƒãƒ£ãƒƒãƒˆã‚„ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰éŸ³å£°ã¸ã®å¤‰æ›ã€ç”»åƒç·¨é›†ãŒå¯èƒ½ã€‚Ming-Omniã¯ã€GPT-4oã«åŒ¹æ•µã™ã‚‹åˆã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ç ”ç©¶ã¨é–‹ç™ºã‚’ä¿ƒé€²ã™ã‚‹ãŸã‚ã«ã‚³ãƒ¼ãƒ‰ã¨ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span><p><img src="https://github.com/user-attachments/assets/62fe9563-ed6b-40bf-ad95-067407534626" alt="image" loading="lazy"></p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1948878025757446389?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>ç¾åœ¨ã¯v1.5ã‚‚å…¬é–‹ã•ã‚Œã¦ãŠã‚Šã•ã‚‰ã«æ€§èƒ½ãŒå‘ä¸Šã—ã¦ã„ã‚‹æ¨¡æ§˜ï¼Ÿ<p>HF:


<a href="https://huggingface.co/inclusionAI/Ming-Lite-Omni" target="_blank" rel="noopener noreferrer">https://huggingface.co/inclusionAI/Ming-Lite-Omni</a>


</p></span><br><br>
<a class="button" href="articles/NaturalLanguageGeneration.html" target="_blank" rel="noopener noreferrer">#NaturalLanguageGeneration</a>
<a class="button" href="articles/Controllable.html" target="_blank" rel="noopener noreferrer">#Controllable</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2297" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning, Kuniaki Saito+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- CaptionSmithsã¯ã€ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ãŒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®ç‰¹æ€§ï¼ˆé•·ã•ã€è¨˜è¿°æ€§ã€å˜èªã®ç‹¬è‡ªæ€§ï¼‰ã‚’æŸ”è»Ÿã«åˆ¶å¾¡ã§ãã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã€‚äººé–“ã®æ³¨é‡ˆãªã—ã§ç‰¹æ€§ã‚’å®šé‡åŒ–ã—ã€çŸ­ã„ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã¨é•·ã„ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®é–“ã§è£œé–“ã™ã‚‹ã“ã¨ã§æ¡ä»¶ä»˜ã‘ã‚’å®Ÿç¾ã€‚å®Ÿè¨¼çµæœã§ã¯ã€å‡ºåŠ›ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®ç‰¹æ€§ã‚’ã‚¹ãƒ ãƒ¼ã‚ºã«å¤‰åŒ–ã•ã›ã€èªå½™çš„æ•´åˆæ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã€èª¤å·®ã‚’506%å‰Šæ¸›ã€‚ã‚³ãƒ¼ãƒ‰ã¯GitHubã§å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/a_hasimoto/status/1948258269668970782?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å¾“æ¥ã¯Discreteã«è¡¨ç¾ã•ã‚Œã¦ã„ãŸcaptioningã«ãŠã‘ã‚‹ç‰¹æ€§ã‚’Condition Caluculatorã‚’å°å…¥ã™ã‚‹ã“ã¨ã§continuousãªrepresentationã«ã‚ˆã£ã¦è¡¨ç¾ã—ã€Caluculatorã«äººé–“ã«ã‚ˆã‚‹input, ã‚ã‚‹ã„ã¯è¡¨ç¾ã—ãŸã„Conditionã‚’æŒã¤exampleã‚’inputã™ã‚‹ã“ã¨ã§ã€ç”Ÿæˆæ™‚ã«åæ˜ ã•ã›ã‚‹ã‚ˆã†ãªæ‰‹æ³•ã‚’ææ¡ˆã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚Conditionã§åˆ©ç”¨ã™ã‚‹propertyã«ã¤ã„ã¦ã¯ã€ææ¡ˆæ‰‹æ³•ã§ã¯Length, Descriptive, Uniqueness of Vocabulariesã®3ã¤ã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹ï¼ˆãŒã€ä»–ã®propertyã§ã‚‚æœ¬æ‰‹æ³•ã¯é©ç”¨å¯èƒ½ã¨æ€ã‚ã‚Œã‚‹ï¼‰ã€‚ã“ã®ã¨ãã€ã‚ã‚‹propertyã®å€¤ã‚’å¤‰ãˆã‚‹ã“ã¨ã§ä»–ã®propertyãŒå¤‰åŒ–ã—ã¦ã—ã¾ã†ã¨åˆ¶å¾¡ãŒã§ããªããªã‚‹ãŸã‚ã€propertyé–“ã®decorrelationã‚’å®Ÿæ–½ã—ã¦ã„ã‚‹ã€‚ã“ã‚Œã¯ã€ã‚ã‚‹property Aã‹ã‚‰åˆ¥ã®property Bã®å€¤ã‚’äºˆæ¸¬ã—ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã®propertyã®å€¤ã‹ã‚‰subtractã™ã‚‹ã€ã¨ã„ã£ãŸå‡¦ç†ã‚’é †æ¬¡propertyã”ã¨ã«å®Ÿæ–½ã™ã‚‹ã“ã¨ã§å®Ÿç¾ã•ã‚Œã‚‹ã€‚Appendixã«è©³ç´°ãŒè¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ã€‚<br><br><img src="https://github.com/user-attachments/assets/673a2b9d-d630-4328-b619-f5382bb74f27" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/a90aa9d1-27f1-45c0-819e-c81b93364c68" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/4D%20Reconstruction.html" target="_blank" rel="noopener noreferrer">#4D Reconstruction</a>
<span class="issue_date">Issue Date: 2025-07-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2246" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Streaming 4D Visual Geometry Transformer, Dong Zhuo+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- å‹•ç”»ã‹ã‚‰4Dç©ºé–“-æ™‚é–“å¹¾ä½•å­¦ã‚’èªè­˜ãƒ»å†æ§‹ç¯‰ã™ã‚‹ãŸã‚ã«ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°4Dãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¸ã‚ªãƒ¡ãƒˆãƒªãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ææ¡ˆã€‚å› æœãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç”¨ã„ã¦ã€éå»ã®æƒ…å ±ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ãªãŒã‚‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§4Då†æ§‹ç¯‰ã‚’å®Ÿç¾ã€‚åŠ¹ç‡çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŸã‚ã«ã€åŒæ–¹å‘ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¸ã‚ªãƒ¡ãƒˆãƒªã‹ã‚‰ã®çŸ¥è­˜è’¸ç•™ã‚’è¡Œã„ã€æ¨è«–é€Ÿåº¦ã‚’å‘ä¸Šã•ã›ã¤ã¤ç«¶äº‰åŠ›ã®ã‚ã‚‹æ€§èƒ½ã‚’ç¶­æŒã€‚ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãª4Dãƒ“ã‚¸ãƒ§ãƒ³ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿç¾ã«å¯„ä¸ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/zhenjun_zhao/status/1945427634642424188?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£<br><img src="https://github.com/user-attachments/assets/4aafda63-cbdb-4823-908b-6ef0732f339b" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<a class="button" href="articles/Finetuning.html" target="_blank" rel="noopener noreferrer">#Finetuning</a>
<span class="issue_date">Issue Date: 2025-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2206" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision   Transformers under Domain Shifts, Samar Khanna+, ICML'25</a>
<span class="snippet"><span>GPT Summary</span>- PEFTæŠ€è¡“ã‚’ç”¨ã„ãŸExPLoRAã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ“ã‚¸ãƒ§ãƒ³ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ï¼ˆViTï¼‰ã‚’æ–°ã—ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ã«é©å¿œã•ã›ã‚‹æ‰‹æ³•ã§ã€æ•™å¸«ãªã—äº‹å‰å­¦ç¿’ã‚’é€šã˜ã¦åŠ¹ç‡çš„ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†ã€‚å®Ÿé¨“ã§ã¯ã€è¡›æ˜Ÿç”»åƒã«ãŠã„ã¦æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã—ã€å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚ˆã‚Šã‚‚å°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ç²¾åº¦ã‚’æœ€å¤§8%å‘ä¸Šã•ã›ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/samar_a_khanna/status/1944781066591748336?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã“ã‚Œã¾ã§ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œã™ã‚‹å ´åˆã«ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿+LoRAã§Finetuningã—ã¦ã„ãŸã®ã‚’ã€ãƒ©ãƒ™ãƒ«ç„¡ã—ãƒ‡ãƒ¼ã‚¿+ç¶™ç¶šäº‹å‰å­¦ç¿’ã®æ çµ„ã¿ã§ã‚„ã‚Šã¾ã—ã‚‡ã†ã€ã¨ã„ã†è©±ã®ã‚ˆã†ã§ã‚ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/dcae10cf-6b5d-4b29-8d9a-a94227f29a11" alt="image" loading="lazy"><br><br>æ‰‹æ³•ã¯ä¸‹è¨˜ã§ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦LoRAã‚’é©ç”¨ã—ç¶™ç¶šäº‹å‰å­¦ç¿’ã™ã‚‹ã€‚ãŸã ã—ã€æœ€å¾Œå°¾ã®Layerã€ã‚ã‚‹ã„ã¯æœ€åˆã¨æœ€å¾Œå°¾ã®Layerã®ä¸¡æ–¹ã‚’unfreezeã—ã¦ã€trainableã«ã™ã‚‹ã€‚ã¾ãŸã€LoRAã¯freezeã—ãŸLayerã®Q,Vã«é©ç”¨ã—ã€ãã‚Œã‚‰ã®Layerã®normalization layerã‚‚unfreezeã™ã‚‹ã€‚æœ€çµ‚çš„ã«ã€ç¶™ç¶šäº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã«ãƒ˜ãƒƒãƒ‰ã‚’concatã—ã¦finetuningã™ã‚‹ã“ã¨ã§ç›®çš„ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚è©³ç´°ã¯Algorithm1ã‚’å‚ç…§ã®ã“ã¨ã€‚<br><br><img src="https://github.com/user-attachments/assets/6b7ef497-2253-46c9-bbe7-ffdd50765fa3" alt="image" loading="lazy"><br><br>åŒã˜ãƒ¢ãƒ‡ãƒ«ã§å˜ã«LoRAã‚’é©ç”¨ã—ãŸã ã‘ã®æ‰‹æ³•ã‚„ã€æ—¢å­˜æ‰‹æ³•ã‚’outperform<br><br>&lt;img width="679" height="364" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/14935879-75a4-4e4a-a176-1b1eabc4b8fd"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/14935879-75a4-4e4a-a176-1b1eabc4b8fd"&lt;/a&gt;


/&gt;</p>
<p>ç”»åƒ+ViTç³»ã®ãƒ¢ãƒ‡ãƒ«ã ã‘ã§å®Ÿé¨“ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã‚‹ãŒã€LLMã¨ã‹ã«ã‚‚å¿œç”¨å¯èƒ½ã ã¨æ€ã‚ã‚Œã‚‹ã€‚<br></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2205" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain  Knowledge, Yueqi Song+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- VisualPuzzlesã¯ã€å°‚é–€çŸ¥è­˜ã¸ã®ä¾å­˜ã‚’æœ€å°é™ã«æŠ‘ãˆãŸè¦–è¦šçš„æ¨è«–ã‚’è©•ä¾¡ã™ã‚‹æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã€5ã¤ã®æ¨è«–ã‚«ãƒ†ã‚´ãƒªãƒ¼ã‹ã‚‰æˆã‚‹å¤šæ§˜ãªè³ªå•ã‚’å«ã‚€ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€VisualPuzzlesã¯ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹æœ‰ã®çŸ¥è­˜ã‚’å¤§å¹…ã«æ¸›å°‘ã•ã›ã€ã‚ˆã‚Šè¤‡é›‘ãªæ¨è«–ã‚’è¦æ±‚ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚æœ€å…ˆç«¯ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã¯ã€VisualPuzzlesã§äººé–“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«é…ã‚Œã‚’ã¨ã‚Šã€çŸ¥è­˜é›†ç´„å‹ã‚¿ã‚¹ã‚¯ã§ã®æˆåŠŸãŒæ¨è«–ã‚¿ã‚¹ã‚¯ã§ã®æˆåŠŸã«å¿…ãšã—ã‚‚ã¤ãªãŒã‚‰ãªã„ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚ã¾ãŸã€ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®é–“ã«æ˜ç¢ºãªç›¸é–¢ã¯è¦‹ã‚‰ã‚Œãšã€VisualPuzzlesã¯äº‹å®Ÿã®è¨˜æ†¶ã‚’è¶…ãˆãŸæ¨è«–èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹æ–°ãŸãªè¦–ç‚¹ã‚’æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/yueqi_song/status/1912510869491101732?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ç”»åƒã¯PJãƒšãƒ¼ã‚¸ã‚ˆã‚Šå¼•ç”¨ã€‚æ–°ãŸã«Visual Puzzleã¨å‘¼ã°ã‚Œã‚‹ç‰¹å®šã®ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ãŒã»ã¨ã‚“ã©å¿…è¦ãªã„ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªreasoningãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã€‚o1ã§ã™ã‚‰ã€äººé–“ã®5th percentileã«æº€ãŸãªã„æ€§èƒ½ã¨ã®ã“ã¨ã€‚<br><br>Chinese Civil Service Examinationä¸­ã®logical reasoning questionã‚’æ‰‹ä½œæ¥­ã§ç¿»è¨³ã—ãŸã¨ã®ã“ã¨ã€‚<br><br><img src="https://github.com/user-attachments/assets/4ee1cd31-2d47-46a2-861b-2a72c5df8529" alt="image" loading="lazy"><br><br>ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®çµ±è¨ˆé‡ã¯ä»¥ä¸‹ã§ã€åˆè¨ˆ1168å•ã§ã€é›£æ˜“åº¦ã¯3æ®µéšã«åˆ†ã‹ã‚Œã¦ã„ã‚‹æ¨¡æ§˜ã€‚<br><img src="https://github.com/user-attachments/assets/332246e3-075f-4d98-b528-c8e4ec865068" alt="image" loading="lazy"><br><br>project page:


<a href="https://neulab.github.io/VisualPuzzles/" target="_blank" rel="noopener noreferrer">https://neulab.github.io/VisualPuzzles/</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2200" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Kimi-VL Technical Report, Kimi Team+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Kimi-VLã¯ã€åŠ¹ç‡çš„ãªã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®Mixture-of-Expertsãƒ“ã‚¸ãƒ§ãƒ³ãƒ»ãƒ©ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€2.8Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨€èªãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’æ´»æ€§åŒ–ã—ã¦é«˜åº¦ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–ã‚’å®Ÿç¾ã€‚ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¿ã‚¹ã‚¯ã‚„å¤§å­¦ãƒ¬ãƒ™ãƒ«ã®ç”»åƒãƒ»å‹•ç”»ç†è§£ã«ãŠã„ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ã€æœ€å…ˆç«¯ã®VLMã¨ç«¶äº‰ã€‚128Kã®æ‹¡å¼µã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’æŒã¡ã€é•·ã„å…¥åŠ›ã‚’å‡¦ç†å¯èƒ½ã€‚Kimi-VL-Thinking-2506ã¯ã€é•·æœŸçš„æ¨è«–èƒ½åŠ›ã‚’å¼·åŒ–ã™ã‚‹ãŸã‚ã«æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨å¼·åŒ–å­¦ç¿’ã‚’ç”¨ã„ã¦é–‹ç™ºã•ã‚Œã€å …ç‰¢ãªä¸€èˆ¬èƒ½åŠ›ã‚’ç²å¾—ã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2201" target="_blank" rel="noopener noreferrer">[Paper Note] Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset, Ke Wang+, NeurIPS'24 Datasets and Benchmarks Track</a>
 <br>ã§ã®æ€§èƒ½ï¼ˆVision+ãƒ†ã‚­ã‚¹ãƒˆã®æ•°å­¦ã®å•é¡Œï¼‰ã€‚ä»–ã®å·¨å¤§ãªãƒ¢ãƒ‡ãƒ«ã¨æ¯”ã¹2.8Bã®Activation paramsã§é«˜ã„æ€§èƒ½ã‚’é”æˆ<br><br>&lt;img width="831" height="431" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/3ec08621-f269-4f1d-97bb-3ebca537f2ea"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/3ec08621-f269-4f1d-97bb-3ebca537f2ea"&lt;/a&gt;


/&gt;<br><br>ãã®ä»–ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã‚‚é«˜ã„æ€§èƒ½ã‚’ç²å¾—<br><br>&lt;img width="833" height="558" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/b30afc4f-efce-4206-b499-f4f089d97226"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/b30afc4f-efce-4206-b499-f4f089d97226"&lt;/a&gt;


/&gt;<br><br>ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚MoonViT (Image Encoder, 1Dã®patchã‚’input, æ§˜ã€…ãªè§£åƒåº¦ã®ã‚µãƒãƒ¼ãƒˆ, FlashAttention,  SigLIP-SO-400Mã‚’ç¶™ç¶šäº‹å‰å­¦ç¿’, RoPEã‚’æ¡ç”¨) + Linear Projector + MoE Language Decoderã®æ§‹æˆ<br>&lt;img width="851" height="590" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/f59d7655-c1c7-4284-b79c-9d62739da889"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/f59d7655-c1c7-4284-b79c-9d62739da889"&lt;/a&gt;


/&gt;<br><br>å­¦ç¿’ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€‚ViTã®äº‹å‰å­¦ç¿’ã§ã¯SigLIP loss (contrastive lossã®äºœç¨®)ã¨captionç”Ÿæˆã®cross-entropy lossã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã€‚joint cooldown stageã«ãŠã„ã¦ã¯ã€é«˜å“è³ªãªQAãƒ‡ãƒ¼ã‚¿ã‚’åˆæˆã™ã‚‹ã“ã¨ã§å®Ÿé¨“çš„ã«å¤§å¹…ã«æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ãŸã®ã§ã€ãã‚Œã‚’æ¡ç”¨ã—ã¦ã„ã‚‹ã¨ã®ã“ã¨ã€‚optimizerã¯ <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2202" target="_blank" rel="noopener noreferrer">[Paper Note] Muon is Scalable for LLM Training, Jingyuan Liu+, arXiv'25</a>
<br><br>&lt;img width="849" height="213" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/720b02f7-a260-497f-85c5-04cf382c2f98"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/720b02f7-a260-497f-85c5-04cf382c2f98"&lt;/a&gt;


/&gt;<br><br>&lt;img width="828" height="402" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/bb78d799-5db4-4904-8669-540d2142c95c"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/bb78d799-5db4-4904-8669-540d2142c95c"&lt;/a&gt;


/&gt;<br><br>post-trainingã«ãŠã‘ã‚‹RLã§ã¯ä»¥ä¸‹ã®ç›®çš„é–¢æ•°ã‚’ç”¨ã„ã¦ãŠã‚Šã€RLVRã‚’ç”¨ã„ã¤ã¤ã€ç¾åœ¨ã®ãƒãƒªã‚·ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’referenceã¨ã—æ›´æ–°ã‚’ã™ã‚‹ã‚ˆã†ãªç›®çš„é–¢æ•°ã«ãªã£ã¦ã„ã‚‹ã€‚curriculum sampling, prioritize samplingã‚’difficulty labelã«åŸºã¥ã„ã¦å®Ÿæ–½ã—ã¦ã„ã‚‹ã€‚<br>&lt;img width="842" height="152" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/298fdef8-9807-4511-96f6-02241393ab9f"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/298fdef8-9807-4511-96f6-02241393ab9f"&lt;/a&gt;


/&gt;<br><br>&lt;img width="822" height="187" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/4ad0d815-ef1c-4945-ae08-ab2b072ec63f"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/4ad0d815-ef1c-4945-ae08-ab2b072ec63f"&lt;/a&gt;


/&gt;</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2191" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Perception-Aware Policy Optimization for Multimodal Reasoning, Zhenhailong Wang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- å¼·åŒ–å­¦ç¿’ã«ãŠã‘ã‚‹æ¤œè¨¼å¯èƒ½ãªå ±é…¬ï¼ˆRLVRï¼‰ã¯ã€LLMsã«å¤šæ®µéšæ¨è«–èƒ½åŠ›ã‚’ä¸ãˆã‚‹ãŒã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–ã§ã¯æœ€é©ãªæ€§èƒ½ã‚’ç™ºæ®ã§ããªã„ã€‚è¦–è¦šå…¥åŠ›ã®èªè­˜ãŒä¸»ãªã‚¨ãƒ©ãƒ¼åŸå› ã§ã‚ã‚‹ãŸã‚ã€çŸ¥è¦šã‚’æ„è­˜ã—ãŸãƒãƒªã‚·ãƒ¼æœ€é©åŒ–ï¼ˆPAPOï¼‰ã‚’ææ¡ˆã€‚PAPOã¯GRPOã®æ‹¡å¼µã§ã€å†…éƒ¨ç›£è¦–ä¿¡å·ã‹ã‚‰å­¦ç¿’ã—ã€è¿½åŠ ã®ãƒ‡ãƒ¼ã‚¿ã‚„å¤–éƒ¨å ±é…¬ã«ä¾å­˜ã—ãªã„ã€‚KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹é …ã‚’å°å…¥ã—ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§4.4%ã®æ”¹å–„ã€è¦–è¦šä¾å­˜ã‚¿ã‚¹ã‚¯ã§ã¯8.0%ã®æ”¹å–„ã‚’é”æˆã€‚çŸ¥è¦šã‚¨ãƒ©ãƒ¼ã‚‚30.5%æ¸›å°‘ã—ã€PAPOã®åŠ¹æœã‚’ç¤ºã™ã€‚ç ”ç©¶ã¯è¦–è¦šã«åŸºã¥ãæ¨è«–ã‚’ä¿ƒé€²ã™ã‚‹æ–°ã—ã„RLãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®åŸºç›¤ã‚’ç¯‰ãã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/aicia_solid/status/1943507735489974596?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>VLMã«ãŠã„ã¦ã€ç”»åƒã‚’ãƒã‚¹ã‚¯ã—ãŸå ´åˆã®ãƒãƒªã‚·ãƒ¼ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã¨ã€ç”»åƒã‚’ãƒã‚¹ã‚¯ã—ãªã„å ´åˆã®ãƒãƒªã‚·ãƒ¼ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã®KL Divergenceã‚’æœ€å¤§åŒ–ã™ã‚‹ã“ã¨ã§ã€ç”»åƒã®èªçŸ¥èƒ½åŠ›ãŒå‘ä¸Šã—æ€§èƒ½å‘ä¸Šã™ã‚‹ã‚ˆã€ã¿ãŸã„ãªè©±ãªæ¨¡æ§˜ã€‚<br><img src="https://github.com/user-attachments/assets/d7844321-d979-497f-84da-5d69fd13233f" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/afe8919c-ea16-48a1-b33b-79b7a3b1ccb0" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/04a3d23c-2eb0-40e2-aa2c-363498976320" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2156" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] VLM2Vec: Training Vision-Language Models for Massive Multimodal  Embedding Tasks, Ziyan Jiang+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒ¦ãƒ‹ãƒãƒ¼ã‚µãƒ«ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã‚’ç›®æŒ‡ã—ã€äºŒã¤ã®è²¢çŒ®ã‚’è¡Œã£ãŸã€‚ç¬¬ä¸€ã«ã€MMEBï¼ˆMassive Multimodal Embedding Benchmarkï¼‰ã‚’ææ¡ˆã—ã€36ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦åˆ†é¡ã‚„è¦–è¦šçš„è³ªå•å¿œç­”ãªã©ã®ãƒ¡ã‚¿ã‚¿ã‚¹ã‚¯ã‚’ç¶²ç¾…ã—ãŸã€‚ç¬¬äºŒã«ã€VLM2Vecã¨ã„ã†ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’é–‹ç™ºã—ã€è¦–è¦š-è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›ã™ã‚‹æ‰‹æ³•ã‚’ç¤ºã—ãŸã€‚å®Ÿé¨“çµæœã¯ã€VLM2VecãŒæ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦10%ã‹ã‚‰20%ã®æ€§èƒ½å‘ä¸Šã‚’é”æˆã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã€VLMã®å¼·åŠ›ãªåŸ‹ã‚è¾¼ã¿èƒ½åŠ›ã‚’è¨¼æ˜ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=TE0KOzWYAF" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=TE0KOzWYAF</a>


</p></span><br><br>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/InformationRetrieval.html" target="_blank" rel="noopener noreferrer">#InformationRetrieval</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/RAG(RetrievalAugmentedGeneration).html" target="_blank" rel="noopener noreferrer">#RAG(RetrievalAugmentedGeneration)</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2155" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and  Visual Documents, Rui Meng+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- VLM2Vec-V2ã¨ã„ã†çµ±ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€å‹•ç”»ã€è¦–è¦šæ–‡æ›¸ã‚’å«ã‚€å¤šæ§˜ãªè¦–è¦šå½¢å¼ã®åŸ‹ã‚è¾¼ã¿ã‚’å­¦ç¿’ã€‚æ–°ãŸã«MMEB-V2ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å°å…¥ã—ã€å‹•ç”»æ¤œç´¢ã‚„è¦–è¦šæ–‡æ›¸æ¤œç´¢ãªã©5ã¤ã®ã‚¿ã‚¹ã‚¯ã‚’è¿½åŠ ã€‚åºƒç¯„ãªå®Ÿé¨“ã«ã‚ˆã‚Šã€VLM2Vec-V2ã¯æ–°ã‚¿ã‚¹ã‚¯ã§å¼·åŠ›ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã€å¾“æ¥ã®ç”»åƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã‚‚æ”¹å–„ã‚’é”æˆã€‚ç ”ç©¶ã¯ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®ä¸€èˆ¬åŒ–å¯èƒ½æ€§ã«é–¢ã™ã‚‹æ´å¯Ÿã‚’æä¾›ã—ã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªè¡¨ç¾å­¦ç¿’ã®åŸºç›¤ã‚’ç¯‰ãã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenhuchen/status/1942501330674647342?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2156" target="_blank" rel="noopener noreferrer">[Paper Note] VLM2Vec: Training Vision-Language Models for Massive Multimodal  Embedding Tasks, Ziyan Jiang+, ICLR'25</a>
</p>
<p>Video Classification, Visual Document Retrievalãªã©ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚‚å«ã¾ã‚Œã¦ã„ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2146" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Energy-Based Transformers are Scalable Learners and Thinkers, Alexi Gladstone+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ï¼ˆEBTsï¼‰ã‚’ç”¨ã„ã¦ã€ç„¡ç›£ç£å­¦ç¿’ã‹ã‚‰æ€è€ƒã‚’å­¦ã¶ãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã€‚EBTsã¯ã€å…¥åŠ›ã¨å€™è£œäºˆæ¸¬ã®äº’æ›æ€§ã‚’æ¤œè¨¼ã—ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ–ã‚’é€šã˜ã¦äºˆæ¸¬ã‚’è¡Œã†ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚ˆã‚Šã‚‚é«˜ã„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç‡ã‚’é”æˆã—ã€è¨€èªã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’29%å‘ä¸Šã•ã›ã€ç”»åƒã®ãƒã‚¤ã‚ºé™¤å»ã§ã‚‚å„ªã‚ŒãŸçµæœã‚’ç¤ºã™ã€‚EBTsã¯ä¸€èˆ¬åŒ–èƒ½åŠ›ãŒé«˜ãã€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’èƒ½åŠ›ã¨æ€è€ƒèƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§ã‚ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1941657099567845696?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Project Page:


<a href="https://energy-based-transformers.github.io" target="_blank" rel="noopener noreferrer">https://energy-based-transformers.github.io</a>


</p>
<p>First Authorã®æ–¹ã«ã‚ˆã‚‹è§£èª¬ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/alexiglad/status/1942231878305714462?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/2D%20(Image).html" target="_blank" rel="noopener noreferrer">#2D (Image)</a>
<a class="button" href="articles/3D%20(Scene).html" target="_blank" rel="noopener noreferrer">#3D (Scene)</a>
<a class="button" href="articles/FeatureMatching.html" target="_blank" rel="noopener noreferrer">#FeatureMatching</a>
<span class="issue_date">Issue Date: 2025-07-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2132" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space, Yingping Liang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æ–°ã—ã„äºŒæ®µéšãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€ŒLift to Match (L2M)ã€ã‚’ææ¡ˆã—ã€2Dç”»åƒã‚’3Dç©ºé–“ã«æŒã¡ä¸Šã’ã‚‹ã“ã¨ã§ã€ç‰¹å¾´ãƒãƒƒãƒãƒ³ã‚°ã®ä¸€èˆ¬åŒ–ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ç¬¬ä¸€æ®µéšã§3Dç‰¹å¾´ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’å­¦ç¿’ã—ã€ç¬¬äºŒæ®µéšã§ç‰¹å¾´ãƒ‡ã‚³ãƒ¼ãƒ€ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€å …ç‰¢ãªç‰¹å¾´ãƒãƒƒãƒãƒ³ã‚°ã‚’å®Ÿç¾ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆè©•ä¾¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å„ªã‚ŒãŸä¸€èˆ¬åŒ–æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/zhenjun_zhao/status/1940399755827270081?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/RLHF.html" target="_blank" rel="noopener noreferrer">#RLHF</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/mid-training.html" target="_blank" rel="noopener noreferrer">#mid-training</a>
<a class="button" href="articles/RewardHacking.html" target="_blank" rel="noopener noreferrer">#RewardHacking</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/CurriculumLearning.html" target="_blank" rel="noopener noreferrer">#CurriculumLearning</a>
<a class="button" href="articles/RLVR.html" target="_blank" rel="noopener noreferrer">#RLVR</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2128" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable  Reinforcement Learning, GLM-V Team+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- è¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«GLM-4.1V-Thinkingã‚’ç™ºè¡¨ã—ã€æ¨è«–ä¸­å¿ƒã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’é–‹ç™ºã€‚å¼·åŠ›ãªè¦–è¦šåŸºç›¤ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã€ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ç”¨ã„ãŸå¼·åŒ–å­¦ç¿’ã§å¤šæ§˜ãªã‚¿ã‚¹ã‚¯ã®èƒ½åŠ›ã‚’å‘ä¸Šã€‚28ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã€ç‰¹ã«é›£ã—ã„ã‚¿ã‚¹ã‚¯ã§ç«¶äº‰åŠ›ã®ã‚ã‚‹çµæœã‚’ç¤ºã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã¨ã—ã¦å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/sinclairwang1/status/1940331927724232712?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Qwen2.5-VLã‚ˆã‚Šã‚‚æ€§èƒ½ãŒè‰¯ã„VLM<br><img src="https://github.com/user-attachments/assets/1215d0cf-3776-4631-a5d5-2c514e7d5a2e" alt="image" loading="lazy"></p>
<p>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã“ã¡ã‚‰ã€‚ãŒã€pretraining(ãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°, ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«â†’long contextç¶™ç¶šäº‹å‰å­¦ç¿’)-&gt;SFT(cold startã¸ã®å¯¾å‡¦, reasoningèƒ½åŠ›ã®ç²å¾—)-&gt;RL(RLVRã¨RLHFã®ä½µç”¨ã«ã‚ˆã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã¨Alignment, RewardHackingã¸ã®å¯¾å‡¦,curriculum sampling)ãªã©ã€å…¨ä½“ã®å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ç´°ã‹ã„ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã®ç©ã¿é‡ã­ã§é«˜ã„æ€§èƒ½ãŒç²å¾—ã•ã‚Œã¦ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/a692b5de-5f4e-42c6-938e-3718dd2fc0e6" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/Findings.html" target="_blank" rel="noopener noreferrer">#Findings</a>
<span class="issue_date">Issue Date: 2025-07-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2125" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Do Vision-Language Models Have Internal World Models? Towards an Atomic   Evaluation, Qiyue Gao+, ACLï¼ˆFindingsï¼‰'25</a>
<span class="snippet"><span>GPT Summary</span>- å†…éƒ¨ä¸–ç•Œãƒ¢ãƒ‡ãƒ«ï¼ˆWMsï¼‰ã¯ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ç†è§£ã¨äºˆæ¸¬ã‚’æ”¯ãˆã‚‹ãŒã€æœ€è¿‘ã®å¤§è¦æ¨¡ãƒ“ã‚¸ãƒ§ãƒ³ãƒ»ãƒ©ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ï¼ˆVLMsï¼‰ã®åŸºæœ¬çš„ãªWMèƒ½åŠ›ã«é–¢ã™ã‚‹è©•ä¾¡ã¯ä¸è¶³ã—ã¦ã„ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€çŸ¥è¦šã¨äºˆæ¸¬ã‚’è©•ä¾¡ã™ã‚‹äºŒæ®µéšã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€WM-ABenchã¨ã„ã†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å°å…¥ã€‚15ã®VLMsã«å¯¾ã™ã‚‹660ã®å®Ÿé¨“ã§ã€ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ãŒåŸºæœ¬çš„ãªWMèƒ½åŠ›ã«é¡•è‘—ãªåˆ¶é™ã‚’ç¤ºã—ã€ç‰¹ã«é‹å‹•è»Œé“ã®è­˜åˆ¥ã«ãŠã„ã¦ã»ã¼ãƒ©ãƒ³ãƒ€ãƒ ãªç²¾åº¦ã§ã‚ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚VLMsã¨äººé–“ã®WMã¨ã®é–“ã«ã¯é‡è¦ãªã‚®ãƒ£ãƒƒãƒ—ãŒå­˜åœ¨ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/qiyuegao123/status/1940097188220297613?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<span class="issue_date">Issue Date: 2025-07-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2122" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning, Yulun Jiang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- MARBLEã¨ã„ã†æ–°ã—ã„ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€MLLMsã®è¤‡é›‘ãªæ¨è«–èƒ½åŠ›ã‚’è©•ä¾¡ã€‚MARBLEã¯ã€ç©ºé–“çš„ãƒ»è¦–è¦šçš„ãƒ»ç‰©ç†çš„åˆ¶ç´„ä¸‹ã§ã®å¤šæ®µéšè¨ˆç”»ã‚’å¿…è¦ã¨ã™ã‚‹M-Portalã¨M-Cubeã®2ã¤ã®ã‚¿ã‚¹ã‚¯ã‹ã‚‰æˆã‚‹ã€‚ç¾åœ¨ã®MLLMsã¯ä½ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã€è¦–è¦šçš„å…¥åŠ›ã‹ã‚‰ã®æƒ…å ±æŠ½å‡ºã«ãŠã„ã¦ã‚‚å¤±æ•—ãŒè¦‹ã‚‰ã‚Œã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ¬¡ä¸–ä»£ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–èƒ½åŠ›å‘ä¸ŠãŒæœŸå¾…ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/michael_d_moor/status/1940062842742526445?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Portal2ã‚’ä½¿ã£ãŸæ–°ãŸãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€‚ç­†è€…ã¯æ˜”ã“ã®ã‚²ãƒ¼ãƒ ã‚’å°‘ã—ã ã‘ãƒ—ãƒ¬ã‚¤ã—ãŸã“ã¨ãŒã‚ã‚‹ãŒã€æ™®é€šã«é›£ã—ã‹ã£ãŸè¨˜æ†¶ãŒã‚ã‚‹ğŸ˜…<br><br>ç´°ã‹ã„ãŒè¡¨ä¸­ã®GPT-o3ã¯æ­£ã—ãã¯o3ã ã¨æ€ã‚ã‚Œã‚‹ã€‚<br>æ™‚é–“ãŒãªãã¦å…¨ç„¶ã—ã£ã‹ã‚Šã¨èª­ã‚ã¦ã„ãªã„ãŒã€reasoning effortã‚„thinkingãƒ¢ãƒ¼ãƒ‰ã¯ã©ã®ã‚ˆã†ã«è¨­å®šã—ã¦è©•ä¾¡ã—ãŸã®ã ã‚ã†ã‹ã€‚<br><img src="https://github.com/user-attachments/assets/a7647007-b718-4b1c-8d8a-396c36d7811d" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/4b996864-7bf8-4ea9-aa3e-84d4e9f3f5d2" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Zero/Few/ManyShotPrompting.html" target="_blank" rel="noopener noreferrer">#Zero/Few/ManyShotPrompting</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2025-07-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2121" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context  Learning, Melanie Rieff+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ï¼ˆICLï¼‰ã¯åŒ»ç™‚åˆ†é‡ã§ã®å¯èƒ½æ€§ãŒã‚ã‚‹ãŒã€ååˆ†ã«æ¢æ±‚ã•ã‚Œã¦ã„ãªã„ã€‚SMMILEã¨ã„ã†åŒ»ç™‚ã‚¿ã‚¹ã‚¯å‘ã‘ã®åˆã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ICLãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å°å…¥ã—ã€111ã®å•é¡Œã‚’å«ã‚€ã€‚15ã®MLLMã®è©•ä¾¡ã§ã€åŒ»ç™‚ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹ICLèƒ½åŠ›ãŒä¸­ç¨‹åº¦ã‹ã‚‰ä½ã„ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ICLã¯SMMILEã§å¹³å‡8%ã€SMMILE++ã§9.4%ã®æ”¹å–„ã‚’ã‚‚ãŸã‚‰ã—ã€ç„¡é–¢ä¿‚ãªä¾‹ãŒãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€å¤§9.5%ä½ä¸‹ã•ã›ã‚‹ã“ã¨ã‚‚ç¢ºèªã€‚ä¾‹ã®é †åºã«ã‚ˆã‚‹æœ€è¿‘æ€§ãƒã‚¤ã‚¢ã‚¹ãŒãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã«å¯„ä¸ã™ã‚‹ã“ã¨ã‚‚æ˜ã‚‰ã‹ã«ãªã£ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/michael_d_moor/status/1939664155813839114?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/OpenSource.html" target="_blank" rel="noopener noreferrer">#OpenSource</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<a class="button" href="articles/Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2105" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision  Encoders for Multimodal Learning, Xianhang Li+, ICCV'25</a>
<span class="snippet"><span>GPT Summary</span>- OpenVisionã¯ã€å®Œå…¨ã«ã‚ªãƒ¼ãƒ—ãƒ³ã§ã‚³ã‚¹ãƒˆåŠ¹æœã®é«˜ã„ãƒ“ã‚¸ãƒ§ãƒ³ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’ææ¡ˆã—ã€CLIPã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚æ—¢å­˜ã®ç ”ç©¶ã‚’åŸºã«æ§‹ç¯‰ã•ã‚Œã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®é€²å±•ã«å®Ÿç”¨çš„ãªåˆ©ç‚¹ã‚’ç¤ºã—ã¾ã™ã€‚5.9Mã‹ã‚‰632.1Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’æä¾›ã—ã€å®¹é‡ã¨åŠ¹ç‡ã®æŸ”è»Ÿãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å®Ÿç¾ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/cihangxie/status/1920575141849030882?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>v2ã¸ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/cihangxie/status/1963297223753494832?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>äº‹å‰å­¦ç¿’æ™‚ã«text, image encoderã®contrastive lossã§å­¦ç¿’ã—ã¦ã„ãŸãŒã€text encoderã‚’ç„¡ãã—image encoderã«å…¥åŠ›ã•ã‚ŒãŸimageã‹ã‚‰captionã‚’ç”Ÿæˆã™ã‚‹caption lossã®ã¿ã«ã™ã‚‹ã“ã¨ã§æ€§èƒ½ã‚’è½ã¨ã™ã“ã¨ãªãåŠ¹ç‡ã‚’æ”¹å–„<br><br>ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒšãƒ¼ãƒ‘ãƒ¼ãŒå‡ºãŸæ¨¡æ§˜<br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2820" target="_blank" rel="noopener noreferrer">[Paper Note] OpenVision 2: A Family of Generative Pretrained Visual Encoders for
  Multimodal Learning, Yanqing Liu+, arXiv'25</a>
<p>HF:


<a href="https://huggingface.co/collections/UCSC-VLAA/openvision-681a4c27ee1f66411b4ae919" target="_blank" rel="noopener noreferrer">https://huggingface.co/collections/UCSC-VLAA/openvision-681a4c27ee1f66411b4ae919</a>


<br>pj page: 


<a href="https://ucsc-vlaa.github.io/OpenVision/" target="_blank" rel="noopener noreferrer">https://ucsc-vlaa.github.io/OpenVision/</a>


</p>
<p>CLIP, SigLIPã¨ã¯ç•°ãªã‚Šå®Œå…¨ã«ã‚ªãƒ¼ãƒ—ãƒ³ãªVision Encoder<br><img src="https://github.com/user-attachments/assets/b7c8eb07-45df-4ab3-9cd2-6b31af46e761" alt="image" loading="lazy"></p>
<p>v2ã®è§£èª¬:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1963442911108084161?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/pretrained-LM.html" target="_blank" rel="noopener noreferrer">#pretrained-LM</a>
<a class="button" href="articles/Scaling%20Laws.html" target="_blank" rel="noopener noreferrer">#Scaling Laws</a>
<a class="button" href="articles/TMLR.html" target="_blank" rel="noopener noreferrer">#TMLR</a>
<span class="issue_date">Issue Date: 2025-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2100" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] An Empirical Study of Pre-trained Model Selection for   Out-of-Distribution Generalization and Calibration, Hiroki Naganuma+, TMLR'25</a>
<span class="snippet"><span>GPT Summary</span>- äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒåˆ†å¸ƒå¤–ä¸€èˆ¬åŒ–ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦é‡è¦ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã€ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®é¸æŠãŒOODç²¾åº¦ã¨ä¿¡é ¼æ€§ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¿æŸ»ã€‚120,000æ™‚é–“ä»¥ä¸Šã®å®Ÿé¨“ã‚’é€šã˜ã¦ã€å¤§ããªãƒ¢ãƒ‡ãƒ«ã¨å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒOODãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ”¹å–„ã™ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã€‚ã“ã‚Œã¯ã€å¾“æ¥ã®ç ”ç©¶ã¨å¯¾ç…§çš„ã§ã‚ã‚Šã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®é¸æŠã®é‡è¦æ€§ã‚’å¼·èª¿ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>OpenReview:


<a href="https://openreview.net/forum?id=tYjoHjShxF" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=tYjoHjShxF</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_hiroki11x/status/1938052113466323134?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<span class="issue_date">Issue Date: 2025-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2099" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers, Weiming Ren+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- VAMBAãƒ¢ãƒ‡ãƒ«ã¯ã€Mamba-2ãƒ–ãƒ­ãƒƒã‚¯ã‚’ç”¨ã„ã¦ãƒ“ãƒ‡ã‚ªãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç·šå½¢ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›ãªã—ã§1024ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†å¯èƒ½ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’50%å‰Šæ¸›ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã‚’å€å¢—ã€‚1æ™‚é–“ã®ãƒ“ãƒ‡ã‚ªç†è§£ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯LVBenchã§4.3%ã®ç²¾åº¦å‘ä¸Šã‚’é”æˆã—ã€æ§˜ã€…ãªãƒ“ãƒ‡ã‚ªç†è§£ã‚¿ã‚¹ã‚¯ã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenhuchen/status/1938064510369280136?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Tokenizer.html" target="_blank" rel="noopener noreferrer">#Tokenizer</a>
<span class="issue_date">Issue Date: 2025-06-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2082" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Vision as a Dialect: Unifying Visual Understanding and Generation via  Text-Aligned Representations, Jiaming Han+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€è¦–è¦šç†è§£ã¨ç”Ÿæˆã‚’çµ±ä¸€ã™ã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯Tarã‚’ææ¡ˆã€‚Text-Aligned Tokenizerï¼ˆTA-Tokï¼‰ã‚’ç”¨ã„ã¦ç”»åƒã‚’é›¢æ•£ãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›ã—ã€è¦–è¦šã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ±ä¸€ç©ºé–“ã«çµ±åˆã€‚ã‚¹ã‚±ãƒ¼ãƒ«é©å¿œå‹ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å°å…¥ã—ã€é«˜å¿ å®Ÿåº¦ã®è¦–è¦šå‡ºåŠ›ã‚’ç”Ÿæˆã€‚è¿…é€Ÿãªè‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã¨æ‹¡æ•£ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸãƒ‡ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’æ´»ç”¨ã—ã€è¦–è¦šç†è§£ã¨ç”Ÿæˆã®æ”¹å–„ã‚’å®Ÿç¾ã€‚å®Ÿé¨“çµæœã§ã¯ã€TarãŒæ—¢å­˜æ‰‹æ³•ã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’ç¤ºã—ã€åŠ¹ç‡çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é”æˆã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_akhaliq/status/1937345768223859139?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>text modalityã¨vision modalityã‚’å…±é€šã®ç©ºé–“ã§è¡¨ç¾ã™ã‚‹<br><img src="https://github.com/user-attachments/assets/356e86e1-cad9-4bee-8398-d68c4fc6ad46" alt="image" loading="lazy"></p>
<p>Visual Understanding/Generationã®ãƒ™ãƒ³ãƒã§å…¨ä½“çš„ã«é«˜ã„æ€§èƒ½ã‚’é”æˆ<br><img src="https://github.com/user-attachments/assets/6e45aec0-ae0b-4327-923f-fdfce8e83ca0" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/RepresentationLearning.html" target="_blank" rel="noopener noreferrer">#RepresentationLearning</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<span class="issue_date">Issue Date: 2025-06-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2079" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual  Retrieval, Michael GÃ¼nther+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- 3.8å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã€Œjina-embeddings-v4ã€ã‚’ææ¡ˆã€‚æ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚Šã€ã‚¯ã‚¨ãƒªãƒ™ãƒ¼ã‚¹ã®æƒ…å ±æ¤œç´¢ã‚„ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«ã®é¡ä¼¼æ€§æ¤œç´¢ã‚’æœ€é©åŒ–ã€‚ã‚¿ã‚¹ã‚¯ç‰¹åŒ–å‹ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’çµ„ã¿è¾¼ã¿ã€è¦–è¦šçš„ã«è±Šã‹ãªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å‡¦ç†ã«å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã€‚æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ŒJina-VDRã€ã‚‚å°å…¥ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/arankomatsuzaki/status/1937342962075378014?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<span class="issue_date">Issue Date: 2025-06-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2074" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Sekai: A Video Dataset towards World Exploration, Zhen Li+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- é«˜å“è³ªãªä¸€äººç§°è¦–ç‚¹ã®ãƒ“ãƒ‡ã‚ªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ŒSekaiã€ã‚’ç´¹ä»‹ã€‚750ã®éƒ½å¸‚ã‹ã‚‰5,000æ™‚é–“ä»¥ä¸Šã®ãƒ“ãƒ‡ã‚ªã‚’åé›†ã—ã€ä½ç½®ã‚„ã‚·ãƒ¼ãƒ³ãªã©ã®è±Šå¯Œãªæ³¨é‡ˆã‚’ä»˜ä¸ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªãƒ“ãƒ‡ã‚ªä¸–ç•Œæ¢æŸ»ãƒ¢ãƒ‡ãƒ«ã€ŒYUMEã€ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€‚Sekaiã¯ãƒ“ãƒ‡ã‚ªç”Ÿæˆã¨ä¸–ç•Œæ¢æŸ»ã«è²¢çŒ®ã™ã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/yongyuanxi/status/1936846469346251068?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/3D%20Reconstruction.html" target="_blank" rel="noopener noreferrer">#3D Reconstruction</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-06-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2068" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] VGGT: Visual Geometry Grounded Transformer, Jianyuan Wang+, CVPR'25</a>
<span class="snippet"><span>GPT Summary</span>- VGGTã¯ã€ã‚·ãƒ¼ãƒ³ã®ä¸»è¦ãª3Då±æ€§ã‚’è¤‡æ•°ã®ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰ç›´æ¥æ¨æ¸¬ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚Šã€3Dã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®åˆ†é‡ã«ãŠã„ã¦æ–°ãŸãªé€²å±•ã‚’ç¤ºã—ã¾ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯åŠ¹ç‡çš„ã§ã€1ç§’æœªæº€ã§ç”»åƒã‚’å†æ§‹ç¯‰ã—ã€è¤‡æ•°ã®3Dã‚¿ã‚¹ã‚¯ã§æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã—ã¾ã™ã€‚ã¾ãŸã€VGGTã‚’ç‰¹å¾´ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ä¸‹æµã‚¿ã‚¹ã‚¯ã®æ€§èƒ½ãŒå¤§å¹…ã«å‘ä¸Šã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1936711294956265820?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<span class="issue_date">Issue Date: 2025-06-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2037" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Seedance 1.0: Exploring the Boundaries of Video Generation Models, Yu Gao+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Seedance 1.0ã¯ã€å‹•ç”»ç”Ÿæˆã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéµå®ˆã€å‹•ãã®å¦¥å½“æ€§ã€è¦–è¦šçš„å“è³ªã‚’åŒæ™‚ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚ä¸»ãªæŠ€è¡“æ”¹å–„ã¨ã—ã¦ã€æ„å‘³ã®ã‚ã‚‹å‹•ç”»ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ç”¨ã„ãŸãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒãƒ«ãƒã‚·ãƒ§ãƒƒãƒˆç”Ÿæˆã®ã‚µãƒãƒ¼ãƒˆã€å‹•ç”»ç‰¹æœ‰ã®RLHFã‚’æ´»ç”¨ã—ãŸãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€æ¨è«–é€Ÿåº¦ã®ç´„10å€å‘ä¸Šã‚’å®Ÿç¾ã™ã‚‹è’¸ç•™æˆ¦ç•¥ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚Seedance 1.0ã¯ã€1080pè§£åƒåº¦ã®5ç§’é–“ã®å‹•ç”»ã‚’41.4ç§’ã§ç”Ÿæˆã—ã€é«˜å“è³ªã‹ã¤è¿…é€Ÿãªå‹•ç”»ç”Ÿæˆã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/scaling01/status/1933048431775527006?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<span class="issue_date">Issue Date: 2025-06-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2021" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Generative Omnimatte: Learning to Decompose Video into Layers, Yao-Chih Lee+, CVPR'25</a>
<span class="snippet"><span>GPT Summary</span>- ã‚ªãƒ ãƒ‹ãƒãƒƒãƒˆæ‰‹æ³•ã¯ã€ãƒ“ãƒ‡ã‚ªã‚’æ„å‘³çš„ã«æœ‰æ„ç¾©ãªå±¤ã«åˆ†è§£ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ãŒã€æ—¢å­˜æ‰‹æ³•ã¯é™çš„èƒŒæ™¯ã‚„æ­£ç¢ºãªãƒãƒ¼ã‚ºã‚’å‰æã¨ã—ã¦ãŠã‚Šã€ã“ã‚ŒãŒç ´ã‚‰ã‚Œã‚‹ã¨æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹ã€‚æ–°ãŸã«ææ¡ˆã™ã‚‹ç”Ÿæˆçš„å±¤çŠ¶ãƒ“ãƒ‡ã‚ªåˆ†è§£ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€é™æ­¢ã‚·ãƒ¼ãƒ³ã‚„æ·±åº¦æƒ…å ±ã‚’å¿…è¦ã¨ã›ãšã€å‹•çš„é ˜åŸŸã®è£œå®Œã‚’è¡Œã†ã€‚æ ¸å¿ƒçš„ãªã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã€ãƒ“ãƒ‡ã‚ªæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€ã‚·ãƒ¼ãƒ³åŠ¹æœã‚’ç‰¹å®šãƒ»é™¤å»ã™ã‚‹ã“ã¨ã§ã‚ã‚Šã€ã“ã‚Œã«ã‚ˆã‚Šé«˜å“è³ªãªåˆ†è§£ã¨ç·¨é›†çµæœã‚’å®Ÿç¾ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/yaochihlee/status/1930473521081397253?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã–ã£ãã‚Šã—ã‹èª­ã‚ã¦ã„ãªã„ãŒã€Inputã¨ã—ã¦å‹•ç”»ã¨maskï¼ˆç™½:æ®‹ã™, é»’:æ¶ˆã™, ã‚°ãƒ¬ãƒ¼: ä¸ç¢ºå®šãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚„ã‚¨ãƒ•ã‚§ã‚¯ãƒˆãŒå«ã¾ã‚Œã‚‹ã‚¨ãƒªã‚¢â‰’èƒŒæ™¯ï¼Ÿ)ã‚’å—ã‘å–ã‚Šã€Casperã¨å‘¼ã°ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«ã§ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¶ˆã—æ¶ˆã—ãŸéƒ¨åˆ†ã‚’inpaintingã™ã‚‹ã“ã¨ã§ã€layerã£ã½ã„ã‚‚ã®ã‚’ä½œæˆã™ã‚‹ã£ã½ã„ï¼ŸCasperã¯&lt;Inputç”»åƒ, maskã€maskã‹ã‚‰ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å‰Šé™¤ã—ãŸç”»åƒï¼ˆå‰Šé™¤ã—ãŸéƒ¨åˆ†ã‚‚ãã¡ã‚“ã¨èƒŒæ™¯ãŒã‚ã‚‹ï¼‰&gt;ã®3çµ„ãƒ‡ãƒ¼ã‚¿ã§Finetuningã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚</p>
<p>project pageãŒã‚µãƒ³ãƒ—ãƒ«ã‚‚ã‚ã‚Šã¨ã¦ã‚‚ã‚ã‹ã‚Šã‚„ã™ã„:


<a href="https://gen-omnimatte.github.io" target="_blank" rel="noopener noreferrer">https://gen-omnimatte.github.io</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/RLVR.html" target="_blank" rel="noopener noreferrer">#RLVR</a>
<a class="button" href="articles/DataMixture.html" target="_blank" rel="noopener noreferrer">#DataMixture</a>
<span class="issue_date">Issue Date: 2025-06-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2015" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement  Learning, Yiqing Liang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æ¤œè¨¼å¯èƒ½ãªå ±é…¬ã‚’ç”¨ã„ãŸå¼·åŒ–å­¦ç¿’ï¼ˆRLVRï¼‰ã‚’ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMsã«é©ç”¨ã™ã‚‹ãŸã‚ã®ãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã€‚ç•°ãªã‚‹è¦–è¦šã¨è¨€èªã®å•é¡Œã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã€æœ€é©ãªãƒ‡ãƒ¼ã‚¿æ··åˆæˆ¦ç•¥ã‚’å°å…¥ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ææ¡ˆã—ãŸæˆ¦ç•¥ãŒMLLMã®æ¨è«–èƒ½åŠ›ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç¤ºã—ã€åˆ†å¸ƒå¤–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å¹³å‡5.24%ã®ç²¾åº¦å‘ä¸Šã‚’é”æˆã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_vztu/status/1930312780701413498?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªè¨­å®šã§RLVRã‚’é©ç”¨ã™ã‚‹ã¨ã€ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å­¦ç¿’ã«åˆ©ç”¨ã™ã‚‹å ´åˆã‚ˆã‚Šã€ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã®ã¿ã®ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ãŸæ–¹ãŒå½“è©²ã‚¿ã‚¹ã‚¯ã§ã¯æ€§èƒ½ãŒé«˜ããªã£ãŸã‚Šï¼ˆã¤ã¾ã‚Šãƒ‡ãƒ¼ã‚¿ãŒå¤šã‘ã‚Œã°å¤šã„ã»ã©è‰¯ã„ã‚ã‘ã§ã¯ç„¡ã„ï¼‰ã€ç‰¹å®šã®ãƒ‡ãƒ¼ã‚¿ã‚’ablationã™ã‚‹ã¨OODã«å¯¾ã™ã‚‹äºˆæ¸¬æ€§èƒ½ãŒæ”¹å–„ã—ãŸã‚Šã™ã‚‹ãªã©ã€ãƒ‡ãƒ¼ã‚¿é–“ã§å¹²æ¸‰ãŒèµ·ãã¦æ•µå¯¾çš„ã«ãªã£ã¦ã—ã¾ã†ã‚ˆã†ãªç¾è±¡ãŒèµ·ãã‚‹ã€‚ã“ã®ã“ã¨ã‹ã‚‰ã€ã©ã®ã‚ˆã†ã«é©åˆ‡ã«ãƒ‡ãƒ¼ã‚¿ã‚’æ··åˆã§ãã‚‹ã‹ï¼Ÿã¨ã„ã†æˆ¦ç•¥ã®å¿…è¦æ€§ãŒæµ®ãå½«ã‚Šã«ãªã‚Šã€ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ãªMixtureæˆ¦ç•¥ï¼ˆã©ã†ã‚„ã‚‰ãƒ‡ãƒ¼ã‚¿ã®æ··åˆåˆ†å¸ƒã‹ã‚‰å­¦ç¿’å¾Œã®æ€§èƒ½ã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãªæ¨¡æ§˜ï¼‰ã®æ€§èƒ½ãŒuniformã«mixã™ã‚‹ã‚ˆã‚Šã‚‚é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ãŸã€ã¿ãŸã„ãªè©±ã‚‰ã—ã„ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2025-05-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1985" target="_blank" rel="noopener noreferrer" class="title-link">LaViDa: A Large Diffusion Language Model for Multimodal Understanding, Shufan Li+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- LaViDaã¯ã€é›¢æ•£æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ï¼ˆDMï¼‰ã‚’åŸºã«ã—ãŸãƒ“ã‚¸ãƒ§ãƒ³ãƒ»ãƒ©ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰ã§ã€é«˜é€Ÿãªæ¨è«–ã¨åˆ¶å¾¡å¯èƒ½ãªç”Ÿæˆã‚’å®Ÿç¾ã€‚æ–°æŠ€è¡“ã‚’å–ã‚Šå…¥ã‚Œã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦AR VLMã¨ç«¶äº‰åŠ›ã®ã‚ã‚‹æ€§èƒ½ã‚’é”æˆã€‚COCOã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã§é€Ÿåº¦å‘ä¸Šã¨æ€§èƒ½æ”¹å–„ã‚’ç¤ºã—ã€AR VLMã®å¼·åŠ›ãªä»£æ›¿æ‰‹æ®µã§ã‚ã‚‹ã“ã¨ã‚’è¨¼æ˜ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1925749919312159167?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Diffusion Modelã®æ³¢ãŒæ¥ãŸ</p>
<p>åŒç¨‹åº¦ã®ã‚µã‚¤ã‚ºã®ARãƒ¢ãƒ‡ãƒ«ã‚’outperform [^1]<br><img src="https://github.com/user-attachments/assets/aeb12147-48ba-4b64-917c-9976ec1ffa0a" alt="image" loading="lazy"><br><br>[^1]:ãŸã ã—ã€ã“ã‚ŒãŒæœ¬å½“ã«Diffusion Modelã‚’ä½¿ã£ãŸã“ã¨ã«ã‚ˆã‚‹æ©æµãªã®ã‹ã¯ã¾ã è«–æ–‡ã‚’èª­ã‚“ã§ã„ãªã„ã®ã§ã‚ã‹ã‚‰ãªã„ã€‚å¿…è¦ã«ãªã£ãŸã‚‰èª­ã‚€ã€‚ãŸã ã€Physics of Language Modelã®ã‚ˆã†ã«ã€å®Œå…¨ã«ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã§ç•°ãªã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¯”è¼ƒã—ãªã„ã¨ãã®è¾ºã¯ã‚ã‹ã‚‰ãªãã†ã§ã¯ã‚ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="articles/DPO.html" target="_blank" rel="noopener noreferrer">#DPO</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/Probing.html" target="_blank" rel="noopener noreferrer">#Probing</a>
<span class="issue_date">Issue Date: 2025-05-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1974" target="_blank" rel="noopener noreferrer" class="title-link">Why Vision Language Models Struggle with Visual Arithmetic? Towards   Enhanced Chart and Geometry Understanding, Kung-Hsiang Huang+, ACL'25</a>
<span class="snippet"><span>GPT Summary</span>- Vision Language Models (VLMs)ã¯è¦–è¦šçš„ç®—è¡“ã«è‹¦åŠ´ã—ã¦ã„ã‚‹ãŒã€CogAlignã¨ã„ã†æ–°ã—ã„ãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã‚’ææ¡ˆã—ã€VLMã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚CogAlignã¯è¦–è¦šçš„å¤‰æ›ã®ä¸å¤‰ç‰¹æ€§ã‚’èªè­˜ã™ã‚‹ã‚ˆã†ã«è¨“ç·´ã—ã€CHOCOLATEã§4.6%ã€MATH-VISIONã§2.9%ã®æ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’60%å‰Šæ¸›ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€åŸºæœ¬çš„ãªè¦–è¦šçš„ç®—è¡“èƒ½åŠ›ã®å‘ä¸Šã¨ä¸‹æµã‚¿ã‚¹ã‚¯ã¸ã®è»¢é€ã®åŠ¹æœãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/steeve__huang/status/1923543884367306763?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ—¢å­˜ã®LLM (proprietary, openweightãã‚Œãã‚Œ)ãŒã€ã‚·ãƒ³ãƒ—ãƒ«ãªvisual arithmeticã‚¿ã‚¹ã‚¯(e.g., ç·šåˆ†ã®é•·ã•æ¯”è¼ƒ, Chartä¸Šã®dotã®ç†è§£)ãªã©ã®æ€§èƒ½ãŒä½ã„ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã—ã€<br><img src="https://github.com/user-attachments/assets/039a48de-67a5-4c81-ba59-174acd508479" alt="image" loading="lazy"><br>ãã‚Œã‚‰ã®åŸå› ã‚’(1)Vision Encoderã®representationã¨(2)Vision Encoderã‚’Freezeã—ãŸä¸Šã§ã®Text Decoderã®finetuningã§åˆ†æã—ãŸã€‚ãã®çµæœã€(1)ã§ã¯ã„ãã¤ã‹ã®ã‚¿ã‚¹ã‚¯ã§linear layerã®probingã§ã¯é«˜ã„æ€§èƒ½ãŒé”æˆã§ããªã„ã“ã¨ãŒã‚ã‹ã£ãŸã€‚ã“ã®ã“ã¨ã‹ã‚‰ã€Vision Encoderã«ã‚ˆã‚‹representationãŒã‚¿ã‚¹ã‚¯ã«é–¢ã™ã‚‹æƒ…å ±ã‚’å†…åŒ…ã§ãã¦ã„ãªã„ã‹ã€ã‚¿ã‚¹ã‚¯ã«é–¢ã™ã‚‹æƒ…å ±ã¯å†…åŒ…ã—ã¦ã„ã‚‹ãŒlinear layerã§ã¯ãã‚Œã‚’ååˆ†ã«å¯èƒ½ã§ããªã„å¯èƒ½æ€§ãŒç¤ºå”†ã•ã‚ŒãŸã€‚<br><img src="https://github.com/user-attachments/assets/0eb90fa2-7b6a-43b6-81d9-b5f7e6fb3ea8" alt="image" loading="lazy"><br><br>ã“ã‚Œã‚’ã•ã‚‰ã«åˆ†æã™ã‚‹ãŸã‚ã«(2)ã‚’å®Ÿæ–½ã—ãŸã¨ã“ã‚ã€Vision Encoderã‚’freezeã—ã¦ã„ã¦ã‚‚finetuningã«ã‚ˆã‚Šquery stringã«é–¢ã‚ã‚‰ãšé«˜ã„æ€§èƒ½ã‚’ç²å¾—ã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ã“ã®ã“ã¨ã‹ã‚‰ã€Vision Encoderå´ã®representationã®å•é¡Œã§ã¯ãªãã€Text Decoderã¨å´ã§ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹éš›ã«Finetuningã—ãªã„ã¨ã†ã¾ãæ´»ç”¨ã§ããªã„ã“ã¨ãŒåˆ¤æ˜ã—ãŸã€‚<br><img src="https://github.com/user-attachments/assets/cd122d99-9228-44b1-9827-cdb56f49d492" alt="image" loading="lazy"></p>
<p>æ‰‹æ³•ã®ã¨ã“ã‚ã¯ã¾ã å…¨ç„¶ã—ã£ã‹ã‚Šèª­ã‚ã¦ã„ãªã„ã®ã ãŒã€ç”»åƒã«é–¢ã™ã‚‹ç‰¹å®šã®å±æ€§ã«é–¢ã™ã‚‹ã‚¯ã‚¨ãƒªã¨å›ç­”ã®ãƒšã‚¢ã‚’åˆæˆã—ã€DPOã™ã‚‹ã“ã¨ã§ã€zero-shotã®æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã€ã¨ã„ã†æ„Ÿã˜ã£ã½ã„ï¼Ÿ<br><img src="https://github.com/user-attachments/assets/707b1cc9-8bbf-45a5-b564-f654503c836e" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/281da17b-c8c3-455a-aa51-043ed297ae1f" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/RepresentationLearning.html" target="_blank" rel="noopener noreferrer">#RepresentationLearning</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/CompressionValleys.html" target="_blank" rel="noopener noreferrer">#CompressionValleys</a>
<span class="issue_date">Issue Date: 2025-05-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1924" target="_blank" rel="noopener noreferrer" class="title-link">Layer by Layer: Uncovering Hidden Representations in Language Models, Oscar Skean+, ICML'25</a>
<span class="snippet"><span>GPT Summary</span>- ä¸­é–“å±¤ã®åŸ‹ã‚è¾¼ã¿ãŒæœ€çµ‚å±¤ã‚’è¶…ãˆã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã™ã“ã¨ã‚’åˆ†æã—ã€æƒ…å ±ç†è«–ã‚„å¹¾ä½•å­¦ã«åŸºã¥ããƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ææ¡ˆã€‚32ã®ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚¿ã‚¹ã‚¯ã§ä¸­é–“å±¤ãŒå¼·åŠ›ãªç‰¹å¾´ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã—ã€AIã‚·ã‚¹ãƒ†ãƒ ã®æœ€é©åŒ–ã«ãŠã‘ã‚‹ä¸­é–“å±¤ã®é‡è¦æ€§ã‚’å¼·èª¿ã€‚</span>
<span class="snippet"><span>Comment</span><p>ç¾ä»£ã®ä»£è¡¨çš„ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆdecoder-only model, encoder-only model, SSMï¼‰ã«ã¤ã„ã¦ã€æœ€çµ‚å±¤ã®embeddingã‚ˆã‚Šã‚‚ä¸­é–“å±¤ã®embeddingã®æ–¹ãŒdownstream taskï¼ˆMTEBã®32Taskã®å¹³å‡ï¼‰ã«ã€ä¸€è²«ã—ã¦ï¼ˆãŸã ã—ã€ã“ã‚Œã¯MTEBã®å¹³å‡ã§è¦‹ãŸã‚‰ãã†ã¨ã„ã†è©±ã§ã‚ã‚Šã€å€‹åˆ¥ã®ã‚¿ã‚¹ã‚¯ã§ä¸€è²«ã—ã¦å¼·ã„ã‹ã¯èª­ã‚“ã§ã¿ãªã„ã¨ã‚ã‹ã‚‰ãªã„ï¼‰å¼·ã„ã“ã¨ã‚’ç¤ºã—ãŸç ”ç©¶ã€‚<br><br>ã“ã®ã“ã¨è‡ªä½“ã¯çµŒé¨“çš„ã«çŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ã®ã§ã‚ã¾ã‚Šé©šãã§ã¯ãªã„ã®ã ãŒï¼ˆãŸã ã€SSMã§ã‚‚ãã†ãªã®ã‹ã€ã¨ã„ã†ã®ã¨ã€ä¸€è²«ã—ã¦å¼·ã„ã¨ã„ã†ã®ã¯èˆˆå‘³æ·±ã„ï¼‰ã€ã“ã®ç ”ç©¶ã¯Matrix Based Entropyã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã«åŸºã¥ã„ã¦ã€ã“ã‚Œã‚‰ã‚’åˆ†æã™ã‚‹ãŸã‚ã®æ§˜ã€…ãªæŒ‡æ¨™ã‚’å®šç¾©ã—ç†è«–çš„ãªæ ¹æ‹ ã‚’ç¤ºã—ã€Autoregressiveãªå­¦ç¿’ã‚ˆã‚Šã‚‚Masked Languageã«ã‚ˆã‚‹å­¦ç¿’ã®æ–¹ãŒã“ã®ã‚ˆã†ãªMiddle Layerã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãŒç·©å’Œã•ã‚Œã€åŒæ§˜ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãŒç”»åƒã®å ´åˆã§ã‚‚èµ·ãã‚‹ã“ã¨ã‚’ç¤ºã—ã€CoTãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸFinetuningã«ã¤ã„ã¦ã‚‚åˆ†æã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚ã“ã®è¾ºã®è²¢çŒ®ãŒéå¸¸ã«å¤§ãã„ã¨æ€ã‚ã‚Œã‚‹ã®ã§ã“ã“ã‚’ç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã ã¨æ€ã‚ã‚Œã‚‹ã€‚ã‚ã¨ã§èª­ã‚€ã€‚<br><br><img src="https://github.com/user-attachments/assets/bda00c50-c97b-45e0-97a5-d98dd98599fd" alt="image" loading="lazy"></p>
<p>openreview:


<a href="https://openreview.net/forum?id=WGXb7UdvTX" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=WGXb7UdvTX</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/ComputerUse.html" target="_blank" rel="noopener noreferrer">#ComputerUse</a>
<span class="issue_date">Issue Date: 2025-04-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1897" target="_blank" rel="noopener noreferrer" class="title-link">AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents, Christopher Rawles+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€116ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦å ±é…¬ä¿¡å·ã‚’æä¾›ã™ã‚‹ã€ŒAndroidWorldã€ã¨ã„ã†å®Œå…¨ãªAndroidç’°å¢ƒã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è‡ªç„¶è¨€èªã§è¡¨ç¾ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚’å‹•çš„ã«æ§‹ç¯‰ã—ã€ç¾å®Ÿçš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿç¾ã€‚åˆæœŸçµæœã§ã¯ã€æœ€è‰¯ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒ30.6%ã®ã‚¿ã‚¹ã‚¯ã‚’å®Œäº†ã—ã€ã•ã‚‰ãªã‚‹ç ”ç©¶ã®ä½™åœ°ãŒç¤ºã•ã‚ŒãŸã€‚ã¾ãŸã€ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—Webã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®Androidé©å¿œãŒåŠ¹æœè–„ã§ã‚ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã€ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿç¾ã«ã¯ã•ã‚‰ãªã‚‹ç ”ç©¶ãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚ŒãŸã€‚ã‚¿ã‚¹ã‚¯ã®å¤‰å‹•ãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ã‚‚ç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>Androidç’°å¢ƒã§ã®Phone Useã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<span class="issue_date">Issue Date: 2025-04-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1883" target="_blank" rel="noopener noreferrer" class="title-link">AM-RADIO: Agglomerative Vision Foundation Model -- Reduce All Domains   Into One, Mike Ranzinger+, CVPR'25</a>
<span class="snippet"><span>GPT Summary</span>- è¦–è¦šåŸºç›¤ãƒ¢ãƒ‡ãƒ«ï¼ˆVFMï¼‰ã‚’ãƒãƒ«ãƒãƒ†ã‚£ãƒ¼ãƒãƒ£ãƒ¼è’¸ç•™ã‚’é€šã˜ã¦çµ±åˆã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒAM-RADIOã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã®è¦–è¦š-è¨€èªç†è§£ã‚„ãƒ”ã‚¯ã‚»ãƒ«ãƒ¬ãƒ™ãƒ«ã®ç†è§£ã‚’å‘ä¸Šã•ã›ã€å€‹ã€…ã®ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è¶…ãˆã‚‹ã€‚æ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£E-RADIOã¯ã€ãƒ†ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚å°‘ãªãã¨ã‚‚7å€é€Ÿã„ã€‚åŒ…æ‹¬çš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æ§˜ã€…ãªä¸‹æµã‚¿ã‚¹ã‚¯ã‚’è©•ä¾¡ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/pavlomolchanov/status/1910391609927360831?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>visionç³»ã®foundation modelã¯ãã‚Œãã‚Œç•°ãªã‚‹ç›®çš„é–¢æ•°ã§è¨“ç·´ã•ã‚Œã¦ãã¦ãŠã‚Šï¼ˆCLIPã¯å¯¾ç…§å­¦ç¿’ <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/550" target="_blank" rel="noopener noreferrer">Learning Transferable Visual Models From Natural Language Supervision, Radford+, OpenAI, ICML'21</a>
, DINOv2ã¯è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1884" target="_blank" rel="noopener noreferrer">DINOv2: Learning Robust Visual Features without Supervision, Maxime Oquab+, TMLR'24</a>
, SAMã¯segmentation <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1885" target="_blank" rel="noopener noreferrer">Segment Anything, Alexander Kirillov+, arXiv'23</a>
)ãã‚Œãã‚Œåˆ¥ã®èƒ½åŠ›ã‚’æŒã£ã¦ãŸãŒã€ãã‚Œã‚‰ã‚’ä¸€å€‹ã®ãƒ¢ãƒ‡ãƒ«ã«è’¸ç•™ã—ã¾ã—ãŸã€ã¨ã„ã†è©±ã‚‰ã—ã„<br><img src="https://github.com/user-attachments/assets/929aaa47-ab88-4912-a59a-579d2f34e886" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/Video.html" target="_blank" rel="noopener noreferrer">#Video</a>
<span class="issue_date">Issue Date: 2025-03-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1843" target="_blank" rel="noopener noreferrer" class="title-link">Qwen2.5-Omni Technical Report, Jin Xu+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã€ŒQwen2.5-Omniã€ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ã€å‹•ç”»ã‚’èªè­˜ã—ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ–¹å¼ã§è‡ªç„¶ãªéŸ³å£°å¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ã€‚éŸ³å£°ã¨è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯ãƒ–ãƒ­ãƒƒã‚¯å‡¦ç†ã‚’ç”¨ã„ã€TMRoPEã«ã‚ˆã‚‹æ–°ã—ã„ä½ç½®åŸ‹ã‚è¾¼ã¿ã§éŸ³å£°ã¨å‹•ç”»ã®åŒæœŸã‚’å®Ÿç¾ã€‚Thinker-Talkerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã¨éŸ³å£°å‡ºåŠ›ã‚’å¹²æ¸‰ãªãè¡Œã†ã€‚Qwen2.5-Omniã¯ã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§è¨“ç·´ã•ã‚Œã€éŸ³å£°æŒ‡ç¤ºã«å¯¾ã™ã‚‹æ€§èƒ½ãŒãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã¨åŒç­‰ã§ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°Talkerã¯æ—¢å­˜æ‰‹æ³•ã‚’ä¸Šå›ã‚‹è‡ªç„¶ã•ã‚’æŒã¤ã€‚</span>
<span class="snippet"><span>Comment</span><p>Qwen Teamã«ã‚ˆã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMã€‚ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€å‹•ç”»éŸ³å£°ã‚’inputã¨ã—ã¦å—ã‘å–ã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆã€éŸ³å£°ã‚’outputã™ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/03e54fd7-2011-4069-aa1b-38d1610169ec" alt="image" loading="lazy"><br><br>weight:


<a href="https://huggingface.co/collections/Qwen/qwen25-omni-67de1e5f0f9464dc6314b36e" target="_blank" rel="noopener noreferrer">https://huggingface.co/collections/Qwen/qwen25-omni-67de1e5f0f9464dc6314b36e</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:


<a href="https://www.linkedin.com/posts/niels-rogge-a3b7a3127_alibabas-qwen-team-has-done-it-again-this-activity-7311036679627132929-HUqy?utm_source=share&utm_medium=member_ios&rcm=ACoAACzQvjwB2FeLVE3yukDiUYtr5J4k-6nlNG4" target="_blank" rel="noopener noreferrer">https://www.linkedin.com/posts/niels-rogge-a3b7a3127_alibabas-qwen-team-has-done-it-again-this-activity-7311036679627132929-HUqy?utm_source=share&utm_medium=member_ios&rcm=ACoAACzQvjwB2FeLVE3yukDiUYtr5J4k-6nlNG4</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<span class="issue_date">Issue Date: 2025-03-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1776" target="_blank" rel="noopener noreferrer" class="title-link">Large Language Diffusion Models, Shen Nie+, NeurIPS'25</a>
<span class="snippet"><span>GPT Summary</span>- LLaDAã¯ã€è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆARMsï¼‰ã«ä»£ã‚ã‚‹æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ã‚¼ãƒ­ã‹ã‚‰è¨“ç·´ã•ã‚Œã€ãƒ‡ãƒ¼ã‚¿ãƒã‚¹ã‚­ãƒ³ã‚°ã‚’é€šã˜ã¦åˆ†å¸ƒã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã€‚åºƒç¯„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å¼·åŠ›ãªã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’ç¤ºã—ã€è‡ªå·±æ§‹ç¯‰ã—ãŸARMãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚‹ã€‚ç‰¹ã«ã€LLaDA 8Bã¯æ–‡è„ˆå†…å­¦ç¿’ã‚„æŒ‡ç¤ºè¿½å¾“èƒ½åŠ›ã«å„ªã‚Œã€é€†è©©ã®å®Œæˆã‚¿ã‚¹ã‚¯ã§GPT-4oã‚’è¶…ãˆã‚‹æ€§èƒ½ã‚’ç™ºæ®ã€‚æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ãŒARMsã®å®Ÿè¡Œå¯èƒ½ãªä»£æ›¿æ‰‹æ®µã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dair_ai/status/1893698288328602022?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å‚è€ƒ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/karpathy/status/1894923254864978091"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>openreview(ICLR'25):


<a href="https://openreview.net/forum?id=W2tWu0aikL" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=W2tWu0aikL</a>


</p>
<p>pj page:


<a href="https://ml-gsai.github.io/LLaDA-demo/" target="_blank" rel="noopener noreferrer">https://ml-gsai.github.io/LLaDA-demo/</a>


</p>
<p>openreview(NeurIPS'25):


<a href="https://openreview.net/forum?id=KnqiC0znVF" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=KnqiC0znVF</a>


</p></span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-01-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1740" target="_blank" rel="noopener noreferrer" class="title-link">SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model   Post-training, Tianzhe Chu+, ICML'25</a>
<span class="snippet"><span>GPT Summary</span>- SFTã¨RLã®ä¸€èˆ¬åŒ–èƒ½åŠ›ã®é•ã„ã‚’ç ”ç©¶ã—ã€GeneralPointsã¨V-IRLã‚’ç”¨ã„ã¦è©•ä¾¡ã€‚RLã¯ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆã¨è¦–è¦šå¤‰ç¨®ã«å¯¾ã—ã¦å„ªã‚ŒãŸä¸€èˆ¬åŒ–ã‚’ç¤ºã™ä¸€æ–¹ã€SFTã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜æ†¶ã—åˆ†å¸ƒå¤–ã‚·ãƒŠãƒªã‚ªã«è‹¦åŠ´ã€‚RLã¯è¦–è¦šèªè­˜èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ãŒã€SFTã¯RLè¨“ç·´ã«ä¸å¯æ¬ ã§ã‚ã‚Šã€å‡ºåŠ›å½¢å¼ã‚’å®‰å®šã•ã›ã‚‹ã“ã¨ã§æ€§èƒ½å‘ä¸Šã‚’ä¿ƒé€²ã€‚ã“ã‚Œã‚‰ã®çµæœã¯ã€è¤‡é›‘ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹RLã®ä¸€èˆ¬åŒ–èƒ½åŠ›ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1884731381517082668?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>openreview:


<a href="https://openreview.net/forum?id=dYur3yabMj&referrer=%5Bthe%20profile%20of%20Yi%20Ma%5D(%2Fprofile%3Fid%3D~Yi_Ma4)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=dYur3yabMj&referrer=%5Bthe%20profile%20of%20Yi%20Ma%5D(%2Fprofile%3Fid%3D~Yi_Ma4)</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-01-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1730" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Humanity's Last Exam, Long Phan+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ã€Œäººé¡ã®æœ€å¾Œã®è©¦é¨“ï¼ˆHLEï¼‰ã€ã‚’å°å…¥ã—ã€LLMã®èƒ½åŠ›ã‚’æ¸¬å®šã™ã‚‹æ–°ã—ã„ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ææ¡ˆã€‚HLEã¯2,500ã®è³ªå•ã‹ã‚‰æˆã‚Šã€æ•°å­¦ã‚„è‡ªç„¶ç§‘å­¦ãªã©åºƒç¯„ãªç§‘ç›®ã‚’ã‚«ãƒãƒ¼ã€‚å°‚é–€å®¶ã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚Œã€è‡ªå‹•æ¡ç‚¹ãŒå¯èƒ½ãªå½¢å¼ã§ã€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¤œç´¢ã§ã¯è¿…é€Ÿã«å›ç­”ã§ããªã„ã€‚æœ€å…ˆç«¯ã®LLMã¯HLEã«å¯¾ã—ã¦ä½ã„ç²¾åº¦ã‚’ç¤ºã—ã€ç¾åœ¨ã®LLMã®èƒ½åŠ›ã¨å°‚é–€å®¶ã®çŸ¥è­˜ã¨ã®é–“ã«å¤§ããªã‚®ãƒ£ãƒƒãƒ—ãŒã‚ã‚‹ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã€‚HLEã¯å…¬é–‹ã•ã‚Œã€ç ”ç©¶ã‚„æ”¿ç­–ç«‹æ¡ˆã«å½¹ç«‹ã¦ã‚‰ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>o1, DeepSeekR1ã®æ­£è§£ç‡ãŒ10%æœªæº€ã®æ–°ãŸãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/TreeSearch.html" target="_blank" rel="noopener noreferrer">#TreeSearch</a>
<span class="issue_date">Issue Date: 2024-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1630" target="_blank" rel="noopener noreferrer" class="title-link">Mulberry: Empowering MLLM with o1-like Reasoning and Reflection via   Collective Monte Carlo Tree Search, Huanjin Yao+, NeurIPS'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€MLLMã‚’ç”¨ã„ã¦è³ªå•è§£æ±ºã®ãŸã‚ã®æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã‚’å­¦ç¿’ã™ã‚‹æ–°æ‰‹æ³•CoMCTSã‚’ææ¡ˆã€‚é›†å›£å­¦ç¿’ã‚’æ´»ç”¨ã—ã€è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã§åŠ¹æœçš„ãªæ¨è«–çµŒè·¯ã‚’æ¢ç´¢ã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆMulberry-260kã‚’æ§‹ç¯‰ã—ã€ãƒ¢ãƒ‡ãƒ«Mulberryã‚’è¨“ç·´ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šææ¡ˆæ‰‹æ³•ã®å„ªä½æ€§ã‚’ç¢ºèªã€‚</span>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/TMLR.html" target="_blank" rel="noopener noreferrer">#TMLR</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<span class="issue_date">Issue Date: 2024-11-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1505" target="_blank" rel="noopener noreferrer" class="title-link">Mixture-of-Transformers: A Sparse and Scalable Architecture for   Multi-Modal Foundation Models, Weixin Liang+, TMLR'25</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å‡¦ç†ã‚’åŠ¹ç‡åŒ–ã™ã‚‹ãŸã‚ã«ã€Mixture-of-Transformersï¼ˆMoTï¼‰ã‚’ææ¡ˆã€‚MoTã¯è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã—ã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã”ã¨ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆ†é›¢ã—ã¦ç‰¹åŒ–ã—ãŸå‡¦ç†ã‚’å®Ÿç¾ã€‚Chameleon 7Bè¨­å®šã§ã¯ã€55.8%ã®FLOPsã§å¯†ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«åŒ¹æ•µã™ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€éŸ³å£°ã‚’å«ã‚€å ´åˆã‚‚37.2%ã®FLOPsã§åŒæ§˜ã®çµæœã‚’é”æˆã€‚ã•ã‚‰ã«ã€Transfusionè¨­å®šã§ã¯ã€7Bã®MoTãƒ¢ãƒ‡ãƒ«ãŒå¯†ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ç”»åƒæ€§èƒ½ã«å¯¾ã—ã¦FLOPsã®3åˆ†ã®1ã§åŒ¹æ•µã—ã€760Mã®ãƒ¢ãƒ‡ãƒ«ã¯ä¸»è¦ãªç”»åƒç”ŸæˆæŒ‡æ¨™ã§ä¸Šå›ã‚‹çµæœã‚’å¾—ãŸã€‚MoTã¯å®Ÿç”¨çš„ãªåˆ©ç‚¹ã‚‚ç¤ºã—ã€ç”»åƒå“è³ªã‚’47.2%ã€ãƒ†ã‚­ã‚¹ãƒˆå“è³ªã‚’75.6%ã®çµŒéæ™‚é–“ã§é”æˆã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ModelMerge.html" target="_blank" rel="noopener noreferrer">#ModelMerge</a>
<span class="issue_date">Issue Date: 2024-03-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1257" target="_blank" rel="noopener noreferrer" class="title-link">Evolutionary Optimization of Model Merging Recipes, Takuya Akiba+, N_A, Nature Machine Intelligence'25</a>
<span class="snippet"><span>GPT Summary</span>- é€²åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨ã—ãŸæ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã€å¼·åŠ›ãªåŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•ç”Ÿæˆã‚’å®Ÿç¾ã€‚LLMã®é–‹ç™ºã«ãŠã„ã¦ã€äººé–“ã®ç›´æ„Ÿã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«ä¾å­˜ã›ãšã€å¤šæ§˜ãªã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®åŠ¹æœçš„ãªçµ„ã¿åˆã‚ã›ã‚’è‡ªå‹•çš„ã«ç™ºè¦‹ã™ã‚‹ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€æ—¥æœ¬èªã®LLMã¨æ•°å­¦æ¨è«–èƒ½åŠ›ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ãªã©ã€ç•°ãªã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³é–“ã®çµ±åˆã‚’å®¹æ˜“ã«ã—ã€æ—¥æœ¬èªVLMã®æ€§èƒ½å‘ä¸Šã«ã‚‚è²¢çŒ®ã€‚ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¸ã®è²¢çŒ®ã¨è‡ªå‹•ãƒ¢ãƒ‡ãƒ«æ§‹æˆã®æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ å°å…¥ã«ã‚ˆã‚Šã€åŸºç›¤ãƒ¢ãƒ‡ãƒ«é–‹ç™ºã«ãŠã‘ã‚‹åŠ¹ç‡çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’æ¨¡ç´¢ã€‚</span>
<span class="snippet"><span>Comment</span><p>è¤‡æ•°ã®LLMã‚’èåˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ã®è©±ã€‚æ—¥æœ¬èªLLMã¨è‹±èªã®æ•°å­¦LLNã‚’ãƒãƒ¼ã‚¸ã•ã›ã‚‹ã“ã¨ã§æ—¥æœ¬èªã®æ•°å­¦æ€§èƒ½ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ãŸã‚Šã€LLMã¨VLMã‚’èåˆã—ãŸã‚Šã™ã‚‹ã“ã¨ã§ã€æ—¥æœ¬ã«ã—ã‹å­˜åœ¨ã—ãªã„æ¦‚å¿µã®ç”»åƒã‚‚ã€ãã¡ã‚“ã¨å›ç­”ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚<br><br>è‘—è€…ã‚¹ãƒ©ã‚¤ãƒ‰ã«ã‚ˆã‚‹ã¨ã€å¾“æ¥ã®ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ã«ã¯base modelãŒåŒä¸€ã§ãªã„ã¨ã†ã¾ãã„ã‹ãªã‹ã£ãŸã‚Šï¼ˆé‡ã¿ã®ç·šå‹çµåˆã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ï¼‰ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¢—æ¸›ã—ãŸã‚Šï¼ˆè¤‡æ•°LLMã®Layerã‚’é‡ã¿ã¯å¼„ã‚‰ãšå†é…ç½®ã™ã‚‹ï¼‰ã€‚ã¾ãŸæ—¥æœ¬èªLLMã«å¯¾ã—ã¦ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ã‚’å®Ÿæ–½ã—ã‚ˆã†ã¨ã™ã‚‹ã¨ã€ãƒãƒ¼ã‚¸å…ƒã®LLMãŒå°‘ãªã‹ã£ãŸã‚Šã€åºƒç¯„å›²ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ‰±ã†ã¨ãƒãƒ¼ã‚¸ãŒã†ã¾ãã„ã‹ãªã„ã€ã¨ã„ã£ãŸèª²é¡ŒãŒã‚ã£ãŸã€‚æœ¬ç ”ç©¶ã§ã¯ã“ã‚Œã‚‰èª²é¡Œã‚’è§£æ±ºã§ãã‚‹ã€‚</p>
<p>è‘—è€…ã«ã‚ˆã‚‹è³‡æ–™ï¼ˆNLPã‚³ãƒ­ã‚­ã‚¦ãƒ ï¼‰:<br>


<a href="https://speakerdeck.com/iwiwi/17-nlpkorokiumu" target="_blank" rel="noopener noreferrer">https://speakerdeck.com/iwiwi/17-nlpkorokiumu</a>


</p></span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Zero/Few/ManyShotPrompting.html" target="_blank" rel="noopener noreferrer">#Zero/Few/ManyShotPrompting</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-10-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3467" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Many-Shot In-Context Learning in Multimodal Foundation Models, Yixing Jiang+, arXiv'24, 2024.05</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®å°‘æ•°ã‚·ãƒ§ãƒƒãƒˆã‹ã‚‰å¤šæ•°ã‚·ãƒ§ãƒƒãƒˆã®ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ï¼ˆICLï¼‰ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã€2,000ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¾‹ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã„ã¦å¤§å¹…ãªæ”¹å–„ã‚’è¦³å¯Ÿã—ã¾ã—ãŸã€‚ç‰¹ã«ã€Gemini 1.5 Proã¯å¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å¯¾æ•°çš„ã«æ€§èƒ½ãŒå‘ä¸Šã—ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚¦ã‚§ã‚¤ãƒˆãƒ¢ãƒ‡ãƒ«ã¯ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¾‹ã‹ã‚‰ã®æ©æµã‚’å—ã‘ãªã„ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã¾ã—ãŸã€‚ã¾ãŸã€è¤‡æ•°ã®ã‚¯ã‚¨ãƒªã‚’ãƒãƒƒãƒå‡¦ç†ã™ã‚‹ã“ã¨ã§ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãŠã‚ˆã³å¤šæ•°ã‚·ãƒ§ãƒƒãƒˆICLã®æ€§èƒ½ãŒå‘ä¸Šã—ã€ã‚³ã‚¹ãƒˆã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒå‰Šæ¸›ã•ã‚Œã¾ã—ãŸã€‚æœ€çµ‚çš„ã«ã€GPT-4oã¨Gemini 1.5 Proã¯é¡ä¼¼ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ€§èƒ½ã‚’ç¤ºã—ã¤ã¤ã€Gemini 1.5 Proã¯ã‚ˆã‚Šæ—©ãå­¦ç¿’ã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚å¤šæ•°ã‚·ãƒ§ãƒƒãƒˆICLã¯æ–°ã—ã„ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¸ã®é©å¿œã‚’åŠ¹ç‡åŒ–ã™ã‚‹å¯èƒ½æ€§ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/marchoelle/status/1982589731260203110?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<a class="button" href="articles/ImageSynthesis.html" target="_blank" rel="noopener noreferrer">#ImageSynthesis</a>
<a class="button" href="articles/GeometryUnderstanding.html" target="_blank" rel="noopener noreferrer">#GeometryUnderstanding</a>
<span class="issue_date">Issue Date: 2025-10-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3409" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Shadows Don't Lie and Lines Can't Bend Generative Models don't know  Projective Geometry...for now, Ayush Sarkar+, CVPR'24, 2023.11</a>
<span class="snippet"><span>GPT Summary</span>- ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ãƒªã‚¢ãƒ«ãªç”»åƒã‚’ç”Ÿæˆã™ã‚‹ãŒã€å¹¾ä½•å­¦çš„ç‰¹å¾´ã«ãŠã„ã¦å®Ÿéš›ã®ç”»åƒã¨ç•°ãªã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚äº‹å‰ã«é¸åˆ¥ã•ã‚ŒãŸç”Ÿæˆç”»åƒã‚’ç”¨ã„ã¦ã€å¹¾ä½•å­¦çš„ç‰¹æ€§ã«åŸºã¥ãåˆ†é¡å™¨ãŒç”Ÿæˆç”»åƒã‚’é«˜ç²¾åº¦ã§è­˜åˆ¥ã§ãã‚‹ã“ã¨ã‚’ç¢ºèªã€‚3ã¤ã®åˆ†é¡å™¨ã‚’ä½¿ç”¨ã—ã€ç”»åƒã®é€è¦–å ´ã€ç·šã€ç‰©ä½“ã¨å½±ã®é–¢ä¿‚ã‚’åˆ†æã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç”Ÿæˆç”»åƒã®æ¤œå‡ºç²¾åº¦ãŒå‘ä¸Šã—ã€ç¾åœ¨ã®ç”Ÿæˆå™¨ã¯å®Ÿéš›ã®ç”»åƒã®å¹¾ä½•å­¦çš„ç‰¹æ€§ã‚’å†ç¾ã§ããªã„ã¨çµè«–ä»˜ã‘ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page: 


<a href="https://projective-geometry.github.io/" target="_blank" rel="noopener noreferrer">https://projective-geometry.github.io/</a>


</p></span><br><br>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/TMLR.html" target="_blank" rel="noopener noreferrer">#TMLR</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/4D%20(Video).html" target="_blank" rel="noopener noreferrer">#4D (Video)</a>
<span class="issue_date">Issue Date: 2025-10-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3295" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Video Diffusion Models: A Survey, Andrew Melnik+, TMLR'24, 2024.05</a>
<span class="snippet"><span>GPT Summary</span>- æ‹¡æ•£ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯é«˜å“è³ªãªå‹•ç”»ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç”Ÿæˆã«ãŠã„ã¦é‡è¦ãªæŠ€è¡“ã§ã‚ã‚Šã€æœ¬èª¿æŸ»ã¯ãã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚„æ™‚é–“çš„ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚’åŒ…æ‹¬çš„ã«ã¾ã¨ã‚ã¦ã„ã‚‹ã€‚ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å‹•ç”»ã¸ã®ç”Ÿæˆã®é€²å±•ã‚„ã€ãƒ¢ãƒ‡ãƒ«ã®åˆ†é¡æ³•ã€è©•ä¾¡æŒ‡æ¨™ã«ã¤ã„ã¦ã‚‚è­°è«–ã—ã€ç¾åœ¨ã®èª²é¡Œã‚„å°†æ¥ã®æ–¹å‘æ€§ã‚’è€ƒå¯Ÿã—ã¦ã„ã‚‹ã€‚ç ”ç©¶è€…ã‚„å®Ÿå‹™è€…ã«ã¨ã£ã¦æœ‰ç›Šãªãƒªã‚½ãƒ¼ã‚¹ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã‚‹ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-10-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3138" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Aria: An Open Multimodal Native Mixture-of-Experts Model, Dongxu Li+, arXiv'24, 2024.10</a>
<span class="snippet"><span>GPT Summary</span>- Ariaã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒã‚¤ãƒ†ã‚£ãƒ–AIãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€è¦–è¦šã¨ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦é«˜ã„æ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚3.9Bã®è¦–è¦šãƒˆãƒ¼ã‚¯ãƒ³ã¨3.5Bã®ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŒã¤ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®æ··åˆãƒ¢ãƒ‡ãƒ«ã§ã€æ—¢å­˜ã®ãƒ—ãƒ­ãƒ—ãƒ©ã‚¤ã‚¨ã‚¿ãƒªãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚Šã¾ã™ã€‚è¨€èªç†è§£ã‚„ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ç†è§£ã‚’å¼·åŒ–ã™ã‚‹4æ®µéšã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã§äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚§ã‚¤ãƒˆã¨ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã¨ã—ã¦æä¾›ã•ã‚Œã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/lijunnan0409/status/1975341550080303467?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>HF:


<a href="https://huggingface.co/rhymes-ai/Aria" target="_blank" rel="noopener noreferrer">https://huggingface.co/rhymes-ai/Aria</a>


</p>
<p>ææ¡ˆã•ã‚ŒãŸå½“æ™‚2024å¹´10æœˆæ™‚ç‚¹ã§ã€Visionã¨Text UnderstandingåŒæ–¹ã§ã«å¼·ã„åˆã‚ã¦ã®ãƒ¢ãƒ‡ãƒ«ã§ã€åˆã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«MoEãƒ¢ãƒ‡ãƒ«ã§ï¼ˆå½“æ™‚ã¾ã è©±é¡Œã«ãªã£ã¦ã„ãªã‹ã£ãŸDeepSeek-V2ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¡ç”¨ï¼‰ã€LongVideoã®Understanidinpã§å½“æ™‚ã®æœ€é«˜æ€§èƒ½ã§ã‚ã£ãŸã¨ã®ã“ã¨ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/ECCV.html" target="_blank" rel="noopener noreferrer">#ECCV</a>
<span class="issue_date">Issue Date: 2025-09-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2813" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Implicit Style-Content Separation using B-LoRA, Yarden Frenkel+, ECCV'24</a>
<span class="snippet"><span>GPT Summary</span>- ç”»åƒã‚¹ã‚¿ã‚¤ãƒ©ã‚¤ã‚ºã«ãŠã„ã¦ã€LoRAã‚’ç”¨ã„ã¦ã‚¹ã‚¿ã‚¤ãƒ«ã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æš—é»™çš„ã«åˆ†é›¢ã™ã‚‹æ‰‹æ³•B-LoRAã‚’ææ¡ˆã€‚ç‰¹å®šã®ãƒ–ãƒ­ãƒƒã‚¯ã®LoRAé‡ã¿ã‚’å…±åŒã§å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€ç‹¬ç«‹ã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯é”æˆã§ããªã„åˆ†é›¢ã‚’å®Ÿç¾ã€‚ã“ã‚Œã«ã‚ˆã‚Šã‚¹ã‚¿ã‚¤ãƒ«æ“ä½œãŒæ”¹å–„ã•ã‚Œã€éå­¦ç¿’ã®å•é¡Œã‚’å…‹æœã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã¯ã€ç‹¬ç«‹ã—ãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¨ã—ã¦æ§˜ã€…ãªã‚¹ã‚¿ã‚¤ãƒ©ã‚¤ã‚ºã‚¿ã‚¹ã‚¯ã«åˆ©ç”¨å¯èƒ½ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://b-lora.github.io/B-LoRA/" target="_blank" rel="noopener noreferrer">https://b-lora.github.io/B-LoRA/</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<span class="issue_date">Issue Date: 2025-09-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2770" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment, Xiwei Hu+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’çµ„ã¿è¾¼ã‚€ã€ŒåŠ¹ç‡çš„ãªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ï¼ˆELLAï¼‰ã€ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¤‡é›‘ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ•´åˆæ€§ã‚’å‘ä¸Šã•ã›ã€æ„å‘³çš„ç‰¹å¾´ã‚’é©å¿œã•ã›ã‚‹æ–°ã—ã„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€Œæ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—èªè­˜ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚³ãƒã‚¯ã‚¿ï¼ˆTSCï¼‰ã€ã‚’å°å…¥ã€‚ELLAã¯å¯†ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹æ€§èƒ½ãŒæœ€å…ˆç«¯æ‰‹æ³•ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’å®Ÿé¨“ã§ç¤ºã—ã€ç‰¹ã«è¤‡æ•°ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ§‹æˆã«ãŠã„ã¦å„ªä½æ€§ã‚’ç™ºæ®ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://ella-diffusion.github.io" target="_blank" rel="noopener noreferrer">https://ella-diffusion.github.io</a>


</p></span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Prompting.html" target="_blank" rel="noopener noreferrer">#Prompting</a>
<span class="issue_date">Issue Date: 2025-08-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2546" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] As Generative Models Improve, People Adapt Their Prompts, Eaman Jahani+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å®Ÿé¨“ã§1893äººã®å‚åŠ è€…ã‚’å¯¾è±¡ã«ã€DALL-E 2ã¨DALL-E 3ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é‡è¦æ€§ã®å¤‰åŒ–ã‚’èª¿æŸ»ã€‚DALL-E 3ã‚’ä½¿ç”¨ã—ãŸå‚åŠ è€…ã¯ã€DALL-E 2ã‚ˆã‚Šã‚‚é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã€ã“ã‚Œã¯æŠ€è¡“çš„èƒ½åŠ›ã®å‘ä¸Šã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è³ªã®å¤‰åŒ–ã«ã‚ˆã‚‹ã‚‚ã®ã€‚ç‰¹ã«ã€DALL-E 3ã®å‚åŠ è€…ã¯ã‚ˆã‚Šé•·ãã€æ„å‘³çš„ã«é¡ä¼¼ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¿®æ­£æ©Ÿèƒ½ã‚’æŒã¤DALL-E 3ã¯ã•ã‚‰ã«é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ãŸãŒã€ãã®åˆ©ç‚¹ã¯æ¸›å°‘ã€‚çµæœã¨ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®é€²åŒ–ã«ä¼´ã„ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚‚é©å¿œã•ã‚Œã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dair_ai/status/1959644116305748388?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2529" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] InternVL: Scaling up Vision Foundation Models and Aligning for Generic   Visual-Linguistic Tasks, Zhe Chen+, CVPR'24</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¦–è¦š-è¨€èªåŸºç›¤ãƒ¢ãƒ‡ãƒ«ï¼ˆInternVLï¼‰ã¯ã€60å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§è¨­è¨ˆã•ã‚Œã€LLMã¨æ•´åˆã•ã›ã‚‹ãŸã‚ã«ã‚¦ã‚§ãƒ–è¦æ¨¡ã®ç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã€‚è¦–è¦šèªçŸ¥ã‚¿ã‚¹ã‚¯ã‚„ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆåˆ†é¡ã€æ¤œç´¢ãªã©32ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’é”æˆã—ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾è©±ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã«å¯„ä¸ã€‚ViT-22Bã®ä»£æ›¿ã¨ã—ã¦å¼·åŠ›ãªè¦–è¦šèƒ½åŠ›ã‚’æŒã¤ã€‚ã‚³ãƒ¼ãƒ‰ã¨ãƒ¢ãƒ‡ãƒ«ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¢å­˜ã®ResNetã®ã‚ˆã†ãªSupervised pretrainingã«åŸºã¥ããƒ¢ãƒ‡ãƒ«ã€CLIPã®ã‚ˆã†ãªcontrastive pretrainingã«åŸºã¥ããƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ã€text encoderéƒ¨åˆ†ã‚’LLMã«ç½®ãæ›ãˆã¦ã€contrastive learningã¨generativeã‚¿ã‚¹ã‚¯ã«ã‚ˆã‚‹å­¦ç¿’ã‚’çµ„ã¿åˆã‚ã›ãŸãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’ææ¡ˆã€‚<br><img src="https://github.com/user-attachments/assets/eca53a4a-1d3b-46f1-a833-07ef16b8d5f7" alt="image" loading="lazy"><br><br>InternVLã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ä¸‹è¨˜ã§ã€3 stageã®å­¦ç¿’ã§æ§‹æˆã•ã‚Œã‚‹ã€‚æœ€åˆã«image text pairã‚’contrastive learningã—å­¦ç¿’ã—ã€ç¶šã„ã¦ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯freezeã—image text retrievalã‚¿ã‚¹ã‚¯ç­‰ã§ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ã®å¤‰æ›ã‚’æ‹…ã†æœ€çµ‚çš„ã«QlLlama(multilingualæ€§èƒ½ã‚’é«˜ã‚ãŸllama)ã‚’vision-languageãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’ç¹‹ããƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã®ã‚ˆã†ã«æ‰ãˆã€Vicunaã‚’ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ã‚³ãƒ¼ãƒ€ã¨ã—ã¦æ¥ç¶šã—ã¦generative cossã§å­¦ç¿’ã™ã‚‹ã€ã¿ãŸã„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ¨¡æ§˜ï¼ˆæ–œã‚èª­ã¿ãªã®ã§å°‘ã—é•ã†å¯èƒ½æ€§ã‚ã‚Š<br><br><img src="https://github.com/user-attachments/assets/46a2a0fe-721c-4336-b5ec-657caa5c4771" alt="image" loading="lazy"></p>
<p>ç¾åœ¨ã®VLMã®ä¸»æµã§ã‚ã‚‹vision encoderã¨LLMã‚’adapterã§æ¥ç¶šã™ã‚‹æ–¹å¼ã¯ã“ã“ã‹ã‚‰ã‹ãªã‚Šã‚·ãƒ³ãƒ—ãƒ«ã«ãªã£ã¦ã„ã‚‹ã“ã¨ãŒä¼ºãˆã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/QuestionAnswering.html" target="_blank" rel="noopener noreferrer">#QuestionAnswering</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/MultiLingual.html" target="_blank" rel="noopener noreferrer">#MultiLingual</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/Cultural.html" target="_blank" rel="noopener noreferrer">#Cultural</a>
<span class="issue_date">Issue Date: 2025-08-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2471" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] CVQA: Culturally-diverse Multilingual Visual Question Answering  Benchmark, David Romero+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- CVQAã¯ã€æ–‡åŒ–çš„ã«å¤šæ§˜ãªå¤šè¨€èªã®Visual Question Answeringãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã€30ã‹å›½ã‹ã‚‰ã®ç”»åƒã¨è³ªå•ã‚’å«ã¿ã€31ã®è¨€èªã¨13ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ã‚«ãƒãƒ¼ã€‚ãƒ‡ãƒ¼ã‚¿åé›†ã«ã¯ãƒã‚¤ãƒ†ã‚£ãƒ–ã‚¹ãƒ”ãƒ¼ã‚«ãƒ¼ã‚’é–¢ä¸ã•ã›ã€åˆè¨ˆ10,000ã®è³ªå•ã‚’æä¾›ã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã—ã€æ–‡åŒ–çš„èƒ½åŠ›ã¨ãƒã‚¤ã‚¢ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®æ–°ãŸãªåŸºæº–ã‚’ç¤ºã™ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/InstructionTuning.html" target="_blank" rel="noopener noreferrer">#InstructionTuning</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/MultiLingual.html" target="_blank" rel="noopener noreferrer">#MultiLingual</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2470" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages, Xiang Yue+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- Pangeaã¯ã€39ã®è¨€èªã«ã‚ãŸã‚‹6MæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆPangeaInsã‚’ç”¨ã„ã¦è¨“ç·´ã•ã‚ŒãŸå¤šè¨€èªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMã§ã‚ã‚Šã€ç•°æ–‡åŒ–é–“ã®ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’ç¢ºä¿ã—ã¦ã„ã¾ã™ã€‚Pangeaã¯ã€47ã®è¨€èªã‚’ã‚«ãƒãƒ¼ã™ã‚‹è©•ä¾¡ã‚¹ã‚¤ãƒ¼ãƒˆPangeaBenchã§æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€è‹±èªãƒ‡ãƒ¼ã‚¿ã®æ¯”ç‡ã‚„ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«ã®é‡è¦æ€§ã‚’æ˜ã‚‰ã‹ã«ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ¼ã‚¿ã€ã‚³ãƒ¼ãƒ‰ã€è¨“ç·´æ¸ˆã¿ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã•ã‚Œã€è¨€èªçš„ãŠã‚ˆã³æ–‡åŒ–çš„å…¬å¹³æ€§ã‚’æ¨é€²ã—ã¾ã™ã€‚</span>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ImageSegmentation.html" target="_blank" rel="noopener noreferrer">#ImageSegmentation</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/ImageClassification.html" target="_blank" rel="noopener noreferrer">#ImageClassification</a>
<span class="issue_date">Issue Date: 2025-08-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2420" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] MambaOut: Do We Really Need Mamba for Vision?, Weihao Yu+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- Mambaã¯RNNã®ã‚ˆã†ãªãƒˆãƒ¼ã‚¯ãƒ³ãƒŸã‚­ã‚µãƒ¼ã‚’æŒã¤ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã€è¦–è¦šã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦æœŸå¾…å¤–ã‚Œã®æ€§èƒ½ã‚’ç¤ºã™ã€‚Mambaã¯é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¨è‡ªå·±å›å¸°çš„ãªç‰¹æ€§ã«é©ã—ã¦ã„ã‚‹ãŒã€ç”»åƒåˆ†é¡ã«ã¯ä¸å‘ãã§ã‚ã‚‹ã¨ä»®å®šã€‚MambaOutãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã€å®Ÿé¨“ã«ã‚ˆã‚ŠMambaOutãŒImageNetã®ç”»åƒåˆ†é¡ã§è¦–è¦šMambaãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ã€æ¤œå‡ºãŠã‚ˆã³ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã§ã¯Mambaã®å¯èƒ½æ€§ã‚’æ¢ã‚‹ä¾¡å€¤ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<span class="issue_date">Issue Date: 2025-08-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2385" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning   Benchmark for Expert AGI, Xiang Yue+, CVPR'24</a>
<span class="snippet"><span>GPT Summary</span>- MMMUã¯ã€å¤§å­¦ãƒ¬ãƒ™ãƒ«ã®å°‚é–€çŸ¥è­˜ã¨æ„å›³çš„ãªæ¨è«–ã‚’å¿…è¦ã¨ã™ã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã®ãŸã‚ã®æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã€11,500ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è³ªå•ã‚’å«ã‚€ã€‚6ã¤ã®ä¸»è¦åˆ†é‡ã‚’ã‚«ãƒãƒ¼ã—ã€30ç¨®é¡ã®ç”»åƒã‚¿ã‚¤ãƒ—ã‚’ä½¿ç”¨ã€‚æ—¢å­˜ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ç•°ãªã‚Šã€å°‚é–€å®¶ãŒç›´é¢ã™ã‚‹ã‚¿ã‚¹ã‚¯ã«é¡ä¼¼ã—ãŸèª²é¡Œã‚’æä¾›ã€‚GPT-4Vã¨Geminiã®è©•ä¾¡ã§ã¯ã€56%ã¨59%ã®ç²¾åº¦ã«ã¨ã©ã¾ã‚Šã€æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚MMMUã¯æ¬¡ä¸–ä»£ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã«å¯„ä¸ã™ã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>MMMUã®ãƒªãƒªãƒ¼ã‚¹ã‹ã‚‰20ãƒ¶æœˆçµŒéã—ãŸãŒã€ã„ã¾ã ã«äººé–“ã®ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã«ã¯åŠã°ãªã„ã¨ã®ã“ã¨<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/xiangyue96/status/1953902213790830931?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>MMMUã®ã‚µãƒ³ãƒ—ãƒ«ã¯ã“ã¡ã‚‰ã€‚å„åˆ†é‡ã”ã¨ã«å°‚é–€å®¶ãƒ¬ãƒ™ãƒ«ã®çŸ¥è­˜ã¨æ¨è«–ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹ã¨ã®ã“ã¨ã€‚<br><img src="https://github.com/user-attachments/assets/90839a16-d7d2-499d-b2d8-52dab8988e52" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Prompting.html" target="_blank" rel="noopener noreferrer">#Prompting</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2372" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Visual Prompting in Multimodal Large Language Models: A Survey, Junda Wu+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆMLLMsï¼‰ã«ãŠã‘ã‚‹è¦–è¦šçš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ‰‹æ³•ã®åŒ…æ‹¬çš„ãªèª¿æŸ»ã‚’è¡Œã„ã€è¦–è¦šçš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç”Ÿæˆã‚„æ§‹æˆçš„æ¨è«–ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå­¦ç¿’ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚æ—¢å­˜ã®è¦–è¦šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ†é¡ã—ã€è‡ªå‹•ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ³¨é‡ˆã®ç”Ÿæˆæ‰‹æ³•ã‚’è­°è«–ã€‚è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¨ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³LLMã®æ•´åˆæ€§ã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã‚„ã€ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨æ–‡è„ˆå†…å­¦ç¿’ã«ã‚ˆã‚‹è¦–è¦šçš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ç†è§£å‘ä¸Šã«ã¤ã„ã¦ã‚‚è¿°ã¹ã¦ã„ã¾ã™ã€‚æœ€å¾Œã«ã€MLLMsã«ãŠã‘ã‚‹è¦–è¦šçš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ‰‹æ³•ã®æœªæ¥ã«é–¢ã™ã‚‹ãƒ“ã‚¸ãƒ§ãƒ³ã‚’æç¤ºã—ã¾ã™ã€‚</span>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/Controllable.html" target="_blank" rel="noopener noreferrer">#Controllable</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<span class="issue_date">Issue Date: 2025-08-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2371" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Controllable Generation with Text-to-Image Diffusion Models: A Survey, Pu Cao+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¯ãƒ†ã‚­ã‚¹ãƒˆèª˜å°ç”Ÿæˆã«ãŠã„ã¦å¤§ããªé€²å±•ã‚’é‚ã’ãŸãŒã€ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã§ã¯å¤šæ§˜ãªè¦æ±‚ã«å¿œãˆã‚‰ã‚Œãªã„ã€‚æœ¬èª¿æŸ»ã§ã¯ã€T2Iæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®åˆ¶å¾¡å¯èƒ½ãªç”Ÿæˆã«é–¢ã™ã‚‹æ–‡çŒ®ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã€ç†è«–çš„åŸºç›¤ã¨å®Ÿè·µçš„é€²å±•ã‚’ã‚«ãƒãƒ¼ã€‚ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°æ‹¡æ•£ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬ã‚’ç´¹ä»‹ã—ã€åˆ¶å¾¡ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’åˆ†æã€‚ç”Ÿæˆæ¡ä»¶ã®ç•°ãªã‚‹ã‚«ãƒ†ã‚´ãƒªã«æ•´ç†ã—ãŸæ–‡çŒ®ãƒªã‚¹ãƒˆã‚’æä¾›ã€‚</span>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<a class="button" href="articles/Scaling%20Laws.html" target="_blank" rel="noopener noreferrer">#Scaling Laws</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/DataFiltering.html" target="_blank" rel="noopener noreferrer">#DataFiltering</a>
<span class="issue_date">Issue Date: 2025-07-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2262" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Scaling Laws for Data Filtering -- Data Curation cannot be Compute   Agnostic, Sachin Goyal+, CVPR'24</a>
<span class="snippet"><span>GPT Summary</span>- è¦–è¦šã¨è¨€èªã®ãƒ¢ãƒ‡ãƒ«ï¼ˆVLMsï¼‰ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦ã€é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãŒé‡è¦ã§ã‚ã‚‹ãŒã€è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã¨ã¯ç„¡é–¢ä¿‚ã«è¡Œã‚ã‚Œã‚‹ã“ã¨ãŒå¤šã„ã€‚æœ¬ç ”ç©¶ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®å“è³ªã¨é‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼ˆQQTï¼‰ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€ã‚¦ã‚§ãƒ–ãƒ‡ãƒ¼ã‚¿ã®éå‡è³ªæ€§ã‚’è€ƒæ…®ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ‡ãƒ¼ã‚¿ã®æœ‰ç”¨æ€§ã®é•ã„ã‚„ç¹°ã‚Šè¿”ã—ä½¿ç”¨ã«ã‚ˆã‚‹åŠ£åŒ–ã‚’è©•ä¾¡ã—ã€è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¼ãƒ«ã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¨å®šå¯èƒ½ã«ã™ã‚‹ã€‚æœ€é©ãªãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¼ãƒ«ã®ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é€šã˜ã¦ã€è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã«å¿œã˜ãŸæœ€é«˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/cloneofsimo/status/1946241642572448174?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§å¤šãã®ç ”ç©¶ãŒãƒ¢ãƒ‡ãƒ«ãŒã‚ˆã‚Šé«˜ã„æ€§èƒ½ã‚’é”æˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ãŒã€é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã«ã¯é™ã‚ŠãŒã‚ã‚‹ã“ã¨ã¨ã€ç¹°ã‚Šè¿”ã—å­¦ç¿’ã‚’ã™ã‚‹ã“ã¨ã§ã™ãã«ãã®åŠ¹ç”¨ãŒä½ä¸‹ã™ã‚‹ï¼ˆQuality-Quantity tradeoff!)ã¨ã„ã†ç‰¹æ€§ãŒã‚ã‚‹ã€‚ã“ã®ã‚ˆã†ãªçŠ¶æ³ã«ãŠã„ã¦ã€ãŸã¨ãˆã°è¨ˆç®—ã®äºˆç®—ãŒãƒ‡ãƒ¼ã‚¿6ãƒ‘ã‚±ãƒƒãƒˆåˆ†ã®æ™‚ã«ã€ã‚ã¡ã‚ƒã‚ã¡ã‚ƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’é ‘å¼µã£gé«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¼ãƒ«Eã®ã¿ã‚’ä½¿ã£ã¦6 epochå­¦ç¿’ã™ã‚‹ã®ãŒè‰¯ã„ã®ã‹ã€å°‘ã—å“è³ªã¯è½ã¡ã‚‹ãƒ‡ãƒ¼ã‚¿Dã‚‚æ··ãœã¦E+Dã‚’3 epochå­¦ç¿’ã™ã‚‹ã®ãŒè‰¯ã„ã®ã‹ã€ã¨ãã«ã©ã¡ã‚‰ãŒè‰¯ã„ã®ã‹ï¼Ÿã¨ã„ã†è©±ã®ã‚ˆã†ã§ã‚ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/06812781-7212-415e-bc7a-dd19ac4ca0d7" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/Mathematics.html" target="_blank" rel="noopener noreferrer">#Mathematics</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2201" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset, Ke Wang+, NeurIPS'24 Datasets and Benchmarks Track</a>
<span class="snippet"><span>GPT Summary</span>- MATH-Visionï¼ˆMATH-Vï¼‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ææ¡ˆã—ã€3,040ã®è¦–è¦šçš„æ–‡è„ˆã‚’æŒã¤æ•°å­¦å•é¡Œã‚’åé›†ã€‚16ã®æ•°å­¦åˆ†é‡ã¨5ã¤ã®é›£æ˜“åº¦ã§æ§‹æˆã•ã‚Œã€LMMsã®æ•°å­¦çš„æ¨è«–èƒ½åŠ›ã‚’è©•ä¾¡ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€LMMsã¨äººé–“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é–“ã«é¡•è‘—ãªã‚®ãƒ£ãƒƒãƒ—ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã€ã•ã‚‰ãªã‚‹é€²å±•ã®å¿…è¦æ€§ã‚’å¼·èª¿ã€‚ã‚¨ãƒ©ãƒ¼åˆ†æã‚’é€šã˜ã¦ä»Šå¾Œã®ç ”ç©¶ã«è²´é‡ãªæ´å¯Ÿã‚’æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview: 


<a href="https://openreview.net/forum?id=QWTCcxMpPA#discussion" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=QWTCcxMpPA#discussion</a>


<br>project page: 


<a href="https://mathllm.github.io/mathvision/" target="_blank" rel="noopener noreferrer">https://mathllm.github.io/mathvision/</a>


</p>
<p>Project Pageã®ãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒšãƒ¼ã‚¸ãŒéå¸¸ã«ã‚ã‹ã‚Šã‚„ã™ã„ã€‚ã“ã¡ã‚‰ã¯äººé–“ã®æ–¹ãŒã¾ã ã¾ã æ€§èƒ½ãŒé«˜ãã†ã€‚<br><br>&lt;img width="671" height="806" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/586edf6d-cd77-48cb-b209-8ea819e725fc"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/586edf6d-cd77-48cb-b209-8ea819e725fc"&lt;/a&gt;


/&gt;</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<a class="button" href="articles/Self-SupervisedLearning.html" target="_blank" rel="noopener noreferrer">#Self-SupervisedLearning</a>
<a class="button" href="articles/TMLR.html" target="_blank" rel="noopener noreferrer">#TMLR</a>
<span class="issue_date">Issue Date: 2025-04-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1884" target="_blank" rel="noopener noreferrer" class="title-link">DINOv2: Learning Robust Visual Features without Supervision, Maxime Oquab+, TMLR'24</a>
<span class="snippet"><span>GPT Summary</span>- è‡ªå·±æ•™å¸«ã‚ã‚Šæ‰‹æ³•ã‚’ç”¨ã„ã¦ã€å¤šæ§˜ãªã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ±ç”¨çš„ãªè¦–è¦šç‰¹å¾´ã‚’ç”Ÿæˆã™ã‚‹æ–°ã—ã„äº‹å‰å­¦ç¿’æ‰‹æ³•ã‚’ææ¡ˆã€‚1Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ViTãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€å°å‹ãƒ¢ãƒ‡ãƒ«ã«è’¸ç•™ã™ã‚‹ã“ã¨ã§ã€OpenCLIPã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’é”æˆã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1699" target="_blank" rel="noopener noreferrer" class="title-link">OlympiadBench: A Challenging Benchmark for Promoting AGI with   Olympiad-Level Bilingual Multimodal Scientific Problems, Chaoqun He+, ACL'24</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã‚„ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆLMMsï¼‰ã®èƒ½åŠ›ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã«ã€ã‚ªãƒªãƒ³ãƒ”ã‚¢ãƒ‰ãƒ¬ãƒ™ãƒ«ã®ãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ç§‘å­¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ŒOlympiadBenchã€ã‚’ææ¡ˆã€‚8,476ã®æ•°å­¦ã¨ç‰©ç†ã®å•é¡Œã‚’å«ã¿ã€å°‚é–€å®¶ãƒ¬ãƒ™ãƒ«ã®æ³¨é‡ˆãŒä»˜ã‘ã‚‰ã‚Œã¦ã„ã‚‹ã€‚ãƒˆãƒƒãƒ—ãƒ¢ãƒ‡ãƒ«ã®GPT-4Vã¯å¹³å‡17.97%ã®ã‚¹ã‚³ã‚¢ã‚’é”æˆã—ãŸãŒã€ç‰©ç†ã§ã¯10.74%ã«ã¨ã©ã¾ã‚Šã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®å³ã—ã•ã‚’ç¤ºã™ã€‚ä¸€èˆ¬çš„ãªå•é¡Œã¨ã—ã¦å¹»è¦šã‚„è«–ç†çš„èª¤è¬¬ãŒæŒ‡æ‘˜ã•ã‚Œã€ä»Šå¾Œã®AGIç ”ç©¶ã«è²´é‡ãªãƒªã‚½ãƒ¼ã‚¹ã¨ãªã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã‚‹ã€‚</span>
<a class="button" href="articles/InformationRetrieval.html" target="_blank" rel="noopener noreferrer">#InformationRetrieval</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/RAG(RetrievalAugmentedGeneration).html" target="_blank" rel="noopener noreferrer">#RAG(RetrievalAugmentedGeneration)</a>
<a class="button" href="articles/MultiLingual.html" target="_blank" rel="noopener noreferrer">#MultiLingual</a>
<a class="button" href="articles/COLING.html" target="_blank" rel="noopener noreferrer">#COLING</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2024-12-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1598" target="_blank" rel="noopener noreferrer" class="title-link">VLR-Bench: Multilingual Benchmark Dataset for Vision-Language Retrieval  Augmented Generation, Hyeonseok Lim+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- è¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯VLR-Benchã‚’ææ¡ˆã€‚ã“ã‚Œã¯5ã¤ã®å…¥åŠ›ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”¨ã„ã¦ã€ç‰¹å®šã®ã‚¯ã‚¨ãƒªã«å¯¾ã™ã‚‹æœ‰ç”¨ãªæƒ…å ±ã®åˆ¤æ–­èƒ½åŠ›ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ã€‚32,000ã®è‡ªå‹•ç”Ÿæˆã•ã‚ŒãŸæŒ‡ç¤ºã‹ã‚‰ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆVLR-IFã‚’æ§‹ç¯‰ã—ã€VLMã®RAGèƒ½åŠ›ã‚’å¼·åŒ–ã€‚Llama3ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã§æ€§èƒ½ã‚’æ¤œè¨¼ã—ã€ä¸¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã§å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>Multilingual VLMã‚’ç”¨ã„ãŸRAGã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<span class="issue_date">Issue Date: 2024-12-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1584" target="_blank" rel="noopener noreferrer" class="title-link">Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale   Prediction, Keyu Tian+, NeurIPS'24</a>
<span class="snippet"><span>GPT Summary</span>- Visual AutoRegressive modeling (VAR)ã‚’ææ¡ˆã—ã€ç”»åƒç”Ÿæˆã«ãŠã„ã¦è‡ªå·±å›å¸°å­¦ç¿’ã‚’æ¬¡ã®ã‚¹ã‚±ãƒ¼ãƒ«äºˆæ¸¬ã¨ã—ã¦å†å®šç¾©ã€‚VARã¯ã€GPTã®ã‚ˆã†ãªARãƒ¢ãƒ‡ãƒ«ãŒæ‹¡æ•£ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’å®Ÿç¾ã—ã€ImageNet 256x256ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§FIDã‚’18.65ã‹ã‚‰1.73ã€ISã‚’80.4ã‹ã‚‰350.2ã«æ”¹å–„ã€‚æ¨è«–é€Ÿåº¦ã¯ç´„20å€å‘ä¸Šã—ã€ç”»åƒå“è³ªã‚„ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã§ã‚‚å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã€‚VARã¯ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆä¸€èˆ¬åŒ–èƒ½åŠ›ã‚’æŒã¡ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã‚’ç¤ºã™ã€‚å…¨ãƒ¢ãƒ‡ãƒ«ã¨ã‚³ãƒ¼ãƒ‰ã‚’å…¬é–‹ã—ã€è¦–è¦šç”Ÿæˆã®ç ”ç©¶ã‚’ä¿ƒé€²ã€‚</span>
<span class="snippet"><span>Comment</span><p>NeurIPS2024ã®ãƒ™ã‚¹ãƒˆãƒšãƒ¼ãƒ‘ãƒ¼</p>
<p>ç¬¬ä¸€è‘—è€…ãŒByteDanceç¤¾ã‹ã‚‰è¨´è¨Ÿã‚’èµ·ã“ã•ã‚Œã¦ã„ã‚‹æ¨¡æ§˜â€¦ï¼Ÿ<br>


<a href="https://var-integrity-report.github.io" target="_blank" rel="noopener noreferrer">https://var-integrity-report.github.io</a>


</p>
<p>OpenReview:


<a href="https://openreview.net/forum?id=gojL67CfS8" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=gojL67CfS8</a>


</p>
<p>Next Token Prediction, Next Image Token Generation (å¾“æ¥æ‰‹æ³•ï¼‰, Next Scale (resolution) prediction (ææ¡ˆæ‰‹æ³•)ã®é•ã„ã®å›³è§£ã€‚éå¸¸ã«åˆ†ã‹ã‚Šã‚„ã™ã„ã€‚next token predictionã§ã¯æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã‚’äºˆæ¸¬ã™ã‚‹ãŒVARã§ã¯ã€æ¬¡ã®è§£åƒåº¦ç”»åƒã®å…¨ä½“ã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒãƒƒãƒ—ã‚’äºˆæ¸¬ã™ã‚‹ã€‚<br><br><img src="https://github.com/user-attachments/assets/668d7523-f262-45c1-a1d0-2dd479c0a708" alt="image" loading="lazy"><br><br>å­¦ç¿’æ–¹æ³•ã®æ¦‚è¦ã€‚2-Stageã§å­¦ç¿’ã•ã‚Œã‚‹ã€‚æœ€åˆã®ã‚¹ãƒ†ãƒ¼ã‚¸ã§Kç¨®é¡ã®è§£åƒåº¦ã®ç”»åƒï¼ˆï¼Kç¨®é¡ã®ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã®token maps r_kï¼‰ã‚’å¾—ã‚‹ãŸã‚ã«AutoEncoderã‚’å­¦ç¿’ã—ã€æ¬¡ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã§block-wiseã®causal attention maskã‚’ç”¨ã„ã¦ã€K_&lt;kå€‹ç›®ã®è§£åƒåº¦ã®ç”»åƒã‹ã‚‰Kå€‹ç›®ã®è§£åƒåº¦ã®ç”»åƒã‚’äºˆæ¸¬ã™ã‚‹ï¼ˆå›³ã‚’è¦‹ã‚‹ã¨ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’æ´ã¿ã‚„ã™ã„ï¼‰ã€‚inferenceæ™‚ã¯KV Cacheã‚’åˆ©ç”¨ã—ã€maskã¯ä¸è¦ã¨ãªã‚‹ã€‚<br>å„r_kã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹éš›ã«r_&lt;kã®ã¿ã«ä¾å­˜ã™ã‚‹è¨­è¨ˆã«ã™ã‚‹ã“ã¨ã§coase-to-fineã«ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã«ç›¸å½“ã—ã€ã“ã‚Œã¯äººé–“ã®ç²—ãæ‰ãˆã¦ã‹ã‚‰è©³ç´°ã‚’è¦‹ã‚‹èªçŸ¥ãƒ—ãƒ­ã‚»ã‚¹ã¨åˆè‡´ã™ã‚‹ã€‚ã¾ãŸã€flattenæ“ä½œãŒå­˜åœ¨ã›ãšã€ãã‚Œãã‚Œã®r_&lt;kå†…ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒr_kç”Ÿæˆæ™‚ã«å…¨ã¦è€ƒæ…®ã•ã‚Œã‚‹ãŸã‚ç©ºé–“çš„å±€æ‰€æ€§ã‚‚æ‹…ä¿ã•ã‚Œã‚‹ã€‚ã¾ãŸã€r_kå†…ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯ä¸¦åˆ—ã«ç”Ÿæˆå¯èƒ½ãªã®ã§è¨ˆç®—é‡ã®ã‚ªãƒ¼ãƒ€ãƒ¼ãŒå¤§å¹…ã«å‰Šæ¸›ã•ã‚Œã‚‹ï¼ˆO(n^4)ã€‚<br><img src="https://github.com/user-attachments/assets/e1a85712-e66a-4c9a-9cf1-6556f2b8e687" alt="image" loading="lazy"><br><br>å¾“æ¥æ‰‹æ³•ã¨æ¯”ã¹ã‚ˆã‚Šå°ã•ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§é«˜ã„æ€§èƒ½ã‚’å®Ÿç¾ã—ã€inference timeã‚‚éå¸¸ã«æ—©ã„ã€‚<br><img src="https://github.com/user-attachments/assets/90a6a7de-995d-49e6-94a2-cd709e68777f" alt="image" loading="lazy"><br><br>ScalingLawsã‚‚æˆç«‹ã™ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/351c2a7b-85aa-4cc7-8ba2-a5e9528cabd4" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<span class="issue_date">Issue Date: 2024-11-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1542" target="_blank" rel="noopener noreferrer" class="title-link">Multimodal Autoregressive Pre-training of Large Vision Encoders, Enrico Fini+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- æ–°ã—ã„æ‰‹æ³•AIMV2ã‚’ç”¨ã„ã¦ã€å¤§è¦æ¨¡ãªãƒ“ã‚¸ãƒ§ãƒ³ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®äº‹å‰å­¦ç¿’ã‚’è¡Œã†ã€‚ã“ã‚Œã¯ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµ„ã¿åˆã‚ã›ãŸãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è¨­å®šã«æ‹¡å¼µã•ã‚Œã€ã‚·ãƒ³ãƒ—ãƒ«ãªäº‹å‰å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã¨å„ªã‚ŒãŸæ€§èƒ½ã‚’ç‰¹å¾´ã¨ã™ã‚‹ã€‚AIMV2-3Bã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯ImageNet-1kã§89.5%ã®ç²¾åº¦ã‚’é”æˆã—ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ç”»åƒç†è§£ã«ãŠã„ã¦æœ€å…ˆç«¯ã®ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã€‚</span>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2024-11-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1526" target="_blank" rel="noopener noreferrer" class="title-link">Tutorial on Diffusion Models for Imaging and Vision, Stanley H. Chan, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- ç”Ÿæˆãƒ„ãƒ¼ãƒ«ã®æˆé•·ã«ã‚ˆã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã‚„å‹•ç”»ã‚’ç”Ÿæˆã™ã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒå¯èƒ½ã«ã€‚æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®åŸç†ãŒã“ã‚Œã‚‰ã®ç”Ÿæˆãƒ„ãƒ¼ãƒ«ã®åŸºç›¤ã§ã‚ã‚Šã€å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®æ¬ ç‚¹ã‚’å…‹æœã€‚ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬çš„ãªã‚¢ã‚¤ãƒ‡ã‚¢ã‚’å­¦éƒ¨ç”Ÿã‚„å¤§å­¦é™¢ç”Ÿå‘ã‘ã«è§£èª¬ã€‚</span>
<span class="snippet"><span>Comment</span><p>ã„ã¤ã‹èª­ã¾ãªã‘ã‚Œã°ãªã‚‰ãªã„</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/InstructionTuning.html" target="_blank" rel="noopener noreferrer">#InstructionTuning</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/Catastrophic%20Forgetting.html" target="_blank" rel="noopener noreferrer">#Catastrophic Forgetting</a>
<span class="issue_date">Issue Date: 2024-11-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1502" target="_blank" rel="noopener noreferrer" class="title-link">Online-LoRA: Task-free Online Continual Learning via Low Rank Adaptation, Xiwen Wei+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- ç ´æ»…çš„å¿˜å´ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€ã‚¿ã‚¹ã‚¯ãƒ•ãƒªãƒ¼ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç¶™ç¶šå­¦ç¿’ï¼ˆOCLï¼‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯Online-LoRAã‚’ææ¡ˆã€‚ãƒªãƒãƒ¼ã‚µãƒ«ãƒãƒƒãƒ•ã‚¡ã®åˆ¶ç´„ã‚’å…‹æœã—ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ“ã‚¸ãƒ§ãƒ³ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ï¼ˆViTï¼‰ãƒ¢ãƒ‡ãƒ«ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§å¾®èª¿æ•´ã€‚æ–°ã—ã„ã‚ªãƒ³ãƒ©ã‚¤ãƒ³é‡ã¿æ­£å‰‡åŒ–æˆ¦ç•¥ã‚’ç”¨ã„ã¦é‡è¦ãªãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç‰¹å®šã—ã€ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã®å¤‰åŒ–ã‚’è‡ªå‹•èªè­˜ã€‚å¤šæ§˜ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p><img src="https://github.com/user-attachments/assets/b789ba71-3941-4d60-9397-46607ddc7712" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<span class="issue_date">Issue Date: 2024-09-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1435" target="_blank" rel="noopener noreferrer" class="title-link">COM Kitchens: An Unedited Overhead-view Video Dataset as a   Vision-Language Benchmark, Koki Maeda+, N_A, ECCV'24</a>
<span class="snippet"><span>GPT Summary</span>- æ‰‹ç¶šãçš„ãªãƒ“ãƒ‡ã‚ªç†è§£ã®ãŸã‚ã«ã€COM Kitchensã¨ã„ã†æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ææ¡ˆã€‚ã“ã‚Œã¯ã€å‚åŠ è€…ãŒãƒ¬ã‚·ãƒ”ã«åŸºã¥ã„ã¦é£Ÿæã‚’æº–å‚™ã™ã‚‹æ§˜å­ã‚’ä¸Šæ–¹è¦–ç‚¹ã§æ’®å½±ã—ãŸç·¨é›†ã•ã‚Œã¦ã„ãªã„ãƒ“ãƒ‡ã‚ªã§æ§‹æˆã•ã‚Œã¦ã„ã‚‹ã€‚å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿åé›†ã®ãŸã‚ã«ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã‚’ä½¿ç”¨ã—ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ¬ã‚·ãƒ”æ¤œç´¢ï¼ˆOnRRï¼‰ã¨å¯†ãªãƒ“ãƒ‡ã‚ªã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ï¼ˆDVC-OVï¼‰ã¨ã„ã†æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’ææ¡ˆã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€æ—¢å­˜ã®ã‚¦ã‚§ãƒ–ãƒ“ãƒ‡ã‚ªãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã®èƒ½åŠ›ã¨é™ç•Œã‚’æ¤œè¨¼ã€‚</span>
<span class="snippet"><span>Comment</span><p>ã¨ã¦ã‚‚ãŠã‚‚ã—ã‚ãã†ï¼</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2024-09-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1434" target="_blank" rel="noopener noreferrer" class="title-link">What matters when building vision-language models?, Hugo LaurenÃ§on+, N_A, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- è¦–è¦šã¨è¨€èªã®ãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰ã®è¨­è¨ˆã«ãŠã‘ã‚‹è£ä»˜ã‘ã®ãªã„æ±ºå®šãŒæ€§èƒ½å‘ä¸Šã®ç‰¹å®šã‚’å¦¨ã’ã¦ã„ã‚‹ã¨æŒ‡æ‘˜ã€‚äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€ãƒ‡ãƒ¼ã‚¿ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã«é–¢ã™ã‚‹å®Ÿé¨“ã‚’è¡Œã„ã€80å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åŸºç›¤VLMã€ŒIdefics2ã€ã‚’é–‹ç™ºã€‚Idefics2ã¯ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’é”æˆã—ã€4å€ã®ã‚µã‚¤ã‚ºã®ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã®æ€§èƒ½ã‚’ç¤ºã™ã€‚ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆã«OpenVLMã®é€²å±•ã®æ­´å²ãŒè¼‰ã£ã¦ã„ã‚‹ã€‚æ§‹ç¯‰ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚å…¬é–‹ã•ã‚Œã‚‹æ¨¡æ§˜ã€‚<br><img src="https://github.com/user-attachments/assets/9675c2ad-650a-460b-9655-1c6347d07f58" alt="image" loading="lazy"><br>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/thom_wolf/status/1840372428855280045?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/CLIP.html" target="_blank" rel="noopener noreferrer">#CLIP</a>
<span class="issue_date">Issue Date: 2024-09-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1432" target="_blank" rel="noopener noreferrer" class="title-link">Long-CLIP: Unlocking the Long-Text Capability of CLIP, Beichen Zhang+, N_A, ECCV'24</a>
<span class="snippet"><span>GPT Summary</span>- Long-CLIPã¯ã€CLIPã®ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã®é•·ã•åˆ¶é™ã‚’å…‹æœã—ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã®ä¸€èˆ¬åŒ–èƒ½åŠ›ã‚’ä¿æŒã¾ãŸã¯è¶…ãˆã‚‹æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã‚’ç”¨ã„ã¦ã€CLIPã®æ€§èƒ½ã‚’ç¶­æŒã—ã¤ã¤ã€é•·æ–‡ãƒ†ã‚­ã‚¹ãƒˆ-ç”»åƒãƒšã‚¢ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ†ã‚­ã‚¹ãƒˆ-ç”»åƒæ¤œç´¢ã‚¿ã‚¹ã‚¯ã§ç´„20%ã®æ€§èƒ½å‘ä¸Šã‚’é”æˆã—ã¾ã—ãŸã€‚ã¾ãŸã€Long-CLIPã¯è©³ç´°ãªãƒ†ã‚­ã‚¹ãƒˆèª¬æ˜ã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆã™ã‚‹èƒ½åŠ›ã‚’å¼·åŒ–ã—ã¾ã™ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2024-09-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1369" target="_blank" rel="noopener noreferrer" class="title-link">Diffusion Models Are Real-Time Game Engines, Dani Valevski+, N_A, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- GameNGenã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å®Œå…¨ã«å‹•ä½œã™ã‚‹ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³ã§ã‚ã‚Šã€é«˜å“è³ªã§é•·ã„è»Œè·¡ä¸Šã§è¤‡é›‘ãªç’°å¢ƒã¨ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚GameNGenã¯ã€å˜ä¸€ã®TPUä¸Šã§ç§’é–“20ãƒ•ãƒ¬ãƒ¼ãƒ ä»¥ä¸Šã§ã‚¯ãƒ©ã‚·ãƒƒã‚¯ã‚²ãƒ¼ãƒ DOOMã‚’ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã«ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚æ¬¡ãƒ•ãƒ¬ãƒ¼ãƒ äºˆæ¸¬ã§ã¯ã€PSNRãŒ29.4ã«é”ã—ã€åŠ£åŒ–JPEGåœ§ç¸®ã¨æ¯”è¼ƒå¯èƒ½ã§ã™ã€‚GameNGenã¯ã€2ã¤ã®æ®µéšã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¾ã™ï¼šï¼ˆ1ï¼‰RLã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒã‚²ãƒ¼ãƒ ã‚’ãƒ—ãƒ¬ã‚¤ã™ã‚‹ã“ã¨ã‚’å­¦ã³ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒè¨˜éŒ²ã•ã‚Œã€ï¼ˆ2ï¼‰æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ãŒéå»ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¿œã˜ã¦æ¬¡ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç”Ÿæˆã™ã‚‹ã‚ˆã†ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¾ã™ã€‚æ¡ä»¶ä»˜ãã®æ‹¡å¼µã«ã‚ˆã‚Šã€é•·ã„è»Œè·¡ä¸Šã§å®‰å®šã—ãŸè‡ªå·±å›å¸°ç”ŸæˆãŒå¯èƒ½ã¨ãªã‚Šã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>Diffusion Modelã§ã‚²ãƒ¼ãƒ æ˜ åƒã‚’ç”Ÿæˆã™ã‚‹å–ã‚Šçµ„ã¿ã‚‰ã—ã„ã€‚ã‚²ãƒ¼ãƒ ã®environmentã«å¯¾ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ã®Actionã¨frameã®ç³»åˆ—ã‚’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã¨ã¿ãªã—ã¦ç”Ÿæˆã™ã‚‹ã£ã½ã„ï¼Ÿ</p>
<p>project pageã«ãƒ‡ãƒ¢ãŒã®ã£ã¦ã„ã‚‹<br><br>


<a href="https://gamengen.github.io/" target="_blank" rel="noopener noreferrer">https://gamengen.github.io/</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<span class="issue_date">Issue Date: 2024-04-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1275" target="_blank" rel="noopener noreferrer" class="title-link">Visualization-of-Thought Elicits Spatial Reasoning in Large Language  Models, Wenshan Wu+, N_A, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- LLMsã®ç©ºé–“æ¨è«–èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€Visualization-of-Thoughtï¼ˆVoTï¼‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã‚’ææ¡ˆã€‚VoTã¯ã€LLMsã®æ¨è«–ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’å¯è¦–åŒ–ã—ã€ç©ºé–“æ¨è«–ã‚¿ã‚¹ã‚¯ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€æ—¢å­˜ã®MLLMsã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã€‚VoTã¯ã€ç©ºé–“æ¨è«–ã‚’ä¿ƒé€²ã™ã‚‹ãŸã‚ã«ã€Œãƒ¡ãƒ³ã‚¿ãƒ«ã‚¤ãƒ¡ãƒ¼ã‚¸ã€ã‚’ç”Ÿæˆã™ã‚‹èƒ½åŠ›ã‚’æŒã¡ã€MLLMsã§ã®æœ‰åŠ¹æ€§ã‚’ç¤ºå”†ã™ã‚‹ã€‚</span>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/InstructionTuning.html" target="_blank" rel="noopener noreferrer">#InstructionTuning</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<a class="button" href="articles/Robotics.html" target="_blank" rel="noopener noreferrer">#Robotics</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<a class="button" href="articles/EmbodiedAI.html" target="_blank" rel="noopener noreferrer">#EmbodiedAI</a>
<span class="issue_date">Issue Date: 2023-12-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1202" target="_blank" rel="noopener noreferrer" class="title-link">Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision,   Language, Audio, and Action, Jiasen Lu+, N_A, CVPR'24</a>
<span class="snippet"><span>GPT Summary</span>- Unified-IO 2ã¯ã€æœ€åˆã®è‡ªå·±å›å¸°å‹ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ç”»åƒã€ãƒ†ã‚­ã‚¹ãƒˆã€éŸ³å£°ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç†è§£ã—ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’çµ±ä¸€ã™ã‚‹ãŸã‚ã«ã€å…±æœ‰ã®æ„å‘³ç©ºé–“ã«å…¥åŠ›ã¨å‡ºåŠ›ã‚’é…ç½®ã—ã€å˜ä¸€ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã§å‡¦ç†ã—ã¾ã™ã€‚ã•ã¾ã–ã¾ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ”¹å–„ã‚’ææ¡ˆã—ã€å¤§è¦æ¨¡ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªäº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚Unified-IO 2ã¯ã€GRITãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å«ã‚€35ä»¥ä¸Šã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>ç”»åƒã€ãƒ†ã‚­ã‚¹ãƒˆã€éŸ³å£°ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç†è§£ã§ãã‚‹åˆã‚ã¦ã®autoregressive modelã€‚AllenAI</p>
<p>ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³<br><img src="https://github.com/user-attachments/assets/4282ffb0-18f1-40c9-b6d7-f004d03b8382" alt="image" loading="lazy"><br><br>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã«æ‹¡å¼µã—ãŸã“ã¨ã§ã€è¨“ç·´ãŒéå¸¸ã«ä¸å®‰å®šã«ãªã£ãŸãŸã‚ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä¸Šã§ã„ãã¤ã‹ã®å·¥å¤«ã‚’åŠ ãˆã¦ã„ã‚‹:<br><br>- 2D Rotary Embedding<br>  - Positional Encodingã¨ã—ã¦RoPEã‚’æ¡ç”¨<br>  - ç”»åƒã®ã‚ˆã†ãª2æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®å ´åˆã¯RoPEã‚’2æ¬¡å…ƒã«æ‹¡å¼µã™ã‚‹ã€‚å…·ä½“çš„ã«ã¯ã€ä½ç½®(i, j)ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ã¤ã„ã¦ã¯ã€Q, Kã®embeddingã‚’åŠåˆ†ã«åˆ†å‰²ã—ã¦ã€ãã‚Œãã‚Œã«å¯¾ã—ã¦ç‹¬ç«‹ã«i, jã®RoPE Embeddingã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§i, jåŒæ–¹ã®æƒ…å ±ã‚’çµ„ã¿è¾¼ã‚€ã€‚<br>- QK Normalization<br>  - image, audioã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§MHAã®logitsãŒéå¸¸ã«å¤§ãããªã‚Šatteetion weightãŒ0/1ã®æ¥µç«¯ãªå€¤ã‚’ã¨ã‚‹ã‚ˆã†ã«ãªã‚Šè¨“ç·´ã®ä¸å®‰å®šã•ã«ã¤ãªãŒã£ãŸã€‚ã“ã®ãŸã‚ã€dot product attentionã‚’é©ç”¨ã™ã‚‹å‰ã«LayerNormã‚’çµ„ã¿è¾¼ã‚“ã ã€‚<br>- Scaled Cosine Attention<br>  - Image Historyãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«ãŠã„ã¦å›ºå®šé•·ã®Embeddingã‚’å¾—ã‚‹ãŸã‚ã«Perceiver Resamplerã‚’æ‰±ã£ãŸã¦ã„ã‚‹ãŒã€ã“ã¡ã‚‰ã‚‚ä¸Šè¨˜ã¨åŒæ§˜ã«Attentionã®logitsãŒæ¥µç«¯ã«å¤§ãããªã£ãŸãŸã‚ã€cosineé¡ä¼¼åº¦ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸScaled Cosine Attention <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2259" target="_blank" rel="noopener noreferrer">[Paper Note] Swin Transformer V2: Scaling Up Capacity and Resolution, Ze Liu+, arXiv'21</a>
 ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€å¤§å¹…ã«è¨“ç·´ã®å®‰å®šæ€§ãŒæ”¹å–„ã•ã‚ŒãŸã€‚<br>- ãã®ä»–<br>  - attention logitsã«ã¯fp32ã‚’é©ç”¨<br>  - äº‹å‰å­¦ç¿’ã•ã‚ŒãŸViTã¨ASTã‚’åŒæ™‚ã«æ›´æ–°ã™ã‚‹ã¨ä¸å®‰å®šã«ã¤ãªãŒã£ãŸãŸã‚ã€äº‹å‰å­¦ç¿’ã®æ®µéšã§ã¯freezeã—ã€instruction tuningã®æœ€å¾Œã«finetuningã‚’å®Ÿæ–½<br><br><img src="https://github.com/user-attachments/assets/74c8fa3a-8fb5-4785-8dd3-6a8cf3c7cfeb" alt="image" loading="lazy"></p>
<p>ç›®çš„é–¢æ•°ã¨ã—ã¦ã¯ã€Mixture of Denoisers (<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1424" target="_blank" rel="noopener noreferrer">UL2: Unifying Language Learning Paradigms, Yi Tay+, N/A, ICLR'23</a>
)ã«ç€æƒ³ã‚’å¾—ã¦ã€Multimodal Mixture of Denoisersã‚’ææ¡ˆã€‚MoDã§ã¯ã€<br>- \[R\]: é€šå¸¸ã®span corruption (1--5 tokenç¨‹åº¦ã®spanã‚’maskã™ã‚‹)<br>- \[S\]: causal language modeling (inputã‚’2ã¤ã®ã‚µãƒ–ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«åˆ†å‰²ã—ã€å‰æ–¹ã‹ã‚‰å¾Œæ–¹ã‚’äºˆæ¸¬ã™ã‚‹ã€‚å‰æ–¹éƒ¨åˆ†ã¯Bi-directionalã§ã‚‚å¯)<br>- \[X\]: extreme span corruption (12&gt;=tokenç¨‹åº¦ã®spanã‚’maskã™ã‚‹)<br><br>ã®3ç¨®é¡ãŒææ¡ˆã•ã‚Œã¦ãŠã‚Šã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã”ã¨ã«ã“ã‚Œã‚‰ã‚’ä½¿ã„åˆ†ã‘ã‚‹:<br>- text modality: UL2 (<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1424" target="_blank" rel="noopener noreferrer">UL2: Unifying Language Learning Paradigms, Yi Tay+, N/A, ICLR'23</a>
)ã‚’è¸è¥²<br>- image, audioãŒtargetã®å ´åˆ: 2ã¤ã®é¡ä¼¼ã—ãŸãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’å®šç¾©ã—åˆ©ç”¨<br>  - \[R\]: patchã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«x%ãƒã‚¹ã‚¯ã—re-constructã™ã‚‹<br>  - \[S\]: inputã®targetã¨ã¯ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®ã¿ã®æƒ…å ±ã‹ã‚‰ã€targetãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’ç”Ÿæˆã™ã‚‹<br><br>è¨“ç·´æ™‚ã«ã¯ prefixã¨ã—ã¦modality token \[Text\], \[Image\], \[Audio\] ã¨paradigm token \[R\], \[S\], \[X\] ã‚’ã‚¿ã‚¹ã‚¯ã‚’æŒ‡ç¤ºã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦åˆ©ç”¨ã—ã¦ã„ã‚‹ã€‚</p>
<p>ã¾ãŸã€image, audioã®ãƒã‚¹ã‚¯éƒ¨åˆ†ã®denoisingã‚’autoregressive modelã§å®Ÿæ–½ã™ã‚‹éš›ã«ã¯æ™®é€šã«ã‚„ã‚‹ã¨decoderå´ã§ãƒªãƒ¼ã‚¯ãŒç™ºç”Ÿã™ã‚‹(a)ã€‚ã“ã‚Œã‚’é˜²ãã«ã¯ã€Encoderå´ã§ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã€Decoderå´ã§teacher-forcingã™ã‚‹éš›ã«ã®å…¨ã¦ãƒã‚¹ã‚¯ã™ã‚‹æ–¹æ³•(b)ãŒã‚ã‚‹ãŒã€ã“ã®å ´åˆã€ç”Ÿæˆã‚¿ã‚¹ã‚¯ã¨denoisingã‚¿ã‚¹ã‚¯ãŒç›¸äº’ã«å¹²æ¸‰ã—ã¦ã—ã¾ã„ã†ã¾ãå­¦ç¿’ã§ããªããªã£ã¦ã—ã¾ã†ï¼ˆç”Ÿæˆã‚¿ã‚¹ã‚¯ã§ã¯é€šå¸¸Decoderã®inputã¨ã—ã¦[mask]ãŒå…¥åŠ›ã•ã‚Œæ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã™ã‚‹ã€ã¨ã„ã£ãŸã“ã¨ã¯èµ·ããˆãªã„ãŒã€æ„šç›´ã«(b)ã‚’ã‚„ã‚‹ã¨ãã†ãªã£ã¦ã—ã¾ã†ï¼‰ã€‚ã®ã§ã€(c)ã«ç¤ºã—ãŸã‚ˆã†ã«ã€ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’inputã¨ã—ã¦ç”Ÿæˆã—ãªã‘ã‚Œã°ãªã‚‰ãªã„æ™‚ã ã‘ã€ãƒã‚¹ã‚¯ã‚’è§£é™¤ã—ã¦decoderå´ã«inputã™ã‚‹ã€ã¨ã„ã†æ–¹æ³• (Dynamic Masking) ã§ã“ã®å•é¡Œã«å¯¾å‡¦ã—ã¦ã„ã‚‹ã€‚<br>&lt;img width="597" height="394" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/0dba8d5d-0c93-4c56-852b-fce9869428e7"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/0dba8d5d-0c93-4c56-852b-fce9869428e7"&lt;/a&gt;


/&gt;</p></span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2023-12-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1186" target="_blank" rel="noopener noreferrer" class="title-link">VILA: On Pre-training for Visual Language Models, Ji Lin+, N_A, CVPR'24</a>
<span class="snippet"><span>GPT Summary</span>- æœ€è¿‘ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®æˆåŠŸã«ã‚ˆã‚Šã€ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰ãŒé€²æ­©ã—ã¦ã„ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€VLMã®äº‹å‰å­¦ç¿’ã®ãŸã‚ã®ãƒ‡ã‚¶ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æ¤œè¨ã—ã€ä»¥ä¸‹ã®çµæœã‚’ç¤ºã—ãŸï¼š(1) LLMã‚’å‡çµã™ã‚‹ã“ã¨ã§ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒé”æˆã§ãã‚‹ãŒã€æ–‡è„ˆã«åŸºã¥ã„ãŸå­¦ç¿’èƒ½åŠ›ãŒä¸è¶³ã—ã¦ã„ã‚‹ã€‚(2) äº¤äº’ã«è¡Œã‚ã‚Œã‚‹äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯æœ‰ç›Šã§ã‚ã‚Šã€ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®ãƒšã‚¢ã ã‘ã§ã¯æœ€é©ã§ã¯ãªã„ã€‚(3) ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®æŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚’ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã«å†ãƒ–ãƒ¬ãƒ³ãƒ‰ã™ã‚‹ã“ã¨ã§ã€VLMã®ã‚¿ã‚¹ã‚¯ã®ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚VILAã¨ã„ã†ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’æ§‹ç¯‰ã—ã€æœ€å…ˆç«¯ãƒ¢ãƒ‡ãƒ«ã‚’å‡Œé§•ã—ã€å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã®äº‹å‰å­¦ç¿’ã¯ã€VILAã®ç‰¹æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1068" target="_blank" rel="noopener noreferrer">Improved Baselines with Visual Instruction Tuning, Haotian Liu+, N/A, CVPR'24</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/AutomaticPromptEngineering.html" target="_blank" rel="noopener noreferrer">#AutomaticPromptEngineering</a>
<a class="button" href="articles/EACL.html" target="_blank" rel="noopener noreferrer">#EACL</a>
<a class="button" href="articles/System%20Demonstration.html" target="_blank" rel="noopener noreferrer">#System Demonstration</a>
<span class="issue_date">Issue Date: 2023-11-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1161" target="_blank" rel="noopener noreferrer" class="title-link">NeuroPrompts: An Adaptive Framework to Optimize Prompts for  Text-to-Image Generation, Shachar Rosenman+, N_A, EACL'24 Sustem Demonstration Track</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å“è³ªã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®é©å¿œå‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯NeuroPromptsã‚’ææ¡ˆã—ã¾ã™ã€‚ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€äº‹å‰å­¦ç¿’ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦åˆ¶ç´„ä»˜ããƒ†ã‚­ã‚¹ãƒˆãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¡Œã„ã€äººé–“ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒç”Ÿæˆã™ã‚‹ã‚‚ã®ã«é¡ä¼¼ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é«˜å“è³ªãªãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®ç”ŸæˆãŒå¯èƒ½ã¨ãªã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯ã‚¹ã‚¿ã‚¤ãƒ«ã®ç‰¹å¾´ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚ã¾ãŸã€å¤§è¦æ¨¡ãªäººé–“ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ãŸå®Ÿé¨“ã«ã‚ˆã‚Šã€å½“ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒè‡ªå‹•çš„ã«å“è³ªã®é«˜ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã€å„ªã‚ŒãŸç”»åƒå“è³ªã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã—ãŸã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/ECCV.html" target="_blank" rel="noopener noreferrer">#ECCV</a>
<span class="issue_date">Issue Date: 2023-11-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1159" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs, Viraj Shah+, N_A, ECCV'24</a>
<span class="snippet"><span>GPT Summary</span>- æ¦‚è¦ï¼šæ¦‚å¿µé§†å‹•å‹ã®ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã®ãŸã‚ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´æ‰‹æ³•ã§ã‚ã‚‹ZipLoRAã‚’ææ¡ˆã€‚ZipLoRAã¯ã€ç‹¬ç«‹ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸã‚¹ã‚¿ã‚¤ãƒ«ã¨ä¸»é¡Œã®LoRAã‚’çµ±åˆã—ã€ä»»æ„ã®ä¸»é¡Œã¨ã‚¹ã‚¿ã‚¤ãƒ«ã®çµ„ã¿åˆã‚ã›ã§ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚å®Ÿé¨“çµæœã¯ã€ZipLoRAãŒä¸»é¡Œã¨ã‚¹ã‚¿ã‚¤ãƒ«ã®å¿ å®Ÿåº¦ã‚’æ”¹å–„ã—ãªãŒã‚‰é­…åŠ›çš„ãªçµæœã‚’ç”Ÿæˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://ziplora.github.io/" target="_blank" rel="noopener noreferrer">https://ziplora.github.io/</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/MultiLingual.html" target="_blank" rel="noopener noreferrer">#MultiLingual</a>
<a class="button" href="articles/NAACL.html" target="_blank" rel="noopener noreferrer">#NAACL</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2023-11-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1131" target="_blank" rel="noopener noreferrer" class="title-link">MEGAVERSE: Benchmarking Large Language Models Across Languages,   Modalities, Models and Tasks, Sanchit Ahuja+, N_A, NAACL'24</a>
<span class="snippet"><span>GPT Summary</span>- LLMsã®ç ”ç©¶ã¯æ€¥é€Ÿã«é€²å±•ã—ã¦ãŠã‚Šã€è‹±èªä»¥å¤–ã®è¨€èªã§ã®è©•ä¾¡ãŒå¿…è¦ã¨ã•ã‚Œã¦ã„ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¿½åŠ ã—ãŸMEGAVERSEãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€ã•ã¾ã–ã¾ãªLLMsã‚’è©•ä¾¡ã™ã‚‹ã€‚å®Ÿé¨“ã®çµæœã€GPT4ã¨PaLM2ãŒå„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ãŸãŒã€ãƒ‡ãƒ¼ã‚¿ã®æ±šæŸ“ãªã©ã®å•é¡ŒãŒã‚ã‚‹ãŸã‚ã€ã•ã‚‰ãªã‚‹å–ã‚Šçµ„ã¿ãŒå¿…è¦ã§ã‚ã‚‹ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/QuestionAnswering.html" target="_blank" rel="noopener noreferrer">#QuestionAnswering</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2023-10-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1068" target="_blank" rel="noopener noreferrer" class="title-link">Improved Baselines with Visual Instruction Tuning, Haotian Liu+, N_A, CVPR'24</a>
<span class="snippet"><span>GPT Summary</span>- LLaVAã¯ã€ãƒ“ã‚¸ãƒ§ãƒ³ã¨è¨€èªã®ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚³ãƒã‚¯ã‚¿ã§ã‚ã‚Šã€ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ãŒé«˜ãå¼·åŠ›ãªæ€§èƒ½ã‚’æŒã¤ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚CLIP-ViT-L-336pxã‚’ä½¿ç”¨ã—ã€å­¦è¡“ã‚¿ã‚¹ã‚¯æŒ‡å‘ã®VQAãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€11ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ç¢ºç«‹ã—ã¾ã—ãŸã€‚13Bã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¯ã‚ãšã‹120ä¸‡ã®å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã€1æ—¥ã§å®Œå…¨ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’çµ‚ãˆã¾ã™ã€‚ã‚³ãƒ¼ãƒ‰ã¨ãƒ¢ãƒ‡ãƒ«ã¯å…¬é–‹ã•ã‚Œã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>ç”»åƒåˆ†æãŒå¯èƒ½ãªã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹LLMã¨ã®ã“ã¨ã€‚</p>
<p># Overview<br><br>ç”»åƒç”Ÿæˆã‚’ã§ãã‚‹ã‚ã‘ã§ã¯ãªãã€inputã¨ã—ã¦ç”»åƒã‚’æ‰±ãˆã‚‹ã®ã¿ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/8d0382b0-8c2b-438d-8de8-ee451f5e2649" alt="image" loading="lazy"><br><br></p>
<p>pj page:


<a href="https://llava-vl.github.io" target="_blank" rel="noopener noreferrer">https://llava-vl.github.io</a>


</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/ImageSegmentation.html" target="_blank" rel="noopener noreferrer">#ImageSegmentation</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<span class="issue_date">Issue Date: 2023-04-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/600" target="_blank" rel="noopener noreferrer" class="title-link">Segment Anything in Medical Images, Jun Ma+, N_A, Nature Communications'24</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€è‡ªç„¶ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã«é©æ–°çš„ãªæ‰‹æ³•ã§ã‚ã‚‹Segment anything model (SAM)ã‚’åŒ»ç™‚ç”»åƒã«æ‹¡å¼µã™ã‚‹ãŸã‚ã®MedSAMã‚’ææ¡ˆã—ã€æ§˜ã€…ãªåŒ»ç™‚ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®æ±ç”¨ãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚MedSAMã¯ã€å¤§è¦æ¨¡ãªåŒ»ç™‚ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦é–‹ç™ºã•ã‚Œã€SAMã‚’ä¸€èˆ¬çš„ãªåŒ»ç™‚ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã«é©å¿œã™ã‚‹ãŸã‚ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã‚’é–‹ç™ºã—ã¾ã—ãŸã€‚21ã®3Dã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã¨9ã®2Dã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹åŒ…æ‹¬çš„ãªå®Ÿé¨“ã«ã‚ˆã‚Šã€MedSAMã¯ã€å¹³å‡Diceé¡ä¼¼ä¿‚æ•°ï¼ˆDSCï¼‰ãŒãã‚Œãã‚Œ22.5ï¼…ã¨17.6ï¼…ã§ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®SAMãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚ã‚³ãƒ¼ãƒ‰ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯ã€\url{https://github.com/bowang-lab/MedSAM}ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>SAMã®æ€§èƒ½ã¯åŒ»ç™‚ç”»åƒã«å¯¾ã—ã¦ã¯é™å®šçš„ã ã£ãŸãŸã‚ã€11ã®ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«å¯¾ã—ã¦200kã®ãƒã‚¹ã‚¯ã‚’ã—ãŸåŒ»ç™‚ç”»åƒã‚’ç”¨æ„ã—finetuningã—ãŸMedSAMã«ã‚ˆã£ã¦ã€åŒ»ç™‚ç”»åƒã®ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®æ€§èƒ½ã‚’å¤§å¹…ã«å‘ä¸Šã€‚<br>ã‚³ãƒ¼ãƒ‰ã¨ãƒ¢ãƒ‡ãƒ«ã¯publicly available</p>
<p><img src="https://github.com/user-attachments/assets/ea394adc-b1da-4764-bf29-534323bfc443" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/AAAI.html" target="_blank" rel="noopener noreferrer">#AAAI</a>
<span class="issue_date">Issue Date: 2023-04-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/547" target="_blank" rel="noopener noreferrer" class="title-link">AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head, AAAI'24</a>
<span class="snippet"><span>GPT Summary</span>- AudioGPTã¯ã€è¤‡é›‘ãªéŸ³å£°æƒ…å ±ã‚’å‡¦ç†ã—ã€éŸ³å£°å¯¾è©±ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIã‚·ã‚¹ãƒ†ãƒ ã§ã‚ã‚‹ã€‚åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã¨ASRã€TTSã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’çµ„ã¿åˆã‚ã›ã€éŸ³å£°ã€éŸ³æ¥½ã€ãƒˆãƒ¼ã‚­ãƒ³ã‚°ãƒ˜ãƒƒãƒ‰ã®ç†è§£ã¨ç”Ÿæˆã‚’è¡Œã†ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€AudioGPTãŒå¤šæ§˜ãªã‚ªãƒ¼ãƒ‡ã‚£ã‚ªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å‰µé€ ã‚’å®¹æ˜“ã«ã™ã‚‹èƒ½åŠ›ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>text, audio, imageã¨ã„ã£ãŸãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªpromptã‹ã‚‰ã€audioã«é–¢ã™ã‚‹æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã‚’å®Ÿç¾ã§ãã‚‹ã‚·ã‚¹ãƒ†ãƒ </p>
<p>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’jointã§å­¦ç¿’ã—ãŸã¨ã„ã†ã‚ã‘ã§ã¯ãªãã€è‰²ã€…ãªãƒ¢ãƒ‡ãƒ«ã®çµ„ã¿åˆã‚ã›ã¦ã‚¿ã‚¹ã‚¯ã‚’å®Ÿç¾ã—ã¦ã„ã‚‹ã£ã½ã„<br><br><img src="https://user-images.githubusercontent.com/12249301/234739859-f833706a-6040-484a-b015-553a719484d7.png" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/RectifiedFlow.html" target="_blank" rel="noopener noreferrer">#RectifiedFlow</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3210" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Flow Straight and Fast: Learning to Generate and Transfer Data with   Rectified Flow, Xingchao Liu+, ICLR'23, 2022.09</a>
<span class="snippet"><span>GPT Summary</span>- rectified flowã¨ã„ã†æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã€2ã¤ã®åˆ†å¸ƒé–“ã§ã®è¼¸é€ã‚’å­¦ç¿’ã™ã‚‹ODEãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã‚‹ã€‚ã“ã‚Œã¯ã€ç›´ç·šçš„ãªçµŒè·¯ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§è¨ˆç®—åŠ¹ç‡ã‚’é«˜ã‚ã€ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³è»¢é€ã«ãŠã„ã¦çµ±ä¸€çš„ãªè§£æ±ºç­–ã‚’æä¾›ã™ã‚‹ã€‚rectificationã‚’é€šã˜ã¦ã€éå¢—åŠ ã®å‡¸è¼¸é€ã‚³ã‚¹ãƒˆã‚’æŒã¤æ–°ã—ã„çµåˆã‚’ç”Ÿæˆã—ã€å†å¸°çš„ã«é©ç”¨ã™ã‚‹ã“ã¨ã§ç›´ç·šçš„ãªãƒ•ãƒ­ãƒ¼ã‚’å¾—ã‚‹ã€‚å®Ÿè¨¼ç ”ç©¶ã§ã¯ã€ç”»åƒç”Ÿæˆã‚„ç¿»è¨³ã«ãŠã„ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ã€é«˜å“è³ªãªçµæœã‚’å¾—ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=XVjTT1nw5z" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=XVjTT1nw5z</a>


</p>
<p>æ—¥æœ¬èªè§£èª¬(fmuuly, zenn):<br>- Rectified Flow 1: 


<a href="https://zenn.dev/fmuuly/articles/37cc3a2f17138e" target="_blank" rel="noopener noreferrer">https://zenn.dev/fmuuly/articles/37cc3a2f17138e</a>


<br>- Rectified Flow 2: 


<a href="https://zenn.dev/fmuuly/articles/a062fcd340207f" target="_blank" rel="noopener noreferrer">https://zenn.dev/fmuuly/articles/a062fcd340207f</a>


<br>- Rectified Flow 3: 


<a href="https://zenn.dev/fmuuly/articles/0f262fc003e202" target="_blank" rel="noopener noreferrer">https://zenn.dev/fmuuly/articles/0f262fc003e202</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-09-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2769" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] GenEval: An Object-Focused Framework for Evaluating Text-to-Image   Alignment, Dhruba Ghosh+, NeurIPS'23</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è‡ªå‹•è©•ä¾¡æ–¹æ³•ã€ŒGenEvalã€ã‚’ææ¡ˆã€‚ç‰©ä½“ã®å…±èµ·ã€ä½ç½®ã€æ•°ã€è‰²ãªã©ã®ç‰¹æ€§ã‚’è©•ä¾¡ã—ã€ç¾åœ¨ã®ç‰©ä½“æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ã¦ç”Ÿæˆã‚¿ã‚¹ã‚¯ã‚’åˆ†æã€‚æœ€è¿‘ã®ãƒ¢ãƒ‡ãƒ«ã¯æ”¹å–„ã‚’ç¤ºã™ãŒã€è¤‡é›‘ãªèƒ½åŠ›ã«ã¯èª²é¡ŒãŒæ®‹ã‚‹ã€‚GenEvalã¯å¤±æ•—ãƒ¢ãƒ¼ãƒ‰ã®ç™ºè¦‹ã«ã‚‚å¯„ä¸ã—ã€æ¬¡ä¸–ä»£ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã«å½¹ç«‹ã¤ã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ä¸­ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=Wbr51vK331&noteId=NpvYJlJFqK" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=Wbr51vK331&noteId=NpvYJlJFqK</a>


</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Regularization.html" target="_blank" rel="noopener noreferrer">#Regularization</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<span class="issue_date">Issue Date: 2025-08-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2604" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Dropout Reduces Underfitting, Zhuang Liu+, ICML'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åˆæœŸã«ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã‚¢ãƒ³ãƒ€ãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚’è»½æ¸›ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã€åˆæœŸãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆæ‰‹æ³•ã‚’ææ¡ˆã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å‹¾é…ã®æ–¹å‘çš„åˆ†æ•£ãŒæ¸›å°‘ã—ã€SGDã®ç¢ºç‡æ€§ã«å¯¾æŠ—ã—ã¾ã™ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€åˆæœŸãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’ç”¨ã„ãŸãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆãªã—ã®ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚ä½ã„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æå¤±ã‚’ç¤ºã—ã€ä¸€èˆ¬åŒ–ç²¾åº¦ãŒå‘ä¸Šã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚ã¾ãŸã€å¾ŒæœŸãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã¨ã„ã†æ‰‹æ³•ã‚‚æ¢æ±‚ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¾ŒåŠã§ã®æ­£å‰‡åŒ–åŠ¹æœã‚’æ¤œè¨¼ã—ã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã®çµæœã¯ã€æ·±å±¤å­¦ç¿’ã«ãŠã‘ã‚‹æ­£å‰‡åŒ–ã®ç†è§£ã‚’æ·±ã‚ã‚‹ã“ã¨ã«å¯„ä¸ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬:


<a href="https://www.docswell.com/s/DeepLearning2023/54QM6D-dldropout-reduces-underfitting" target="_blank" rel="noopener noreferrer">https://www.docswell.com/s/DeepLearning2023/54QM6D-dldropout-reduces-underfitting</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-08-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2564" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Scalable Diffusion Models with Transformers, William Peebles+, ICCV'23</a>
<span class="snippet"><span>GPT Summary</span>- æ–°ã—ã„ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã«åŸºã¥ãæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ï¼ˆDiffusion Transformers, DiTsï¼‰ã‚’ææ¡ˆã—ã€U-Netã‚’ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã«ç½®ãæ›ãˆãŸã€‚DiTsã¯é«˜ã„Gflopsã‚’æŒã¡ã€ä½ã„FIDã‚’ç¶­æŒã—ãªãŒã‚‰è‰¯å¥½ãªã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’ç¤ºã™ã€‚æœ€å¤§ã®DiT-XL/2ãƒ¢ãƒ‡ãƒ«ã¯ã€ImageNetã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å¾“æ¥ã®æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚Šã€æœ€å…ˆç«¯ã®FID 2.27ã‚’é”æˆã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬:


<a href="https://qiita.com/sasgawy/items/8546c784bc94d94ef0b2" target="_blank" rel="noopener noreferrer">https://qiita.com/sasgawy/items/8546c784bc94d94ef0b2</a>


</p>
<p>ã‚ˆãè¦‹ã‚‹DiT<br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2526" target="_blank" rel="noopener noreferrer">[Paper Note] DiT: Self-supervised Pre-training for Document Image Transformer, Junlong Li+, ACMMM'22</a>
<br><br>ã‚‚åŒæ§˜ã®å‘¼ç§°ã ãŒå…¨ãç•°ãªã‚‹è©±ãªã®ã§æ³¨æ„</p></span><br><br>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Deduplication.html" target="_blank" rel="noopener noreferrer">#Deduplication</a>
<span class="issue_date">Issue Date: 2025-08-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2445" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] SemDeDup: Data-efficient learning at web-scale through semantic  deduplication, Amro Abbas+, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- SemDeDupã¯ã€äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®åŸ‹ã‚è¾¼ã¿ã‚’ç”¨ã„ã¦æ„å‘³çš„ã«é‡è¤‡ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãƒšã‚¢ã‚’ç‰¹å®šã—å‰Šé™¤ã™ã‚‹æ‰‹æ³•ã€‚LAIONã®ã‚µãƒ–ã‚»ãƒƒãƒˆã§50%ã®ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ã‚’å®Ÿç¾ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“ã‚’åŠåˆ†ã«çŸ­ç¸®ã€‚åˆ†å¸ƒå¤–æ€§èƒ½ã‚‚å‘ä¸Šã—ã€C4ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚‚åŠ¹ç‡æ€§ã‚’æ”¹å–„ã€‚è³ªã®é«˜ã„åŸ‹ã‚è¾¼ã¿ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ‡ãƒ¼ã‚¿å‰Šæ¸›ã¨å­¦ç¿’åŠ é€Ÿã‚’ä¸¡ç«‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>embeddingç©ºé–“ã«ãŠã„ã¦è¿‘å‚ã®ã‚µãƒ³ãƒ—ãƒ«(near-duplicates)ã‚’å‰Šé™¤ã™ã‚‹ã“ã¨ã§ã€å­¦ç¿’åŠ¹ç‡ãŒå‘ä¸Šã—ã¾ã™ã€ã¨ã„ã†è©±ãªæ¨¡æ§˜ã€‚<br>&lt;img width="957" height="535" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/11511a7e-feaa-4e7b-8276-628fe5099be9"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/11511a7e-feaa-4e7b-8276-628fe5099be9"&lt;/a&gt;


/&gt;<br><br>openreview:


<a href="https://openreview.net/forum?id=IRSesTQUtb&noteId=usQjFYYAZJ" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=IRSesTQUtb&noteId=usQjFYYAZJ</a>


<br><br>openreviewã«ã‚ˆã‚‹ã¨ã€embeddingç©ºé–“ã«ãŠã„ã¦near-duplicatesã‚’å‰Šé™¤ã™ã‚‹ã¨ã„ã†ã‚¢ã‚¤ãƒ‡ã‚¢ã¯èˆˆå‘³æ·±ã„ãŒã€ææ¡ˆæ‰‹æ³•ã¯æ—¢å­˜ç ”ç©¶ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã‚’çµ„ã¿åˆã‚ã›ã¦ã„ã‚‹ã«ç•™ã¾ã£ã¦ãŠã‚Šï¼ˆå¤šãã®ãƒ–ãƒ­ã‚°ãƒã‚¹ãƒˆã‚„deduplicationã®ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚‚å­˜åœ¨ã™ã‚‹ï¼‰æ–°è¦æ€§ãŒæ˜ç¢ºã§ã¯ãªã„ç‚¹ã‚„ã€å®Ÿé¨“çµæœãŒä¸è¶³ã—ã¦ã„ã‚‹ï¼ˆi.e., å…¨ã¦ã®ã‚±ãƒ¼ã‚¹ã§SoTAã¨ã„ã†ã‚ã‘ã§ã‚‚ãªãã€å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã®å®Ÿé¨“ã‚„strong baselineã®ä¸åœ¨ï¼ˆå®Ÿé¨“çµæœã¯random pruningã«å¯¾ã—ã¦outperformã™ã‚‹ã“ã¨ãŒä¸»ã«ç¤ºã•ã‚Œã¦ã„ã‚‹ï¼‰ãªã©ã€è«–æ–‡ã®ä¸»å¼µã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãŸã‚ã®çµæœãŒè¶³ã‚Šãªã„ï¼‰ã¨ã„ã†æŒ‡æ‘˜ãŒã•ã‚Œã¦ã„ã‚‹ã€‚<br>å®Ÿç”¨çš„ã«ã¯well-writtenã§exampleã‚‚è±Šå¯Œã¨ã®ã“ã¨ãªã®ã§ã€Deduplicationã®ç†è§£ã‚’æ·±ã‚ã‚‹ã®ã«è‰¯ã•ãã†ã€‚</p>
<p>å…ˆè¡Œç ”ç©¶:<br>- ï¼ˆç”»åƒï¼‰<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2688" target="_blank" rel="noopener noreferrer">[Paper Note] Beyond neural scaling laws: beating power law scaling via data pruning, Ben Sorscher+, NeurIPS'22</a>
 <br>- ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2689" target="_blank" rel="noopener noreferrer">[Paper Note] Deduplicating Training Data Makes Language Models Better, Katherine Lee+, ACL'22</a>
<br><br><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2688" target="_blank" rel="noopener noreferrer">[Paper Note] Beyond neural scaling laws: beating power law scaling via data pruning, Ben Sorscher+, NeurIPS'22</a>
 ã§ã¯ã€åˆ†é¡ãŒé›£ã—ã„ç”»åƒã®ãƒ‡ãƒ¼ã‚¿ã¨ã„ã†è¦³ç‚¹ã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã—ã¦ãŠã‚Šã€<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2689" target="_blank" rel="noopener noreferrer">[Paper Note] Deduplicating Training Data Makes Language Models Better, Katherine Lee+, ACL'22</a>
 ã§ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã®è¡¨å±¤çš„ãªæƒ…å ±ã®ä¸€è‡´ã«åŸºã¥ã„ã¦Deduplicationã‚’å®Ÿæ–½ã—ã¦ã„ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Controllable.html" target="_blank" rel="noopener noreferrer">#Controllable</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<span class="issue_date">Issue Date: 2025-08-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2373" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Adding Conditional Control to Text-to-Image Diffusion Models, Lvmin Zhang+, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- ControlNetã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«ç©ºé–“çš„ãªæ¡ä»¶åˆ¶å¾¡ã‚’è¿½åŠ ã™ã‚‹ãŸã‚ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚ã‚Šã€äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å±¤ã‚’å†åˆ©ç”¨ã—ã¦å¤šæ§˜ãªæ¡ä»¶åˆ¶å¾¡ã‚’å­¦ç¿’ã—ã¾ã™ã€‚ã‚¼ãƒ­ç•³ã¿è¾¼ã¿ã‚’ç”¨ã„ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¾ã€…ã«å¢—åŠ ã•ã›ã€æœ‰å®³ãªãƒã‚¤ã‚ºã®å½±éŸ¿ã‚’è»½æ¸›ã—ã¾ã™ã€‚Stable Diffusionã‚’ç”¨ã„ã¦æ§˜ã€…ãªæ¡ä»¶åˆ¶å¾¡ã‚’ãƒ†ã‚¹ãƒˆã—ã€å°è¦æ¨¡ãŠã‚ˆã³å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦å …ç‰¢æ€§ã‚’ç¤ºã—ã¾ã—ãŸã€‚ControlNetã¯ç”»åƒæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®åˆ¶å¾¡ã«ãŠã‘ã‚‹åºƒç¯„ãªå¿œç”¨ã®å¯èƒ½æ€§ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>ControlNetè«–æ–‡</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/FlowMatching.html" target="_blank" rel="noopener noreferrer">#FlowMatching</a>
<a class="button" href="articles/OptimalTransport.html" target="_blank" rel="noopener noreferrer">#OptimalTransport</a>
<span class="issue_date">Issue Date: 2025-07-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2166" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Flow Matching for Generative Modeling, Yaron Lipman+, ICLR'23</a>
<span class="snippet"><span>GPT Summary</span>- Continuous Normalizing Flowsï¼ˆCNFsï¼‰ã«åŸºã¥ãæ–°ã—ã„ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´æ‰‹æ³•Flow Matchingï¼ˆFMï¼‰ã‚’ææ¡ˆã€‚FMã¯å›ºå®šã•ã‚ŒãŸæ¡ä»¶ä»˜ãç¢ºç‡çµŒè·¯ã®ãƒ™ã‚¯ãƒˆãƒ«å ´ã‚’å›å¸°ã—ã€ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¸è¦ã§è¨“ç·´å¯èƒ½ã€‚æ‹¡æ•£çµŒè·¯ã¨ä½µç”¨ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šå …ç‰¢ãªè¨“ç·´ãŒå®Ÿç¾ã€‚æœ€é©è¼¸é€ã‚’ç”¨ã„ãŸæ¡ä»¶ä»˜ãç¢ºç‡çµŒè·¯ã¯åŠ¹ç‡çš„ã§ã€è¨“ç·´ã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒé€Ÿãã€ä¸€èˆ¬åŒ–æ€§èƒ½ã‚‚å‘ä¸Šã€‚ImageNetã§ã®å®Ÿé¨“ã«ã‚ˆã‚Šã€FMã¯æ‹¡æ•£ãƒ™ãƒ¼ã‚¹æ‰‹æ³•ã‚ˆã‚Šã‚‚å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ã€è¿…é€Ÿãªã‚µãƒ³ãƒ—ãƒ«ç”Ÿæˆã‚’å¯èƒ½ã«ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3208" target="_blank" rel="noopener noreferrer">[Paper Note] High-Resolution Image Synthesis with Latent Diffusion Models, Robin Rombach+, CVPR'22, 2021.12</a>
</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<span class="issue_date">Issue Date: 2025-06-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2111" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Sigmoid Loss for Language Image Pre-Training, Xiaohua Zhai+, ICCV'23</a>
<span class="snippet"><span>GPT Summary</span>- ã‚·ãƒ³ãƒ—ãƒ«ãªãƒšã‚¢ãƒ¯ã‚¤ã‚ºã‚·ã‚°ãƒ¢ã‚¤ãƒ‰æå¤±ï¼ˆSigLIPï¼‰ã‚’ææ¡ˆã—ã€ç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ã«åŸºã¥ãè¨€èª-ç”»åƒäº‹å‰å­¦ç¿’ã‚’æ”¹å–„ã€‚ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰æå¤±ã¯ãƒãƒƒãƒã‚µã‚¤ã‚ºã®æ‹¡å¤§ã‚’å¯èƒ½ã«ã—ã€å°ã•ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã‚‚æ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ã€‚SigLiTãƒ¢ãƒ‡ãƒ«ã¯84.5%ã®ImageNetã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆç²¾åº¦ã‚’é”æˆã€‚ãƒãƒƒãƒã‚µã‚¤ã‚ºã®å½±éŸ¿ã‚’ç ”ç©¶ã—ã€32kãŒåˆç†çš„ãªã‚µã‚¤ã‚ºã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã€‚ãƒ¢ãƒ‡ãƒ«ã¯å…¬é–‹ã•ã‚Œã€ã•ã‚‰ãªã‚‹ç ”ç©¶ã®ä¿ƒé€²ã‚’æœŸå¾…ã€‚</span>
<span class="snippet"><span>Comment</span><p>SigLIPè«–æ–‡</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/Normalization.html" target="_blank" rel="noopener noreferrer">#Normalization</a>
<span class="issue_date">Issue Date: 2025-04-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1899" target="_blank" rel="noopener noreferrer" class="title-link">Foundation Transformers, Hongyu Wang+, PMLR'23</a>
<span class="snippet"><span>GPT Summary</span>- è¨€èªã€è¦–è¦šã€éŸ³å£°ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã«ãŠã‘ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®åæŸãŒé€²ã‚€ä¸­ã€ç•°ãªã‚‹å®Ÿè£…ã®ã€ŒTransformersã€ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚æ±ç”¨ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ãŸã‚ã«ã€å®‰å®šæ€§ã‚’æŒã¤Foundation Transformerã®é–‹ç™ºãŒæå”±ã•ã‚Œã€Magnetoã¨ã„ã†æ–°ã—ã„Transformerå¤‰ç¨®ãŒç´¹ä»‹ã•ã‚Œã‚‹ã€‚Sub-LayerNormã¨ç†è«–ã«åŸºã¥ãåˆæœŸåŒ–æˆ¦ç•¥ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ã•ã¾ã–ã¾ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ãŠã„ã¦å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨å®‰å®šæ€§ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªãƒ¢ãƒ‡ãƒ«ãªãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ã«ãŠã„ã¦ã€PostLNã¯vision encodingã«ãŠã„ã¦sub-optimalã§ã€PreLNã¯text encodingã«ãŠã„ã¦sub-optimalã§ã‚ã‚‹ã“ã¨ãŒå…ˆè¡Œç ”ç©¶ã§ç¤ºã•ã‚Œã¦ãŠã‚Šã€ãƒãƒ«ã‚¿ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’å˜ä¸€ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã€é«˜æ€§èƒ½ã€ã‹ã¤å­¦ç¿’ã®å®‰å®šæ€§ãªé«˜ãã€try and errorç„¡ã—ã§é©ç”¨ã§ãã‚‹åŸºç›¤ã¨ãªã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒå¿…è¦ã¨ã„ã†ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã§ææ¡ˆã•ã‚ŒãŸæ‰‹æ³•ã€‚å…·ä½“çš„ã«ã¯ã€Sub-LayerNorm(Sub-LN)ã¨å‘¼ã°ã‚Œã‚‹ã€self attentionã¨FFNéƒ¨åˆ†ã«è¿½åŠ ã®LayerNormã‚’é©ç”¨ã™ã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã€DeepNetã‚’è¸è¥²ã—Layeræ•°ãŒéå¸¸ã«å¤§ãã„å ´åˆã§ã‚‚å­¦ç¿’ãŒå®‰å®šã™ã‚‹ã‚ˆã†ãªé‡ã¿ã®åˆæœŸåŒ–æ–¹æ³•ã‚’ç†è«–çš„ã«åˆ†æã—ææ¡ˆã—ã¦ã„ã‚‹ã€‚<br><br>å…·ä½“çš„ã«ã¯ã€Sub-LNã®å ´åˆã€LayerNormã‚’<br>- SelfAttentionè¨ˆç®—ã«ãŠã‘ã‚‹QKVã‚’æ±‚ã‚ã‚‹ãŸã‚ã®input Xã®projectionã®å‰ã¨Attentionã®å‡ºåŠ›projectionã®å‰<br>- FFNã§ã®å„Linear Layerã®å‰<br>ã«é©ç”¨ã—ã€<br><br>åˆæœŸåŒ–ã‚’ã™ã‚‹éš›ã«ã¯ã€FFNã®W, ãŠã‚ˆã³self-attentionã®V_projã¨å‡ºåŠ›ã®out_projã®åˆæœŸåŒ–ã‚’Î³ï¼ˆï¼sqrt(log(2N))ã«ã‚ˆã£ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹æ–¹æ³•ã‚’ææ¡ˆã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚<br><br><img src="https://github.com/user-attachments/assets/2847f982-3266-4394-9920-01d9977e505e" alt="image" loading="lazy"></p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1900" target="_blank" rel="noopener noreferrer">DeepNet: Scaling Transformers to 1,000 Layers, Hongyu Wang+, arXiv'22</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/ImageSegmentation.html" target="_blank" rel="noopener noreferrer">#ImageSegmentation</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<span class="issue_date">Issue Date: 2025-04-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1885" target="_blank" rel="noopener noreferrer" class="title-link">Segment Anything, Alexander Kirillov+, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- Segment Anything (SA)ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã€ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ææ¡ˆã—ã€1å„„ä»¥ä¸Šã®ãƒã‚¹ã‚¯ã‚’å«ã‚€1,100ä¸‡ã®ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼å°Šé‡ã—ãŸç”»åƒã‹ã‚‰ãªã‚‹æœ€å¤§ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã¯ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã§æ–°ã—ã„ç”»åƒåˆ†å¸ƒã‚„ã‚¿ã‚¹ã‚¯ã«é©å¿œã§ãã€è©•ä¾¡ã®çµæœã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ€§èƒ½ãŒé«˜ãã€å¾“æ¥ã®ç›£è¦–ã•ã‚ŒãŸçµæœã‚’ä¸Šå›ã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ã€‚SAMã¨SA-1Bãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ç ”ç©¶ä¿ƒé€²ã®ãŸã‚ã«å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>SAMè«–æ–‡</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<span class="issue_date">Issue Date: 2025-04-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1881" target="_blank" rel="noopener noreferrer" class="title-link">PaLI-3 Vision Language Models: Smaller, Faster, Stronger, Xi Chen+, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- PaLI-3ã¯ã€å¾“æ¥ã®ãƒ¢ãƒ‡ãƒ«ã«æ¯”ã¹ã¦10å€å°å‹ã§é«˜é€Ÿãªè¦–è¦šè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰ã§ã‚ã‚Šã€ç‰¹ã«ãƒ­ãƒ¼ã‚«ãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚„è¦–è¦šçš„ãƒ†ã‚­ã‚¹ãƒˆç†è§£ã«ãŠã„ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã€‚SigLIPãƒ™ãƒ¼ã‚¹ã®PaLIã¯ã€20å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã•ã‚Œã€å¤šè¨€èªã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«æ¤œç´¢ã§æ–°ãŸãªæœ€å…ˆç«¯ã‚’é”æˆã€‚50å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®PaLI-3ã¯ã€VLMã®ç ”ç©¶ã‚’å†ç‡ƒã•ã›ã‚‹ã“ã¨ã‚’æœŸå¾…ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>OpenReview:


<a href="https://openreview.net/forum?id=JpyWPfzu0b" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=JpyWPfzu0b</a>


<br><br>å®Ÿé¨“çš„ã«ç´ æ™´ã‚‰ã—ã„æ€§èƒ½ãŒå®Ÿç¾ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã¯èªã‚ã‚‰ã‚Œã¤ã¤ã‚‚<br>- æ¯”è¼ƒå¯¾è±¡ãŒSigLIPã®ã¿ã§ã‚ˆã‚Šåºƒç¯„ãªæ¯”è¼ƒå®Ÿé¨“ã¨åˆ†æãŒå¿…è¦ãªã“ã¨<br>- Backboneãƒ¢ãƒ‡ãƒ«ã‚’Contrastive Learningã™ã‚‹ã“ã¨è‡ªä½“ã®æœ‰ç”¨æ€§ã¯æ—¢ã«çŸ¥ã‚‰ã‚Œã¦ãŠã‚Šã€æ–°è¦æ€§ã«ä¹ã—ã„ã“ã¨<br><br>ã¨ã—ã¦ICLR'24ã«Rejectã•ã‚Œã¦ã„ã‚‹</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Zero/Few/ManyShotPrompting.html" target="_blank" rel="noopener noreferrer">#Zero/Few/ManyShotPrompting</a>
<a class="button" href="articles/Self-SupervisedLearning.html" target="_blank" rel="noopener noreferrer">#Self-SupervisedLearning</a>
<span class="issue_date">Issue Date: 2024-10-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1448" target="_blank" rel="noopener noreferrer" class="title-link">SINC: Self-Supervised In-Context Learning for Vision-Language Tasks, Yi-Syuan Chen+, N_A, ICCV'23</a>
<span class="snippet"><span>GPT Summary</span>- è‡ªå·±æ•™å¸«ã‚ã‚Šæ–‡è„ˆå†…å­¦ç¿’ï¼ˆSINCï¼‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«ä¾å­˜ã›ãšã«æ–‡è„ˆå†…å­¦ç¿’ã‚’å®Ÿç¾ã€‚ç‰¹åˆ¥ã«èª¿æ•´ã•ã‚ŒãŸãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”¨ã„ãŸãƒ¡ã‚¿ãƒ¢ãƒ‡ãƒ«ãŒã€è¦–è¦šã¨è¨€èªã®ã‚¿ã‚¹ã‚¯ã§å°‘æ•°ã‚·ãƒ§ãƒƒãƒˆè¨­å®šã«ãŠã„ã¦å‹¾é…ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã€‚SINCã¯æ–‡è„ˆå†…å­¦ç¿’ã®åˆ©ç‚¹ã‚’æ¢æ±‚ã—ã€é‡è¦ãªè¦ç´ ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/GenerativeAI.html" target="_blank" rel="noopener noreferrer">#GenerativeAI</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<span class="issue_date">Issue Date: 2023-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1169" target="_blank" rel="noopener noreferrer" class="title-link">SEINE: Short-to-Long Video Diffusion Model for Generative Transition and  Prediction, Xinyuan Chen+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒ“ãƒ‡ã‚ªç”Ÿæˆã«ãŠã„ã¦é€£ç¶šã—ãŸé•·ã„ãƒ“ãƒ‡ã‚ªã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®ã‚¸ã‚§ãƒãƒ¬ãƒ¼ãƒ†ã‚£ãƒ–ãªãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³ã¨äºˆæ¸¬ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸãƒ¢ãƒ‡ãƒ«SEINEã‚’ææ¡ˆã™ã‚‹ã€‚SEINEã¯ãƒ†ã‚­ã‚¹ãƒˆã®èª¬æ˜ã«åŸºã¥ã„ã¦ãƒˆãƒ©ãƒ³ã‚¸ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã—ã€ä¸€è²«æ€§ã¨è¦–è¦šçš„å“è³ªã‚’ç¢ºä¿ã—ãŸé•·ã„ãƒ“ãƒ‡ã‚ªã‚’ç”Ÿæˆã™ã‚‹ã€‚ã•ã‚‰ã«ã€ææ¡ˆæ‰‹æ³•ã¯ä»–ã®ã‚¿ã‚¹ã‚¯ã«ã‚‚æ‹¡å¼µå¯èƒ½ã§ã‚ã‚Šã€å¾¹åº•çš„ãªå®Ÿé¨“ã«ã‚ˆã‚Šãã®æœ‰åŠ¹æ€§ãŒæ¤œè¨¼ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>


<a href="https://huggingface.co/spaces/Vchitect/SEINE" target="_blank" rel="noopener noreferrer">https://huggingface.co/spaces/Vchitect/SEINE</a>


<br><br>ç”»åƒ + ãƒ†ã‚­ã‚¹ãƒˆpromptã§ã€å‹•ç”»ã‚’ç”Ÿæˆã™ã‚‹ãƒ‡ãƒ¢</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ImageSegmentation.html" target="_blank" rel="noopener noreferrer">#ImageSegmentation</a>
<a class="button" href="articles/Prompting.html" target="_blank" rel="noopener noreferrer">#Prompting</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2023-11-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1160" target="_blank" rel="noopener noreferrer" class="title-link">Visual In-Context Prompting, Feng Li+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒ“ã‚¸ãƒ§ãƒ³é ˜åŸŸã«ãŠã‘ã‚‹æ±ç”¨çš„ãªãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã¾ã™ã€‚ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½¿ç”¨ã—ã€ã•ã¾ã–ã¾ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’é–‹ç™ºã—ã¾ã—ãŸã€‚ã•ã‚‰ã«ã€ä»»æ„ã®æ•°ã®å‚ç…§ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦å—ã‘å–ã‚‹ã‚ˆã†ã«æ‹¡å¼µã—ã¾ã—ãŸã€‚å®Ÿé¨“çµæœã‹ã‚‰ã€ææ¡ˆæ‰‹æ³•ãŒéå‡¡ãªå‚ç…§ãŠã‚ˆã³ä¸€èˆ¬çš„ãªã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³èƒ½åŠ›ã‚’å¼•ãå‡ºã—ã€ç«¶äº‰åŠ›ã®ã‚ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã™ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>Image Segmentationã«ã¯ã€ãƒ¦ãƒ¼ã‚¶ãŒä¸ãˆãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å…±é€šã®ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’æŒã¤ã™ã¹ã¦ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹ã‚¿ã‚¹ã‚¯ã¨ã€ãƒ¦ãƒ¼ã‚¶ã®å…¥åŠ›ã®ç‰¹å®šã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã‚’ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹ã‚¿ã‚¹ã‚¯ãŒã‚ã‚‹ã€‚å¾“æ¥ã¯å€‹åˆ¥ã®ã‚¿ã‚¹ã‚¯ã”ã¨ã«ã€ç‰¹å®šã®å…¥åŠ›æ–¹æ³•ï¼ˆVisual Prompt, Image Promptï¼‰ã‚’å‰æã¨ã—ãŸæ‰‹æ³•ã‚„ã€å€‹ã€…ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿæ–½ã§ãã‚‹ãŒIn-Context Promptã—ã‹ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„æ‰‹æ³•ã—ã‹ãªã‹ã£ãŸãŒã€ã“ã®ç ”ç©¶ã§ã¯ã€Visual Prompt, Image Prompt, In-Context Promptã‚’ãã‚Œãã‚Œã‚µãƒãƒ¼ãƒˆã—ä¸¡ã‚¿ã‚¹ã‚¯ã‚’å®Ÿæ–½ã§ãã‚‹ã¨ã„ã†ä½ç½®ä»˜ã‘ã®æ¨¡æ§˜ã€‚ã¾ãŸã€ææ¡ˆæ‰‹æ³•ã§ã¯ã‚¹ãƒˆãƒ­ãƒ¼ã‚¯ã€ç‚¹ã€ãƒœãƒƒã‚¯ã‚¹ã¨ã„ã£ãŸãƒ¦ãƒ¼ã‚¶ã®ç”»åƒã«å¯¾ã™ã‚‹æç”»ã«åŸºã¥ãPromptingã‚’ã‚µãƒãƒ¼ãƒˆã—ã€Promptingã«ãŠã‘ã‚‹å‚ç…§ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°ã‚‚ä»»æ„ã®æ•°æŒ‡å®šã§ãã‚‹ã¨ã®ã“ã¨ã€‚<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/f5da3d7b-68aa-4120-a37c-7c42be1704f8" alt="image" loading="lazy"><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/e6992dde-e10b-41cb-a190-eb78376bef31" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LayoutGeneration.html" target="_blank" rel="noopener noreferrer">#LayoutGeneration</a>
<span class="issue_date">Issue Date: 2023-11-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1133" target="_blank" rel="noopener noreferrer" class="title-link">LayoutPrompter: Awaken the Design Ability of Large Language Models, Jiawei Lin+, N_A, NeurIPS'23</a>
<span class="snippet"><span>GPT Summary</span>- LayoutPrompterã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã‚’ä½¿ç”¨ã—ã¦æ¡ä»¶ä»˜ãã®ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆç”Ÿæˆã‚’è¡Œã†æ‰‹æ³•ã§ã‚ã‚Šã€å…¥åŠ›-å‡ºåŠ›ã®ã‚·ãƒªã‚¢ãƒ«åŒ–ã€å‹•çš„ãªæ¨¡ç¯„çš„é¸æŠã€ãŠã‚ˆã³ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®3ã¤ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚LayoutPrompterã¯ã€æ—¢å­˜ã®æ‰‹æ³•ã¨ç«¶åˆã—ãŸã‚Šä¸Šå›ã£ãŸã‚Šã™ã‚‹æ€§èƒ½ã‚’æŒã¡ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„å¾®èª¿æ•´ãªã—ã§ä½¿ç”¨ã§ãã‚‹æ±ç”¨æ€§ã®ã‚ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã‚ã‚‹ã“ã¨ãŒå®Ÿé¨“çµæœã‹ã‚‰ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚ã¾ãŸã€ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã«ã‚‚å„ªã‚Œã¦ãŠã‚Šã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ˆã‚Šã‚‚æœ‰æ„ã«å„ªã‚Œã¦ã„ã‚‹ã“ã¨ã‚‚ç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€https://github.com/microsoft/LayoutGeneration/tree/main/LayoutPrompterã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>Conditional Graphic Layout Generation</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultitaskLearning.html" target="_blank" rel="noopener noreferrer">#MultitaskLearning</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<span class="issue_date">Issue Date: 2023-11-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1127" target="_blank" rel="noopener noreferrer" class="title-link">Florence-2: Advancing a Unified Representation for a Variety of Vision  Tasks, Bin Xiao+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- Florence-2ã¯ã€ãƒ“ã‚¸ãƒ§ãƒ³åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ã•ã¾ã–ã¾ãªãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã«å¯¾å¿œã™ã‚‹ãŸã‚ã®çµ±ä¸€ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ™ãƒ¼ã‚¹ã®è¡¨ç¾ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å—ã‘å–ã‚Šã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡ºã€ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã€ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãªã©ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã€ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§çµæœã‚’ç”Ÿæˆã—ã¾ã™ã€‚ã¾ãŸã€FLD-5Bã¨ã„ã†å¤§è¦æ¨¡ãªæ³¨é‡ˆä»˜ããƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚é–‹ç™ºã•ã‚Œã¾ã—ãŸã€‚Florence-2ã¯ã€å¤šç›®çš„ã‹ã¤åŒ…æ‹¬çš„ãªãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ„ãƒ¼ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ§‹é€ ã‚’æ¡ç”¨ã—ã¦ãŠã‚Šã€å‰ä¾‹ã®ãªã„ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãŠã‚ˆã³ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®èƒ½åŠ›ã‚’æŒã¤å¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>Vison Foundation Modelã€‚Spatialãªéšå±¤æ§‹é€ ã‚„ã€Semanticã‚’æ‰ãˆã‚‰ã‚Œã‚‹ã‚ˆã†ã«è¨“ç·´ã€‚Image/Prompt Encoderã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã€outputã¯text + location informationã¨ãªã‚‹ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/9fbfba62-190f-46eb-a893-5ebe76dda030" alt="image" loading="lazy"><br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/f7497161-6b9a-4adc-aa6b-53debe1e9318" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/OCR.html" target="_blank" rel="noopener noreferrer">#OCR</a>
<span class="issue_date">Issue Date: 2023-10-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1093" target="_blank" rel="noopener noreferrer" class="title-link">Exploring OCR Capabilities of GPT-4Vï¼ˆisionï¼‰ : A Quantitative and  In-depth Evaluation, Yongxin Shi+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- ã“ã®è«–æ–‡ã§ã¯ã€GPT-4Vã¨ã„ã†å¤§è¦æ¨¡ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å…‰å­¦æ–‡å­—èªè­˜ï¼ˆOCRï¼‰èƒ½åŠ›ã‚’è©•ä¾¡ã—ã¾ã™ã€‚ã•ã¾ã–ã¾ãªOCRã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã—ã€ãƒ©ãƒ†ãƒ³æ–‡å­—ã®èªè­˜ã¨ç†è§£ã«ãŠã„ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ä¸€æ–¹ã€å¤šè¨€èªã‚„è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã«ã¯è‹¦æˆ¦ã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚ã“ã‚Œã«åŸºã¥ã„ã¦ã€å°‚é–€ã®OCRãƒ¢ãƒ‡ãƒ«ã®å¿…è¦æ€§ã‚„GPT-4Vã‚’æ´»ç”¨ã™ã‚‹æˆ¦ç•¥ã«ã¤ã„ã¦ã‚‚æ¤œè¨ã—ã¾ã™ã€‚ã“ã®ç ”ç©¶ã¯ã€å°†æ¥ã®LMMã‚’ç”¨ã„ãŸOCRã®ç ”ç©¶ã«å½¹ç«‹ã¤ã‚‚ã®ã§ã™ã€‚è©•ä¾¡ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨çµæœã¯ã€GitHubã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>GPT4-Vã‚’ã•ã¾ã–ã¾ãªOCRã‚¿ã‚¹ã‚¯ã€Œæ‰‹æ›¸ãã€æ•°å¼ã€ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ èªè­˜ç­‰ã‚’å«ã‚€ï¼‰ã§æ€§èƒ½æ¤œè¨¼ã—ãŸç ”ç©¶ã€‚<br>MLT19ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã£ãŸè©•ä¾¡ã§ã¯ã€æ—¥æœ¬èªã®æ€§èƒ½ã¯éå¸¸ã«ä½ãã€è‹±èªã¨ãƒ•ãƒ©ãƒ³ã‚¹èªãŒæ€§èƒ½é«˜ã„ã€‚æ‰‹æ›¸ãæ–‡å­—èªè­˜ã§ã¯è‹±èªã¨ä¸­å›½èªã§ã®ã¿è©•ä¾¡ã€‚<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/c433b921-c527-441f-8925-00f4ac5fc6c3" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<span class="issue_date">Issue Date: 2023-08-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/914" target="_blank" rel="noopener noreferrer" class="title-link">Foundational Models Defining a New Era in Vision: A Survey and Outlook, Muhammad Awais+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€è¦–è¦šã‚·ã‚¹ãƒ†ãƒ ã®åŸºç¤ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦åŒ…æ‹¬çš„ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’æä¾›ã—ã¾ã™ã€‚ã“ã‚Œã«ã¯ã€ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’çµ„ã¿åˆã‚ã›ã‚‹ãŸã‚ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆã‚„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç›®æ¨™ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãªã©ãŒå«ã¾ã‚Œã¾ã™ã€‚ã¾ãŸã€åŸºç¤ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚„èª²é¡Œã€æœ€è¿‘ã®ç™ºå±•ã«ã¤ã„ã¦ã‚‚è­°è«–ã—ã¾ã™ã€‚è©³ç´°ãªãƒªã‚¹ãƒˆã¯ã€\url{https://github.com/awaisrauf/Awesome-CV-Foundational-Models}ã§å…¥æ‰‹ã§ãã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>CVã«ãŠã‘ã‚‹foundation modelã®surveyã€‚æ®‹ã•ã‚ŒãŸãƒãƒ£ãƒ¬ãƒ³ã‚¸ã¨ç ”ç©¶ã®æ–¹å‘æ€§ãŒè­°è«–ã•ã‚Œã¦ã„ã‚‹</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<span class="issue_date">Issue Date: 2023-07-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/897" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction  Tuning, Lili Yu+, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- CM3Leonã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®ç”Ÿæˆãƒ»è£œå®ŒãŒå¯èƒ½ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã€ãƒªãƒˆãƒªãƒ¼ãƒãƒ«æ‹¡å¼µå‹ã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒ™ãƒ¼ã‚¹ã®ãƒ‡ã‚³ãƒ¼ãƒ€ã‚’ä½¿ç”¨ã€‚CM3ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’åŸºã«ã€å¤šæ§˜ãªæŒ‡ç¤ºã‚¹ã‚¿ã‚¤ãƒ«ã§ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¨ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«å„ªã‚Œã€åˆã®ãƒ†ã‚­ã‚¹ãƒˆå°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰é©å¿œã•ã‚ŒãŸãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã€‚é«˜å“è³ªãªå‡ºåŠ›ã‚’ç”Ÿæˆã™ã‚‹å¯¾ç…§çš„ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ‰‹æ³•ã‚’å°å…¥ã—ã€å°‘ãªã„è¨ˆç®—é‡ã§æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’é”æˆã€‚SFTå¾Œã¯ã€ç”»åƒç·¨é›†ã‚„ç”Ÿæˆã«ãŠã„ã¦é«˜ã„åˆ¶å¾¡æ€§ã‚’ç¤ºã™ã€‚</span>
<a class="button" href="articles/NaturalLanguageGeneration.html" target="_blank" rel="noopener noreferrer">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<span class="issue_date">Issue Date: 2023-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/891" target="_blank" rel="noopener noreferrer" class="title-link">InfoMetIC: An Informative Metric for Reference-free Image Caption Evaluation, ACL'23</a>
<span class="snippet"><span>GPT Summary</span>- è‡ªå‹•ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®è©•ä¾¡ã«ã¯ã€æƒ…å ±è±Šã‹ãªãƒ¡ãƒˆãƒªãƒƒã‚¯ï¼ˆInfoMetICï¼‰ãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®èª¤ã‚Šã‚„æ¬ è½ã—ãŸæƒ…å ±ã‚’è©³ç´°ã«ç‰¹å®šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚InfoMetICã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã®ç²¾åº¦ã‚¹ã‚³ã‚¢ã€ãƒ“ã‚¸ãƒ§ãƒ³ã®å†ç¾ã‚¹ã‚³ã‚¢ã€ãŠã‚ˆã³å…¨ä½“ã®å“è³ªã‚¹ã‚³ã‚¢ã‚’æä¾›ã—ã€äººé–“ã®åˆ¤æ–­ã¨ã®ç›¸é–¢ã‚‚é«˜ã„ã§ã™ã€‚ã¾ãŸã€ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚æ§‹ç¯‰ã•ã‚Œã¦ã„ã¾ã™ã€‚è©³ç´°ã¯GitHubã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/LLMAgent.html" target="_blank" rel="noopener noreferrer">#LLMAgent</a>
<span class="issue_date">Issue Date: 2023-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/883" target="_blank" rel="noopener noreferrer" class="title-link">Towards A Unified Agent with Foundation Models, Norman Di Palo+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ãƒ“ã‚¸ãƒ§ãƒ³è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’å¼·åŒ–å­¦ç¿’ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«çµ„ã¿è¾¼ã¿ã€åŠ¹ç‡çš„ãªæ¢ç´¢ã‚„çµŒé¨“ãƒ‡ãƒ¼ã‚¿ã®å†åˆ©ç”¨ãªã©ã®èª²é¡Œã«å–ã‚Šçµ„ã‚€æ–¹æ³•ã‚’èª¿æŸ»ã—ã¾ã—ãŸã€‚ã‚¹ãƒ‘ãƒ¼ã‚¹ãªå ±é…¬ã®ãƒ­ãƒœãƒƒãƒˆæ“ä½œç’°å¢ƒã§ã®ãƒ†ã‚¹ãƒˆã«ãŠã„ã¦ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«æ¯”ã¹ã¦å¤§å¹…ãªæ€§èƒ½å‘ä¸Šã‚’å®Ÿè¨¼ã—ã€å­¦ç¿’æ¸ˆã¿ã®ã‚¹ã‚­ãƒ«ã‚’æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã®è§£æ±ºã‚„äººé–“ã®å°‚é–€å®¶ã®ãƒ“ãƒ‡ã‚ªã®æ¨¡å€£ã«æ´»ç”¨ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/aa40d0e3-9499-4804-9046-a9ad795c2d52" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Personalization.html" target="_blank" rel="noopener noreferrer">#Personalization</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2023-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/878" target="_blank" rel="noopener noreferrer" class="title-link">FABRIC: Personalizing Diffusion Models with Iterative Feedback, Dimitri von RÃ¼tte+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€æ‹¡æ•£ãƒ™ãƒ¼ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®å¤‰æ›ãƒ¢ãƒ‡ãƒ«ã«äººé–“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’çµ„ã¿è¾¼ã‚€æˆ¦ç•¥ã‚’ææ¡ˆã™ã‚‹ã€‚è‡ªå·±æ³¨æ„å±¤ã‚’åˆ©ç”¨ã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒªãƒ¼ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã‚ã‚‹FABRICã‚’ææ¡ˆã—ã€ã•ã¾ã–ã¾ãªæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«é©ç”¨å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ã¾ãŸã€åŒ…æ‹¬çš„ãªè©•ä¾¡æ–¹æ³•ã‚’å°å…¥ã—ã€äººé–“ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’çµ±åˆã—ãŸç”Ÿæˆãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®šé‡åŒ–ã™ã‚‹ãŸã‚ã®å …ç‰¢ãªãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æä¾›ã™ã‚‹ã€‚å¾¹åº•çš„ãªåˆ†æã«ã‚ˆã‚Šã€åå¾©çš„ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®è¤‡æ•°ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã‚’é€šã˜ã¦ç”ŸæˆçµæœãŒæ”¹å–„ã•ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å€‹åˆ¥åŒ–ã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä½œæˆã‚„ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãªã©ã®é ˜åŸŸã«å¿œç”¨ãŒå¯èƒ½ã¨ãªã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>upvote downvoteã‚’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã—ã€iterativeãªmannerã§Diffusionãƒ¢ãƒ‡ãƒ«ã®ç”Ÿæˆçµæœã‚’æ”¹å–„ã§ãã‚‹æ‰‹æ³•ã€‚å¤šãã®Diffusion based Modelã«å¯¾ã—ã¦é©ç”¨å¯èƒ½<br>ãƒ‡ãƒ¢: 


<a href="https://huggingface.co/spaces/dvruette/fabric" target="_blank" rel="noopener noreferrer">https://huggingface.co/spaces/dvruette/fabric</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/SpokenLanguageProcessing.html" target="_blank" rel="noopener noreferrer">#SpokenLanguageProcessing</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<span class="issue_date">Issue Date: 2023-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/875" target="_blank" rel="noopener noreferrer" class="title-link">Meta-Transformer: A Unified Framework for Multimodal Learning, Yiyuan Zhang+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å­¦ç¿’ã®ãŸã‚ã®Meta-Transformerã¨ã„ã†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®æƒ…å ±ã‚’å‡¦ç†ã—é–¢é€£ä»˜ã‘ã‚‹ãŸã‚ã®çµ±ä¸€ã•ã‚ŒãŸãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚Meta-Transformerã¯ã€å¯¾å¿œã®ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦12ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£é–“ã§çµ±ä¸€ã•ã‚ŒãŸå­¦ç¿’ã‚’è¡Œã†ã“ã¨ãŒã§ãã€ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€ãƒã‚¤ãƒ³ãƒˆã‚¯ãƒ©ã‚¦ãƒ‰ã€éŸ³å£°ã€ãƒ“ãƒ‡ã‚ªãªã©ã®åŸºæœ¬çš„ãªãƒ‘ãƒ¼ã‚»ãƒ—ã‚·ãƒ§ãƒ³ã‹ã‚‰ã€Xç·šã€èµ¤å¤–ç·šã€é«˜åˆ†å…‰ã€IMUãªã©ã®å®Ÿç”¨çš„ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã€ã‚°ãƒ©ãƒ•ã€è¡¨å½¢å¼ã€æ™‚ç³»åˆ—ãªã©ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ‹ãƒ³ã‚°ã¾ã§ã€å¹…åºƒã„ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚Meta-Transformerã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ç”¨ã„ãŸçµ±ä¸€ã•ã‚ŒãŸãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ã‚¹ã®é–‹ç™ºã«å‘ã‘ãŸæœ‰æœ›ãªæœªæ¥ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>12ç¨®é¡ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«å¯¾ã—ã¦å­¦ç¿’ã§ãã‚‹Transformerã‚’ææ¡ˆ<br>Dataã‚’sequenceã«tokenizeã—ã€unifiedã«featureã‚’encodingã—ã€ãã‚Œãã‚Œã®downstreamã‚¿ã‚¹ã‚¯ã§å­¦ç¿’<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/8734073a-573e-442e-8b9f-fed559199d56" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Personalization.html" target="_blank" rel="noopener noreferrer">#Personalization</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Conversation.html" target="_blank" rel="noopener noreferrer">#Conversation</a>
<span class="issue_date">Issue Date: 2023-07-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/839" target="_blank" rel="noopener noreferrer" class="title-link">MPCHAT: Towards Multimodal Persona-Grounded Conversation, ACL'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã®ä¸¡æ–¹ã‚’ä½¿ç”¨ã—ã¦ãƒ‘ãƒ¼ã‚½ãƒŠã‚’æ‹¡å¼µã—ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªå¯¾è©±ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹MPCHATã‚’ææ¡ˆã—ã¾ã™ã€‚ã•ã‚‰ã«ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‘ãƒ¼ã‚½ãƒŠã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€å¿œç­”äºˆæ¸¬ã€ãƒ‘ãƒ¼ã‚½ãƒŠã®ã‚°ãƒ©ã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°äºˆæ¸¬ã€è©±è€…ã®è­˜åˆ¥ã¨ã„ã£ãŸã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’çµ±è¨ˆçš„ã«æœ‰æ„ã«æ”¹å–„ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚ã“ã®ç ”ç©¶ã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªå¯¾è©±ç†è§£ã«ãŠã„ã¦ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‘ãƒ¼ã‚½ãƒŠã®é‡è¦æ€§ã‚’å¼·èª¿ã—ã€MPCHATãŒé«˜å“è³ªãªãƒªã‚½ãƒ¼ã‚¹ã¨ã—ã¦å½¹ç«‹ã¤ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚</span>
<a class="button" href="articles/NaturalLanguageGeneration.html" target="_blank" rel="noopener noreferrer">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/TabularData.html" target="_blank" rel="noopener noreferrer">#TabularData</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<span class="issue_date">Issue Date: 2023-07-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/835" target="_blank" rel="noopener noreferrer" class="title-link">Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models, ACL'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€Visionï¼†Languageï¼ˆVï¼†Lï¼‰ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®çŸ¥è­˜ã®ä¿æŒæ–¹æ³•ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã«ã€ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ç”»åƒã®ç”Ÿæˆã‚¿ã‚¹ã‚¯ã‚’ææ¡ˆã—ã¾ã™ã€‚ã“ã®ã‚¿ã‚¹ã‚¯ã§ã¯ã€ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã¨é–¢é€£ã™ã‚‹ç”»åƒã®çŸ¥è­˜ã‚’å«ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç”Ÿæˆã™ã‚‹ç¬¬ä¸€ã®éƒ¨åˆ†ã¨ã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã¨ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®é–¢é€£çŸ¥è­˜ã‚’å«ã‚€ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ç¬¬äºŒã®éƒ¨åˆ†ãŒã‚ã‚Šã¾ã™ã€‚ææ¡ˆã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«ã€Wikipediaã®ç´„20ä¸‡ã®infoboxã‹ã‚‰WikiTIGãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã—ãŸã€‚æœ€å…ˆç«¯ã®Vï¼†Lãƒ¢ãƒ‡ãƒ«OFAã‚’ä½¿ç”¨ã—ã¦ã€ææ¡ˆã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚å®Ÿé¨“çµæœã¯ã€OFAãŒä¸€éƒ¨ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£çŸ¥è­˜ã‚’å¿˜ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚</span>
<a class="button" href="articles/NaturalLanguageGeneration.html" target="_blank" rel="noopener noreferrer">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<span class="issue_date">Issue Date: 2023-07-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/831" target="_blank" rel="noopener noreferrer" class="title-link">Learning to Imagine: Visually-Augmented Natural Language Generation, ACL'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€è¦–è¦šæƒ…å ±ã‚’æ´»ç”¨ã—ãŸè‡ªç„¶è¨€èªç”Ÿæˆã®ãŸã‚ã®LIVEã¨ã„ã†æ‰‹æ³•ã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚LIVEã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦å ´é¢ã‚’æƒ³åƒã—ã€é«˜å“è³ªãªç”»åƒã‚’åˆæˆã™ã‚‹æ–¹æ³•ã§ã™ã€‚ã¾ãŸã€CLIPã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã®æƒ³åƒåŠ›ã‚’è©•ä¾¡ã—ã€æ®µè½ã”ã¨ã«ç”»åƒã‚’ç”Ÿæˆã—ã¾ã™ã€‚ã•ã¾ã–ã¾ãªå®Ÿé¨“ã«ã‚ˆã‚Šã€LIVEã®æœ‰åŠ¹æ€§ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚³ãƒ¼ãƒ‰ã€ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã¯å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>&gt;ã¾ãšã€ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦å ´é¢ã‚’æƒ³åƒã—ã¾ã™ã€‚å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ã„ã¦é«˜å“è³ªãªç”»åƒã‚’åˆæˆã™ã‚‹ãŸã‚ã«æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚æ¬¡ã«ã€CLIPã‚’ä½¿ç”¨ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆãŒæƒ³åƒåŠ›ã‚’å–šèµ·ã§ãã‚‹ã‹ã‚’äº‹å¾Œçš„ã«åˆ¤æ–­ã—ã¾ã™ã€‚æœ€å¾Œã«ã€ç§ãŸã¡ã®æƒ³åƒåŠ›ã¯å‹•çš„ã§ã‚ã‚Šã€æ®µè½å…¨ä½“ã«1ã¤ã®ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã®ã§ã¯ãªãã€å„æ–‡ã«å¯¾ã—ã¦åˆæˆã‚’è¡Œã„ã¾ã™ã€‚<br><br><br><br>èˆˆå‘³æ·±ã„</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<span class="issue_date">Issue Date: 2023-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/806" target="_blank" rel="noopener noreferrer" class="title-link">Generative Pretraining in Multimodality, Quan Sun+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- Emuã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€å˜ä¸€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã¾ãŸã¯ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªãƒ‡ãƒ¼ã‚¿å…¥åŠ›ã‚’å—ã‘å…¥ã‚Œã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚Emuã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã€ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã¸ã®ã‚¿ã‚¹ã‚¯ã‚„ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®ã‚¿ã‚¹ã‚¯ãªã©ã€ã•ã¾ã–ã¾ãªã‚¿ã‚¹ã‚¯ã§å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã¾ã™ã€‚ã¾ãŸã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãªã©ã®æ‹¡å¼µæ©Ÿèƒ½ã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚</span>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<span class="issue_date">Issue Date: 2023-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/805" target="_blank" rel="noopener noreferrer" class="title-link">EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the  Backbone, Shraman Pramanick+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- ã‚¨ã‚´ã‚»ãƒ³ãƒˆãƒªãƒƒã‚¯ãƒ“ãƒ‡ã‚ªè¨€èªã®äº‹å‰å­¦ç¿’ã®ç¬¬2ä¸–ä»£ï¼ˆEgoVLPv2ï¼‰ã¯ã€ãƒ“ãƒ‡ã‚ªã¨è¨€èªã®ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã«ã‚¯ãƒ­ã‚¹ãƒ¢ãƒ¼ãƒ€ãƒ«ã®èåˆã‚’ç›´æ¥çµ„ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã‚‹ã€‚EgoVLPv2ã¯å¼·åŠ›ãªãƒ“ãƒ‡ã‚ªãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¾ã‚’å­¦ç¿’ã—ã€æŸ”è»Ÿã‹ã¤åŠ¹ç‡çš„ãªæ–¹æ³•ã§ã•ã¾ã–ã¾ãªãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¿ã‚¹ã‚¯ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã€‚ã•ã‚‰ã«ã€ææ¡ˆã•ã‚ŒãŸãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³æˆ¦ç•¥ã¯è»½é‡ã§è¨ˆç®—åŠ¹ç‡ãŒé«˜ã„ã€‚EgoVLPv2ã¯å¹…åºƒã„VLã‚¿ã‚¹ã‚¯ã§æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã¦ã„ã‚‹ã€‚è©³ç´°ã¯https://shramanpramanick.github.io/EgoVLPv2/ã‚’å‚ç…§ã€‚</span>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<a class="button" href="articles/Navigation.html" target="_blank" rel="noopener noreferrer">#Navigation</a>
<span class="issue_date">Issue Date: 2023-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/802" target="_blank" rel="noopener noreferrer" class="title-link">ViNT: A Foundation Model for Visual Navigation, Dhruv Shah+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€æ±ç”¨äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹Visual Navigation Transformerï¼ˆViNTï¼‰ã‚’ææ¡ˆã—ã€ãƒ“ã‚¸ãƒ§ãƒ³ãƒ™ãƒ¼ã‚¹ã®ãƒ­ãƒœãƒƒãƒˆãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã«æˆåŠŸã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚ViNTã¯ã€å¤§è¦æ¨¡ãªãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¨“ç·´ã•ã‚Œã€æŸ”è»ŸãªTransformerãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½¿ç”¨ã—ã¦ã•ã¾ã–ã¾ãªãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã«é©å¿œã—ã¾ã™ã€‚ViNTã¯ã€æ‹¡æ•£ãƒ™ãƒ¼ã‚¹ã®ã‚µãƒ–ã‚´ãƒ¼ãƒ«ææ¡ˆã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€æ–°ã—ã„ç’°å¢ƒã‚’æ¢ç´¢ã—ã€ã‚­ãƒ­ãƒ¡ãƒ¼ãƒˆãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ã®ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³å•é¡Œã‚’è§£æ±ºã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€ViNTã¯ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«è§¦ç™ºã•ã‚ŒãŸæŠ€è¡“ã‚’ä½¿ç”¨ã—ã¦ã€æ–°ã—ã„ã‚¿ã‚¹ã‚¯ä»•æ§˜ã«é©å¿œã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ViNTã¯ãƒ¢ãƒã‚¤ãƒ«ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã®ãŸã‚ã®åŠ¹æœçš„ãªåŸºç¤ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ç¢ºç«‹ã•ã‚Œã¦ã„ã¾ã™ã€‚è©³ç´°ã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒšãƒ¼ã‚¸ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚</span>
<span class="snippet"><span>Comment</span><p>äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è¦–è¦šãƒ™ãƒ¼ã‚¹ã®ãƒ­ãƒœãƒƒãƒˆãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã«æ´»ç”¨ã™ã‚‹Foundation Modelã€‚FlexibleãªTransformerãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«åŸºã¥ã„ã¦æ§‹ç¯‰ã•ã‚Œã¦ãŠã‚Šã€ã•ã¾ã–ã¾ãªãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã«å–ã‚Šçµ„ã‚€ã“ã¨ãŒå¯èƒ½<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/fcb59d61-9a89-4ac8-989c-ffb125e90cbd" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/QuestionAnswering.html" target="_blank" rel="noopener noreferrer">#QuestionAnswering</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<span class="issue_date">Issue Date: 2023-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/800" target="_blank" rel="noopener noreferrer" class="title-link">SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen  LLMs, Lijun Yu+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- ã“ã®ç ”ç©¶ã§ã¯ã€Semantic Pyramid AutoEncoderï¼ˆSPAEï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€å‡çµã•ã‚ŒãŸLLMsãŒéè¨€èªçš„ãªãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’å«ã‚€ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚SPAEã¯ã€LLMã®èªå½™ã‹ã‚‰æŠ½å‡ºã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã¨ç”Ÿã®ãƒ”ã‚¯ã‚»ãƒ«ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ›ã‚’è¡Œã„ã¾ã™ã€‚ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã¯ã€è¦–è¦šå†æ§‹æˆã«å¿…è¦ãªæ„å‘³ã¨è©³ç´°ã‚’æ‰ãˆã€LLMãŒç†è§£ã§ãã‚‹è¨€èªã«å¤‰æ›ã—ã¾ã™ã€‚å®Ÿé¨“çµæœã§ã¯ã€æˆ‘ã€…ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒç”»åƒç†è§£ã¨ç”Ÿæˆã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’25ï¼…ä»¥ä¸Šä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>ç”»åƒã‚’LLMã®tokenã‚¹ãƒšãƒ¼ã‚¹ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€LLMãŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°ãªã—ã«visual taskã‚’è§£ãã“ã¨ã‚’å¯èƒ½ã«ã—ãŸã€‚in context learningã«ã‚ˆã£ã¦ã€æ§˜ã€…ãªvisuataskã‚’è§£ãã“ã¨ãŒã§ãã‚‹ã€‚<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/1e0f962f-e661-44e6-bc59-73d9ae87d6dd" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/QuestionAnswering.html" target="_blank" rel="noopener noreferrer">#QuestionAnswering</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<span class="issue_date">Issue Date: 2023-06-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/775" target="_blank" rel="noopener noreferrer" class="title-link">Towards Language Models That Can See: Computer Vision Through the LENS  of Natural Language, William Berrios+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- ç§ãŸã¡ã¯ã€LENSã¨ã„ã†ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®å•é¡Œã«å–ã‚Šçµ„ã¿ã¾ã™ã€‚LENSã¯ã€ç‹¬ç«‹ã—ãŸãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å‡ºåŠ›ã«å¯¾ã—ã¦è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦æ¨è«–ã‚’è¡Œã„ã¾ã™ã€‚ç§ãŸã¡ã¯ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãŠã‚ˆã³ãƒ•ãƒ¥ãƒ¼ã‚·ãƒ§ãƒƒãƒˆã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆèªè­˜ãªã©ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®è¨­å®šã§LENSã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚LENSã¯å¸‚è²©ã®LLMã«é©ç”¨ã§ãã€éå¸¸ã«ç«¶äº‰åŠ›ã®ã‚ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã¾ã™ã€‚ã‚³ãƒ¼ãƒ‰ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§æä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å‚è€ƒ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1674878733264781312?s=46&t=KFT8cWTu8vV69iD6Qt0NGw"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/e96f9a8a-6ce2-4985-8b0a-8daf4a6e477c" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Personalization.html" target="_blank" rel="noopener noreferrer">#Personalization</a>
<span class="issue_date">Issue Date: 2023-06-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/751" target="_blank" rel="noopener noreferrer" class="title-link">Photoswap: Personalized Subject Swapping in Images, Jing Gu+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€Photoswapã¨ã„ã†æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã€æ—¢å­˜ã®ç”»åƒã«ãŠã„ã¦å€‹äººçš„ãªå¯¾è±¡ç‰©ã®äº¤æ›ã‚’å¯èƒ½ã«ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚Photoswapã¯ã€å‚ç…§ç”»åƒã‹ã‚‰å¯¾è±¡ç‰©ã®è¦–è¦šçš„ãªæ¦‚å¿µã‚’å­¦ç¿’ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒªãƒ¼ã§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”»åƒã«äº¤æ›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€PhotoswapãŒåŠ¹æœçš„ã§åˆ¶å¾¡å¯èƒ½ã§ã‚ã‚Šã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ‰‹æ³•ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹äººé–“ã®è©•ä¾¡ã‚’å¾—ã¦ã„ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚Photoswapã¯ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆã‹ã‚‰ãƒ—ãƒ­ã®ç·¨é›†ã¾ã§å¹…åºƒã„å¿œç”¨å¯èƒ½æ€§ã‚’æŒã£ã¦ã„ã¾ã™ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Personalization.html" target="_blank" rel="noopener noreferrer">#Personalization</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<span class="issue_date">Issue Date: 2023-06-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/741" target="_blank" rel="noopener noreferrer" class="title-link">ViCo: Detail-Preserving Visual Condition for Personalized Text-to-Image  Generation, Shaozhe Hao+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºã•ã‚ŒãŸç”»åƒç”Ÿæˆã«ãŠã„ã¦ã€é«˜é€Ÿã§è»½é‡ãªãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãƒ¡ã‚½ãƒƒãƒ‰ã§ã‚ã‚‹ViCoã‚’ææ¡ˆã€‚æ³¨ç›®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å°å…¥ã—ã€æ³¨ç›®ãƒ™ãƒ¼ã‚¹ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãƒã‚¹ã‚¯ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ä¸€èˆ¬çš„ãªéå­¦ç¿’ã®åŠ£åŒ–ã‚’è»½æ¸›ã€‚å…ƒã®æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¾®èª¿æ•´ã›ãšã€è»½é‡ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã ã‘ã§ã€æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã¾ãŸã¯ãã‚Œä»¥ä¸Šã®æ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/QuestionAnswering.html" target="_blank" rel="noopener noreferrer">#QuestionAnswering</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<span class="issue_date">Issue Date: 2023-06-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/732" target="_blank" rel="noopener noreferrer" class="title-link">AVIS: Autonomous Visual Information Seeking with Large Language Models, Ziniu Hu+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€è‡ªå¾‹çš„ãªæƒ…å ±åé›†ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«è³ªå•å¿œç­”ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚‹AVISã‚’ææ¡ˆã™ã‚‹ã€‚AVISã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’æ´»ç”¨ã—ã¦å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ã®åˆ©ç”¨æˆ¦ç•¥ã‚’å‹•çš„ã«æ±ºå®šã—ã€è³ªå•ã«å¯¾ã™ã‚‹å›ç­”ã«å¿…è¦ãªä¸å¯æ¬ ãªçŸ¥è­˜ã‚’ç²å¾—ã™ã‚‹ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¹ã‚¿ãƒ‡ã‚£ã‚’å®Ÿæ–½ã—ã¦åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ã€ãƒ—ãƒ©ãƒ³ãƒŠãƒ¼ã‚„æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’æ”¹å–„ã—ã€çŸ¥è­˜é›†ç´„å‹ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«è³ªå•å¿œç­”ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/9df9b0ce-1f95-4e48-a4c9-b4c6b87d0ac6" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Controllable.html" target="_blank" rel="noopener noreferrer">#Controllable</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<span class="issue_date">Issue Date: 2023-05-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/677" target="_blank" rel="noopener noreferrer" class="title-link">Sketching the Future ï¼ˆSTFï¼‰: Applying Conditional Control Techniques to  Text-to-Video Models, Rohan Dhesikan+, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å‹•ç”»ç”Ÿæˆã‚’ControlNetã¨çµ„ã¿åˆã‚ã›ã€ã‚¹ã‚±ãƒƒãƒã•ã‚ŒãŸãƒ•ãƒ¬ãƒ¼ãƒ ã‚’åŸºã«å‹•ç”»ã‚’ç”Ÿæˆã™ã‚‹æ–°æ‰‹æ³•ã‚’ææ¡ˆã€‚ãƒ•ãƒ¬ãƒ¼ãƒ è£œé–“ã‚’è¡Œã„ã€Text-to-Video Zeroã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ´»ç”¨ã—ã¦é«˜å“è³ªã§ä¸€è²«æ€§ã®ã‚ã‚‹å‹•ç”»ã‚’ç”Ÿæˆã€‚ãƒ‡ãƒ¢å‹•ç”»ã‚„ãƒªã‚½ãƒ¼ã‚¹ã‚’æä¾›ã—ã€ã•ã‚‰ãªã‚‹ç ”ç©¶ã‚’ä¿ƒé€²ã€‚</span>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/RepresentationLearning.html" target="_blank" rel="noopener noreferrer">#RepresentationLearning</a>
<a class="button" href="articles/ContrastiveLearning.html" target="_blank" rel="noopener noreferrer">#ContrastiveLearning</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/Semi-Supervised.html" target="_blank" rel="noopener noreferrer">#Semi-Supervised</a>
<span class="issue_date">Issue Date: 2023-04-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/602" target="_blank" rel="noopener noreferrer" class="title-link">SemPPL: Predicting pseudo-labels for better contrastive representations, Matko BoÅ¡njak+, N_A, ICLR'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã«ãŠã‘ã‚‹åŠæ•™å¸«ã‚ã‚Šå­¦ç¿’ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€Semantic Positives via Pseudo-Labels (SemPPL)ã¨ã„ã†æ–°ã—ã„æ‰‹æ³•ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚ã“ã®æ‰‹æ³•ã¯ã€ãƒ©ãƒ™ãƒ«ä»˜ãã¨ãƒ©ãƒ™ãƒ«ãªã—ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµ„ã¿åˆã‚ã›ã¦æƒ…å ±è±Šå¯Œãªè¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒã§ãã€ResNet-$50$ã‚’ä½¿ç”¨ã—ã¦ImageNetã®$1\%$ãŠã‚ˆã³$10\%$ã®ãƒ©ãƒ™ãƒ«ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å ´åˆã€ç«¶åˆã™ã‚‹åŠæ•™å¸«ã‚ã‚Šå­¦ç¿’æ‰‹æ³•ã‚’ä¸Šå›ã‚‹æœ€é«˜æ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚SemPPLã¯ã€å¼·åŠ›ãªé ‘å¥æ€§ã€åˆ†å¸ƒå¤–ãŠã‚ˆã³è»¢ç§»æ€§èƒ½ã‚’ç¤ºã™ã“ã¨ãŒã§ãã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å¾Œã»ã©èª¬æ˜ã‚’è¿½è¨˜ã™ã‚‹<br><img src="https://github.com/user-attachments/assets/4441dc6c-a7b2-4ec9-9748-b6558a96e1af" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/8a78a40e-f5c4-4742-9e5d-36cd1b8d0e60" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/04ded9aa-c875-4282-9e3b-7ce456a6cc44" alt="image" loading="lazy"></p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1975" target="_blank" rel="noopener noreferrer">A Simple Framework for Contrastive Learning of Visual Representations, Ting Chen+, ICML'20</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<span class="issue_date">Issue Date: 2023-04-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/563" target="_blank" rel="noopener noreferrer" class="title-link">Stable and low-precision training for large-scale vision-language models, Wortsman+, University of Washington, NeurIPS'23</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡ãªè¨€èª-è¦–è¦šãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’åŠ é€Ÿã—å®‰å®šã•ã›ã‚‹æ–°æ‰‹æ³•ã‚’ææ¡ˆã€‚SwitchBackã‚’ç”¨ã„ãŸint8é‡å­åŒ–ã§ã€CLIP ViT-Hugeã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã‚’13-25%å‘ä¸Šã•ã›ã€bfloat16ã¨åŒç­‰ã®æ€§èƒ½ã‚’ç¶­æŒã€‚float8ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚‚åŠ¹æœçš„ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã€åˆæœŸåŒ–æ–¹æ³•ãŒæˆåŠŸã«å¯„ä¸ã€‚æå¤±ã®ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’åˆ†æã—ã€AdamW-Adafactorãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚’æ¨å¥¨ã™ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®‰å®šæ€§ã‚’å‘ä¸Šã•ã›ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p><img src="https://user-images.githubusercontent.com/12249301/235149432-1c818dc6-174c-4666-a26c-2ab9683b438b.png" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/ImageSegmentation.html" target="_blank" rel="noopener noreferrer">#ImageSegmentation</a>
<a class="button" href="articles/TechnicalReport.html" target="_blank" rel="noopener noreferrer">#TechnicalReport</a>
<span class="issue_date">Issue Date: 2023-04-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/535" target="_blank" rel="noopener noreferrer" class="title-link">Track Anything: Segment Anything Meets Videos, yang+, SUSTech VIP Lab, arXiv'23</a>
<span class="snippet"><span>Comment</span><p>Metaã®SAMã‚’ã€videoã«é©ç”¨ã—ã€videowå†…ã®segmentationã‚’è¿½åŠ å­¦ç¿’ãªã—ã§ã‚„ã‚Šã¾ã—ãŸã€ã¨ã„ã†è©±ã ã¨æ€ã‚ã‚Œã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/SIGGRAPH.html" target="_blank" rel="noopener noreferrer">#SIGGRAPH</a>
<span class="issue_date">Issue Date: 2022-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/496" target="_blank" rel="noopener noreferrer" class="title-link">Sketch-Guided Text-to-Image Diffusion Models, Andrey+, Google Research, SIGGRAPH'23</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®ãƒ¢ãƒ‡ãƒ«ã¯é«˜å“è³ªãªç”»åƒåˆæˆã‚’å®Ÿç¾ã™ã‚‹ãŒã€ç©ºé–“çš„ç‰¹æ€§ã®åˆ¶å¾¡ãŒä¸è¶³ã—ã¦ã„ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€ã‚¹ã‚±ãƒƒãƒã‹ã‚‰ã®ç©ºé–“ãƒãƒƒãƒ—ã‚’ç”¨ã„ã¦äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’å°ãæ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã€‚å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’å¿…è¦ã¨ã›ãšã€æ½œåœ¨ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹äºˆæ¸¬å™¨ï¼ˆLGPï¼‰ã‚’è¨“ç·´ã—ã€ç”»åƒã‚’ç©ºé–“ãƒãƒƒãƒ—ã«ä¸€è‡´ã•ã›ã‚‹ã€‚ãƒ”ã‚¯ã‚»ãƒ«ã”ã¨ã®è¨“ç·´ã«ã‚ˆã‚ŠæŸ”è»Ÿæ€§ã‚’æŒã¡ã€ã‚¹ã‚±ãƒƒãƒã‹ã‚‰ç”»åƒã¸ã®ç¿»è¨³ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦åŠ¹æœçš„ãªç”ŸæˆãŒå¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>ã‚¹ã‚±ãƒƒãƒã¨promptã‚’å…¥åŠ›ã™ã‚‹ã“ã¨ã§ã€ã‚¹ã‚±ãƒƒãƒ biasedãªç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹æŠ€è¡“ã€‚ã™ã”ã„ã€‚<br><br><img src="https://user-images.githubusercontent.com/12249301/205189823-66052368-60a8-4f03-a4b6-37111bd1b361.png" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/4D%20(Video).html" target="_blank" rel="noopener noreferrer">#4D (Video)</a>
<span class="issue_date">Issue Date: 2025-10-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3294" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Video Diffusion Models, Jonathan Ho+, arXiv'22, 2022.04</a>
<span class="snippet"><span>GPT Summary</span>- é«˜å¿ å®Ÿåº¦ã§ä¸€è²«ã—ãŸå‹•ç”»ç”Ÿæˆã®ãŸã‚ã®æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã€‚ç”»åƒã¨å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚’å…±åŒã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€æœ€é©åŒ–ã‚’åŠ é€Ÿã€‚æ–°ã—ã„æ¡ä»¶ä»˜ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æŠ€è¡“ã«ã‚ˆã‚Šã€é•·ãé«˜è§£åƒåº¦ã®å‹•ç”»ç”Ÿæˆã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã€‚å¤§è¦æ¨¡ãªãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ä»˜ãå‹•ç”»ç”Ÿæˆã‚¿ã‚¹ã‚¯ã§ã®åˆæœŸçµæœã¨ã€æ—¢å­˜ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®æœ€å…ˆç«¯çµæœã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>Surveyã¯ã“ã¡ã‚‰:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3295" target="_blank" rel="noopener noreferrer">[Paper Note] Video Diffusion Models: A Survey, Andrew Melnik+, TMLR'24, 2024.05</a>
<br><br></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/VariationalAutoEncoder.html" target="_blank" rel="noopener noreferrer">#VariationalAutoEncoder</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<a class="button" href="articles/ImageSynthesis.html" target="_blank" rel="noopener noreferrer">#ImageSynthesis</a>
<a class="button" href="articles/U-Net.html" target="_blank" rel="noopener noreferrer">#U-Net</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3208" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] High-Resolution Image Synthesis with Latent Diffusion Models, Robin Rombach+, CVPR'22, 2021.12</a>
<span class="snippet"><span>GPT Summary</span>- æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ï¼ˆDMsï¼‰ã¯ã€é€æ¬¡çš„ãªãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’ç”¨ã„ã¦ç”»åƒç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã‚’åŠ¹ç‡åŒ–ã—ã€æœ€å…ˆç«¯ã®åˆæˆçµæœã‚’é”æˆã€‚å¾“æ¥ã®ãƒ”ã‚¯ã‚»ãƒ«ç©ºé–“ã§ã®è¨“ç·´ã«æ¯”ã¹ã€å¼·åŠ›ãªäº‹å‰è¨“ç·´ã•ã‚ŒãŸã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®æ½œåœ¨ç©ºé–“ã§ã®è¨“ç·´ã«ã‚ˆã‚Šã€è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’å‰Šæ¸›ã—ã¤ã¤è¦–è¦šçš„å¿ å®Ÿåº¦ã‚’å‘ä¸Šã€‚ã‚¯ãƒ­ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³å±¤ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€ãƒ†ã‚­ã‚¹ãƒˆã‚„ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã«åŸºã¥ãæŸ”è»Ÿãªç”ŸæˆãŒå¯èƒ½ã¨ãªã‚Šã€ç”»åƒã‚¤ãƒ³ãƒšã‚¤ãƒ³ãƒ†ã‚£ãƒ³ã‚°ã‚„ç„¡æ¡ä»¶ç”»åƒç”Ÿæˆãªã©ã§ç«¶äº‰åŠ›ã®ã‚ã‚‹æ€§èƒ½ã‚’ç™ºæ®ã€‚</span>
<span class="snippet"><span>Comment</span><p>ã“ã“ã‹ã‚‰textç­‰ã«ã‚ˆã‚‹æ¡ä»¶ä»˜ã‘ã‚’ã—ãŸä¸Šã§ã®ç”ŸæˆãŒå¯èƒ½ã«ãªã£ãŸï¼ˆã‚‰ã—ã„ï¼‰</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3204" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Classifier-Free Diffusion Guidance, Jonathan Ho+, arXiv'22, 2022.07</a>
<span class="snippet"><span>GPT Summary</span>- åˆ†é¡å™¨ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã¯æ¡ä»¶ä»˜ãæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã§ã€ãƒ¢ãƒ¼ãƒ‰ã‚«ãƒãƒ¬ãƒƒã‚¸ã¨ã‚µãƒ³ãƒ—ãƒ«å¿ å®Ÿåº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å›³ã‚‹ã€‚è‘—è€…ã¯ã€åˆ†é¡å™¨ãªã—ã§ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ãŒå¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã€ã“ã‚Œã‚’åˆ†é¡å™¨ãƒ•ãƒªãƒ¼ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã¨å‘¼ã¶ã€‚æ¡ä»¶ä»˜ããŠã‚ˆã³ç„¡æ¡ä»¶ã®æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’å…±åŒã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ã‚µãƒ³ãƒ—ãƒ«å“è³ªã¨å¤šæ§˜æ€§ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’é”æˆã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬:


<a href="https://qiita.com/UMAboogie/items/160c1159811743c49d99" target="_blank" rel="noopener noreferrer">https://qiita.com/UMAboogie/items/160c1159811743c49d99</a>


</p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3205" target="_blank" rel="noopener noreferrer">[Paper Note] Diffusion Models Beat GANs on Image Synthesis, Prafulla Dhariwal+, NeurIPS'21 Spotlight, 2021.05</a>
</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/Scaling%20Laws.html" target="_blank" rel="noopener noreferrer">#Scaling Laws</a>
<a class="button" href="articles/Deduplication.html" target="_blank" rel="noopener noreferrer">#Deduplication</a>
<span class="issue_date">Issue Date: 2025-09-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2688" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Beyond neural scaling laws: beating power law scaling via data pruning, Ben Sorscher+, NeurIPS'22</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã«å¯¾ã™ã‚‹èª¤å·®ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’ç ”ç©¶ã—ã€é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’ç”¨ã„ã‚‹ã“ã¨ã§èª¤å·®ã‚’æŒ‡æ•°ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«æ¸›å°‘ã•ã›ã‚‹å¯èƒ½æ€§ã‚’ç¤ºã™ã€‚CIFAR-10ã€SVHNã€ImageNetã§ã®å®Ÿé¨“ã«ã‚ˆã‚Šã€å†ªæ³•å‰‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’è¶…ãˆã‚‹æ”¹å–„ã‚’ç¢ºèªã€‚ImageNetã«ãŠã‘ã‚‹10ç¨®é¡ã®ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¡ãƒˆãƒªãƒƒã‚¯ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿæ–½ã—ã€å¾“æ¥ã®ãƒ¡ãƒˆãƒªãƒƒã‚¯ã«ä»£ã‚ã‚‹æ–°ã—ã„è‡ªå·±æ•™å¸«ã‚ã‚Šãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’é–‹ç™ºã€‚è‰¯å¥½ãªãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¡ãƒˆãƒªãƒƒã‚¯ãŒãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã®æ”¹å–„ã¨ãƒªã‚½ãƒ¼ã‚¹ã‚³ã‚¹ãƒˆå‰Šæ¸›ã«å¯„ä¸ã™ã‚‹å¯èƒ½æ€§ã‚’ç¤ºå”†ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview: 


<a href="https://openreview.net/forum?id=UmvSlP-PyV" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=UmvSlP-PyV</a>


</p>
<p>æ—¥æœ¬èªè§£èª¬ã‚¹ãƒ©ã‚¤ãƒ‰: 


<a href="https://speakerdeck.com/takase/snlp2023-beyond-neural-scaling-laws" target="_blank" rel="noopener noreferrer">https://speakerdeck.com/takase/snlp2023-beyond-neural-scaling-laws</a>


</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-08-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2597" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] A ConvNet for the 2020s, Zhuang Liu+, arXiv'22</a>
<span class="snippet"><span>GPT Summary</span>- ConvNetã¯Vision Transformersã®ç™»å ´ã«ã‚ˆã‚Šåœ°ä½ã‚’å¤±ã£ãŸãŒã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®åŠ¹æœã¯ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®å„ªä½æ€§ã«ä¾å­˜ã—ã¦ã„ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€ConvNetã®é™ç•Œã‚’ãƒ†ã‚¹ãƒˆã—ã€ConvNeXtã¨ã„ã†æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã€‚ConvNeXtã¯æ¨™æº–çš„ãªConvNetãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã¿ã§æ§‹æˆã•ã‚Œã€ç²¾åº¦ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã§ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¨ç«¶äº‰ã—ã€ImageNetã§87.8%ã®ç²¾åº¦ã‚’é”æˆã—ã€COCOæ¤œå‡ºãŠã‚ˆã³ADE20Kã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã§Swin Transformersã‚’ä¸Šå›ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>ConvNeXt</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/OCR.html" target="_blank" rel="noopener noreferrer">#OCR</a>
<a class="button" href="articles/ACMMM.html" target="_blank" rel="noopener noreferrer">#ACMMM</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-08-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2526" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] DiT: Self-supervised Pre-training for Document Image Transformer, Junlong Li+, ACMMM'22</a>
<span class="snippet"><span>GPT Summary</span>- è‡ªå·±ç›£è¦–å‹äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«DiTã‚’ææ¡ˆã—ã€ãƒ©ãƒ™ãƒ«ãªã—ãƒ†ã‚­ã‚¹ãƒˆç”»åƒã‚’ç”¨ã„ã¦æ–‡æ›¸AIã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹æ€§èƒ½ã‚’å‘ä¸Šã€‚æ–‡æ›¸ç”»åƒåˆ†é¡ã‚„ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆåˆ†æã€è¡¨æ¤œå‡ºã€OCRãªã©ã§æ–°ãŸãªæœ€å…ˆç«¯çµæœã‚’é”æˆã€‚ã‚³ãƒ¼ãƒ‰ã¨ãƒ¢ãƒ‡ãƒ«ã¯å…¬é–‹ä¸­ã€‚</span>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultitaskLearning.html" target="_blank" rel="noopener noreferrer">#MultitaskLearning</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<span class="issue_date">Issue Date: 2025-07-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2183" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Perceiver IO: A General Architecture for Structured Inputs &amp; Outputs, Andrew Jaegle+, ICLR'22</a>
<span class="snippet"><span>GPT Summary</span>- æ±ç”¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£Perceiver IOã‚’ææ¡ˆã—ã€ä»»æ„ã®ãƒ‡ãƒ¼ã‚¿è¨­å®šã«å¯¾å¿œã—ã€å…¥åŠ›ã¨å‡ºåŠ›ã®ã‚µã‚¤ã‚ºã«å¯¾ã—ã¦ç·šå½¢ã«ã‚¹ã‚±ãƒ¼ãƒ«å¯èƒ½ã€‚æŸ”è»Ÿãªã‚¯ã‚¨ãƒªãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’è¿½åŠ ã—ã€ã‚¿ã‚¹ã‚¯ç‰¹æœ‰ã®è¨­è¨ˆã‚’ä¸è¦ã«ã€‚è‡ªç„¶è¨€èªã€è¦–è¦šç†è§£ã€ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ã§å¼·åŠ›ãªçµæœã‚’ç¤ºã—ã€GLUEãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§BERTã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’é”æˆã€‚</span>
<span class="snippet"><span>Comment</span><p>å½“æ™‚ç›¸å½“è©±é¡Œã¨ãªã£ãŸã•ã¾ã–ã¾ãªãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’çµ±ä¸€ã•ã‚ŒãŸæ çµ„ã¿ã§æ‰±ãˆã‚‹Perceiver IOè«–æ–‡<br><img src="https://github.com/user-attachments/assets/d7893f14-d69c-4af8-8117-08c2a6095e8e" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/CLIP.html" target="_blank" rel="noopener noreferrer">#CLIP</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<span class="issue_date">Issue Date: 2025-05-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1928" target="_blank" rel="noopener noreferrer" class="title-link">LAION-5B: An open large-scale dataset for training next generation   image-text models, Christoph Schuhmann+, NeurIPS'22</a>
<span class="snippet"><span>GPT Summary</span>- LAION-5Bã¯ã€5.85å„„ã®CLIPãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆãƒšã‚¢ã‹ã‚‰æˆã‚‹å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã€è‹±èªã®ãƒšã‚¢ãŒ2.32Bå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€CLIPã‚„GLIDEãªã©ã®ãƒ¢ãƒ‡ãƒ«ã®å†ç¾ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«åˆ©ç”¨ã•ã‚Œã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ç ”ç©¶ã‚’æ°‘ä¸»åŒ–ã—ã¾ã™ã€‚ã¾ãŸã€ãƒ‡ãƒ¼ã‚¿æ¢ç´¢ã‚„ã‚µãƒ–ã‚»ãƒƒãƒˆç”Ÿæˆã®ãŸã‚ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚„ã€ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ¤œå‡ºã®ãŸã‚ã®ã‚¹ã‚³ã‚¢ã‚‚æä¾›ã•ã‚Œã¾ã™ã€‚</span>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/CLIP.html" target="_blank" rel="noopener noreferrer">#CLIP</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/OOD.html" target="_blank" rel="noopener noreferrer">#OOD</a>
<span class="issue_date">Issue Date: 2023-05-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/681" target="_blank" rel="noopener noreferrer" class="title-link">Fine-Tuning can Distort Pretrained Features and Underperform   Out-of-Distribution, Ananya Kumar+, N_A, ICLR'22</a>
<span class="snippet"><span>GPT Summary</span>- äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚¿ã‚¹ã‚¯ã«è»¢ç§»ã™ã‚‹éš›ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ç·šå½¢ãƒ—ãƒ­ãƒ¼ãƒ“ãƒ³ã‚°ã®2ã¤ã®æ–¹æ³•ãŒã‚ã‚‹ãŒã€æœ¬ç ”ç©¶ã§ã¯ã€åˆ†å¸ƒã®ã‚·ãƒ•ãƒˆãŒå¤§ãã„å ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒç·šå½¢ãƒ—ãƒ­ãƒ¼ãƒ“ãƒ³ã‚°ã‚ˆã‚Šã‚‚åˆ†å¸ƒå¤–ã§ç²¾åº¦ãŒä½ããªã‚‹ã“ã¨ã‚’ç™ºè¦‹ã—ãŸã€‚LP-FTã¨ã„ã†2æ®µéšæˆ¦ç•¥ã®ç·šå½¢ãƒ—ãƒ­ãƒ¼ãƒ“ãƒ³ã‚°å¾Œã®å…¨ä½“ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒã€ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ç·šå½¢ãƒ—ãƒ­ãƒ¼ãƒ“ãƒ³ã‚°ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚’finetuningã™ã‚‹æ–¹æ³•ã¯å¤§ããåˆ†ã‘ã¦<br>1. linear layerã‚’ãƒ˜ãƒƒãƒ‰ã¨ã—ã¦concatã—ãƒ˜ãƒƒãƒ‰ã®ã¿ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å­¦ç¿’<br>2. äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å­¦ç¿’<br><br>ã®2ç¨®é¡ãŒã‚ã‚‹ã€‚<br>å‰è€…ã¯in-distributionãƒ‡ãƒ¼ã‚¿ã«å¼·ã„ãŒã€out-of-distributionã«å¼±ã„ã€‚å¾Œè€…ã¯é€†ã¨ã„ã†äº’ã„ãŒäº’ã„ã‚’è£œå®Œã—åˆã†é–¢ä¿‚ã«ã‚ã£ãŸã€‚<br>ãã“ã§ã€ã¾ãš1ã‚’å®Ÿæ–½ã—ã€ãã®å¾Œ2ã‚’å®Ÿæ–½ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã€‚in-distribution, out-of-distributionã®ä¸¡æ–¹ã§é«˜ã„æ€§èƒ½ã‚’å‡ºã™ã“ã¨ã‚’ç¤ºã—ãŸï¼ˆå®Ÿé¨“ã§ã¯ç”»åƒå‡¦ç†ç³»ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã¯ImageNet+CLIPã§äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ViTã‚’ç”¨ã„ã¦ã„ã‚‹)ã€‚<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/059d9056-bd3c-45f2-abd9-00c9f2a3d630" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<a class="button" href="articles/PMLR.html" target="_blank" rel="noopener noreferrer">#PMLR</a>
<a class="button" href="articles/ScoreMatching.html" target="_blank" rel="noopener noreferrer">#ScoreMatching</a>
<a class="button" href="articles/U-Net.html" target="_blank" rel="noopener noreferrer">#U-Net</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3207" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Improved Denoising Diffusion Probabilistic Models, Alex Nichol+, PMLR'21, 2021.02</a>
<span class="snippet"><span>GPT Summary</span>- DDPMã¯é«˜å“è³ªãªã‚µãƒ³ãƒ—ãƒ«ç”ŸæˆãŒå¯èƒ½ãªç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ç°¡å˜ãªä¿®æ­£ã«ã‚ˆã‚Šç«¶äº‰åŠ›ã®ã‚ã‚‹å¯¾æ•°å°¤åº¦ã‚’é”æˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚é€†æ‹¡æ•£ãƒ—ãƒ­ã‚»ã‚¹ã®åˆ†æ•£ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å›æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã¤ã¤ã‚µãƒ³ãƒ—ãƒ«å“è³ªã‚’ç¶­æŒã€‚DDPMã¨GANã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ†å¸ƒã®ã‚«ãƒãƒ¼èƒ½åŠ›ã‚’æ¯”è¼ƒã—ã€ãƒ¢ãƒ‡ãƒ«ã®å®¹é‡ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨ˆç®—é‡ã«å¯¾ã—ã¦ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã—ãŸã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3206" target="_blank" rel="noopener noreferrer">[Paper Note] Denoising Diffusion Probabilistic Models, Jonathan Ho+, NeurIPS'20, 2020.06</a>
</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<a class="button" href="articles/ScoreMatching.html" target="_blank" rel="noopener noreferrer">#ScoreMatching</a>
<a class="button" href="articles/U-Net.html" target="_blank" rel="noopener noreferrer">#U-Net</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3205" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Diffusion Models Beat GANs on Image Synthesis, Prafulla Dhariwal+, NeurIPS'21 Spotlight, 2021.05</a>
<span class="snippet"><span>GPT Summary</span>- æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ãŒæœ€å…ˆç«¯ã®ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ç”»åƒã‚µãƒ³ãƒ—ãƒ«å“è³ªã‚’é”æˆã€‚ç„¡æ¡ä»¶ç”»åƒåˆæˆã§ã¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ”¹å–„ã€æ¡ä»¶ä»˜ãç”»åƒåˆæˆã§ã¯åˆ†é¡å™¨ã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚’ç”¨ã„ã¦å“è³ªå‘ä¸Šã€‚ImageNetã§ã®FIDã‚¹ã‚³ã‚¢ã¯ã€128Ã—128ã§2.97ã€256Ã—256ã§4.59ã€512Ã—512ã§7.72ã‚’é”æˆã—ã€BigGAN-deepã«åŒ¹æ•µã€‚åˆ†é¡å™¨ã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã¯ã‚¢ãƒƒãƒ—ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã•ã‚‰ã«æ”¹å–„ã•ã‚Œã€256Ã—256ã§3.94ã€512Ã—512ã§3.85ã‚’è¨˜éŒ²ã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ä¸­ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=AAWuCvzaVt" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=AAWuCvzaVt</a>


</p>
<p>æ—¥æœ¬èªè§£èª¬:


<a href="https://qiita.com/UMAboogie/items/160c1159811743c49d99" target="_blank" rel="noopener noreferrer">https://qiita.com/UMAboogie/items/160c1159811743c49d99</a>


</p>
<p>ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã¦ä½¿ã‚ã‚Œã¦ã„ã‚‹U-Netã¯ã“ã¡ã‚‰:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2931" target="_blank" rel="noopener noreferrer">[Paper Note] U-Net: Convolutional Networks for Biomedical Image Segmentation, Olaf Ronneberger+, MICCAI'15, 2015.05</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-08-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2545" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] An Image is Worth 16x16 Words: Transformers for Image Recognition at   Scale, Alexey Dosovitskiy+, ICLR'21</a>
<span class="snippet"><span>GPT Summary</span>- ç´”ç²‹ãªãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ç”»åƒãƒ‘ãƒƒãƒã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ç›´æ¥é©ç”¨ã™ã‚‹ã“ã¨ã§ã€CNNã¸ã®ä¾å­˜ãªã—ã«ç”»åƒåˆ†é¡ã‚¿ã‚¹ã‚¯ã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã§ãã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’ã—ã€è¤‡æ•°ã®ç”»åƒèªè­˜ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®CNNã¨æ¯”è¼ƒã—ã¦å„ªã‚ŒãŸçµæœã‚’é”æˆã—ã€è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’å¤§å¹…ã«å‰Šæ¸›ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=YicbFdNTTy" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=YicbFdNTTy</a>


</p>
<p>ViTã‚’ææ¡ˆã—ãŸç ”ç©¶</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-07-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2259" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Swin Transformer V2: Scaling Up Capacity and Resolution, Ze Liu+, arXiv'21</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€å¤§è¦æ¨¡ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨å¿œç”¨ã«ãŠã‘ã‚‹èª²é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã®3ã¤ã®æŠ€è¡“ã‚’ææ¡ˆã€‚å…·ä½“çš„ã«ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®‰å®šæ€§å‘ä¸Šã®ãŸã‚ã®æ®‹å·®å¾Œæ­£è¦åŒ–æ³•ã€ä½è§£åƒåº¦ã‹ã‚‰é«˜è§£åƒåº¦ã¸ã®è»¢é€ã‚’å¯èƒ½ã«ã™ã‚‹ä½ç½®ãƒã‚¤ã‚¢ã‚¹æ³•ã€ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã®å¿…è¦æ€§ã‚’æ¸›å°‘ã•ã›ã‚‹è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’æ³•ã‚’ç”¨ã„ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€30å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®Swin Transformer V2ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€è¤‡æ•°ã®ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã§æ–°è¨˜éŒ²ã‚’æ¨¹ç«‹ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚‚å‘ä¸Šã—ã€ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã¨æ™‚é–“ã‚’å¤§å¹…ã«å‰Šæ¸›ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-07-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2258" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, Ze Liu+, ICCV'21</a>
<span class="snippet"><span>GPT Summary</span>- Swin Transformerã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®æ–°ã—ã„ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹éšå±¤çš„ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ææ¡ˆã€‚ã‚·ãƒ•ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ–¹å¼ã«ã‚ˆã‚Šã€åŠ¹ç‡çš„ãªè‡ªå·±æ³¨æ„è¨ˆç®—ã‚’å®Ÿç¾ã—ã€ã•ã¾ã–ã¾ãªã‚¹ã‚±ãƒ¼ãƒ«ã§ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãŒå¯èƒ½ã€‚ç”»åƒåˆ†é¡ã‚„ç‰©ä½“æ¤œå‡ºã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãªã©ã§å¾“æ¥ã®æœ€å…ˆç«¯ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®ãƒ“ã‚¸ãƒ§ãƒ³ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã¦ã®å¯èƒ½æ€§ã‚’ç¤ºå”†ã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬:


<a href="https://qiita.com/m_sugimura/items/139b182ee7c19c83e70a" target="_blank" rel="noopener noreferrer">https://qiita.com/m_sugimura/items/139b182ee7c19c83e70a</a>


</p>
<p>ç”»åƒå‡¦ç†ã«ãŠã„ã¦ã€ç‰©ä½“ã®ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã‚„ã€è§£åƒåº¦ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€PatchMergeã¨å‘¼ã°ã‚Œã‚‹ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã®ã‚ˆã†ãªå‡¦ç†ã¨ã€å›ºå®šã‚µã‚¤ã‚ºã®ãƒ­ãƒ¼ã‚«ãƒ«ãªwindowã«åˆ†å‰²ã—ã¦Self-Attentionã‚’å®Ÿæ–½ã—ã€layerã”ã¨ã«é€šå¸¸ã®windowã¨ã‚·ãƒ•ãƒˆã•ã‚ŒãŸwindowã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§ã€windowé–“ã‚’è·¨ã„ã é–¢ä¿‚æ€§ã‚‚è€ƒæ…®ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹æ©Ÿæ§‹ã‚’å°å…¥ã—ãŸãƒ¢ãƒ‡ãƒ«ã€‚<br><img src="https://github.com/user-attachments/assets/a2d5f78c-27ec-4f18-bd7d-5475085cfa7b" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/92fb10e1-614e-44ef-9e65-3920cd863d46" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/2b8a543a-069e-468a-bc3c-1f288cdcf577" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<span class="issue_date">Issue Date: 2023-08-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1009" target="_blank" rel="noopener noreferrer" class="title-link">ViLT: Vision-and-Language Transformer Without Convolution or Region   Supervision, Wonjae Kim+, N_A, ICML'21</a>
<span class="snippet"><span>GPT Summary</span>- VLPï¼ˆVision-and-Language Pre-trainingï¼‰ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ãƒ“ã‚¸ãƒ§ãƒ³ã¨è¨€èªã®ã‚¿ã‚¹ã‚¯ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã¦ã„ã‚‹ãŒã€ç¾åœ¨ã®æ–¹æ³•ã¯åŠ¹ç‡æ€§ã¨è¡¨ç¾åŠ›ã®é¢ã§å•é¡ŒãŒã‚ã‚‹ã€‚ãã“ã§ã€æœ¬ç ”ç©¶ã§ã¯ç•³ã¿è¾¼ã¿ãƒ•ãƒªãƒ¼ã®ãƒ“ã‚¸ãƒ§ãƒ³ã¨è¨€èªã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒï¼ˆViLTï¼‰ãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã™ã‚‹ã€‚ViLTã¯é«˜é€Ÿã§ã‚ã‚ŠãªãŒã‚‰ç«¶äº‰åŠ›ã®ã‚ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã€ã‚³ãƒ¼ãƒ‰ã¨äº‹å‰å­¦ç¿’æ¸ˆã¿ã®é‡ã¿ã¯GitHubã§åˆ©ç”¨å¯èƒ½ã§ã‚ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬:


<a href="https://tech.fusic.co.jp/posts/2021-12-29-vilt/" target="_blank" rel="noopener noreferrer">https://tech.fusic.co.jp/posts/2021-12-29-vilt/</a>


</p></span><br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/ContrastiveLearning.html" target="_blank" rel="noopener noreferrer">#ContrastiveLearning</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<span class="issue_date">Issue Date: 2023-04-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/550" target="_blank" rel="noopener noreferrer" class="title-link">Learning Transferable Visual Models From Natural Language Supervision, Radford+, OpenAI, ICML'21</a>
<span class="snippet"><span>Comment</span><p>CLIPè«–æ–‡ã€‚å¤§é‡ã®ç”»åƒã¨ç”»åƒã«å¯¾å¿œã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®ãƒšã‚¢ã‹ã‚‰ã€å¯¾è±¡å­¦ç¿’ã‚’è¡Œã„ã€ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆé–“ã®similarityã‚’ã¯ã‹ã‚Œã‚‹ã‚ˆã†ã«ã—ãŸãƒ¢ãƒ‡ãƒ«<br><br><img src="https://user-images.githubusercontent.com/12249301/234729329-dfa5dc1e-c5fc-452c-8ead-76df7d1aeda4.png" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/NaturalLanguageGeneration.html" target="_blank" rel="noopener noreferrer">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Game.html" target="_blank" rel="noopener noreferrer">#Game</a>
<span class="issue_date">Issue Date: 2022-09-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/487" target="_blank" rel="noopener noreferrer" class="title-link">Generating Racing Game Commentary from Vision, Language, and Structured Data, Tatsuya+, INLG'21</a>
<span class="snippet"><span>Comment</span><p>ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: 


<a href="https://kirt.airc.aist.go.jp/corpus/ja/RacingCommentary" target="_blank" rel="noopener noreferrer">https://kirt.airc.aist.go.jp/corpus/ja/RacingCommentary</a>


</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<span class="issue_date">Issue Date: 2021-11-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/431" target="_blank" rel="noopener noreferrer" class="title-link">ResNet strikes back: An improved training procedure in timm, Wightman+, NeurIPS'21 Workshop ImageNet PPF</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€Residual Networksï¼ˆResNet-50ï¼‰ã®æ€§èƒ½ã‚’æ–°ãŸãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã‚’ç”¨ã„ã¦å†è©•ä¾¡ã—ã€ç«¶äº‰åŠ›ã®ã‚ã‚‹è¨­å®šã§80.4%ã®ãƒˆãƒƒãƒ—1ç²¾åº¦ã‚’é”æˆã—ãŸã“ã¨ã‚’å ±å‘Šã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å°†æ¥ã®ç ”ç©¶ã®ãŸã‚ã®ã‚ˆã‚Šè‰¯ã„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>2015å¹´ä»¥å¾Œã€æ§˜ã€…ãªæœ€é©åŒ–ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€æ­£å‰‡åŒ–æ‰‹æ³•ã€ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãªã©ãŒææ¡ˆã•ã‚Œã‚‹ä¸­ã§ã€æœ€æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ãƒ¢ãƒ‡ãƒ«ã«ã¯ãã‚Œã‚‰ãŒé©ç”¨ã•ã‚Œã‚‹ä¸€æ–¹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ãªã‚‹ResNetã§ã¯ãã‚Œã‚‰ãŒé©ç”¨ã•ã‚Œãšã€è«–æ–‡ã®å€¤ã®ã¿ãŒå‚ç…§ã•ã‚Œã‚‹ç¾çŠ¶ã¯ãƒ•ã‚§ã‚¢ã§ã¯ãªã„ã®ã§ã€ResNetã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã‚ˆã†ãªè¨“ç·´æ‰‹æ³•ã‚’è¿½æ±‚ã—ãŸç ”ç©¶ã€‚<br><br><br><br>ResNetã«ãŠã‘ã‚‹æœ‰åŠ¹ãªè¨“ç·´æ‰‹æ³•ã¨ã—ã¦ä¸‹è¨˜ã‚’æ¨¡ç´¢ï¼š<br><br><br><br>æå¤±é–¢æ•°ã¨ã—ã¦ã€MixUpï¼ˆè¨“ç·´ç”»åƒã‚’é‡ã­åˆã‚ã›ã€çµ„ã¿åˆã‚ã›ãŸç”»åƒã®ãƒ©ãƒ™ãƒ«ã‚’ãƒŸãƒƒã‚¯ã‚¹ã—ã¦æ–°ã—ã„å­¦ç¿’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œã‚‹ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µæ‰‹æ³•ï¼‰ã¨ã€CutMixï¼ˆç”»åƒã‚’åˆ‡ã‚Šè²¼ã‚Šã—ã¦ã€åˆ‡ã‚Šè²¼ã‚Šéƒ¨åˆ†ã®é¢ç©ã«å¿œã˜ã¦ãƒ©ãƒ™ãƒ«ã®ã‚¹ã‚³ã‚¢ã‚’èª¿æ•´ã™ã‚‹ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µæ‰‹æ³•ï¼‰ã‚’é©ç”¨ã—ã€CutMixã«ã‚ˆã£ã¦å¤§å¹…ã«æ€§èƒ½ãŒæ”¹å–„ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚ã“ã®ã¨ãã€ãƒ©ãƒ™ãƒ«ã®ç¢ºç‡ã®å’ŒãŒ1ã¨ãªã‚‹å‰æã®å…ƒã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã§å­¦ç¿’ã™ã‚‹ã®ã§ã¯ãªãã€å…ƒç”»åƒã«å«ã¾ã‚Œã‚‹ç‰©ä½“ãŒä¸¡æ–¹å­˜åœ¨ã™ã‚‹ã¨ã„ã†å…¨ä½“ã®å…ƒBinaryCrossEntropyã‚’é©ç”¨ã—ãƒãƒ«ãƒãƒ©ãƒ™ãƒ«å•é¡Œã¨ã—ã¦å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€æ€§èƒ½ãŒå‘ä¸Šã€‚<br><br><br><br>ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µæ‰‹æ³•ã¨ã—ã¦ã€MixUp, CutMixã ã‘ã§ãªãã€é€šå¸¸ã®ãƒªã‚µã‚¤ã‚ºãƒ»åˆ‡ã‚ŠæŠœãã¨ã€æ°´å¹³æ–¹å‘ã®åè»¢ã‚’é©ç”¨ã—ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã™ã‚‹ã€‚åŠ ãˆã¦RandAugmentï¼ˆ14ç¨®é¡ã®ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µæ“ä½œã‹ã‚‰ã€Nå€‹ã‚µãƒ³ãƒ—ãƒ«ã—ã€å¼·ã•Mã§é †ç•ªã«é©ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µæ‰‹æ³•ã€‚N,Mã¯ãã‚Œãã‚Œ0ã€œ10ã®æ•´æ•°ãªã®ã§ã€10ã®äºŒä¹—ã‚ªãƒ¼ãƒ€ãƒ¼ã§ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã™ã‚Œã°ã€æœ€é©ãªN,Mã‚’å¾—ã‚‹ã€‚ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã™ã‚‹ã ã‘ã§ãŠæ‰‹è»½ã ãŒéå¸¸ã«å¼·åŠ›ï¼‰ã‚’é©ç”¨ã—ãŸã€‚<br><br><br><br>æ­£å‰‡åŒ–ã¨ã—ã¦ã€Weight Decayï¼ˆå­¦ç¿’éç¨‹ã§é‡ã¿ãŒå¤§ãããªã‚Šã™ããªã„ã‚ˆã†ã«ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’èª²ã—ã€éå­¦ç¿’ã‚’é˜²æ­¢ã™ã‚‹æ‰‹æ³•ã€‚L2æ­£å‰‡åŒ–ãªã©ã€‚ï¼‰ã¨ã€label smoothingï¼ˆæ­£è§£ãƒ©ãƒ™ãƒ«ãŒ1ã€ãã®ä»–ã¯0ã¨ãƒ©ãƒ™ãƒ«ä»˜ã‘ã™ã‚‹ã®ã§ã¯ãªãã€ãƒ©ãƒ™ãƒ«ã«ä¸€å®šã®ãƒã‚¤ã‚ºã‚’å…¥ã‚Œã€æ­£è§£ãƒ©ãƒ™ãƒ«ä»¥å¤–ã«ã‚‚é‡ã¿ãŒå…¥ã£ã¦ã„ã‚‹çŠ¶æ…‹ã«ã—ã€ãƒ©ãƒ™ãƒ«ä»˜ã‘ã®ãƒã‚¤ã‚ºã«ãƒ­ãƒã‚¹ãƒˆãªãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã€‚ãƒã‚¤ã‚ºã®å¼·ã•ã¯å®šæ•°ã§èª¿æ•´ã™ã‚‹ï¼‰ã€Repeated Augmentationï¼ˆåŒã˜ãƒãƒƒãƒå†…ã®ç”»åƒã«ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’é©ç”¨ã—ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤§ããã™ã‚‹ï¼‰ã€Stochastic Depthï¼ˆãƒ©ãƒ³ãƒ€ãƒ ã§ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å‰Šé™¤ã—ã€ãã®é–“ã‚’æ’ç­‰é–¢æ•°ã§ç¹‹ãè¨“ç·´ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–èƒ½åŠ›ã¨è¨“ç·´æ™‚é–“ã‚’å‘ä¸Šã™ã‚‹ï¼‰ã‚’é©ç”¨ã€‚<br><br></p>
<p>Optimizerã¨ã—ã¦ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ResNetã§ã¯ã€SGDã‚„AdamWã§è¨“ç·´ã•ã‚Œã‚‹ã“ã¨ãŒå¤šã„ãŒã€Repeated Augmentationã¨ãƒã‚¤ãƒŠãƒªã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’çµ„ã¿åˆã‚ã›ãŸå ´åˆã¯LAMBãŒæœ‰åŠ¹ã§ã‚ã£ãŸã€‚ã¾ãŸã€å¾“æ¥ã‚ˆã‚Šã‚‚é•·ã„è¨“ç·´æ™‚é–“ï¼ˆ600epochã€æ§˜ã€…ãªæ­£å‰‡åŒ–æ‰‹æ³•ã‚’ä½¿ã£ã¦ã„ã‚‹ã®ã§éå­¦ç¿’ã—ã¥ã‚‰ã„ãŸã‚ï¼‰ã§å­¦ç¿’ã—ã€æœ€åˆã«ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚’ä½¿ã„å¾ã€…ã«å­¦ç¿’ç‡ã‚’ä¸Šã’ï¼ˆfinetuningã®å†èªè­˜ã“ã‚Œã¾ã§ã®weightã‚’ãªã‚‹ã¹ãå£Šã—ãŸããªã„ã‹ã‚‰å°ã•ã„å­¦ç¿’ç‡ã‹ã‚‰å§‹ã‚ã‚‹ã€ã‚ã‚‹ã„ã¯Momentumã‚„Adamã¨ã„ã£ãŸç§»å‹•å¹³å‡ã‚’ä½¿ã†æ‰‹æ³•ã§ã¯ç§»å‹•å¹³å‡ã‚’å–ã‚‹ãŸã‚ã®å£°å€ã®è“„ç©ãŒè¶³ã‚Šãªã„å ´åˆå­¦ç¿’ã®ä¿¡é ¼åº¦ãŒä½ã„ã®ã§æœ€åˆã®æ–¹ã¯å­¦ç¿’ç‡å°ã•ãã™ã‚‹ã¿ãŸã„ãªã€ã‚¤ãƒ¡ãƒ¼ã‚¸ï¼‰ãã®å¾Œã‚³ã‚µã‚¤ãƒ³é–¢æ•°ã«å¾“ã„å­¦ç¿’ç‡ã‚’æ¸›ã‚‰ã—ã¦ã„ãã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°æ³•ã§å­¦ç¿’ã€‚<br><br><br><br>è«–æ–‡ä¸­ã§ã¯ä¸Šè¨˜æ‰‹æ³•ã®3ç¨®é¡ã®çµ„ã¿åˆã‚ã›ï¼ˆA1,A2,A3ï¼‰ã‚’ææ¡ˆã—å®Ÿé¨“ã—ã¦ã„ã‚‹ã€‚<br><br>ResNet-50ã«å¯¾ã—ã¦A1,2,3ã‚’é©ç”¨ã—ãŸçµæœã€A1ã‚’é©ç”¨ã—ãŸå ´åˆã«ImageNetã®ãƒˆãƒƒãƒ—1ç²¾åº¦ãŒ80.4%ã§ã‚ã‚Šã€ã“ã‚Œã¯ResNet-50ã‚’ä½¿ã£ãŸå ´åˆã®SoTAã€‚å…ƒã®ResNetã®ç²¾åº¦ãŒ76%ç¨‹åº¦ã ã£ãŸã®ã§å¤§å¹…ã«å‘ä¸Šã—ãŸã€‚<br><br>åŒã˜å®Ÿé¨“è¨­å®šã‚’ä½¿ã£ãŸå ´åˆã®ä»–ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆViTã‚„EfficientNetãªã©ï¼‰ã¨æ¯”ã¹ã¦ã‚‚éœè‰²ã®ãªã„æ€§èƒ½ã‚’é”æˆã€‚<br><br><br><br><img src="https://user-images.githubusercontent.com/12249301/140302112-05392bbb-7014-4518-a001-55e91933a065.png" alt="image" loading="lazy"><br><br><br><br>ã¾ãŸã€æœ¬è«–æ–‡ã§ææ¡ˆã•ã‚Œã¦ã„ã‚‹A2ã¨ã€DeiTã¨å‘¼ã°ã‚Œã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ææ¡ˆã•ã‚Œã¦ã„ã‚‹è¨“ç·´æ‰‹æ³•ï¼ˆT2ï¼‰ã‚’ãã‚Œãã‚Œã®ãƒ¢ãƒ‡ãƒ«ã«é©ç”¨ã—ãŸçµæœã€ResNetã§ã¯A2ã€DeiTã§ã¯T2ã®æ€§èƒ½ãŒè‰¯ã‹ã£ãŸã€‚ã¤ã¾ã‚Šã€ã€Œã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨è¨“ç·´æ–¹æ³•ã¯åŒæ™‚ã«æœ€é©åŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€ã¨ã„ã†ã“ã¨ã€‚ã“ã‚ŒãŒã“ã®è«–æ–‡ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è‚ã¨ã®ã“ã¨ã€‚<br><br><br><br>ï¼ˆã‚¹ãƒ†ãƒ¼ãƒˆã‚ªãƒ–AIã‚¬ã‚¤ãƒ‰ã®å†…å®¹ã‚’ä¸€éƒ¨è£œè¶³ã—ã¦è¨˜è¿°ã—ã¾ã—ãŸã€‚ã„ã¤ã‚‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚ï¼‰<br><br><br><br><img src="https://user-images.githubusercontent.com/12249301/140302160-c31717ae-a225-47a4-ae33-f1cd081c419b.png" alt="image" loading="lazy"></p>
<p>ç”»åƒç³»ã§ã©ã†ã„ã£ãŸè¨“ç·´æ‰‹æ³•ãŒåˆ©ç”¨ã•ã‚Œã‚‹ã‹è‰²ã€…æ›¸ã‹ã‚Œã¦ã„ãŸã®ã§å‹‰å¼·ã«ãªã£ãŸã€‚ç‰¹ã«ç”»åƒç³»ã®ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µæ‰‹æ³•ãªã‚“ã‹ã¯æ™®æ®µè§¦ã‚‰ãªã„ã®ã§å‹‰å¼·ã«ãªã‚‹ã€‚</p>
<p>OpenReview:


<a href="https://openreview.net/forum?id=NG6MJnVl6M5" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=NG6MJnVl6M5</a>


</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<a class="button" href="articles/ScoreMatching.html" target="_blank" rel="noopener noreferrer">#ScoreMatching</a>
<a class="button" href="articles/ImageSynthesis.html" target="_blank" rel="noopener noreferrer">#ImageSynthesis</a>
<a class="button" href="articles/U-Net.html" target="_blank" rel="noopener noreferrer">#U-Net</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3206" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Denoising Diffusion Probabilistic Models, Jonathan Ho+, NeurIPS'20, 2020.06</a>
<span class="snippet"><span>GPT Summary</span>- æ‹¡æ•£ç¢ºç‡ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸé«˜å“è³ªãªç”»åƒåˆæˆã‚’ææ¡ˆã€‚æ–°ã—ã„é‡ã¿ä»˜ãå¤‰åˆ†å¢ƒç•Œã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚Šã€å„ªã‚ŒãŸçµæœã‚’å¾—ã‚‹ã€‚ç„¡æ¡ä»¶CIFAR10ã§9.46ã®Inceptionã‚¹ã‚³ã‚¢ã€256x256ã®LSUNã§ProgressiveGANã«åŒ¹æ•µã™ã‚‹å“è³ªã‚’é”æˆã€‚å®Ÿè£…ã¯GitHubã§å…¬é–‹ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DataAugmentation.html" target="_blank" rel="noopener noreferrer">#DataAugmentation</a>
<a class="button" href="articles/ContrastiveLearning.html" target="_blank" rel="noopener noreferrer">#ContrastiveLearning</a>
<a class="button" href="articles/Self-SupervisedLearning.html" target="_blank" rel="noopener noreferrer">#Self-SupervisedLearning</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-05-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1975" target="_blank" rel="noopener noreferrer" class="title-link">A Simple Framework for Contrastive Learning of Visual Representations, Ting Chen+, ICML'20</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€è¦–è¦šè¡¨ç¾ã®å¯¾æ¯”å­¦ç¿’ã®ãŸã‚ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯SimCLRã‚’ææ¡ˆã—ã€ç‰¹åˆ¥ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚„ãƒ¡ãƒ¢ãƒªãƒãƒ³ã‚¯ãªã—ã§å¯¾æ¯”è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã‚’ç°¡ç´ åŒ–ã—ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®é‡è¦æ€§ã€å­¦ç¿’å¯èƒ½ãªéç·šå½¢å¤‰æ›ã®å°å…¥ã«ã‚ˆã‚‹è¡¨ç¾ã®è³ªå‘ä¸Šã€å¯¾æ¯”å­¦ç¿’ãŒå¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºã¨å¤šãã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰åˆ©ç›Šã‚’å¾—ã‚‹ã“ã¨ã‚’ç¤ºã—ã€ImageNetã§å¾“æ¥ã®æ‰‹æ³•ã‚’ä¸Šå›ã‚‹çµæœã‚’é”æˆã—ã¾ã—ãŸã€‚SimCLRã«ã‚ˆã‚‹è‡ªå·±æ•™å¸«ã‚ã‚Šè¡¨ç¾ã‚’ç”¨ã„ãŸç·šå½¢åˆ†é¡å™¨ã¯76.5%ã®ãƒˆãƒƒãƒ—1ç²¾åº¦ã‚’é”æˆã—ã€æ•™å¸«ã‚ã‚ŠResNet-50ã«åŒ¹æ•µã—ã¾ã™ã€‚ãƒ©ãƒ™ãƒ«ã®1%ã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸå ´åˆã€85.8%ã®ãƒˆãƒƒãƒ—5ç²¾åº¦ã‚’é”æˆã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬:


<a href="https://techblog.cccmkhd.co.jp/entry/2022/08/30/163625" target="_blank" rel="noopener noreferrer">https://techblog.cccmkhd.co.jp/entry/2022/08/30/163625</a>


</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/KnowledgeEditing.html" target="_blank" rel="noopener noreferrer">#KnowledgeEditing</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-05-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1934" target="_blank" rel="noopener noreferrer" class="title-link">Editable Neural Networks, Anton Sinitsin+, ICLR'20</a>
<span class="snippet"><span>GPT Summary</span>- æ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®èª¤ã‚Šã‚’è¿…é€Ÿã«ä¿®æ­£ã™ã‚‹ãŸã‚ã«ã€Editable Trainingã¨ã„ã†ãƒ¢ãƒ‡ãƒ«éä¾å­˜ã®è¨“ç·´æ‰‹æ³•ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç‰¹å®šã®ã‚µãƒ³ãƒ—ãƒ«ã®èª¤ã‚Šã‚’åŠ¹ç‡çš„ã«ä¿®æ­£ã—ã€ä»–ã®ã‚µãƒ³ãƒ—ãƒ«ã¸ã®å½±éŸ¿ã‚’é¿ã‘ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚å¤§è¦æ¨¡ãªç”»åƒåˆ†é¡ã¨æ©Ÿæ¢°ç¿»è¨³ã‚¿ã‚¹ã‚¯ã§ãã®æœ‰åŠ¹æ€§ã‚’å®Ÿè¨¼ã€‚</span>
<span class="snippet"><span>Comment</span><p>ï¼ˆãŠãã‚‰ãï¼‰Knowledge Editingã‚’åˆã‚ã¦ææ¡ˆã—ãŸç ”ç©¶</p>
<p>OpenReview:


<a href="https://openreview.net/forum?id=HJedXaEtvS" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=HJedXaEtvS</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-08-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2500" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Supervised Multimodal Bitransformers for Classifying Images and Text, Douwe Kiela+, arXiv'19</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒæƒ…å ±ã‚’èåˆã™ã‚‹ç›£è¦–å‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ“ãƒƒãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã—ã€ã•ã¾ã–ã¾ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åˆ†é¡ã‚¿ã‚¹ã‚¯ã§æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’é”æˆã€‚ç‰¹ã«ã€é›£æ˜“åº¦ã®é«˜ã„ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã‚‚å¼·åŠ›ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚‹çµæœã‚’å¾—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>ãƒ†ã‚­ã‚¹ãƒˆ+imageã‚’ç”¨ã„ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªtransformer</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-05-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1957" target="_blank" rel="noopener noreferrer" class="title-link">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, Mingxing Tan+, ICML'19</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€ConvNetsã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’æ·±ã•ã€å¹…ã€è§£åƒåº¦ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®ã—ã¦ä½“ç³»çš„ã«ç ”ç©¶ã—ã€æ–°ã—ã„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ‰‹æ³•ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€MobileNetsã‚„ResNetã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¨¼ã—ã€EfficientNetsã¨ã„ã†æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’è¨­è¨ˆã€‚ç‰¹ã«EfficientNet-B7ã¯ã€ImageNetã§84.3%ã®ãƒˆãƒƒãƒ—1ç²¾åº¦ã‚’é”æˆã—ã€å¾“æ¥ã®ConvNetsã‚ˆã‚Šã‚‚å°å‹ã‹ã¤é«˜é€Ÿã§ã‚ã‚‹ã€‚CIFAR-100ã‚„Flowersãªã©ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚‚æœ€å…ˆç«¯ã®ç²¾åº¦ã‚’è¨˜éŒ²ã€‚ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒè«–æ–‡ã‚’ãƒ¡ãƒ¢ã£ã¦ãªã‹ã£ãŸã®ã§è¿½åŠ ã€‚<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/346" target="_blank" rel="noopener noreferrer">EfficientNetè§£èª¬, omiita (ã‚ªãƒŸãƒ¼ã‚¿), 2019</a>
<br><br>ã‚‚å‚ç…§ã®ã“ã¨ã€‚</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<span class="issue_date">Issue Date: 2021-06-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/388" target="_blank" rel="noopener noreferrer" class="title-link">On Empirical Comparisons of Optimizers for Deep Learning, Dami Choi+, N_A, arXiv'19</a>
<span class="snippet"><span>GPT Summary</span>- æ·±å±¤å­¦ç¿’ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®æ¯”è¼ƒã¯é‡è¦ã§ã‚ã‚Šã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ¢ç´¢ç©ºé–“ãŒæ€§èƒ½ã«å½±éŸ¿ã™ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚Œã¦ã„ã‚‹ã€‚ç‰¹ã«ã€é©å¿œçš„å‹¾é…æ³•ã¯å¸¸ã«ä»–ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚ˆã‚Šã‚‚æ€§èƒ½ãŒä½ä¸‹ã—ãªã„ã“ã¨ãŒå®Ÿé¨“ã§ç¤ºã•ã‚Œã¦ãŠã‚Šã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«é–¢ã™ã‚‹å®Ÿç”¨çš„ãªãƒ’ãƒ³ãƒˆã‚‚æä¾›ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>SGD, Momentum,RMSProp, Adam,NAdamç­‰ã®ä¸­ã‹ã‚‰ã€ã©ã®æœ€é©åŒ–æ‰‹æ³•(Optimizer)ãŒå„ªã‚Œã¦ã„ã‚‹ã‹ã‚’ç”»åƒåˆ†é¡ã¨è¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦æ¯”è¼ƒã—ãŸç ”ç©¶ï¼ˆä¸‹è¨˜æ—¥æœ¬èªè§£èª¬è¨˜äº‹ã‹ã‚‰å¼•ç”¨ï¼‰</p>
<p>æ—¥æœ¬èªã§ã®è§£èª¬: 


<a href="https://akichan-f.medium.com/optimizer%E3%81%AF%E3%81%A9%E3%82%8C%E3%81%8C%E5%84%AA%E3%82%8C%E3%81%A6%E3%81%84%E3%82%8B%E3%81%8B-on-empirical-comparisons-of-optimizers-for-deep-learning%E3%81%AE%E7%B4%B9%E4%BB%8B-f843179e8a8d" target="_blank" rel="noopener noreferrer">https://akichan-f.medium.com/optimizerã¯ã©ã‚ŒãŒå„ªã‚Œã¦ã„ã‚‹ã‹-on-empirical-comparisons-of-optimizers-for-deep-learningã®ç´¹ä»‹-f843179e8a8d</a>


</p>
<p>AdamãŒè‰¯ã„ã®ã ã‘ã©ã€å­¦ç¿’ç‡ä»¥å¤–ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãªã„ã¨æœ¬æ¥ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç™ºæ®ã•ã‚Œãªã„ã‹ã‚‚ã‚ˆã€ã¨ã„ã†æ„Ÿã˜ã£ã½ã„</p>
<p>ICLR 2020 Open Review: 


<a href="https://openreview.net/forum?id=HygrAR4tPS" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=HygrAR4tPS</a>


</p>
<p>OpenReview:


<a href="https://openreview.net/forum?id=HygrAR4tPS" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=HygrAR4tPS</a>


</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Batch.html" target="_blank" rel="noopener noreferrer">#Batch</a>
<span class="issue_date">Issue Date: 2025-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2196" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Revisiting Small Batch Training for Deep Neural Networks, Dominic Masters+, arXiv'18</a>
<span class="snippet"><span>GPT Summary</span>- ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒæ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ€§èƒ½ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’å®Ÿé¨“çš„ã«æ¯”è¼ƒã€‚å¤§ããªãƒŸãƒ‹ãƒãƒƒãƒã¯è¨ˆç®—ã®ä¸¦åˆ—æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŒã€å°ã•ãªãƒŸãƒ‹ãƒãƒƒãƒã¯ä¸€èˆ¬åŒ–æ€§èƒ½ã‚’é«˜ã‚ã€å®‰å®šã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿç¾ã€‚æœ€è‰¯ã®æ€§èƒ½ã¯ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚º$m = 2$ã‹ã‚‰$m = 32$ã®ç¯„å›²ã§å¾—ã‚‰ã‚Œã€æ•°åƒã®ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¨å¥¨ã™ã‚‹ç ”ç©¶ã¨ã¯å¯¾ç…§çš„ã€‚</span>
<span class="snippet"><span>Comment</span><p>{Res, Reduced Alex}Netã«ãŠã„ã¦ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤§ããã™ã‚‹ã¨ã€å­¦ç¿’ãŒå®‰å®šã—ã‹ã¤é«˜ã„äºˆæ¸¬æ€§èƒ½ã‚’ç²å¾—ã§ãã‚‹å­¦ç¿’ç‡ã®rangeãŒå°ã•ããªã‚‹ã€‚ä¸€æ–¹ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå°ã•ã„ã¨æœ‰åŠ¹ãªå­¦ç¿’ç‡ã®rangeãŒåºƒã„ã€‚ã¾ãŸã€ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå°ã•ã„å ´åˆã¯ã€å‹¾é…è¨ˆç®—ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆãŒã‚ˆã‚Šé »ç¹ã«è¡Œã‚ã‚Œã‚‹ã€‚ã“ã®ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŒã‚ˆã‚Šé€²ã‚“ã çŠ¶æ…‹ã§å€‹ã€…ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å‹¾é…è¨ˆç®—ãŒè¡Œã‚ã‚Œã‚‹ãŸã‚ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå¤§ãã„å ´åˆã¨æ¯”ã¹ã‚‹ã¨ãƒ¢ãƒ‡ãƒ«ãŒã‚ˆã‚Šæ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹ã§å„ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å‹¾é…ãŒè¨ˆç®—ã•ã‚Œã‚‹ã“ã¨ã«ãªã‚‹ãŸã‚ã€å­¦ç¿’ãŒå®‰å®šã—è‰¯ã„æ±åŒ–æ€§èƒ½ã«ã¤ãªãŒã‚‹ã€ã¨ã„ã£ãŸè©±ã®æ¨¡æ§˜ã€‚<br><br><img src="https://github.com/user-attachments/assets/f02f9016-6e9f-476d-a4c1-4f64bd51e9d5" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Normalization.html" target="_blank" rel="noopener noreferrer">#Normalization</a>
<span class="issue_date">Issue Date: 2025-04-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1856" target="_blank" rel="noopener noreferrer" class="title-link">Group Normalization, Yuxin Wu+, arXiv'18</a>
<span class="snippet"><span>GPT Summary</span>- ã‚°ãƒ«ãƒ¼ãƒ—æ­£è¦åŒ–ï¼ˆGNï¼‰ã¯ã€ãƒãƒƒãƒæ­£è¦åŒ–ï¼ˆBNï¼‰ã®ä»£æ›¿æ‰‹æ®µã¨ã—ã¦ææ¡ˆã•ã‚Œã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã«ä¾å­˜ã›ãšå®‰å®šã—ãŸç²¾åº¦ã‚’æä¾›ã—ã¾ã™ã€‚ç‰¹ã«ã€ãƒãƒƒãƒã‚µã‚¤ã‚º2ã®ResNet-50ã§ã¯ã€GNãŒBNã‚ˆã‚Šã‚‚10.6%ä½ã„èª¤å·®ã‚’ç¤ºã—ã€ä¸€èˆ¬çš„ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã‚‚åŒç­‰ã®æ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚GNã¯ç‰©ä½“æ¤œå‡ºã‚„ãƒ“ãƒ‡ã‚ªåˆ†é¡ãªã©ã®ã‚¿ã‚¹ã‚¯ã§BNã‚’ä¸Šå›ã‚‹çµæœã‚’ç¤ºã—ã€ç°¡å˜ã«å®Ÿè£…å¯èƒ½ã§ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>BatchNormalizationã¯ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå°ã•ã„ã¨ã†ã¾ãã„ã‹ãšã€ãƒ¡ãƒ¢ãƒªã®åˆ¶ç´„ã§å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºãŒè¨­å®šã§ããªã„å ´åˆã«å›°ã‚‹ã‹ã‚‰ãƒãƒƒãƒã‚µã‚¤ã‚ºã«ä¾å­˜ã—ãªã„normalizationã‚’è€ƒãˆãŸã‚ˆã€‚LayerNormã¨InstanceNormã‚‚ãƒãƒƒãƒã‚µã‚¤ã‚ºã«ä¾å­˜ã—ãªã„ã‘ã©ææ¡ˆæ‰‹æ³•ã®æ–¹ãŒç”»åƒç³»ã®ã‚¿ã‚¹ã‚¯ã ã¨æ€§èƒ½ãŒè‰¯ã„ã‚ˆã€ã¨ã„ã†è©±ã‚‰ã—ã„ã€‚<br><br>å„normalizationã¨ã®æ¯”è¼ƒã€‚åˆ†ã‹ã‚Šã‚„ã™ã„ã€‚<br><img src="https://github.com/user-attachments/assets/128a6a2e-cac7-4d6a-9cf6-31119fb6b187" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Optimizer.html" target="_blank" rel="noopener noreferrer">#Optimizer</a>
<span class="issue_date">Issue Date: 2023-12-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1185" target="_blank" rel="noopener noreferrer" class="title-link">Large Batch Training of Convolutional Networks, Yang You+, N_A, arXiv'17</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡ãªç•³ã¿è¾¼ã¿ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã«ã€æ–°ã—ã„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆã—ã¾ã—ãŸã€‚ã“ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€Layer-wise Adaptive Rate Scalingï¼ˆLARSï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ãªãŒã‚‰ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’æãªã‚ãšã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€Alexnetã‚’8Kã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã¾ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ã€Resnet-50ã‚’32Kã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã¾ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>BatchSizeã‚’å¤§ããã™ã‚‹ã¨æ€§èƒ½ãŒè½ã¡ã¾ã™ã‚ˆã€ç³»ã®è©±ï¼ˆCNNï¼‰<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/deeb60b7-548c-4e50-94db-ce98eaf268e3" alt="image" loading="lazy"></p>
<p>OpenReview:


<a href="https://openreview.net/forum?id=rJ4uaX2aW" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=rJ4uaX2aW</a>


<br><br>ICLR'18ã«rejectã•ã‚Œã¦ã„ã‚‹<br><br>å…ˆè¡Œç ”ç©¶ã§ææ¡ˆã‚ˆã‚Šã‚‚å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ‰±ãˆã‚‹synchronized SGDã¯å¼·ã¿ã ãŒã€è©•ä¾¡ãŒä¸€ã¤ã®ã‚¿ã‚¹ã‚¯ã®ã¿ãªã®ã§ã‚ˆã‚Šå¢—ã‚„ã—ãŸæ–¹ãŒconvincingã ã¨ã„ã†ã“ã¨ã€ææ¡ˆæ‰‹æ³•ã«è¿½åŠ ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¿…è¦ãªç‚¹ãŒæ‰‹æ³•ã‚’less appealingã«ã—ã¦ã—ã¾ã£ã¦ã„ã‚‹ã“ã¨ã€layer wise rate scailng (LARS)ã®ç†è«–çš„ãªjustificationãŒä½•ã‹æ¬²ã—ã„ã“ã¨ã€å…ˆè¡Œç ”ç©¶ã¨ã®æ¯”è¼ƒãŒã‚¯ãƒªã‚¢ã§ã¯ãªã„ã“ã¨ã€ãªã©ãŒç†ç”±ãªæ¨¡æ§˜ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/CommentGeneration.html" target="_blank" rel="noopener noreferrer">#CommentGeneration</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<span class="issue_date">Issue Date: 2019-09-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/327" target="_blank" rel="noopener noreferrer" class="title-link">Attend to You: Personalized Image Captioning with Context Sequence Memory Networks, Park+, CVPR'17</a>
<span class="snippet"><span>Comment</span><p>ç”»åƒãŒä¸ãˆã‚‰ã‚ŒãŸã¨ãã«ã€ãã®ç”»åƒã«å¯¾ã™ã‚‹Hashtag predictionã¨ã€personalizedãªpost generationã‚’è¡Œã†ã‚¿ã‚¹ã‚¯ã‚’ææ¡ˆã€‚<br><br>Instagramã®Postã®ç°¡æ˜“åŒ–ãªã©ã«å¿œç”¨ã§ãã‚‹ã€‚<br><br>Postã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã«ã¯ã€è‡ªèº«ã®è¨€è‘‰ã§ã€ç”»åƒã«ã¤ã„ã¦ã®èª¬æ˜ã‚„ã€contextã¨ã„ã£ãŸã“ã¨ã‚’èª¬æ˜ã—ãªã‘ã‚Œã°ãªã‚‰ãšã€image captioningã‚’ã™ã‚‹éš›ã«Personalization IssueãŒç”Ÿã˜ã‚‹ã“ã¨ã‚’æŒ‡æ‘˜ã€‚<br><br><br><br></p>
<p>official implementation: 


<a href="https://github.com/cesc-park/attend2u" target="_blank" rel="noopener noreferrer">https://github.com/cesc-park/attend2u</a>


</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/NaturalLanguageGeneration.html" target="_blank" rel="noopener noreferrer">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/90" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Multi-Task Video Captioning with Video and Entailment Generation, Pasunuru+, ACL'17</a>
<span class="snippet"><span>Comment</span><p>è§£èª¬ã‚¹ãƒ©ã‚¤ãƒ‰ï¼š


<a href="https://www.slideshare.net/HangyoMasatsugu/hangyo-acl-paperreading2017multitask-video-captioning-with-video-and-entailment-generation/1" target="_blank" rel="noopener noreferrer">https://www.slideshare.net/HangyoMasatsugu/hangyo-acl-paperreading2017multitask-video-captioning-with-video-and-entailment-generation/1</a>


</p>
<p>multitask learningã§å‹•ç”»ï¼ˆã‹ãªã‚ŠçŸ­ã‚ï¼‰ã®ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚’è¡Œãªã£ãŸè©±<br><br>(2025.05.12)<br>ä¸Šè¨˜è§£èª¬è³‡æ–™ä¸­ã®ã‚¹ã‚¯ã‚·ãƒ§ãŒã„ãã¤ã‹æ²è¼‰ã•ã‚Œã¦ã„ã¾ã—ãŸãŒå‰Šé™¤ã—ã¾ã—ãŸã€‚</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/GenerativeAdversarialNetwork.html" target="_blank" rel="noopener noreferrer">#GenerativeAdversarialNetwork</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/60" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Generative Adversarial Networks: An Overview, Antonia Creswell+, IEEE-SPM'17, 2017.10</a>
<span class="snippet"><span>GPT Summary</span>- GANã¯ã€æ³¨é‡ˆãªã—ã®ãƒ‡ãƒ¼ã‚¿ã§æ·±ã„è¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã§ã€ç«¶äº‰ãƒ—ãƒ­ã‚»ã‚¹ã‚’é€šã˜ã¦é€†ä¼æ’­ä¿¡å·ã‚’å°å‡ºã—ã¾ã™ã€‚ç”»åƒåˆæˆã‚„ã‚¹ã‚¿ã‚¤ãƒ«è»¢é€ãªã©å¤šæ§˜ãªå¿œç”¨ãŒå¯èƒ½ã§ã™ã€‚æœ¬ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯ã€ä¿¡å·å‡¦ç†ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å‘ã‘ã«GANã®æ¦‚è¦ã‚’æä¾›ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ–¹æ³•ã‚„æ®‹ã•ã‚ŒãŸèª²é¡Œã«ã¤ã„ã¦ã‚‚è¨€åŠã—ã¾ã™ã€‚</span>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Visual%20Words.html" target="_blank" rel="noopener noreferrer">#Visual Words</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/63" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Image Captioning with Semantic Attention, You+, CVPR'16</a>
<span class="snippet"><span>Comment</span><p>ç”»åƒãã®ã‚‚ã®ã ã‘ã§ãªãã€ãƒ¢ãƒ‡ãƒ«ã¸ã®Inputã«Visual Wordsã‚’æ˜ç¤ºçš„ã«åŠ ãˆã‚‹ã“ã¨ã§ã€captioningã®ç²¾åº¦ãŒä¸ŠãŒã‚Šã¾ã—ãŸã¨ã„ã†è«–æ–‡</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/ECCV.html" target="_blank" rel="noopener noreferrer">#ECCV</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/61" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Generating Visual Explanations, Hendrickks+, ECCV'16</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<a class="button" href="articles/U-Net.html" target="_blank" rel="noopener noreferrer">#U-Net</a>
<span class="issue_date">Issue Date: 2025-09-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2931" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] U-Net: Convolutional Networks for Biomedical Image Segmentation, Olaf Ronneberger+, MICCAI'15, 2015.05</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’æ´»ç”¨ã—ãŸæ–°ã—ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ææ¡ˆã—ã€å°‘ãªã„æ³¨é‡ˆä»˜ãã‚µãƒ³ãƒ—ãƒ«ã‹ã‚‰ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚é›»å­é¡•å¾®é¡ã‚¹ã‚¿ãƒƒã‚¯ã®ç¥çµŒæ§‹é€ ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã§å¾“æ¥æ‰‹æ³•ã‚’ä¸Šå›ã‚Šã€é€éå…‰é¡•å¾®é¡ç”»åƒã§ã‚‚å„ªã‚ŒãŸçµæœã‚’é”æˆã€‚512x512ç”»åƒã®ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã¯1ç§’æœªæº€ã§å®Œäº†ã€‚å®Ÿè£…ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-08-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2544" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Very Deep Convolutional Networks for Large-Scale Image Recognition, Karen Simonyan+, ICLR'15</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€3x3ã®ç•³ã¿è¾¼ã¿ãƒ•ã‚£ãƒ«ã‚¿ã‚’ç”¨ã„ãŸæ·±ã„ç•³ã¿è¾¼ã¿ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ç²¾åº¦å‘ä¸Šã‚’è©•ä¾¡ã—ã€16-19å±¤ã®é‡ã¿å±¤ã§å¾“æ¥ã®æœ€å…ˆç«¯æ§‹æˆã‚’å¤§å¹…ã«æ”¹å–„ã—ãŸã“ã¨ã‚’ç¤ºã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ImageNet Challenge 2014ã§1ä½ã¨2ä½ã‚’ç²å¾—ã—ã€ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚‚å„ªã‚ŒãŸä¸€èˆ¬åŒ–æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚æœ€ã‚‚æ€§èƒ½ã®è‰¯ã„2ã¤ã®ConvNetãƒ¢ãƒ‡ãƒ«ã‚’å…¬é–‹ã—ã€æ·±å±¤è¦–è¦šè¡¨ç¾ã®ç ”ç©¶ã‚’ä¿ƒé€²ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>ã„ã‚ã‚†ã‚‹VGGNetã‚’ææ¡ˆã—ãŸè«–æ–‡</p></span><br><br>
<a class="button" href="articles/DocumentSummarization.html" target="_blank" rel="noopener noreferrer">#DocumentSummarization</a>
<a class="button" href="articles/NaturalLanguageGeneration.html" target="_blank" rel="noopener noreferrer">#NaturalLanguageGeneration</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/ImageCaptioning.html" target="_blank" rel="noopener noreferrer">#ImageCaptioning</a>
<a class="button" href="articles/Reference-based.html" target="_blank" rel="noopener noreferrer">#Reference-based</a>
<span class="issue_date">Issue Date: 2023-05-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/670" target="_blank" rel="noopener noreferrer" class="title-link">CIDEr: Consensus-based Image Description Evaluation, Ramakrishna Vedantam+, N_A, CVPR'15</a>
<span class="snippet"><span>GPT Summary</span>- ç”»åƒã‚’æ–‡ç« ã§è‡ªå‹•çš„ã«èª¬æ˜ã™ã‚‹ã“ã¨ã¯ã€é•·å¹´ã®èª²é¡Œã§ã‚ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€äººé–“ã®åˆæ„ã‚’åˆ©ç”¨ã—ãŸç”»åƒèª¬æ˜ã®è©•ä¾¡ã®ãŸã‚ã®æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’ææ¡ˆã—ã€æ–°ã—ã„è‡ªå‹•è©•ä¾¡æŒ‡æ¨™ã¨2ã¤ã®æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å«ã‚€ã€‚ææ¡ˆæ‰‹æ³•ã¯ã€äººé–“ã®åˆ¤æ–­ã‚’ã‚ˆã‚Šæ­£ç¢ºã«æ‰ãˆã‚‹ã“ã¨ãŒã§ãã€5ã¤ã®æœ€å…ˆç«¯ã®ç”»åƒèª¬æ˜æ‰‹æ³•ã‚’è©•ä¾¡ã—ã€å°†æ¥ã®æ¯”è¼ƒã®ãŸã‚ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’æä¾›ã™ã‚‹ã€‚CIDEr-Dã¯ã€MS COCOè©•ä¾¡ã‚µãƒ¼ãƒãƒ¼ã®ä¸€éƒ¨ã¨ã—ã¦åˆ©ç”¨å¯èƒ½ã§ã‚ã‚Šã€ã‚·ã‚¹ãƒ†ãƒãƒ†ã‚£ãƒƒã‚¯ãªè©•ä¾¡ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚</span>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Visual%20Words.html" target="_blank" rel="noopener noreferrer">#Visual Words</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/62" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] What value do explicit high level concepts have in vision to language  problems?, Qi Wu+, arXiv'15</a>
<span class="snippet"><span>GPT Summary</span>- CNN-RNNã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«é«˜æ¬¡ã®æ¦‚å¿µã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã¨è¦–è¦šçš„è³ªå•å¿œç­”ã®æ€§èƒ½ã‚’å‘ä¸Šã€‚å¤–éƒ¨ã®æ„å‘³æƒ…å ±ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã•ã‚‰ãªã‚‹æ”¹å–„ã‚’å®Ÿç¾ã—ã€V2Lå•é¡Œã«ãŠã‘ã‚‹é«˜æ¬¡ã®æ„å‘³æƒ…å ±ã®é‡è¦æ€§ã‚’åˆ†æã€‚</span>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ImageClassification.html" target="_blank" rel="noopener noreferrer">#ImageClassification</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-05-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1958" target="_blank" rel="noopener noreferrer" class="title-link">ImageNet Classification with Deep Convolutional Neural Networks, Krizhevsky+, NIPS'12</a>
<span class="snippet"><span>Comment</span><p>ILSVRC 2012ã«ãŠã„ã¦åœ§å€’çš„ãªæ€§èƒ½ç¤ºã—ãŸã“ã¨ã§ç¾ä»£ã®DeepLearningã®ç«ä»˜ã‘å½¹ã¨ãªã£ãŸç ”ç©¶AlexNetã€‚ãƒ¡ãƒ¢ã£ã¦ãªã‹ã£ãŸã®ã§ä»Šæ›´ãªãŒã‚‰è¿½åŠ ã—ãŸã€‚</p>
<p>AlexNetä»¥å‰ã®ç”»åƒèªè­˜æŠ€è¡“ã«ã¤ã„ã¦ã¯ç‰›ä¹…å…ˆç”ŸãŒã¾ã¨ã‚ã¦ãã ã•ã£ã¦ã„ã‚‹ï¼ˆå½“æ™‚ã®èª²é¡Œã¨ãã‚Œã«å¯¾ã™ã‚‹è§£æ±ºæ³•ã€ã—ã‹ã—ã¾ã èª²é¡ŒãŒâ€¦ã¨æ¬¡ã€…ã¨èª²é¡Œã«ç›´é¢ã—è§£æ±ºã—ã¦ã„ãæ§˜å­ãŒæã‹ã‚Œã¦ãŠã‚Šéå¸¸ã«èˆˆå‘³æ·±ã‹ã£ãŸ)ã€‚ç¾åœ¨ã§ã‚‚æ®‹ã£ã¦ã„ã‚‹æŠ€è¡“ã‚‚ç´¹ä»‹ã•ã‚Œã¦ã„ã‚‹ã€‚:<br>


<a href="https://speakerdeck.com/yushiku/pre_alexnet" target="_blank" rel="noopener noreferrer">https://speakerdeck.com/yushiku/pre_alexnet</a>


<br><br>&gt; éå»ã®æŠ€è¡“ã ã‹ã‚‰ã¨ã„ã£ã¦èãæµã—ã¦ã„ã‚‹ã¨æ™‚ä»£èƒŒæ™¯ã®å¤‰åŒ–ã«ã‚ˆã£ã¦ãªã—å¾—ãŸã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é€ƒã™ã‹ã‚‚<br><br>ã“ã‚Œã¯è‚ã«éŠ˜ã˜ãŸã„ã€‚</p></span><br><br>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ImageClassification.html" target="_blank" rel="noopener noreferrer">#ImageClassification</a>
<a class="button" href="articles/ObjectRecognition.html" target="_blank" rel="noopener noreferrer">#ObjectRecognition</a>
<a class="button" href="articles/ObjectLocalization.html" target="_blank" rel="noopener noreferrer">#ObjectLocalization</a>
<span class="issue_date">Issue Date: 2025-05-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1959" target="_blank" rel="noopener noreferrer" class="title-link">ImageNet: A Large-Scale Hierarchical Image Database, Deng+, CVPR'09</a>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/Slide.html" target="_blank" rel="noopener noreferrer">#Slide</a>
<a class="button" href="articles/ObjectLocalization.html" target="_blank" rel="noopener noreferrer">#ObjectLocalization</a>
<a class="button" href="articles/Geometric.html" target="_blank" rel="noopener noreferrer">#Geometric</a>
<a class="button" href="articles/Mapping.html" target="_blank" rel="noopener noreferrer">#Mapping</a>
<span class="issue_date">Issue Date: 2025-11-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3567" target="_blank" rel="noopener noreferrer" class="title-link">Do we still need geometry for Visual Localization and Mapping?, Paul-Edouard Sarlin, 50th Pattern Recognition and Computer Vision Colloquium - CVUT, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gabriberton/status/1985019542511497683?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/Slide.html" target="_blank" rel="noopener noreferrer">#Slide</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<span class="issue_date">Issue Date: 2025-11-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3538" target="_blank" rel="noopener noreferrer" class="title-link">ICCV 2025 Report, Kataoka+, LIMIT.Lab, cvpaper.challenge, Visual Geometry Group ï¼ˆVGGï¼‰, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hirokatukataoka/status/1984311854693564915?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/WorldModels.html" target="_blank" rel="noopener noreferrer">#WorldModels</a>
<span class="issue_date">Issue Date: 2025-11-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3537" target="_blank" rel="noopener noreferrer" class="title-link">Awesome World Models, Siqiao Huang, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/knightnemo_/status/1984436712433426692?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/2D%20(Image).html" target="_blank" rel="noopener noreferrer">#2D (Image)</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<a class="button" href="articles/4D%20(Video).html" target="_blank" rel="noopener noreferrer">#4D (Video)</a>
<a class="button" href="articles/Omni.html" target="_blank" rel="noopener noreferrer">#Omni</a>
<a class="button" href="articles/audio.html" target="_blank" rel="noopener noreferrer">#audio</a>
<a class="button" href="articles/text.html" target="_blank" rel="noopener noreferrer">#text</a>
<span class="issue_date">Issue Date: 2025-11-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3536" target="_blank" rel="noopener noreferrer" class="title-link">LongCat-Flash-Omni Technical Report, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/meituan_longcat/status/1984398560973242733?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>HF:


<a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Omni" target="_blank" rel="noopener noreferrer">https://huggingface.co/meituan-longcat/LongCat-Flash-Omni</a>


</p>
<p>text, image/video, audioã‚’inputã—ã€audioã‚’ç”Ÿæˆã™ã‚‹omniãƒ¢ãƒ‡ãƒ«<br><img src="https://github.com/user-attachments/assets/7af9971e-c75f-4330-aca7-7f00547f97ae" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-10-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3498" target="_blank" rel="noopener noreferrer" class="title-link">Nemotron-VLM-Dataset-v2, Nvidia, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/vanstriendaniel/status/1983238971644608924?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<span class="issue_date">Issue Date: 2025-10-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3495" target="_blank" rel="noopener noreferrer" class="title-link">From Egocentric Perception to Embodied Intelligence: Building the World in First Person, Ziwei Liu, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/liuziwei7/status/1982824198600056929?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<span class="issue_date">Issue Date: 2025-10-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3494" target="_blank" rel="noopener noreferrer" class="title-link">Multimodal Reasoning for Human-Centric Generative Models, Ziwei Liu, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/liuziwei7/status/1982824198600056929?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<span class="issue_date">Issue Date: 2025-10-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3493" target="_blank" rel="noopener noreferrer" class="title-link">Native Multimodal Models: Architecture, Post-Training, and Evaluation, Ziwei Liu, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/liuziwei7/status/1982824198600056929?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/AutomaticSpeechRecognition(ASR).html" target="_blank" rel="noopener noreferrer">#AutomaticSpeechRecognition(ASR)</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/Editing.html" target="_blank" rel="noopener noreferrer">#Editing</a>
<a class="button" href="articles/TTS.html" target="_blank" rel="noopener noreferrer">#TTS</a>
<a class="button" href="articles/Routing.html" target="_blank" rel="noopener noreferrer">#Routing</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<a class="button" href="articles/Omni.html" target="_blank" rel="noopener noreferrer">#Omni</a>
<a class="button" href="articles/Sparse.html" target="_blank" rel="noopener noreferrer">#Sparse</a>
<a class="button" href="articles/ImageSynthesis.html" target="_blank" rel="noopener noreferrer">#ImageSynthesis</a>
<span class="issue_date">Issue Date: 2025-10-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3489" target="_blank" rel="noopener noreferrer" class="title-link">Ming-flash-omni-Preview, inclusionAI, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1982987141773713445?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2300" target="_blank" rel="noopener noreferrer">[Paper Note] Ming-Omni: A Unified Multimodal Model for Perception and Generation, Inclusion AI+, arXiv'25</a>
</p>
<p>éå»ä¸€ç•ªå¤šãã®ã‚¿ã‚°ã‚’ä»˜ä¸ã—ãŸæ°—ãŒã™ã‚‹ãŒã€æœãŸã—ã¦å¤§è¦æ¨¡ã€Omniãƒ¢ãƒ‡ãƒ«ã‹ã¤ã€UMMã«ã—ãŸã“ã¨ã«ã‚ˆã‚‹æ©æµï¼ˆï¼æ§˜ã€…ãªãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’çµ±ä¸€ã•ã‚ŒãŸç©ºé–“ä¸Šã«å­¦ç¿’ã•ã›ã‚‹æ©æµï¼‰ã¯ã©ã®ç¨‹åº¦ã‚ã‚‹ã®ã ã‚ã†ã‹ï¼Ÿ<br><br>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è¦‹ã‚‹ã¨ã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã”ã¨ã«ï¼ˆãƒ¢ãƒ€ãƒªãƒ†ã‚£å˜ä½ã§ã®ãƒã‚¤ã‚¢ã‚¹ãŒã‹ã‹ã£ãŸï¼‰RouterãŒç”¨æ„ã•ã‚Œexpertã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã•ã‚Œã‚‹ã‚ˆã†ãªæ§‹é€ ã«ãªã£ã¦ã„ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-10-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3454" target="_blank" rel="noopener noreferrer" class="title-link">LMMs Engine, EvolvingLMMs-Lab, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/liuziwei7/status/1982446267646239148?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>äº‹å‰å­¦ç¿’æ¸ˆã¿ã®LLM, VLM, dLM, DiffusionModelãªã©ã‹ã‚‰UMMã‚’å­¦ç¿’ã§ãã‚‹äº‹å¾Œå­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€‚<br>LigerKernelã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’30%å‰Šæ¸›ã—ã€SparseAttentionã‚‚ã‚µãƒãƒ¼ãƒˆã—ã€Muon Optimizerã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/VariationalAutoEncoder.html" target="_blank" rel="noopener noreferrer">#VariationalAutoEncoder</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<span class="issue_date">Issue Date: 2025-10-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3440" target="_blank" rel="noopener noreferrer" class="title-link">LongCat-Video Techcal Report, Meituan LongCat Team, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/teortaxestex/status/1982013157926125724?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>HF:


<a href="https://huggingface.co/meituan-longcat/LongCat-Video" target="_blank" rel="noopener noreferrer">https://huggingface.co/meituan-longcat/LongCat-Video</a>


</p>
<p>å…¬å¼ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/meituan_longcat/status/1982083998852763838?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/OCR.html" target="_blank" rel="noopener noreferrer">#OCR</a>
<span class="issue_date">Issue Date: 2025-10-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3412" target="_blank" rel="noopener noreferrer" class="title-link">Supercharge your OCR Pipelines with Open Models, merve+, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/webbigdata/status/1981526749143036255?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/DocParser.html" target="_blank" rel="noopener noreferrer">#DocParser</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/OCR.html" target="_blank" rel="noopener noreferrer">#OCR</a>
<span class="issue_date">Issue Date: 2025-10-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3410" target="_blank" rel="noopener noreferrer" class="title-link">LightOnOCR-1B: The Case for End-to-End and Efficient Domain-Specific Vision-Language Models for OCR, Taghadouini+, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/staghado/status/1981379888301867299?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/MultiLingual.html" target="_blank" rel="noopener noreferrer">#MultiLingual</a>
<a class="button" href="articles/Japanese.html" target="_blank" rel="noopener noreferrer">#Japanese</a>
<a class="button" href="articles/GRPO.html" target="_blank" rel="noopener noreferrer">#GRPO</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/DocParser.html" target="_blank" rel="noopener noreferrer">#DocParser</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/OCR.html" target="_blank" rel="noopener noreferrer">#OCR</a>
<span class="issue_date">Issue Date: 2025-10-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3395" target="_blank" rel="noopener noreferrer" class="title-link">olmOCR 2: Unit test rewards for document OCR, Ai2, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/allen_ai/status/1981029163394797618?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒ¢ãƒ‡ãƒ«:


<a href="https://huggingface.co/allenai/olmOCR-2-7B-1025-FP8" target="_blank" rel="noopener noreferrer">https://huggingface.co/allenai/olmOCR-2-7B-1025-FP8</a>


</p>
<p>Apache2.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã§SoTAæ›´æ–°ã€‚ãã—ã¦ã•ã™ãŒã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã‚³ãƒ¼ãƒ‰ã‚‚å…¬é–‹</p>
<p>ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆ:


<a href="https://github.com/allenai/olmocr/blob/main/olmOCR-2-Unit-Test-Rewards-for-Document-OCR.pdf" target="_blank" rel="noopener noreferrer">https://github.com/allenai/olmocr/blob/main/olmOCR-2-Unit-Test-Rewards-for-Document-OCR.pdf</a>


</p>
<p>æœãŸã—ã¦æ—¥æœ¬èªã¯â€¦SFT Datasetã®top5ã«jaã¯ãªã‹ã£ãŸã‚ˆã†ã«è¦‹ãˆã‚‹</p>
<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/kylelostat/status/1981380820658180310?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>demoã‚’è©¦ã—ãŸè¦‹ãŸãŒæ—¥æœ¬èªã‚¹ãƒ©ã‚¤ãƒ‰ã§ã‚‚éå¸¸ã«æ€§èƒ½ãŒè‰¯ã„</p>
<p>DeepSeekOCRã¨ã®æ¯”è¼ƒ:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/askalphaxiv/status/1983343306152259888?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/MultiLingual.html" target="_blank" rel="noopener noreferrer">#MultiLingual</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-10-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3386" target="_blank" rel="noopener noreferrer" class="title-link">LFM2-VL-3B: A New Efficient Vision-Language for the Edge, LiquidAI, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/liquidai_/status/1980985540196393211?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>HF:


<a href="https://huggingface.co/LiquidAI/LFM2-VL-3B" target="_blank" rel="noopener noreferrer">https://huggingface.co/LiquidAI/LFM2-VL-3B</a>


</p>
<p>SigLIP2ã¨LFM2ãŒãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2999" target="_blank" rel="noopener noreferrer">Introducing LFM2: The Fastest On-Device Foundation Models on the Market, LiquidAI, 2025.07</a>
</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/MultiLingual.html" target="_blank" rel="noopener noreferrer">#MultiLingual</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/DocParser.html" target="_blank" rel="noopener noreferrer">#DocParser</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/OCR.html" target="_blank" rel="noopener noreferrer">#OCR</a>
<span class="issue_date">Issue Date: 2025-10-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3381" target="_blank" rel="noopener noreferrer" class="title-link">dots.ocr, rednote-hilab, 2025.07</a>
<span class="snippet"><span>Comment</span><p>100+è¨€èªã®dots.ocr benchã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã§ã®æ€§èƒ½ã‚‚å ±å‘Šã•ã‚Œã¦ã„ã‚‹ãŒã€æ—¥æœ¬èªæ€§èƒ½ã¯ã©ã®ãã‚‰ã„ãªã®ã ã‚ã†ã‹<br><br>MIT Licence</p>
<p>å‚è€ƒ:VLMã‚’ä½¿ã£ãŸå¤šè¨€èªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ‘ãƒ¼ã‚µã€Œdots.ocrã€ã‚’è©¦ã™, kun432, Zenn<br>


<a href="https://zenn.dev/kun432/scraps/b91fce6fbeb30c" target="_blank" rel="noopener noreferrer">https://zenn.dev/kun432/scraps/b91fce6fbeb30c</a>


<br><br>æ—¥æœ¬èªã‚‚ã‹ãªã‚Šã„ã‘ã¦ãã†</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiLingual.html" target="_blank" rel="noopener noreferrer">#MultiLingual</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/DocParser.html" target="_blank" rel="noopener noreferrer">#DocParser</a>
<a class="button" href="articles/OCR.html" target="_blank" rel="noopener noreferrer">#OCR</a>
<span class="issue_date">Issue Date: 2025-10-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3380" target="_blank" rel="noopener noreferrer" class="title-link">Chandra, datalab-to, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/vikparuchuri/status/1980667137606971423?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>SoTA.ã ã£ãŸdots.ocrã¨ã„ã†ãƒ¢ãƒ‡ãƒ«ã‚’outperformã—ã¦ã„ã‚‹æ¨¡æ§˜<br><br>40+ languagesã‚’ã‚µãƒãƒ¼ãƒˆ<p>AI PUBS OpenRAIL-M Modifiedãƒ©ã‚¤ã‚»ãƒ³ã‚¹ğŸ¤”<br>


<a href="https://huggingface.co/datalab-to/chandra/blob/main/LICENSE" target="_blank" rel="noopener noreferrer">https://huggingface.co/datalab-to/chandra/blob/main/LICENSE</a>


</p>
<p>dots.ocrã¯MIT Licence<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3381" target="_blank" rel="noopener noreferrer">dots.ocr, rednote-hilab, 2025.07</a>
</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiLingual.html" target="_blank" rel="noopener noreferrer">#MultiLingual</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/DocParser.html" target="_blank" rel="noopener noreferrer">#DocParser</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<a class="button" href="articles/OCR.html" target="_blank" rel="noopener noreferrer">#OCR</a>
<a class="button" href="articles/Reference%20Collection.html" target="_blank" rel="noopener noreferrer">#Reference Collection</a>
<span class="issue_date">Issue Date: 2025-10-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3344" target="_blank" rel="noopener noreferrer" class="title-link">DeepSeek-OCR: Contexts Optical Compression, DeepSeek, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/vllm_project/status/1980235518706401405?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è‹±èªã¨ä¸­å›½èªã§ã¯ä½¿ãˆãã†ã ãŒã€æ—¥æœ¬èªã§ã¯ä½¿ãˆã‚‹ã®ã ã‚ã†ã‹ï¼Ÿp.17 Figure11ã‚’è¦‹ã‚‹ã¨100è¨€èªã«å¯¾ã—ã¦å­¦ç¿’ã—ãŸã¨æ›¸ã‹ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã‚‹ã€‚</p>
<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/teortaxestex/status/1980160624140456370?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/willccbb/status/1980160732236042604?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>OCRãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3360" target="_blank" rel="noopener noreferrer">[Paper Note] OmniDocBench: Benchmarking Diverse PDF Document Parsing with   Comprehensive Annotations, Linke Ouyang+, CVPR'25, 2024.12</a>
<br><br>ï¼ˆDeepSeek-OCRã®ä¸»é¡Œã¯OCRã®æ€§èƒ½å‘ä¸Šã¨ã„ã†ã‚ã‘ã§ã¯ãªã„ã‚ˆã†ã ãŒï¼‰</p>
<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/nrehiew_/status/1980635199273889895?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹+ãƒã‚¤ãƒ³ãƒˆè§£èª¬:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rasbt/status/1980642191950090585?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/kangwook_lee/status/1980709454522744902?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>textxã‚’imageã¨ã—ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹è©±ã¯ä»¥ä¸‹ã®2023å¹´ã®ICLRã®ç ”ç©¶ã§ã‚‚ã‚„ã‚‰ã‚Œã¦ã„ã‚‹ã‚ˆã¨ã„ã†ãƒã‚¹ãƒˆ:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3370" target="_blank" rel="noopener noreferrer">[Paper Note] Language Modelling with Pixels, Phillip Rust+, ICLR'23, 2022.07</a>
<br><br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/nielsrogge/status/1980559120760791125?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3375" target="_blank" rel="noopener noreferrer">[Paper Note] Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text
  Inputs in Multimodal LLMs, Yanhong Li+, arXiv'25, 2025.10</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3363" target="_blank" rel="noopener noreferrer">[Paper Note] PixelWorld: Towards Perceiving Everything as Pixels, Zhiheng Lyu+, arXiv'25, 2025.01</a>
</p>
<p>é–¢é€£:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/askalphaxiv/status/1980722479405678593?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3357" target="_blank" rel="noopener noreferrer">[Paper Note] Glyph: Scaling Context Windows via Visual-Text Compression, Jiale Cheng+, arXiv'25, 2025.10</a>
</p>
<p>literature:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/awinyimgprocess/status/1980506449706119642?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>ä¸Šè¨˜ãƒã‚¹ãƒˆã§ã¯æœ¬ç ”ç©¶ã¯ã“ã‚Œã‚‰literatureã‚’å®Œå…¨ã«ç„¡è¦–ã— â€œan initial investigation into the feasibility of compressing long contexts via optical 2D mapping.â€ ã¨ä¸»å¼µã—ã¦ã„ã‚‹ã®ã§ã€å…ˆè¡Œç ”ç©¶ã‚’èªè­˜ã—å¼•ç”¨ã™ã¹ãã ã¨è¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹ã‚ˆã†ã ã€‚<p>karpathyæ°ã®ãƒã‚¹ãƒˆ:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/karpathy/status/1980397031542989305?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br>



<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/karpathy/status/1980764296016720094?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/ScoreMatching.html" target="_blank" rel="noopener noreferrer">#ScoreMatching</a>
<span class="issue_date">Issue Date: 2025-10-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3341" target="_blank" rel="noopener noreferrer" class="title-link">Generative Modeling by Estimating Gradients of the Data Distribution, Yang Song, 2021.05</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/karancl/status/1979955896986776048?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/ObjectLocalization.html" target="_blank" rel="noopener noreferrer">#ObjectLocalization</a>
<a class="button" href="articles/3D%20(Scene).html" target="_blank" rel="noopener noreferrer">#3D (Scene)</a>
<span class="issue_date">Issue Date: 2025-10-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3337" target="_blank" rel="noopener noreferrer" class="title-link">Find3D: Localizing Semantic Concepts in the 3D Space , Ziqi Ma, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/ziqi__ma/status/1979973156652900631?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/Samplers.html" target="_blank" rel="noopener noreferrer">#Samplers</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3212" target="_blank" rel="noopener noreferrer" class="title-link">ç”»åƒç”ŸæˆAIã«ãŠã‘ã‚‹Eulerã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã®è©³ç´°è§£èª¬, ã‚ã‚‰ã‚‚ã‚Š, 2024.07</a>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/Samplers.html" target="_blank" rel="noopener noreferrer">#Samplers</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3211" target="_blank" rel="noopener noreferrer" class="title-link">Stable Diffusionã«ãŠã‘ã‚‹ã‚µãƒ³ãƒ—ãƒ©ãƒ¼ã®å½¹å‰²ã‚’ç†è§£ã™ã‚‹, moykeen, 2024.01</a>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3202" target="_blank" rel="noopener noreferrer" class="title-link">Introducing Stable Diffusion 3.5, StabilityAI, 2024.10</a>
<span class="snippet"><span>Comment</span><p>SD3.5</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<span class="issue_date">Issue Date: 2025-10-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3081" target="_blank" rel="noopener noreferrer" class="title-link">Ming-UniVision: Joint Image Understanding and Generation via a Unified Continuous Tokenizer, inclusionAI, 2025.10</a>
<span class="snippet"><span>Comment</span><p>HF:


<a href="https://huggingface.co/inclusionAI/Ming-UniVision-16B-A3B" target="_blank" rel="noopener noreferrer">https://huggingface.co/inclusionAI/Ming-UniVision-16B-A3B</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1973894009551810952?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-10-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3058" target="_blank" rel="noopener noreferrer" class="title-link">Apriel-1.5-15b-Thinker, ServiceNow-AI, 2025.09</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/artificialanlys/status/1973104687806378048?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Artificial Analysisã«ã‚ˆã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚­ãƒ³ã‚°ã§ã¯ç¾çŠ¶&lt;20Bã§SoTAãªReasoningãƒ¢ãƒ‡ãƒ«ãªæ¨¡æ§˜ã€‚<br>MIT License</p>
<p>å…¬å¼ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/servicenowrsrch/status/1973100536280027586?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Nvidiaã«ã‚ˆã‚‹ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/nvidiaaidev/status/1973113351158047150?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-09-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3038" target="_blank" rel="noopener noreferrer" class="title-link">InternVL3.5-Flash, OpenGVLab, 2025.09</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1972555712308953385?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-09-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3026" target="_blank" rel="noopener noreferrer" class="title-link">HunyuanImage-3.0, Tencent, 2025.09</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/tencenthunyuan/status/1972130405160833334?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/teortaxestex/status/1972469371839860954?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ+ç”»åƒç†è§£ãƒ»ç”ŸæˆãŒå¯èƒ½ãªUnified Multimodal Models (UMMs)ã€‚ãƒ†ã‚­ã‚¹ãƒˆã¯tokenizerã€ç”»åƒã¯ç”Ÿæˆç”¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã€ç†è§£ç”¨ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’ç”¨æ„ã—ã¦ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—Decoder-Only Tranformerã«å…¥åŠ›ã€‚auto-regressiveã«ç”Ÿæˆã—ã€ãƒ†ã‚­ã‚¹ãƒˆã¯De-Tokenizerã§ãƒ†ã‚­ã‚¹ãƒˆåŒ–ã€ç”»åƒã®å ´åˆã¯å°‚ç”¨ã®Decoderã§ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹ã€‚<br><br>&lt;img width="638" height="232" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/8e06f188-3885-4eed-8837-eb560dcc6b67"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/8e06f188-3885-4eed-8837-eb560dcc6b67"&lt;/a&gt;


/&gt;</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<a class="button" href="articles/Editing.html" target="_blank" rel="noopener noreferrer">#Editing</a>
<span class="issue_date">Issue Date: 2025-09-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2949" target="_blank" rel="noopener noreferrer" class="title-link">Qwen-Image-Edit-2509, Qwen Team, 2025.09</a>
<span class="snippet"><span>Comment</span><p>ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆ:


<a href="https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Qwen_Image.pdf" target="_blank" rel="noopener noreferrer">https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-Image/Qwen_Image.pdf</a>


</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-09-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2945" target="_blank" rel="noopener noreferrer" class="title-link">Qwen3-VL, Qwen Team, 2025.09</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/alibaba_qwen/status/1970594923503391182?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>DocVQAã®ã‚ªãƒ©ã‚¯ãƒ«ã¯ãƒ©ãƒ™ãƒ«ãƒã‚¤ã‚ºã¨æ›–æ˜§æ€§ã®è¦³ç‚¹ã‹ã‚‰94--95ã¨ã„ã†ä¸»å¼µ:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/vikhyatk/status/1970585801600967009?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Qwen3 VL cookbook:<br>


<a href="https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks" target="_blank" rel="noopener noreferrer">https://github.com/QwenLM/Qwen3-VL/tree/main/cookbooks</a>


<br><br>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/alibaba_qwen/status/1976479304814145877?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ç¶šå ±:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/alibaba_qwen/status/1980665932625383868?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<span class="issue_date">Issue Date: 2025-09-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2893" target="_blank" rel="noopener noreferrer" class="title-link">MagicBench, ByteDance-Seed, 2025.09</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1968972092445008183?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è‹±æ–‡ã¨ä¸­æ–‡ä¸¡æ–¹å­˜åœ¨ã™ã‚‹</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-09-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2858" target="_blank" rel="noopener noreferrer" class="title-link">Magistral-Small-2509, MistralAI, 2025.09</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/mistralai/status/1968670593412190381?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/DocParser.html" target="_blank" rel="noopener noreferrer">#DocParser</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-09-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2848" target="_blank" rel="noopener noreferrer" class="title-link">granite-docling-258M, IBM, 2025.09</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1968433933210763440?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Apache 2.0, è¨€èªã¯è‹±èªã®ã¿</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/ComputerUse.html" target="_blank" rel="noopener noreferrer">#ComputerUse</a>
<a class="button" href="articles/GRPO.html" target="_blank" rel="noopener noreferrer">#GRPO</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-09-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2821" target="_blank" rel="noopener noreferrer" class="title-link">Holo1.5 - Open Foundation Models for Computer Use Agents, H Company, 2025.09</a>
<span class="snippet"><span>Comment</span><p>7Bã®ã¿Apache 2.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã€‚3Bã¯Qwenã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’ç¶™æ‰¿ã—ã€72Bã¯non-commercialãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚‰ã—ã„</p>
<p>ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã¨ãƒ–ãƒ­ã‚°ã«ã‚ˆã‚‹ã¨ä¸‹è¨˜ãƒ¢ãƒ‡ãƒ«ç¾¤ã¨Sonnet 4 ã‚ˆã‚Šã‚‚Computer Useé–¢é€£ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯(GUIä¸Šã§ã®ä½ç½®ã‚’ç‰¹å®šã™ã‚‹UI Localizationã¨Screen Contentã®ç†è§£ãŠã‚ˆã³QAé–¢é€£ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯)ã§é«˜æ€§èƒ½ã¨ã®ã“ã¨:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2447" target="_blank" rel="noopener noreferrer">[Paper Note] UI-Venus Technical Report: Building High-performance UI Agents with RFT, Zhangxuan Gu+, arXiv'25</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1896" target="_blank" rel="noopener noreferrer">Introducing UI-TARS-1.5, ByteDance, 2025.04</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1835" target="_blank" rel="noopener noreferrer">Qwen2.5-VL-32B-Instruct, Qwen Team, 2025.03</a>
</p>
<p>ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã«ã‚ˆã‚‹ã¨open sourceãƒ‡ãƒ¼ã‚¿ã®mixã¨ã€åˆæˆãƒ‡ãƒ¼ã‚¿ã€äººæ‰‹ã§ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ã€SFT-&gt;GRPOã«ã‚ˆã£ã¦å­¦ç¿’ã•ã‚ŒãŸã¨ã ã‘æ›¸ã‹ã‚Œã¦ã„ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-09-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2788" target="_blank" rel="noopener noreferrer" class="title-link">ç”»åƒãƒ¢ãƒ‡ãƒ«ã®ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã¦æœ€åˆã«ä½•ã‚’é¸ã¶ã¹ãã‹ï¼Ÿ, ã¡ãã‚ã¶, 2025.09</a>
<span class="snippet"><span>Comment</span><p>ã“ã¡ã‚‰ã®è«–æ–‡ã‚’å‚è€ƒã«ã—ã¦ã„ã‚‹:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2789" target="_blank" rel="noopener noreferrer">[Paper Note] Battle of the Backbones: A Large-Scale Comparison of Pretrained Models   across Computer Vision Tasks, Micah Goldblum+, NeurIPS'23</a>
</p>
<p>Backboneé¸å®šã®éš›ã¯å‚ç…§ã®ã“ã¨ã€‚2024å¹´ä»¥å¾Œã®ãƒ¢ãƒ‡ãƒ«ã¯å«ã¾ã‚Œã¦ã„ãªã„ç‚¹ã«æ³¨æ„ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/Contamination-free.html" target="_blank" rel="noopener noreferrer">#Contamination-free</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-09-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2719" target="_blank" rel="noopener noreferrer" class="title-link">CLOCKBENCH: VISUAL TIME BENCHMARK WHERE HUMANS BEAT THE CLOCK, LLMS DONâ€™T ALEK SAFAR ï¼ˆOLEG CHICHIGINï¼‰, 2025.09</a>
<span class="snippet"><span>Comment</span><p>ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰:


<a href="https://clockbench.ai" target="_blank" rel="noopener noreferrer">https://clockbench.ai</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/alek_safar/status/1964383077792141390?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ§˜ã€…ãªç¨®é¡ã®æ™‚è¨ˆï¼ˆe.g., åè»¢ã€ãƒ•ã‚©ãƒ³ãƒˆã®é•ã„, invalidãªæ™‚åˆ»ã®å­˜åœ¨, å¤§ãã•, ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãªã©; p.2å‚ç…§ã®ã“ã¨)ã®æ™‚åˆ»ã‚’èª­ã¿å–ã‚Šï¼ˆã‚ã‚‹ã„ã¯validãªæ™‚åˆ»ã‹å¦ã‹ã‚’åˆ¤å®šã—)ã€èª­ã¿å–ã£ãŸæ™‚åˆ»ã«å¯¾ã—ã¦QAï¼ˆe.g., Xæ™‚é–“Yåˆ†Zç§’é€²ã‚ã‚‹ã€æˆ»ã—ãŸæ™‚åˆ»ã¯ï¼Ÿé•·é‡ã‚’30/60/90åº¦å‹•ã‹ã—ãŸæ™‚åˆ»ã¯ï¼Ÿã“ã®æ™‚åˆ»ãŒãƒ‹ãƒ¥ãƒ¼ãƒ¨ãƒ¼ã‚¯ã®æ™‚é–“ã ã¨ã—ãŸã‚‰ãƒ­ãƒ³ãƒ‰ãƒ³ã®æ™‚åˆ»ã¯ï¼Ÿ)ã‚’å®Ÿæ–½ã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€‚äººé–“ã®æ­£è§£ç‡ã¯89.1%ã«å¯¾ã—ã¦SoTAãƒ¢ãƒ‡ãƒ«ã§ã‚‚13.3%ç¨‹åº¦ã€‚contaminationã«é…æ…®ã—ã¦å…¨ã¦ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰ä½œæˆã•ã‚Œã€å…¨ä½“ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã¯privateãªã¾ã¾ã«ã—ã¦ã„ã‚‹ã¨ã®ã“ã¨ã€‚<br><img src="https://github.com/user-attachments/assets/aa2ca43c-97c9-49c3-a93b-d1897858d598" alt="image" loading="lazy"></p>
<p>ç¶šå ±:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/alek_safar/status/1972697598155706443?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>Qwen3-VL-235B-InstructãŒGPT-5 Chatè¶…ãˆ</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-09-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2695" target="_blank" rel="noopener noreferrer" class="title-link">FineVision: Open Data Is All You Need, Wiedmann+, Hugging Face, 2025.09</a>
<span class="snippet"><span>Comment</span><p>HF:


<a href="https://huggingface.co/datasets/HuggingFaceM4/FineVision" target="_blank" rel="noopener noreferrer">https://huggingface.co/datasets/HuggingFaceM4/FineVision</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/andimarafioti/status/1963610135328104945?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Video.html" target="_blank" rel="noopener noreferrer">#Video</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-09-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2680" target="_blank" rel="noopener noreferrer" class="title-link">ã€è«–æ–‡è§£èª¬ã€‘é«˜é€Ÿãƒ»é«˜å“è³ªãªç”Ÿæˆã‚’å®Ÿç¾ã™ã‚‹Flow Map Modelsï¼ˆPart 1: æ¦‚è¦ç·¨ï¼‰, Masato Ishii ï¼ˆSony AIï¼‰, 2025.09</a>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/WorldModels.html" target="_blank" rel="noopener noreferrer">#WorldModels</a>
<span class="issue_date">Issue Date: 2025-09-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2654" target="_blank" rel="noopener noreferrer" class="title-link">HunyuanWorld-Voyager: Technical Report, Tencent, 2025.09</a>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://3d-models.hunyuan.tencent.com/world/" target="_blank" rel="noopener noreferrer">https://3d-models.hunyuan.tencent.com/world/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/tencenthunyuan/status/1962741518797836708?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-09-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2652" target="_blank" rel="noopener noreferrer" class="title-link">August 2025 - China Open Source  Highlights, 2025.09</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/adinayakup/status/1962508234549329969?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-09-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2641" target="_blank" rel="noopener noreferrer" class="title-link">RLinf: Reinforcement Learning Infrastructure for Agentic AI, RLinf, 2025.09</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1962441512207491217?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Slide.html" target="_blank" rel="noopener noreferrer">#Slide</a>
<a class="button" href="articles/Chip.html" target="_blank" rel="noopener noreferrer">#Chip</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/Robotics.html" target="_blank" rel="noopener noreferrer">#Robotics</a>
<a class="button" href="articles/VisionLanguageActionModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageActionModel</a>
<a class="button" href="articles/EmbodiedAI.html" target="_blank" rel="noopener noreferrer">#EmbodiedAI</a>
<span class="issue_date">Issue Date: 2025-09-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2633" target="_blank" rel="noopener noreferrer" class="title-link">AIãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹æ¤œè¨ä¼š ç¬¬1å›äº‹å‹™å±€è³‡æ–™, çµŒæ¸ˆç”£æ¥­çœ, 2025.08</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gclue_akira/status/1962298561451958546?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Nvidiaã®æŠ•è³‡é¡ãŒæ–‡å­—é€šã‚Šæ¡é•ã„ã®5000å„„ãƒ‰ãƒ«</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2606" target="_blank" rel="noopener noreferrer" class="title-link">fastvlm-webgpu, Apple, 2025.08</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/fartashfg/status/1961441954157244448?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>pj page:


<a href="https://fastvlm.net" target="_blank" rel="noopener noreferrer">https://fastvlm.net</a>


</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/ProprietaryLLM.html" target="_blank" rel="noopener noreferrer">#ProprietaryLLM</a>
<a class="button" href="articles/Editing.html" target="_blank" rel="noopener noreferrer">#Editing</a>
<span class="issue_date">Issue Date: 2025-08-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2579" target="_blank" rel="noopener noreferrer" class="title-link">Introducing Gemini 2.5 Flash Image, our state-of-the-art image model, Google, 2025.08</a>
<span class="snippet"><span>Comment</span><p>nano banana</p>
<p>ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_philschmid/status/1961809165191397863?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¬ã‚¤ãƒ‰ã¨æˆ¦ç•¥:


<a href="https://ai.google.dev/gemini-api/docs/image-generation?hl=ja#prompt-guide" target="_blank" rel="noopener noreferrer">https://ai.google.dev/gemini-api/docs/image-generation?hl=ja#prompt-guide</a>


<br><br>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/beatinaniwa/status/1960911250344526264?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<span class="issue_date">Issue Date: 2025-08-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2563" target="_blank" rel="noopener noreferrer" class="title-link">Wan-S2V: Audio-Driven Cinematic Video Generation, Alibaba, 2025.08</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/alibaba_wan/status/1960350593660367303?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2312" target="_blank" rel="noopener noreferrer">Wan2.2, Alibaba Wan, 2025.07</a>
</p>
<p>image+Audio-to-video generation</p>
<p>Audioãƒ¢ãƒ€ãƒªãƒ†ã‚£: wav2vec+AudioEncoder<br>Visionãƒ¢ãƒ€ãƒªãƒ†ã‚£: 3D VAE Encoder<br>Textãƒ¢ãƒ€ãƒªãƒ†ã‚£: T5 Encoder<br>ãƒ¢ãƒ€ãƒªãƒ†ã‚£çµ±åˆ: DiT Block(ãŠãã‚‰ãT5 Encoderã®å‡ºåŠ›ã‚’ç”¨ã„ã¦promptæƒ…å ±ã‚’æ¡ä»¶ä»˜ã‘ï¼‰ã¨Audio Block?<br>3D VAE Decoderã§ãƒ‡ã‚³ãƒ¼ãƒ‰ã¨ã„ã†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼Ÿè©³ç´°ãŒæ›¸ã‹ã‚Œã¦ãŠã‚‰ãšã‚ˆãã‚ã‹ã‚‰ãªã„ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2561" target="_blank" rel="noopener noreferrer" class="title-link">MiniCPM-V-4_5, openbmb, 2025.08</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/adinayakup/status/1960292853453672886?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Editing.html" target="_blank" rel="noopener noreferrer">#Editing</a>
<span class="issue_date">Issue Date: 2025-08-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2484" target="_blank" rel="noopener noreferrer" class="title-link">Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing, Liu+, SIGGRAPH, 2025.07</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1957730005041197564?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>pj page:


<a href="http://geometrylearning.com/Sketch3DVE/" target="_blank" rel="noopener noreferrer">http://geometrylearning.com/Sketch3DVE/</a>


</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/Editing.html" target="_blank" rel="noopener noreferrer">#Editing</a>
<span class="issue_date">Issue Date: 2025-08-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2483" target="_blank" rel="noopener noreferrer" class="title-link">Qwen-Image-Edit, Qwen, 2025.05</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/adinayakup/status/1957503617931317618?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å…¬å¼ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/alibaba_qwen/status/1957500569029079083?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Imageã‚’å…¥åŠ›ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã§æ¡ä»¶ã¥ã‘ã‚‹ã“ã¨ã§ç·¨é›†ã§ãã‚‹OpenWeightãƒ¢ãƒ‡ãƒ«<br>&lt;img width="810" height="393" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/8c4ed7a1-1604-4365-bdbf-ef64ad8298ce"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/8c4ed7a1-1604-4365-bdbf-ef64ad8298ce"&lt;/a&gt;


/&gt;</p>
<p>å‚è€ƒ:25/08/20 ã¨ã‚Šã¾QwenImageEditã‚’è©¦ã™<br>


<a href="https://six-loganberry-ba7.notion.site/25-08-20-QwenImageEdit-255f7e7600e980f48e09cc7252ea1677" target="_blank" rel="noopener noreferrer">https://six-loganberry-ba7.notion.site/25-08-20-QwenImageEdit-255f7e7600e980f48e09cc7252ea1677</a>


<br><br>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/umiyuki_ai/status/1958308200333332849?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Image Edit Arenaã§ï¼’ä½:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/alibaba_qwen/status/1958725835818770748?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Self-SupervisedLearning.html" target="_blank" rel="noopener noreferrer">#Self-SupervisedLearning</a>
<a class="button" href="articles/Distillation.html" target="_blank" rel="noopener noreferrer">#Distillation</a>
<a class="button" href="articles/Regularization.html" target="_blank" rel="noopener noreferrer">#Regularization</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<a class="button" href="articles/Reference%20Collection.html" target="_blank" rel="noopener noreferrer">#Reference Collection</a>
<span class="issue_date">Issue Date: 2025-08-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2431" target="_blank" rel="noopener noreferrer" class="title-link">DINOv3: Self-supervised learning for vision at unprecedented scale, Meta, 2025.08</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1956067392846749723?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>paper:


<a href="https://arxiv.org/abs/2508.10104" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2508.10104</a>


<br><br>HF:


<a href="https://huggingface.co/docs/transformers/main/en/model_doc/dinov3" target="_blank" rel="noopener noreferrer">https://huggingface.co/docs/transformers/main/en/model_doc/dinov3</a>


</p>
<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1958285463313347071?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã‚µãƒãƒª:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1960840635289886958?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>v2:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1884" target="_blank" rel="noopener noreferrer">DINOv2: Learning Robust Visual Features without Supervision, Maxime Oquab+, TMLR'24</a>
</p>
<p>æœ¬æ—¥é…ä¿¡ã•ã‚ŒãŸå²¡é‡åŸæ°ã®ãƒ©ãƒ³ãƒã‚¿ã‚¤ãƒ ãƒˆãƒ¼ã‚¯ã«ã‚ˆã‚‹ã¨ã€å­¦ç¿’ãŒé€²ã‚“ã§ã„ãã¨å…¨éƒ¨ã®ç‰¹å¾´é‡ãŒä¼¼é€šã£ã¦ãã¦ã—ã¾ã†å•é¡ŒãŒã‚ã£ãŸãŒã€Gram Anchoringã¨å‘¼ã°ã‚Œã‚‹ã€å­¦ç¿’åˆæœŸæ™‚ç‚¹ã§ã®ãƒ‘ãƒƒãƒé–“ã®é¡ä¼¼åº¦åº¦è¡Œåˆ—ã‚’ä¿æŒã—ã¦ãŠãæ­£å‰‡åŒ–ã¨ã—ã¦æå¤±ã«åŠ ãˆã‚‹ã“ã¨ã§ã€ãã“ã‹ã‚‰é›¢ã‚Œã™ããªã„ã‚ˆã†ã«å­¦ç¿’ã™ã‚‹ã¨ã„ã£ãŸå·¥å¤«ã‚’å®Ÿæ–½ã—ã¦ã„ã‚‹ã¨ã®ã“ã¨ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/QuestionAnswering.html" target="_blank" rel="noopener noreferrer">#QuestionAnswering</a>
<a class="button" href="articles/ImageCaptioning.html" target="_blank" rel="noopener noreferrer">#ImageCaptioning</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/OCR.html" target="_blank" rel="noopener noreferrer">#OCR</a>
<span class="issue_date">Issue Date: 2025-08-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2414" target="_blank" rel="noopener noreferrer" class="title-link">NVIDIA Releases 3 Million Sample Dataset for OCR, Visual Question Answering, and Captioning Tasks, NVIDIA, 2025.08</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/nvidiaaidev/status/1955332008890208540?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Llama Nemotron VLM Dataset V1<br><br>VQA, OCRã®æ¯”ç‡ãŒå¤šã‚ã§ã€Imase Captioningã¯å°‘ãªã‚ã€‚<br><img src="https://github.com/user-attachments/assets/973af13e-50a8-4c8e-9260-64140792e444" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/Slide.html" target="_blank" rel="noopener noreferrer">#Slide</a>
<span class="issue_date">Issue Date: 2025-08-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2409" target="_blank" rel="noopener noreferrer" class="title-link">ç¬¬62å›åå¤å±‹CVãƒ»PRMLå‹‰å¼·ä¼šï¼šCVPR2025è«–æ–‡ç´¹ä»‹ ï¼ˆMambaOutï¼‰, Naoki Okamoto, 2025.08</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/yu4u/status/1955192808769532351?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å…ƒè«–æ–‡ã¯ä»¥ä¸‹:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2420" target="_blank" rel="noopener noreferrer">[Paper Note] MambaOut: Do We Really Need Mamba for Vision?, Weihao Yu+, arXiv'24</a>
</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Online/Interactive.html" target="_blank" rel="noopener noreferrer">#Online/Interactive</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/WorldModels.html" target="_blank" rel="noopener noreferrer">#WorldModels</a>
<span class="issue_date">Issue Date: 2025-08-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2369" target="_blank" rel="noopener noreferrer" class="title-link">Genie 3: A new frontier for world models, Google DeepMind, 2025.08</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/shanegjp/status/1952908595261259929?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>ãƒ©ã‚¤ãƒ–æ“ä½œãŒå¯èƒ½ãªä¸–ç•Œãƒ¢ãƒ‡ãƒ«<p>æ—¥æœ¬èªè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1953223065787351272?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒ‡ãƒ¢:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/umiyuki_ai/status/1954175128750686224?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>ã™ã”ã„ãªã‚</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<span class="issue_date">Issue Date: 2025-07-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2312" target="_blank" rel="noopener noreferrer" class="title-link">Wan2.2, Alibaba Wan, 2025.07</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/alibaba_wan/status/1949827662416937443?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>åˆã®MoEã«ã‚ˆã‚‹Open WeightãªVideo generationãƒ¢ãƒ‡ãƒ«ã§ã€ç›´æ¥çš„ã«æ˜ã‚‹ã•ã‚„ã€ã‚«ãƒ©ãƒ¼ã€ã‚«ãƒ¡ãƒ©ã®å‹•ããªã©ã‚’åˆ¶å¾¡ã§ãã€text to video, image to video, unified video generationã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹æ¨¡æ§˜</p>
<p>ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒšãƒ¼ãƒ‘ãƒ¼:<br>


<a href="https://arxiv.org/abs/2503.20314" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2503.20314</a>


</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Document.html" target="_blank" rel="noopener noreferrer">#Document</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/DocParser.html" target="_blank" rel="noopener noreferrer">#DocParser</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2294" target="_blank" rel="noopener noreferrer" class="title-link">LLM APIs Are Not Complete Document Parsers, Jerry Liu, 2025.07</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jerryjliu0/status/1948475176062255504?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<span class="issue_date">Issue Date: 2025-06-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2115" target="_blank" rel="noopener noreferrer" class="title-link">ERNIE 4.5 Series, ERNIE TEAM, 2025.06</a>
<span class="snippet"><span>Comment</span><p>Tech Report:


<a href="https://yiyan.baidu.com/blog/publication/ERNIE_Technical_Report.pdf" target="_blank" rel="noopener noreferrer">https://yiyan.baidu.com/blog/publication/ERNIE_Technical_Report.pdf</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/paddlepaddle/status/1939535276197744952?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1939576393098023188?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/Slide.html" target="_blank" rel="noopener noreferrer">#Slide</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<span class="issue_date">Issue Date: 2025-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2098" target="_blank" rel="noopener noreferrer" class="title-link">CVPR 2025 é€Ÿå ±, Kataoka+, 2025.06</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hirokatukataoka/status/1937815247923950079?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã™ã”ã„ã¾ã¨ã‚ã â€¦</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<span class="issue_date">Issue Date: 2025-06-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2075" target="_blank" rel="noopener noreferrer" class="title-link">Kimi-VL-A3B-Thinking-2506, moonshotai, 2025.06</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/reach_vb/status/1937159672932286950?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ§˜ã€…ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§SoTA(gpt4o, Qwen2.5-VL-7B)ã‚’é”æˆã—ãŸReasoning VLM</p>
<p>ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒšãƒ¼ãƒ‘ãƒ¼:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2200" target="_blank" rel="noopener noreferrer">[Paper Note] Kimi-VL Technical Report, Kimi Team+, arXiv'25</a>
</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/Video.html" target="_blank" rel="noopener noreferrer">#Video</a>
<span class="issue_date">Issue Date: 2025-06-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2034" target="_blank" rel="noopener noreferrer" class="title-link">V-JEPA 2, Meta, 2025.06</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/mervenoyann/status/1932814909722800196?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Physical Reasoning Leaderboardãªã‚‹ã‚‚ã®ã§ç¾åœ¨ãƒˆãƒƒãƒ—ãªæ¨¡æ§˜ã€‚<br><br>


<a href="https://huggingface.co/spaces/facebook/physical_reasoning_leaderboard" target="_blank" rel="noopener noreferrer">https://huggingface.co/spaces/facebook/physical_reasoning_leaderboard</a>


</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Slide.html" target="_blank" rel="noopener noreferrer">#Slide</a>
<span class="issue_date">Issue Date: 2025-05-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1987" target="_blank" rel="noopener noreferrer" class="title-link">ã€DLè¼ªèª­ä¼šã€‘ Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models, Deep Learning JP, 2025.05</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/kym384/status/1925852937835737569?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1986" target="_blank" rel="noopener noreferrer">Masked Diffusion Modelã®é€²å±•, Deep Learning JP, 2025.03</a>
 ã§Literatureã‚’ã–ã£ãã‚ŠæŠŠæ¡ã—ã¦ã‹ã‚‰ã“ã¡ã‚‰ã‚’èª­ã‚€ã®ãŒè‰¯ã•ãã†ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Slide.html" target="_blank" rel="noopener noreferrer">#Slide</a>
<span class="issue_date">Issue Date: 2025-05-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1986" target="_blank" rel="noopener noreferrer" class="title-link">Masked Diffusion Modelã®é€²å±•, Deep Learning JP, 2025.03</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/kym384/status/1925852884656099572?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã‚¹ãƒ©ã‚¤ãƒ‰ä¸­ã®ARã®ã‚ˆã†ã«KV CacheãŒä½¿ãˆãªã„å•é¡Œã«å¯¾å‡¦ã—ãŸç ”ç©¶ãŒ<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1984" target="_blank" rel="noopener noreferrer">dKV-Cache: The Cache for Diffusion Language Models, Xinyin Ma+, arXiv'25</a>
<br><br>ã“ã®è¾ºã¯dLLMãŒæœ‰æœ›ã§ã‚ã‚Œã°ã€ã©ã‚“ã©ã‚“é€²åŒ–ã—ã¦ã„ãã®ã ã‚ã†ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/AWS.html" target="_blank" rel="noopener noreferrer">#AWS</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/Japanese.html" target="_blank" rel="noopener noreferrer">#Japanese</a>
<span class="issue_date">Issue Date: 2025-05-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1976" target="_blank" rel="noopener noreferrer" class="title-link">Webã‚¹ã‚±ãƒ¼ãƒ«ã®æ—¥æœ¬èª-ç”»åƒã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ŒMOMIJIã€ã®æ§‹ç¯‰ _å·¨å¤§ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’AWSã§é«˜é€Ÿã«å‡¦ç†ã™ã‚‹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³, Turing ï¼ˆstudio_graphï¼‰, 2025.05</a>
<span class="snippet"><span>Comment</span><p>è²´é‡ãªVLMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹ç¯‰ãƒã‚¦ãƒã‚¦</p>
<p>é’å¡—ã‚Šã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã‚’å…·ä½“çš„ã«ã©ã†ã‚„ã£ã¦ã„ã‚‹ã®ã‹æ°—ã«ãªã‚‹</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/LLMAgent.html" target="_blank" rel="noopener noreferrer">#LLMAgent</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/ComputerUse.html" target="_blank" rel="noopener noreferrer">#ComputerUse</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-04-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1896" target="_blank" rel="noopener noreferrer" class="title-link">Introducing UI-TARS-1.5, ByteDance, 2025.04</a>
<span class="snippet"><span>GPT Summary</span>- UI-TARSã¯ã€ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’å…¥åŠ›ã¨ã—ã¦äººé–“ã®ã‚ˆã†ã«ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¡Œã†ãƒã‚¤ãƒ†ã‚£ãƒ–GUIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€å¾“æ¥ã®å•†æ¥­ãƒ¢ãƒ‡ãƒ«ã«ä¾å­˜ã›ãšã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚å®Ÿé¨“ã§ã¯ã€10ä»¥ä¸Šã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§SOTAæ€§èƒ½ã‚’é”æˆã—ã€ç‰¹ã«OSWorldã‚„AndroidWorldã§ä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã‚¹ã‚³ã‚¢ã‚’è¨˜éŒ²ã—ã¾ã—ãŸã€‚UI-TARSã¯ã€å¼·åŒ–ã•ã‚ŒãŸçŸ¥è¦šã€çµ±ä¸€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã€ã‚·ã‚¹ãƒ†ãƒ -2æ¨è«–ã€åå°„çš„ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒˆãƒ¬ãƒ¼ã‚¹ã«ã‚ˆã‚‹åå¾©ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãªã©ã®é©æ–°ã‚’å–ã‚Šå…¥ã‚Œã€æœ€å°é™ã®äººé–“ã®ä»‹å…¥ã§é©å¿œã—ç¶šã‘ã‚‹èƒ½åŠ›ã‚’æŒã£ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>paper:


<a href="https://arxiv.org/abs/2501.12326" target="_blank" rel="noopener noreferrer">https://arxiv.org/abs/2501.12326</a>


</p>
<p>è‰²ã€…ã¨æ›¸ã„ã¦ã‚ã‚‹ãŒã€ã–ã£ãã‚Šè¨€ã†ã¨ByteDanceã«ã‚ˆã‚‹ã€Imageã¨Textã‚’inputã¨ã—ã¦å—ã‘å–ã‚Šã€Textã‚’outputã™ã‚‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMã«ã‚ˆã‚‹Computer Use Agent (CUA)</p>
<p>é–¢é€£<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1794" target="_blank" rel="noopener noreferrer">OpenAI API ã§ã® Computer use ã®ä½¿ã„æ–¹, npaka, 2025.03</a>
</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_akhaliq/status/1912913195607663049?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2025-04-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1880" target="_blank" rel="noopener noreferrer" class="title-link">Large Vision Language Model ï¼ˆLVLMï¼‰ ã«é–¢ã™ã‚‹æœ€æ–°çŸ¥è¦‹ã¾ã¨ã‚ ï¼ˆPart 1ï¼‰, Daiki Shiono, 2024.11</a>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/Reference%20Collection.html" target="_blank" rel="noopener noreferrer">#Reference Collection</a>
<span class="issue_date">Issue Date: 2025-04-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1863" target="_blank" rel="noopener noreferrer" class="title-link">Llama 4 Series, Meta, 2025.04</a>
<span class="snippet"><span>Comment</span><p>Downloads:


<a href="https://www.llama.com/?utm_source=twitter&utm_medium=organic_social&utm_content=image&utm_campaign=llama4" target="_blank" rel="noopener noreferrer">https://www.llama.com/?utm_source=twitter&utm_medium=organic_social&utm_content=image&utm_campaign=llama4</a>


</p>
<p>Huggingface:<br>


<a href="https://huggingface.co/collections/meta-llama/llama-4-67f0c30d9fe03840bc9d0164" target="_blank" rel="noopener noreferrer">https://huggingface.co/collections/meta-llama/llama-4-67f0c30d9fe03840bc9d0164</a>


</p>
<p>è§£èª¬ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1908601269004230763?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Artificial Analysisã«ã‚ˆã‚‹æ€§èƒ½æ¤œè¨¼:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/artificialanlys/status/1908890796415414430?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>MaverickãŒGPT4oã¨åŒç­‰ã€ScoutãŒGPT4o-miniã¨åŒç­‰<br><br>Update:



<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/artificialanlys/status/1909624239747182989?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ€§èƒ½ã«é–¢ã—ã¦ä¸å¯è§£ãªç‚¹ãŒå¤šãã†ãªã®ã§æ§˜å­è¦‹ã‚’ã—ã¦ã‚‚è‰¯ã„ã‹ã‚‚ã€‚</p>
<p>æ€§èƒ½æ¤œè¨¼ï¼ˆMath-Perturb):



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/kaixuanhuang1/status/1909387970773234088?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ—¥æœ¬èªã«ã‚ã¾ã‚Šå¼·ããªã„ã¨ã„ã†æƒ…å ±ã‚‚<br>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gosrum/status/1909626761098494060?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã©ã†ã‚„ã‚‰vLLMã®Llama4ã®inferenceã«ãƒã‚°ãŒã‚ã£ãŸã‚„ã†ã§ã€vLLMã®Issue 16311ã«ã¦ã€Llama4ã®inferenceã«é–¢ã™ã‚‹ãƒã‚°ãŒä¿®æ­£ã•ã‚Œã€æ€§èƒ½ãŒå‘ä¸Šã—ãŸæ¨¡æ§˜ã€‚ã©ã®ãƒ™ãƒ³ãƒã‚’ä¿¡ã˜ãŸã‚‰è‰¯ã„ã‹ã¾ã‚‹ã§ã‚ã‹ã‚‰ã‚“ã€‚</p>
<p>2025.0413ç¾åœ¨ã®chatbot arenaã®ãƒ©ãƒ³ã‚¯ã¯ã€32ä½ã¨ãªã‚Šï¼ˆchatbot arenaå‘ã‘ã«tuningã•ã‚Œã¦ã„ãŸã§ã‚ã‚ã†ãƒ¢ãƒ‡ãƒ«ã¯2ä½ã ã£ãŸï¼‰GPT-4oãŒ29ä½ã§ã‚ã‚‹ã“ã¨ã‚’è€ƒæ…®ã™ã‚‹ã¨ä¸Šè¨˜ã®Artificial Intelligenceã®è©•ä¾¡ã¨ã‚‚å¤§ä½“ä¸€è‡´ã—ã¦ã„ã‚‹ã€‚<br><br>


<a href="https://lmarena.ai" target="_blank" rel="noopener noreferrer">https://lmarena.ai</a>


<br><br>é–¢é€£ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/tunguz/status/1911142310160855541?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<span class="issue_date">Issue Date: 2025-03-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1835" target="_blank" rel="noopener noreferrer" class="title-link">Qwen2.5-VL-32B-Instruct, Qwen Team, 2025.03</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/alibaba_qwen/status/1904227859616641534?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-03-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1830" target="_blank" rel="noopener noreferrer" class="title-link">Nemotron-H: A Family of Accurate, Efficient Hybrid Mamba-Transformer Models, Nvidia, 2025.03</a>
<span class="snippet"><span>Comment</span><p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1820" target="_blank" rel="noopener noreferrer">Hunyuan T1, Tencent, 2025.03</a>
</p>
<p>Transformerã®Self-attention Layerã‚’Mamba2 Layerã«ç½®æ›ã™ã‚‹ã“ã¨ã§ã€æ§˜ã€…ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§åŒç­‰ã®æ€§èƒ½ã€ã‚ã‚‹ã„ã¯ä¸Šå›ã‚‹æ€§èƒ½ã§3å€ç¨‹åº¦ã®Inference timeã®é«˜é€ŸåŒ–ã‚’ã—ã¦ã„ã‚‹ï¼ˆ65536 input, 1024 outputï¼‰ã€‚<br><br>56Bç¨‹åº¦ã®mediumã‚µã‚¤ã‚ºã®ãƒ¢ãƒ‡ãƒ«ã¨ã€8Bç¨‹åº¦ã®è»½é‡ãªãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦è¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹ã€‚ç‰¹ã«ã€8Bãƒ¢ãƒ‡ãƒ«ã§Mambaã¨Transformerã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã¨ã€é€šå¸¸ã®Transformerãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã—ã¦ã„ã‚‹ã€‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«15 Trillion Tokenã‚’åˆ©ç”¨ã—ã¦ãŠã‚Šã€ã“ã®ãƒ‡ãƒ¼ã‚¿é‡ã§ã®Apple to Appleã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£é–“ã®æ¯”è¼ƒã¯ã€ç¾çŠ¶ã§ã¯æœ€ã‚‚å¤§è¦æ¨¡ãªã‚‚ã®ã¨ã®ã“ã¨ã€‚æ€§èƒ½ã¯å¤šãã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã«ã—ã¦ã‚‚åŒç­‰ã€Commonsense Understandingã§ã¯ä¸Šå›ã£ã¦ã„ã‚‹ã€‚<br><br>ã¾ãŸã€å­¦ç¿’ã—ãŸNemotron-Hã‚’ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æŒã¤VLMã«ã¤ã„ã¦ã‚‚ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒè¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<span class="issue_date">Issue Date: 2025-03-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1809" target="_blank" rel="noopener noreferrer" class="title-link">SmolDocling-256M, IBM Research, 2025.03</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:


<a href="https://www.linkedin.com/posts/andimarafioti_we-just-dropped-%F0%9D%97%A6%F0%9D%97%BA%F0%9D%97%BC%F0%9D%97%B9%F0%9D%97%97%F0%9D%97%BC%F0%9D%97%B0%F0%9D%97%B9%F0%9D%97%B6%F0%9D%97%BB%F0%9D%97%B4-activity-7307415358427013121-wS8m?utm_source=share&utm_medium=member_ios&rcm=ACoAACzQvjwB2FeLVE3yukDiUYtr5J4k-6nlNG4" target="_blank" rel="noopener noreferrer">https://www.linkedin.com/posts/andimarafioti_we-just-dropped-%F0%9D%97%A6%F0%9D%97%BA%F0%9D%97%BC%F0%9D%97%B9%F0%9D%97%97%F0%9D%97%BC%F0%9D%97%B0%F0%9D%97%B9%F0%9D%97%B6%F0%9D%97%BB%F0%9D%97%B4-activity-7307415358427013121-wS8m?utm_source=share&utm_medium=member_ios&rcm=ACoAACzQvjwB2FeLVE3yukDiUYtr5J4k-6nlNG4</a>


</p>
<p>Apache-2.0ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã€‚è¨€èªã¯Englishã®ã¿ãªæ¨¡æ§˜</p>
<p>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªImage-To-Textãƒ¢ãƒ‡ãƒ«ã€‚ã‚µãƒ³ãƒ—ãƒ«ã¯ã“ã¡ã‚‰<br><img src="https://github.com/user-attachments/assets/d16ce5a9-4336-4daa-ab6f-94d67ae77c41" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/ProprietaryLLM.html" target="_blank" rel="noopener noreferrer">#ProprietaryLLM</a>
<span class="issue_date">Issue Date: 2025-03-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1808" target="_blank" rel="noopener noreferrer" class="title-link">ERNIE4.5_X1, Baidu, 2025.03</a>
<span class="snippet"><span>Comment</span><p>è§£èª¬ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/ai_for_success/status/1901149459826045223?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>- ERNIE4.5ã¯GPT4.5ã‚’ã•ã¾ã–ã¾ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ä¸Šå›ã‚Šã€ä¾¡æ ¼ãŒãªã‚“ã¨GPT4.5ã®1%<br>- X1ã¯ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªreasoningãƒ¢ãƒ‡ãƒ«ã§DeepSeek-R1ã¨åŒç­‰ã®æ€§èƒ½ã§åŠé¡<br><br>ã‚‰ã—ã„</p>
<p>ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯6æœˆ30æ—¥ã«ã‚ªãƒ¼ãƒ—ãƒ³ï¼ˆã‚¦ã‚§ã‚¤ãƒˆï¼Ÿï¼‰ã«ãªã‚‹ã¨ã‚¹ãƒ¬ãƒƒãƒ‰ã§è¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-03-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1807" target="_blank" rel="noopener noreferrer" class="title-link">sarashina2-vision-{8b, 14b}, SB Intuitions, 2025.03</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/sei_shinagawa/status/1901467733331701966?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>VLMã€‚Xã«æ•£è¦‹ã•ã‚Œã‚‹è©¦è¡Œä¾‹ã‚’è¦‹ã‚‹ã¨æ—¥æœ¬èªã®èª­ã¿å–ã‚Šæ€§èƒ½ã¯çµæ§‹é«˜ãã†ã«è¦‹ãˆã‚‹ã€‚</p>
<p>ãƒ¢ãƒ‡ãƒ«æ§‹æˆã€å­¦ç¿’ã®è©³ç´°ã€ãŠã‚ˆã³è©•ä¾¡:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/sbintuitions/status/1901472307421278604?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>LLMï¼ˆsarashina2ï¼‰, Vision Encoderï¼ˆQwen2-VLï¼‰, Projectorã®3ã¤ã§æ§‹æˆã•ã‚Œã¦ãŠã‚Šã€3æ®µéšã®å­¦ç¿’ã‚’è¸ã‚“ã§ã„ã‚‹ã€‚<br>æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦Projectorã®ã¿ã‚’å­¦ç¿’ã—Vision Encoderã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’å¯¾å¿œã¥ã‘ã‚‹ã€‚ç¶šã„ã¦ã€æ—¥æœ¬èªã‚’å«ã‚€ç”»åƒã‚„æ—¥æœ¬ç‰¹æœ‰ã®é¢¨æ™¯ãªã©ã‚’ã†ã¾ãæ‰±ãˆã‚‹ã‚ˆã†ã«ã€ã“ã‚Œã‚‰ã‚’å¤šãæ´»ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ï¼ˆå†…è£½æ—¥æœ¬èªOCRãƒ‡ãƒ¼ã‚¿ã€å›³è¡¨ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼‰ã‚’ç”¨ã„ã¦ã€Vision Encoderã¨Projectorã‚’å­¦ç¿’ã€‚æœ€å¾Œã«LLMã®Alignmentã‚’ã¨ã‚‹ãŸã‚ã«ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚¿ãƒ¼ã¨LLMã‚’å‰æ®µã®ãƒ‡ãƒ¼ã‚¿ã«åŠ ãˆã¦VQAãƒ‡ãƒ¼ã‚¿ï¼ˆå†…è£½åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ï¼‰ã‚„æ—¥æœ¬èªã®æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦å­¦ç¿’ã€‚</p>
<p>Projectorã‚„MMLLMã‚’å…·ä½“çš„ã«ã©ã®ã‚ˆã†ã«å­¦ç¿’ã™ã‚‹ã‹ã¯<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1225" target="_blank" rel="noopener noreferrer">MM-LLMs: Recent Advances in MultiModal Large Language Models, Duzhen Zhang+, N/A, ACL'24 Findings</a>
<br><br>ã‚’å‚ç…§ã®ã“ã¨ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<span class="issue_date">Issue Date: 2025-01-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1737" target="_blank" rel="noopener noreferrer" class="title-link">Janus-Series: Unified Multimodal Understanding and Generation Models, DeepSeek, 2025.01</a>
<span class="snippet"><span>Comment</span><p>DeepSeekã«ã‚ˆã‚‹æ–°ãŸãªUMMã€Janus-ProãŒæœ¬æ—¥ãƒªãƒªãƒ¼ã‚¹ã€‚MIT License</p>
<p>Janus-Proã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€‚<br><br>githubä¸Šã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å›³è§£ã‹ã‚‰å¼•ç”¨ã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ï¼ˆãƒ†ã‚­ã‚¹ãƒˆ+ç”»åƒï¼‰ã®ç†è§£ã«é–¢ã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§LLaVAè¶…ãˆã€‚GenEval, DPG Benchã¨å‘¼ã°ã‚Œã‚‹ç”»åƒç”Ÿæˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§DALL-E 3è¶…ãˆã€‚<br><img src="https://github.com/user-attachments/assets/39b51e99-723d-4105-a113-e4bfa847c69b" alt="image" loading="lazy"><br><br><br>ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆä¸­ã§ã®è©³ç´°ã‹ã‚‰å¼•ç”¨ã€‚ã©ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã‚‚åŸºæœ¬çš„ã«æœ€é«˜æ€§èƒ½ãªã‚ˆã†ã«è¦‹ãˆã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/4c1bd071-966f-4d51-99f4-e60fa2f36b0a" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/a0b22d6e-debb-420a-bf8d-fe8833583d09" alt="image" loading="lazy"><br><br>ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆ: 


<a href="https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf" target="_blank" rel="noopener noreferrer">https://github.com/deepseek-ai/Janus/blob/main/janus_pro_tech_report.pdf</a>


</p>
<p>ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2769" target="_blank" rel="noopener noreferrer">[Paper Note] GenEval: An Object-Focused Framework for Evaluating Text-to-Image   Alignment, Dhruba Ghosh+, NeurIPS'23</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2770" target="_blank" rel="noopener noreferrer">[Paper Note] ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment, Xiwei Hu+, arXiv'24</a>
</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<span class="issue_date">Issue Date: 2025-01-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1662" target="_blank" rel="noopener noreferrer" class="title-link">Killed by LLM, R0bk</a>
<span class="snippet"><span>Comment</span><p>Saturationã¨ãªã£ã¦ã„ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ã€æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’ã™ã§ã«æ¸¬å®šã§ããªããªã£ã¦ã—ã¾ã£ãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã®ã“ã¨ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/ProprietaryLLM.html" target="_blank" rel="noopener noreferrer">#ProprietaryLLM</a>
<span class="issue_date">Issue Date: 2025-01-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1633" target="_blank" rel="noopener noreferrer" class="title-link">2024-ai-timeline, reach-vb, 2025.01</a>
<span class="snippet"><span>Comment</span><p>æœˆåˆ¥ã§2024å¹´ã«ãƒªãƒªãƒ¼ã‚¹ã•ã‚ŒãŸä¸»è¦ãªLLMï¼ˆãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªLLMã‚‚å«ã‚€ï¼‰ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ãŒã¾ã¨ã‚ã‚‰ã‚Œã¦ã„ã‚‹ã€‚<br>API Onlyï¼ˆãƒ—ãƒ­ãƒ—ãƒ©ã‚¤ã‚¨ã‚¿ãƒªï¼‰ãªã®ã‹ã€OpenWeightãªã®ã‹ã‚‚ã‚¿ã‚°ä»˜ã‘ã•ã‚Œã¦ã„ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<a class="button" href="articles/MultiLingual.html" target="_blank" rel="noopener noreferrer">#MultiLingual</a>
<span class="issue_date">Issue Date: 2024-12-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1571" target="_blank" rel="noopener noreferrer" class="title-link">Introducing Amazon Nova, our new generation of foundation models, AWS, 2024.12</a>
<span class="snippet"><span>Comment</span><p>å‚è€ƒ:


<a href="https://qiita.com/ysit/items/8433d149dbaab702d526" target="_blank" rel="noopener noreferrer">https://qiita.com/ysit/items/8433d149dbaab702d526</a>


</p>
<p>ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆ: 


<a href="https://assets.amazon.science/9f/a3/ae41627f4ab2bde091f1ebc6b830/the-amazon-nova-family-of-models-technical-report-and-model-card.pdf" target="_blank" rel="noopener noreferrer">https://assets.amazon.science/9f/a3/ae41627f4ab2bde091f1ebc6b830/the-amazon-nova-family-of-models-technical-report-and-model-card.pdf</a>


</p>
<p>å¾Œã§å€‹ã€…ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’ã¾ã¨ã‚ãŸã„ã€‚<br><br>ã¾ã‚ã§ã‚‚ã–ã£ãã‚Šè¨€ã†ã¨ã€ä»–ã®proprietaryãƒ¢ãƒ‡ãƒ«ã¨ã‚‚ãŠãŠã‚€ã­åŒç­‰ã®æ€§èƒ½ã§ã™ã€ã¨ã„ã†æ„Ÿã˜ã«è¦‹ãˆã‚‹ã€‚å€‹ã€…ã®ã‚¿ã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã§è¦‹ã‚‹ã¨ã€å¾—æ„ãªã‚‚ã®ã¨ä¸å¾—æ„ãªã‚‚ã®ã¯ã‚ã‚Šãã†ã§ã¯ã‚ã‚‹ã€‚<br><br><img src="https://github.com/user-attachments/assets/c0c633d8-c64d-4a14-95cf-0d8b0d52a7f6" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/560f8c3e-65ff-4742-b7da-bc2b242dafcd" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/481a9635-128d-4931-a891-5f46d55b82bc" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/fc9b1bc0-b857-4a27-ad90-4940213c6ec6" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/a349b154-1844-41c2-84e3-7f981b1f6b72" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/a4381740-600c-402f-be0d-59ce60b7a562" alt="image" loading="lazy"><br><br>ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã¨ã‹ã‚‚ã€Proã¨GPT4oã‚’ãƒ‘ãƒƒã¨è¦‹ã§æ¯”è¼ƒã—ãŸæ„Ÿã˜ã€å„ªã‚Œã¦ã„ã‚‹ã‚ã‘ã§ã‚‚ãªã•ãã†ã€‚Liteã«å¯¾å¿œã™ã‚‹GPTã¯ãŠãã‚‰ãGPT4o-miniã ã¨æ€ã‚ã‚Œã‚‹ãŒã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã¯Liteã®æ–¹ãŒé«˜ãã†ã€‚<br><img src="https://github.com/user-attachments/assets/734ee26f-2f16-46e4-a6e8-f5f2f0d65be3" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/fe1768e8-b417-4b89-a0c4-f6dffa99cf11" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/6334ee92-e426-49f5-8e1f-050e0b77fcf2" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/5c9ec797-ef7a-43e1-8540-42ccab265208" alt="image" loading="lazy"><br><br>ï¼ˆç”»åƒã¯è«–æ–‡ä¸­ã‹ã‚‰ã‚¹ã‚¯ã‚·ãƒ§ã—å¼•ç”¨ï¼‰</p>
<p>ä¸‹è¨˜ãƒã‚¹ãƒˆã¯ç‹¬è‡ªã«è©•ä¾¡ã—ãŸçµæœã‚„ã€ã‚³ã‚¹ãƒˆã¨æ€§èƒ½ã®ãƒãƒ©ãƒ³ã‚¹ã«ã¤ã„ã¦è¨€åŠã—ã¦ã„ã‚‹ã€‚<br><br>- Proã¯GPT4oã®ã‚³ã‚¹ãƒˆã®ç´„1/3<br>- Pro, Lite, Flashã¯ã»ã‚Œãã‚Œã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«éå¸¸ã«å„ªã‚Œã¦ã„ã‚‹ï¼ˆQuality vs. Priceå‚ç…§ï¼‰<br><br>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/artificialanlys/status/1864023052818030814?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<span class="issue_date">Issue Date: 2024-11-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1551" target="_blank" rel="noopener noreferrer" class="title-link">ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ï¼šMamba, Vision Mamba ï¼ˆVimï¼‰, Hironobu Fujiyoshi, 2024.11</a>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="articles/Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<a class="button" href="articles/OCR.html" target="_blank" rel="noopener noreferrer">#OCR</a>
<span class="issue_date">Issue Date: 2024-11-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1547" target="_blank" rel="noopener noreferrer" class="title-link">YomiToku, Kotaro Kinoshita, 2024.11</a>
<span class="snippet"><span>Comment</span><p>ã„ã‚ã‚†ã‚‹AI-OCRã§ã€ç¸¦æ›¸ãã®èªè­˜ã‚‚å¯èƒ½ã§ã€è¡¨ãªã©ã®æ§‹é€ åŒ–ã•ã‚ŒãŸæƒ…å ±ã‚‚èªè­˜å¯èƒ½ã¨ã®ã“ã¨ã€‚<br>æ‰‹æ›¸ãã¯èªè­˜ã§ãã‚‹ã®ã ã‚ã†ã‹?<br>CC BY-NC-SA 4.0 </p>
<p>å…ƒãƒ„ã‚¤ãƒ¼ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/kinocoai/status/1861386062175838303?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Slide.html" target="_blank" rel="noopener noreferrer">#Slide</a>
<span class="issue_date">Issue Date: 2024-11-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1528" target="_blank" rel="noopener noreferrer" class="title-link">Large Vision Language Model ï¼ˆLVLMï¼‰ã«é–¢ã™ã‚‹çŸ¥è¦‹ã¾ã¨ã‚, Daiki Shiono, 2024.11</a>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/GenerativeAI.html" target="_blank" rel="noopener noreferrer">#GenerativeAI</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<span class="issue_date">Issue Date: 2024-10-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1444" target="_blank" rel="noopener noreferrer" class="title-link">MovieGen, Meta, 2024.10</a>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<span class="issue_date">Issue Date: 2024-09-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1437" target="_blank" rel="noopener noreferrer" class="title-link">ECCV2024-Papers-with-Code, 2024.09</a>
<span class="snippet"><span>Comment</span><p>ECCV2024ã®å…¨ä½“åƒã‚’æ¦‚è¦³ã™ã‚‹ã®ã«æœ‰ç”¨</p>
<p>ä»¥ä¸‹ã€Claude 3.5 Sonnetã«ç›®æ¬¡ã‚’å…¥åŠ›ã—ä¸€è¨€ã§å„é …ç›®ã‚’èª¬æ˜ã•ã›ãŸå†…å®¹ã€‚<br>hallucinationãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã®ã§å‚è€ƒç¨‹åº¦ã§ã€‚<br><br>--------------------<br>å„é …ç›®ã®æ¦‚è¦ã‚’ä¸€è¨€ã§èª¬æ˜ã„ãŸã—ã¾ã™ï¼š<br><br>1. 3DGS(Gaussian Splatting): 3Dç©ºé–“å†…ã®ã‚¬ã‚¦ã‚¹é–¢æ•°ã‚’ç”¨ã„ãŸæ–°ã—ã„3Dãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°æ‰‹æ³•ã€‚<br><br>2. Mamba / SSM: é•·æœŸä¾å­˜é–¢ä¿‚ã‚’åŠ¹ç‡çš„ã«å‡¦ç†ã™ã‚‹æ–°ã—ã„ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚<br><br>3. Avatars: ãƒ‡ã‚¸ã‚¿ãƒ«ç’°å¢ƒã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è¡¨ç¾ã™ã‚‹ä»®æƒ³ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã€‚<br><br>4. Backbone: ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®ä¸»è¦ãªç‰¹å¾´æŠ½å‡ºéƒ¨åˆ†ã€‚<br><br>5. CLIP: ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’åŒã˜ç©ºé–“ã«åŸ‹ã‚è¾¼ã‚€å¤§è¦æ¨¡ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã€‚<br><br>6. MAE: ç”»åƒã®ä¸€éƒ¨ã‚’éš ã—ã¦ã‹ã‚‰å†æ§‹ç¯‰ã™ã‚‹è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’æ‰‹æ³•ã€‚<br><br>7. Embodied AI: ç‰©ç†çš„ãªç’°å¢ƒã¨ç›¸äº’ä½œç”¨ã™ã‚‹ AI ã‚·ã‚¹ãƒ†ãƒ ã€‚<br><br>8. GAN: ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¨è­˜åˆ¥ãƒ¢ãƒ‡ãƒ«ã‚’ç«¶äº‰ã•ã›ã¦å­¦ç¿’ã™ã‚‹ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã€‚<br><br>9. GNN: ã‚°ãƒ©ãƒ•æ§‹é€ ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã®ç¥çµŒãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€‚<br><br>10. å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM): ãƒ†ã‚­ã‚¹ãƒˆã€ç”»åƒã€éŸ³å£°ãªã©è¤‡æ•°ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’æ‰±ã†å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã€‚<br><br>11. å¤§è¯­è¨€æ¨¡å‹(LLM): å¤§é‡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã•ã‚ŒãŸå¤§è¦æ¨¡ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã€‚<br><br>12. NAS: æœ€é©ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è‡ªå‹•æ¢ç´¢ã™ã‚‹æŠ€è¡“ã€‚<br><br>13. OCR: ç”»åƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’èªè­˜ã—ã€ãƒ‡ã‚¸ã‚¿ãƒ«ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹æŠ€è¡“ã€‚<br><br>14. NeRF: 3Dç©ºé–“ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¡¨ç¾ã™ã‚‹æ‰‹æ³•ã€‚<br><br>15. DETR: Transformerã‚’ç”¨ã„ãŸæ–°ã—ã„ç‰©ä½“æ¤œå‡ºã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€‚<br><br>16. Prompt: AIãƒ¢ãƒ‡ãƒ«ã«ä¸ãˆã‚‹æŒ‡ç¤ºã‚„æ–‡è„ˆã‚’è¨­å®šã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã€‚<br><br>17. æ‰©æ•£æ¨¡å‹(Diffusion Models): ãƒã‚¤ã‚ºã‚’å¾ã€…ã«é™¤å»ã—ã¦ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã€‚<br><br>18. ReID(é‡è¯†åˆ«): ç•°ãªã‚‹ç”»åƒã‚„æ˜ åƒé–“ã§åŒä¸€ã®äººç‰©ã‚„ç‰©ä½“ã‚’å†è­˜åˆ¥ã™ã‚‹æŠ€è¡“ã€‚<br><br>19. é•¿å°¾åˆ†å¸ƒ(Long-Tail): ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã§é »åº¦ã®ä½ã„ã‚¯ãƒ©ã‚¹ã‚„äº‹ä¾‹ã‚’æ‰±ã†å•é¡Œã€‚<br><br>20. Vision Transformer: ç”»åƒå‡¦ç†ã«Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’é©ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã€‚<br><br>21. è§†è§‰å’Œè¯­è¨€(Vision-Language): ç”»åƒã¨è¨€èªã‚’çµ„ã¿åˆã‚ã›ã¦å‡¦ç†ã™ã‚‹ã‚¿ã‚¹ã‚¯ã€‚<br><br>22. è‡ªç›‘ç£å­¦ä¹ (Self-supervised Learning): ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ‰ç”¨ãªè¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã€‚<br><br>23. æ•°æ®å¢å¼º(Data Augmentation): å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’äººå·¥çš„ã«å¢—ã‚„ã™æŠ€è¡“ã€‚<br><br>24. ç›®æ ‡æ£€æµ‹(Object Detection): ç”»åƒå†…ã®ç‰©ä½“ã®ä½ç½®ã¨ç¨®é¡ã‚’ç‰¹å®šã™ã‚‹æŠ€è¡“ã€‚<br><br>25. å¼‚å¸¸æ£€æµ‹(Anomaly Detection): é€šå¸¸ã¨ã¯ç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚„ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºã™ã‚‹æŠ€è¡“ã€‚<br><br>26. ç›®æ ‡è·Ÿè¸ª(Visual Tracking): æ˜ åƒå†…ã®ç‰©ä½“ã®å‹•ãã‚’è¿½è·¡ã™ã‚‹æŠ€è¡“ã€‚<br><br>27. è¯­ä¹‰åˆ†å‰²(Semantic Segmentation): ç”»åƒå†…ã®å„ãƒ”ã‚¯ã‚»ãƒ«ã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã™ã‚‹æŠ€è¡“ã€‚<br><br>28. å®ä¾‹åˆ†å‰²(Instance Segmentation): ç”»åƒå†…ã®å€‹ã€…ã®ç‰©ä½“ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆ†å‰²ã™ã‚‹æŠ€è¡“ã€‚<br><br>29. å…¨æ™¯åˆ†å‰²(Panoptic Segmentation): æ„å‘³åˆ†å‰²ã¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åˆ†å‰²ã‚’çµ„ã¿åˆã‚ã›ãŸæŠ€è¡“ã€‚<br><br>30. åŒ»å­¦å›¾åƒ(Medical Image): åŒ»ç™‚ç›®çš„ã§æ’®å½±ã•ã‚ŒãŸç”»åƒã€‚<br><br>31. åŒ»å­¦å›¾åƒåˆ†å‰²(Medical Image Segmentation): åŒ»ç™‚ç”»åƒå†…ã®è‡“å™¨ã‚„ç—…å¤‰éƒ¨ä½ã‚’åˆ†å‰²ã™ã‚‹æŠ€è¡“ã€‚<br><br>32. è§†é¢‘ç›®æ ‡åˆ†å‰²(Video Object Segmentation): å‹•ç”»å†…ã®ç‰©ä½“ã‚’è¿½è·¡ã—åˆ†å‰²ã™ã‚‹æŠ€è¡“ã€‚<br><br>33. è§†é¢‘å®ä¾‹åˆ†å‰²(Video Instance Segmentation): å‹•ç”»å†…ã®å€‹ã€…ã®ç‰©ä½“ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆ†å‰²ã™ã‚‹æŠ€è¡“ã€‚<br><br>34. å‚è€ƒå›¾åƒåˆ†å‰²(Referring Image Segmentation): è¨€èªè¨˜è¿°ã«åŸºã¥ã„ã¦ç”»åƒå†…ã®ç‰©ä½“ã‚’åˆ†å‰²ã™ã‚‹æŠ€è¡“ã€‚<br><br>35. å›¾åƒæŠ å›¾(Image Matting): ç”»åƒã‹ã‚‰å‰æ™¯ã‚’ç²¾å¯†ã«æŠ½å‡ºã™ã‚‹æŠ€è¡“ã€‚<br><br>36. å›¾åƒç¼–è¾‘(Image Editing): ç”»åƒã®å†…å®¹ã‚’å¤‰æ›´ã¾ãŸã¯æ“ä½œã™ã‚‹æŠ€è¡“ã€‚<br><br>37. Low-level Vision: ç”»åƒã®ä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´ã‚„å‡¦ç†ã‚’æ‰±ã†åˆ†é‡ã€‚<br><br>38. è¶…åˆ†è¾¨ç‡(Super-Resolution): ä½è§£åƒåº¦ç”»åƒã‹ã‚‰é«˜è§£åƒåº¦ç”»åƒã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã€‚<br><br>39. å»å™ª(Denoising): ç”»åƒã‹ã‚‰ãƒã‚¤ã‚ºã‚’é™¤å»ã™ã‚‹æŠ€è¡“ã€‚<br><br>40. å»æ¨¡ç³Š(Deblur): ã¼ã‘ãŸç”»åƒã‚’ã‚·ãƒ£ãƒ¼ãƒ—ã«ã™ã‚‹æŠ€è¡“ã€‚<br><br>41. è‡ªåŠ¨é©¾é©¶(Autonomous Driving): äººé–“ã®æ“ä½œãªã—ã§è»Šä¸¡ã‚’åˆ¶å¾¡ã™ã‚‹æŠ€è¡“ã€‚<br><br>42. 3Dç‚¹äº‘(3D Point Cloud): 3Dç©ºé–“å†…ã®ç‚¹ã®é›†åˆã§ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚„ç’°å¢ƒã‚’è¡¨ç¾ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å½¢å¼ã€‚<br><br>43. 3Dç›®æ ‡æ£€æµ‹(3D Object Detection): 3Dç©ºé–“å†…ã®ç‰©ä½“ã®ä½ç½®ã¨ç¨®é¡ã‚’ç‰¹å®šã™ã‚‹æŠ€è¡“ã€‚<br><br>44. 3Dè¯­ä¹‰åˆ†å‰²(3D Semantic Segmentation): 3Dãƒ‡ãƒ¼ã‚¿ã®å„ç‚¹ã‚’ã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã™ã‚‹æŠ€è¡“ã€‚<br><br>45. 3Dç›®æ ‡è·Ÿè¸ª(3D Object Tracking): 3Dç©ºé–“å†…ã®ç‰©ä½“ã®å‹•ãã‚’è¿½è·¡ã™ã‚‹æŠ€è¡“ã€‚<br><br>46. 3Dè¯­ä¹‰åœºæ™¯è¡¥å…¨(3D Semantic Scene Completion): éƒ¨åˆ†çš„ãª3Dãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å®Œå…¨ãª3Dã‚·ãƒ¼ãƒ³ã‚’æ¨å®šã™ã‚‹æŠ€è¡“ã€‚<br><br>47. 3Dé…å‡†(3D Registration): è¤‡æ•°ã®3Dãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ•´åˆ—ã•ã›ã‚‹æŠ€è¡“ã€‚<br><br>48. 3Däººä½“å§¿æ€ä¼°è®¡(3D Human Pose Estimation): 3Dç©ºé–“å†…ã®äººä½“ã®å§¿å‹¢ã‚’æ¨å®šã™ã‚‹æŠ€è¡“ã€‚<br><br>49. 3Däººä½“Meshä¼°è®¡(3D Human Mesh Estimation): 3Däººä½“ãƒ¡ãƒƒã‚·ãƒ¥ãƒ¢ãƒ‡ãƒ«ã‚’æ¨å®šã™ã‚‹æŠ€è¡“ã€‚<br><br>50. å›¾åƒç”Ÿæˆ(Image Generation): AIã‚’ç”¨ã„ã¦æ–°ã—ã„ç”»åƒã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã€‚<br><br>51. è§†é¢‘ç”Ÿæˆ(Video Generation): AIã‚’ç”¨ã„ã¦æ–°ã—ã„å‹•ç”»ã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã€‚<br><br>52. 3Dç”Ÿæˆ(3D Generation): AIã‚’ç”¨ã„ã¦æ–°ã—ã„3Dãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã€‚<br><br>53. è§†é¢‘ç†è§£(Video Understanding): å‹•ç”»ã®å†…å®¹ã‚’è§£æã—ç†è§£ã™ã‚‹æŠ€è¡“ã€‚<br><br>54. è¡Œä¸ºè¯†åˆ«(Action Recognition): å‹•ç”»å†…ã®äººç‰©ã®è¡Œå‹•ã‚’è­˜åˆ¥ã™ã‚‹æŠ€è¡“ã€‚<br><br>55. è¡Œä¸ºæ£€æµ‹(Action Detection): å‹•ç”»å†…ã®ç‰¹å®šã®è¡Œå‹•ã‚’æ¤œå‡ºã—ä½ç½®ç‰¹å®šã™ã‚‹æŠ€è¡“ã€‚<br><br>56. æ–‡æœ¬æ£€æµ‹(Text Detection): ç”»åƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã®ä½ç½®ã‚’æ¤œå‡ºã™ã‚‹æŠ€è¡“ã€‚<br><br>57. çŸ¥è¯†è’¸é¦(Knowledge Distillation): å¤§ããªãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã‚’å°ã•ãªãƒ¢ãƒ‡ãƒ«ã«è»¢ç§»ã™ã‚‹æŠ€è¡“ã€‚<br><br>58. æ¨¡å‹å‰ªæ(Model Pruning): ãƒ¢ãƒ‡ãƒ«ã®é‡è¦ã§ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‰Šé™¤ã—ã¦è»½é‡åŒ–ã™ã‚‹æŠ€è¡“ã€‚<br><br>59. å›¾åƒå‹ç¼©(Image Compression): ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’åŠ¹ç‡çš„ã«åœ§ç¸®ã™ã‚‹æŠ€è¡“ã€‚<br><br>60. ä¸‰ç»´é‡å»º(3D Reconstruction): 2Dç”»åƒã‹ã‚‰3Dãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹æŠ€è¡“ã€‚<br><br>61. æ·±åº¦ä¼°è®¡(Depth Estimation): 2Dç”»åƒã‹ã‚‰å¥¥è¡Œãæƒ…å ±ã‚’æ¨å®šã™ã‚‹æŠ€è¡“ã€‚<br><br>62. è½¨è¿¹é¢„æµ‹(Trajectory Prediction): ç‰©ä½“ã‚„äººã®å°†æ¥ã®å‹•ãã‚’äºˆæ¸¬ã™ã‚‹æŠ€è¡“ã€‚<br><br>63. è½¦é“çº¿æ£€æµ‹(Lane Detection): é“è·¯ä¸Šã®è»Šç·šã‚’æ¤œå‡ºã™ã‚‹æŠ€è¡“ã€‚<br><br>64. å›¾åƒæè¿°(Image Captioning): ç”»åƒã®å†…å®¹ã‚’è‡ªç„¶è¨€èªã§èª¬æ˜ã™ã‚‹æŠ€è¡“ã€‚<br><br>65. è§†è§‰é—®ç­”(Visual Question Answering): ç”»åƒã«é–¢ã™ã‚‹è³ªå•ã«ç­”ãˆã‚‹æŠ€è¡“ã€‚<br><br>66. æ‰‹è¯­è¯†åˆ«(Sign Language Recognition): æ‰‹è©±ã®ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã‚’èªè­˜ã—è§£é‡ˆã™ã‚‹æŠ€è¡“ã€‚<br><br>67. è§†é¢‘é¢„æµ‹(Video Prediction): å‹•ç”»ã®å°†æ¥ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’äºˆæ¸¬ã™ã‚‹æŠ€è¡“ã€‚<br><br>68. æ–°è§†ç‚¹åˆæˆ(Novel View Synthesis): æ—¢å­˜ã®ç”»åƒã‹ã‚‰æ–°ã—ã„è¦–ç‚¹ã®ç”»åƒã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã€‚<br><br>69. Zero-Shot Learning(é›¶æ ·æœ¬å­¦ä¹ ): å­¦ç¿’æ™‚ã«è¦‹ãŸã“ã¨ã®ãªã„ã‚¯ãƒ©ã‚¹ã‚’èªè­˜ã™ã‚‹å­¦ç¿’æ‰‹æ³•ã€‚<br><br>70. ç«‹ä½“åŒ¹é…(Stereo Matching): ã‚¹ãƒ†ãƒ¬ã‚ªç”»åƒã‹ã‚‰æ·±åº¦æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹æŠ€è¡“ã€‚<br><br>71. ç‰¹å¾åŒ¹é…(Feature Matching): ç•°ãªã‚‹ç”»åƒé–“ã§å¯¾å¿œã™ã‚‹ç‰¹å¾´ç‚¹ã‚’è¦‹ã¤ã‘ã‚‹æŠ€è¡“ã€‚<br><br>72. åœºæ™¯å›¾ç”Ÿæˆ(Scene Graph Generation): ç”»åƒå†…ã®ç‰©ä½“é–“ã®é–¢ä¿‚ã‚’æ§‹é€ åŒ–ã—ã¦è¡¨ç¾ã™ã‚‹æŠ€è¡“ã€‚<br><br>73. è®¡æ•°(Counting): ç”»åƒå†…ã®ç‰¹å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®æ•°ã‚’æ•°ãˆã‚‹æŠ€è¡“ã€‚<br><br>74. éšå¼ç¥ç»è¡¨ç¤º(Implicit Neural Representations): é€£ç¶šé–¢æ•°ã¨ã—ã¦ã‚·ãƒ¼ãƒ³ã‚„å½¢çŠ¶ã‚’è¡¨ç¾ã™ã‚‹æ‰‹æ³•ã€‚<br><br>75. å›¾åƒè´¨é‡è¯„ä»·(Image Quality Assessment): ç”»åƒã®å“è³ªã‚’å®¢è¦³çš„ã«è©•ä¾¡ã™ã‚‹æŠ€è¡“ã€‚<br><br>76. è§†é¢‘è´¨é‡è¯„ä»·(Video Quality Assessment): å‹•ç”»ã®å“è³ªã‚’å®¢è¦³çš„ã«è©•ä¾¡ã™ã‚‹æŠ€è¡“ã€‚<br><br>77. æ•°æ®é›†(Datasets): æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚„è©•ä¾¡ã«ä½¿ç”¨ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã®é›†åˆã€‚â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹</p>
<p>ç¶šã„ã¦ã€å„é …ç›®ã‚’ã‚‚ã†å°‘ã—è©³ç´°ã«èª¬æ˜ã™ã‚‹ã‚ˆã†ã«ç”Ÿæˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆ<br><br>---------<br>ã¯ã„ã€å„é …ç›®ã«ã¤ã„ã¦ã‚ˆã‚Šè©³ç´°ãªèª¬æ˜ã‚’æä¾›ã„ãŸã—ã¾ã™ã€‚<br><br>1. 3DGS(Gaussian Splatting):<br>3Dç©ºé–“å†…ã«ã‚¬ã‚¦ã‚¹é–¢æ•°ã‚’é…ç½®ã—ã€ã“ã‚Œã‚‰ã‚’2Dç”»åƒã«æŠ•å½±ã™ã‚‹ã“ã¨ã§3Dã‚·ãƒ¼ãƒ³ã‚’è¡¨ç¾ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚å¾“æ¥ã®NeRFã‚ˆã‚Šã‚‚é«˜é€Ÿã§ã‚¯ã‚ªãƒªãƒ†ã‚£ã®é«˜ã„3Dãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚’å®Ÿç¾ã—ã¾ã™ã€‚<br><br>2. Mamba / SSM:<br>State Space Model (SSM)ã«åŸºã¥ãæ–°ã—ã„ç³»åˆ—ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚Transformerã¨æ¯”è¼ƒã—ã¦é•·ã„ç³»åˆ—ã‚’åŠ¹ç‡çš„ã«å‡¦ç†ã§ãã€æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã§é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚<br><br>3. Avatars:<br>ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ç©ºé–“ã‚„ã‚²ãƒ¼ãƒ ã€VRç’°å¢ƒãªã©ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’è¡¨ç¾ã™ã‚‹ä»®æƒ³ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã§ã™ã€‚æœ€è¿‘ã§ã¯ã€AIã‚’ç”¨ã„ã¦ã‚ˆã‚Šãƒªã‚¢ãƒ«ã§è‡ªç„¶ãªã‚¢ãƒã‚¿ãƒ¼ã®ç”Ÿæˆã‚„åˆ¶å¾¡ãŒå¯èƒ½ã«ãªã£ã¦ã„ã¾ã™ã€‚<br><br>4. Backbone:<br>æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ä¸»è¦ãªç‰¹å¾´æŠ½å‡ºéƒ¨åˆ†ã‚’æŒ‡ã—ã¾ã™ã€‚ResNetã€VGGã€EfficientNetãªã©ãŒä»£è¡¨çš„ã§ã€æ§˜ã€…ãªã‚¿ã‚¹ã‚¯ã®åŸºç¤ã¨ãªã‚‹é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚<br><br>5. CLIP:<br>OpenAIãŒé–‹ç™ºã—ãŸå¤§è¦æ¨¡ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã€ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’åŒã˜ç‰¹å¾´ç©ºé–“ã«åŸ‹ã‚è¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æŸ”è»Ÿãªç”»åƒæ¤œç´¢ã‚„åˆ†é¡ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚<br><br>6. MAE (Masked Autoencoder):<br>ç”»åƒã®ä¸€éƒ¨ã‚’ãƒã‚¹ã‚¯ã—ã€ãã‚Œã‚’å†æ§‹ç¯‰ã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚’é€šã˜ã¦è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã‚’è¡Œã†æ‰‹æ³•ã§ã™ã€‚äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚<br><br>7. Embodied AI:<br>ç‰©ç†çš„ãªç’°å¢ƒã¨ç›´æ¥ç›¸äº’ä½œç”¨ã™ã‚‹AIã‚·ã‚¹ãƒ†ãƒ ã‚’æŒ‡ã—ã¾ã™ã€‚ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã‚„è‡ªå‹•é‹è»¢ãªã©ã€å®Ÿä¸–ç•Œã§ã®ã‚¿ã‚¹ã‚¯é‚è¡Œã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚<br><br>8. GAN (Generative Adversarial Networks):<br>ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¨è­˜åˆ¥ãƒ¢ãƒ‡ãƒ«ã‚’ç«¶äº‰ã•ã›ã‚‹ã“ã¨ã§å­¦ç¿’ã‚’è¡Œã†ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚é«˜å“è³ªãªç”»åƒç”Ÿæˆãªã©ã€æ§˜ã€…ãªåˆ†é‡ã§å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>9. GNN (Graph Neural Networks):<br>ã‚°ãƒ©ãƒ•æ§‹é€ ã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã™ã‚‹ãŸã‚ã®ç¥çµŒãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆ†æã‚„åˆ†å­æ§‹é€ äºˆæ¸¬ãªã©ã€é–¢ä¿‚æ€§ã®ã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã«é©ã—ã¦ã„ã¾ã™ã€‚<br><br>10. å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹(MLLM):<br>ãƒ†ã‚­ã‚¹ãƒˆã ã‘ã§ãªãã€ç”»åƒã€éŸ³å£°ã€å‹•ç”»ãªã©ã®è¤‡æ•°ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’ç†è§£ã—å‡¦ç†ã§ãã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ã‚ˆã‚Šè±Šã‹ãªã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚„ç†è§£ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚<br><br>11. å¤§è¯­è¨€æ¨¡å‹(LLM):<br>GPT-3ã‚„LLaMAãªã©ã€å¤§é‡ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã•ã‚ŒãŸå·¨å¤§ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚è‡ªç„¶è¨€èªå‡¦ç†ã®å¤šãã®ã‚¿ã‚¹ã‚¯ã§é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚<br><br>12. NAS (Neural Architecture Search):<br>æ©Ÿæ¢°å­¦ç¿’ã‚’ç”¨ã„ã¦æœ€é©ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ§‹é€ ã‚’è‡ªå‹•çš„ã«æ¢ç´¢ã™ã‚‹æŠ€è¡“ã§ã™ã€‚äººæ‰‹ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«è¨­è¨ˆã®åŠ´åŠ›ã‚’è»½æ¸›ã—ã€ã‚ˆã‚ŠåŠ¹ç‡çš„ãªãƒ¢ãƒ‡ãƒ«ã®ç™ºè¦‹ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚<br><br>13. OCR (Optical Character Recognition):<br>ç”»åƒå†…ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’èªè­˜ã—ã€æ©Ÿæ¢°å¯èª­ãªãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹æŠ€è¡“ã§ã™ã€‚æ–‡æ›¸ã®ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã‚„è‡ªå‹•ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ãªã©ã«åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>14. NeRF (Neural Radiance Fields):<br>3Dç©ºé–“ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¡¨ç¾ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚å°‘æ•°ã®2Dç”»åƒã‹ã‚‰é«˜å“è³ªãª3Dã‚·ãƒ¼ãƒ³ã®å†æ§‹ç¯‰ã¨æ–°è¦–ç‚¹ã®åˆæˆãŒå¯èƒ½ã§ã™ã€‚<br><br>15. DETR (DEtection TRansformer):<br>Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç‰©ä½“æ¤œå‡ºã‚¿ã‚¹ã‚¯ã«é©ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚å¾“æ¥ã®æ‰‹æ³•ã¨æ¯”ã¹ã¦ã‚·ãƒ³ãƒ—ãƒ«ã§ã‚ã‚ŠãªãŒã‚‰é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚<br><br>16. Prompt:<br>AIãƒ¢ãƒ‡ãƒ«ã€ç‰¹ã«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«ä¸ãˆã‚‹æŒ‡ç¤ºã‚„æ–‡è„ˆã‚’è¨­å®šã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚é©åˆ‡ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­è¨ˆã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’åˆ¶å¾¡ã—ã€æœ›ã¾ã—ã„çµæœã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚<br><br>17. æ‰©æ•£æ¨¡å‹(Diffusion Models):<br>ãƒã‚¤ã‚ºã‚’å¾ã€…ã«é™¤å»ã—ãªãŒã‚‰ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚DALL-E 2ã‚„Stable Diffusionãªã©ã€é«˜å“è³ªãªç”»åƒç”Ÿæˆã§æ³¨ç›®ã‚’é›†ã‚ã¦ã„ã¾ã™ã€‚<br><br>18. ReID (é‡è¯†åˆ«):<br>ç•°ãªã‚‹ç”»åƒã‚„æ˜ åƒé–“ã§åŒä¸€ã®äººç‰©ã‚„ç‰©ä½“ã‚’å†è­˜åˆ¥ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ç›£è¦–ã‚«ãƒ¡ãƒ©ã‚·ã‚¹ãƒ†ãƒ ã‚„é¡§å®¢è¿½è·¡ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>19. é•¿å°¾åˆ†å¸ƒ(Long-Tail):<br>ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã§é »åº¦ã®ä½ã„ã‚¯ãƒ©ã‚¹ã‚„äº‹ä¾‹ã‚’æ‰±ã†å•é¡Œã§ã™ã€‚ç¾å®Ÿä¸–ç•Œã®ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã«å¯¾å¿œã™ã‚‹ãŸã‚ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å…¬å¹³æ€§ã¨æ±åŒ–æ€§èƒ½ã®å‘ä¸ŠãŒèª²é¡Œã¨ãªã£ã¦ã„ã¾ã™ã€‚<br><br>20. Vision Transformer:<br>è‡ªç„¶è¨€èªå‡¦ç†ã§æˆåŠŸã‚’åã‚ãŸTransformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç”»åƒå‡¦ç†ã«é©ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚CNNã¨æ¯”è¼ƒã—ã¦ã€å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å­¦ç¿’æ™‚ã«é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚<br><br>21. è§†è§‰å’Œè¯­è¨€(Vision-Language):<br>ç”»åƒã¨è¨€èªã‚’çµ„ã¿åˆã‚ã›ã¦å‡¦ç†ã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚„ç ”ç©¶åˆ†é‡ã§ã™ã€‚ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆã€è¦–è¦šçš„è³ªå•å¿œç­”ã€ç”»åƒ-ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ãªã©ãŒå«ã¾ã‚Œã¾ã™ã€‚<br><br>22. è‡ªç›‘ç£å­¦ä¹ (Self-supervised Learning):<br>å¤§é‡ã®ãƒ©ãƒ™ãƒ«ãªã—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ‰ç”¨ãªç‰¹å¾´è¡¨ç¾ã‚’å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã«åºƒãä½¿ç”¨ã•ã‚Œã€å°‘é‡ã®ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã§ã® fine-tuning ã§é«˜ã„æ€§èƒ½ã‚’å®Ÿç¾ã—ã¾ã™ã€‚<br><br>23. æ•°æ®å¢å¼º(Data Augmentation):<br>æ—¢å­˜ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å¤‰å½¢ã‚„å¤‰æ›´ã‚’åŠ ãˆã¦äººå·¥çš„ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ‹¡å¼µã™ã‚‹æŠ€è¡“ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®æ±åŒ–æ€§èƒ½å‘ä¸Šã‚„ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã®æŠ‘åˆ¶ã«åŠ¹æœãŒã‚ã‚Šã¾ã™ã€‚<br><br>24. ç›®æ ‡æ£€æµ‹(Object Detection):<br>ç”»åƒå†…ã®ç‰©ä½“ã®ä½ç½®ã¨ç¨®é¡ã‚’ç‰¹å®šã™ã‚‹æŠ€è¡“ã§ã™ã€‚çŸ©å½¢ã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã§ç‰©ä½“ã®ä½ç½®ã‚’ç¤ºã—ã€å„ç‰©ä½“ã®ã‚¯ãƒ©ã‚¹ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚è‡ªå‹•é‹è»¢ã‚„ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ãªã©ã§åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>25. å¼‚å¸¸æ£€æµ‹(Anomaly Detection):<br>ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®é€šå¸¸ã¨ã¯ç•°ãªã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚„ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆã‚’æ¤œå‡ºã™ã‚‹æŠ€è¡“ã§ã™ã€‚ä¸æ­£æ¤œçŸ¥ã€ç”£æ¥­ç”¨æ©Ÿå™¨ã®æ•…éšœäºˆæ¸¬ã€åŒ»ç™‚è¨ºæ–­ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>26. ç›®æ ‡è·Ÿè¸ª(Visual Tracking):<br>å‹•ç”»ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã§ç‰©ä½“ã®å‹•ãã‚’è¿½è·¡ã™ã‚‹æŠ€è¡“ã§ã™ã€‚è‡ªå‹•é‹è»¢ã€ã‚¹ãƒãƒ¼ãƒ„åˆ†æã€ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ãªã©ã€æ§˜ã€…ãªåˆ†é‡ã§æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>27. è¯­ä¹‰åˆ†å‰²(Semantic Segmentation):<br>ç”»åƒå†…ã®å„ãƒ”ã‚¯ã‚»ãƒ«ã‚’ã‚ã‚‰ã‹ã˜ã‚å®šç¾©ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã™ã‚‹æŠ€è¡“ã§ã™ã€‚è‡ªå‹•é‹è»¢ã«ãŠã‘ã‚‹é“è·¯ç’°å¢ƒã®ç†è§£ã‚„åŒ»ç™‚ç”»åƒè§£æãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>28. å®ä¾‹åˆ†å‰²(Instance Segmentation):<br>ç”»åƒå†…ã®å€‹ã€…ã®ç‰©ä½“ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åˆ†å‰²ã—ã€ãã‚Œãã‚Œã«å›ºæœ‰ã®ãƒ©ãƒ™ãƒ«ã‚’ä»˜ä¸ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ç‰©ä½“æ¤œå‡ºã¨æ„å‘³åˆ†å‰²ã‚’çµ„ã¿åˆã‚ã›ãŸã‚¿ã‚¹ã‚¯ã¨è¨€ãˆã¾ã™ã€‚<br><br>29. å…¨æ™¯åˆ†å‰²(Panoptic Segmentation):<br>æ„å‘³åˆ†å‰²ã¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åˆ†å‰²ã‚’çµ±åˆã—ãŸæŠ€è¡“ã§ã€ç”»åƒå†…ã®ã™ã¹ã¦ã®ãƒ”ã‚¯ã‚»ãƒ«ã«å¯¾ã—ã¦ã‚¯ãƒ©ã‚¹ã¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹IDã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚ã‚·ãƒ¼ãƒ³ã®å®Œå…¨ãªç†è§£ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚<br><br>30. åŒ»å­¦å›¾åƒ(Medical Image):<br>Xç·šã€CTã€MRIã€è¶…éŸ³æ³¢ãªã©ã®åŒ»ç™‚ç›®çš„ã§æ’®å½±ã•ã‚ŒãŸç”»åƒã‚’æŒ‡ã—ã¾ã™ã€‚è¨ºæ–­ã€æ²»ç™‚è¨ˆç”»ã€åŒ»å­¦ç ”ç©¶ãªã©ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚<br><br>31. åŒ»å­¦å›¾åƒåˆ†å‰²(Medical Image Segmentation):<br>åŒ»ç™‚ç”»åƒå†…ã®è‡“å™¨ã€è…«ç˜ã€è¡€ç®¡ãªã©ã®ç‰¹å®šã®æ§‹é€ ã‚„ç—…å¤‰éƒ¨ä½ã‚’åˆ†å‰²ã™ã‚‹æŠ€è¡“ã§ã™ã€‚è¨ºæ–­æ”¯æ´ã‚„æ‰‹è¡“è¨ˆç”»ç«‹æ¡ˆã«é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚<br><br>32. è§†é¢‘ç›®æ ‡åˆ†å‰²(Video Object Segmentation):<br>å‹•ç”»ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å†…ã®ç‰¹å®šã®ç‰©ä½“ã‚’è¿½è·¡ã—ã€ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«åˆ†å‰²ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ãƒ“ãƒ‡ã‚ªç·¨é›†ã‚„ã‚¢ã‚¦ã‚°ãƒ¡ãƒ³ãƒ†ãƒƒãƒ‰ãƒªã‚¢ãƒªãƒ†ã‚£ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>33. è§†é¢‘å®ä¾‹åˆ†å‰²(Video Instance Segmentation):<br>å‹•ç”»å†…ã®å€‹ã€…ã®ç‰©ä½“ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¿½è·¡ã—ã€ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«åˆ†å‰²ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§ã™ã€‚ãƒ“ãƒ‡ã‚ªè§£æã‚„è‡ªå‹•é‹è»¢ã‚·ã‚¹ãƒ†ãƒ ã§ã®ç’°å¢ƒç†è§£ã«å½¹ç«‹ã¡ã¾ã™ã€‚<br><br>34. å‚è€ƒå›¾åƒåˆ†å‰²(Referring Image Segmentation):<br>è‡ªç„¶è¨€èªã«ã‚ˆã‚‹è¨˜è¿°ã«åŸºã¥ã„ã¦ã€ç”»åƒå†…ã®ç‰¹å®šã®ç‰©ä½“ã‚„é ˜åŸŸã‚’åˆ†å‰²ã™ã‚‹æŠ€è¡“ã§ã™ã€‚äººé–“ã¨AIã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’ä¿ƒé€²ã—ã¾ã™ã€‚<br><br>35. å›¾åƒæŠ å›¾(Image Matting):<br>ç”»åƒã‹ã‚‰å‰æ™¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ç²¾å¯†ã«æŠ½å‡ºã™ã‚‹æŠ€è¡“ã§ã™ã€‚èƒŒæ™¯ç½®æ›ã‚„åˆæˆãªã©ã€ç”»åƒç·¨é›†ã‚¿ã‚¹ã‚¯ã§é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚<br><br>36. å›¾åƒç¼–è¾‘(Image Editing):<br>ç”»åƒã®å†…å®¹ã‚’å¤‰æ›´ã¾ãŸã¯æ“ä½œã™ã‚‹æŠ€è¡“ã®ç·ç§°ã§ã™ã€‚ç‰©ä½“ã®é™¤å»ãƒ»è¿½åŠ ã€ã‚¹ã‚¿ã‚¤ãƒ«å¤‰æ›ã€è‰²èª¿æ•´ãªã©ã€æ§˜ã€…ãªç·¨é›†æ“ä½œãŒå«ã¾ã‚Œã¾ã™ã€‚<br><br>37. Low-level Vision:<br>ç”»åƒã®ä½ãƒ¬ãƒ™ãƒ«ç‰¹å¾´ã‚„åŸºæœ¬çš„ãªå‡¦ç†ã‚’æ‰±ã†åˆ†é‡ã§ã™ã€‚ãƒã‚¤ã‚ºé™¤å»ã€è¶…è§£åƒã€ã‚¨ãƒƒã‚¸æ¤œå‡ºãªã©ã®åŸºç¤çš„ãªã‚¿ã‚¹ã‚¯ãŒå«ã¾ã‚Œã¾ã™ã€‚<br><br>38. è¶…åˆ†è¾¨ç‡(Super-Resolution):<br>ä½è§£åƒåº¦ã®ç”»åƒã‹ã‚‰é«˜è§£åƒåº¦ã®ç”»åƒã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚ç›£è¦–ã‚«ãƒ¡ãƒ©æ˜ åƒã®é®®æ˜åŒ–ã‚„å¤ã„å†™çœŸã®å¾©å…ƒãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>39. å»å™ª(Denoising):<br>ç”»åƒã‹ã‚‰ãƒã‚¤ã‚ºã‚’é™¤å»ã—ã€ã‚¯ãƒªã‚¢ãªç”»åƒã‚’å¾—ã‚‹æŠ€è¡“ã§ã™ã€‚ä½å…‰é‡æ’®å½±ã‚„åŒ»ç™‚ç”»åƒã®å“è³ªå‘ä¸Šãªã©ã€æ§˜ã€…ãªå ´é¢ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>40. å»æ¨¡ç³Š(Deblur):<br>ã¼ã‘ãŸç”»åƒã‚’ã‚·ãƒ£ãƒ¼ãƒ—ã«ã™ã‚‹æŠ€è¡“ã§ã™ã€‚æ‰‹ãƒ–ãƒ¬ã‚„è¢«å†™ä½“ãƒ–ãƒ¬ã®è£œæ­£ã€å¤ã„å†™çœŸã®å¾©å…ƒãªã©ã«æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>41. è‡ªåŠ¨é©¾é©¶(Autonomous Driving):<br>äººé–“ã®æ“ä½œãªã—ã§è»Šä¸¡ã‚’åˆ¶å¾¡ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã€ã‚»ãƒ³ã‚µãƒ¼èåˆã€æ±ºå®šã‚·ã‚¹ãƒ†ãƒ ãªã©ã€å¤šå²ã«ã‚ãŸã‚‹æŠ€è¡“ã®çµ±åˆãŒå¿…è¦ã§ã™ã€‚<br><br>42. 3Dç‚¹äº‘(3D Point Cloud):<br>3Dç©ºé–“å†…ã®ç‚¹ã®é›†åˆã§ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚„ç’°å¢ƒã‚’è¡¨ç¾ã™ã‚‹ãƒ‡ãƒ¼ã‚¿å½¢å¼ã§ã™ã€‚LiDARãªã©ã®ã‚»ãƒ³ã‚µãƒ¼ã‹ã‚‰å–å¾—ã•ã‚Œã€3Dèªè­˜ã‚¿ã‚¹ã‚¯ã®åŸºç¤ã¨ãªã‚Šã¾ã™ã€‚<br><br>43. 3Dç›®æ ‡æ£€æµ‹(3D Object Detection):<br>3Dç©ºé–“å†…ã®ç‰©ä½“ã®ä½ç½®ã€ã‚µã‚¤ã‚ºã€å‘ãã‚’ç‰¹å®šã™ã‚‹æŠ€è¡“ã§ã™ã€‚è‡ªå‹•é‹è»¢ã‚„æ‹¡å¼µç¾å®Ÿãªã©ã®åˆ†é‡ã§é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚<br><br>44. 3Dè¯­ä¹‰åˆ†å‰²(3D Semantic Segmentation):<br>3Dãƒ‡ãƒ¼ã‚¿ã®å„ç‚¹ã‚„é ˜åŸŸã‚’ã‚ã‚‰ã‹ã˜ã‚å®šç¾©ã•ã‚ŒãŸã‚«ãƒ†ã‚´ãƒªã«åˆ†é¡ã™ã‚‹æŠ€è¡“ã§ã™ã€‚è‡ªå‹•é‹è»¢ã§ã®ç’°å¢ƒç†è§£ã‚„ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã§ã®ç‰©ä½“èªè­˜ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>45. 3Dç›®æ ‡è·Ÿè¸ª(3D Object Tracking):<br>æ™‚ç³»åˆ—ã®3Dãƒ‡ãƒ¼ã‚¿å†…ã§ç‰©ä½“ã®å‹•ãã‚’è¿½è·¡ã™ã‚‹æŠ€è¡“ã§ã™ã€‚è‡ªå‹•é‹è»¢ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹ä»–ã®è»Šä¸¡ã‚„æ­©è¡Œè€…ã®å‹•ãã®äºˆæ¸¬ãªã©ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚<br><br>46. 3Dè¯­ä¹‰åœºæ™¯è¡¥å…¨(3D Semantic Scene Completion):<br>éƒ¨åˆ†çš„ãª3Dãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€ã‚ªã‚¯ãƒ«ãƒ¼ã‚¸ãƒ§ãƒ³ã‚„æ¬ æã®ã‚ã‚‹é ˜åŸŸã‚’å«ã‚€å®Œå…¨ãª3Dã‚·ãƒ¼ãƒ³ã‚’æ¨å®šã™ã‚‹æŠ€è¡“ã§ã™ã€‚ãƒ­ãƒœãƒƒãƒˆãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ã‚„æ‹¡å¼µç¾å®Ÿã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>47. 3Dé…å‡†(3D Registration):<br>è¤‡æ•°ã®3Dãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆç‚¹ç¾¤ã‚„è¡¨é¢ãƒ¢ãƒ‡ãƒ«ãªã©ï¼‰ã‚’æ­£ç¢ºã«æ•´åˆ—ã•ã›ã‚‹æŠ€è¡“ã§ã™ã€‚3Dã‚¹ã‚­ãƒ£ãƒ³ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆã‚„ä½ç½®åˆã‚ã›ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚<br><br>48. 3Däººä½“å§¿æ€ä¼°è®¡(3D Human Pose Estimation):<br>2Dç”»åƒã‚„3Dãƒ‡ãƒ¼ã‚¿ã‹ã‚‰äººä½“ã®3æ¬¡å…ƒçš„ãªå§¿å‹¢ã‚’æ¨å®šã™ã‚‹æŠ€è¡“ã§ã™ã€‚ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³ã‚­ãƒ£ãƒ—ãƒãƒ£ã€ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ã€ã‚¹ãƒãƒ¼ãƒ„åˆ†æãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>49. 3Däººä½“Meshä¼°è®¡(3D Human Mesh Estimation):<br>2Dç”»åƒã‚„3Dã‚¹ã‚­ãƒ£ãƒ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è©³ç´°ãª3Däººä½“ãƒ¡ãƒƒã‚·ãƒ¥ãƒ¢ãƒ‡ãƒ«ã‚’æ¨å®šã™ã‚‹æŠ€è¡“ã§ã™ã€‚ãƒãƒ¼ãƒãƒ£ãƒ«ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚„ã‚¢ãƒ‹ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³åˆ¶ä½œãªã©ã«æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>50. å›¾åƒç”Ÿæˆ(Image Generation):<br>AIã‚’ç”¨ã„ã¦æ–°ã—ã„ç”»åƒã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚GANã‚„Diffusion Modelãªã©ãŒä»£è¡¨çš„ã§ã€ã‚¢ãƒ¼ãƒˆå‰µä½œã‚„ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>51. è§†é¢‘ç”Ÿæˆ(Video Generation):<br>AIã‚’ç”¨ã„ã¦æ–°ã—ã„å‹•ç”»ã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚çŸ­ã„å…¥åŠ›ã‚¯ãƒªãƒƒãƒ—ã‹ã‚‰ã®å‹•ç”»ã®å»¶é•·ã‚„ã€ãƒ†ã‚­ã‚¹ãƒˆè¨˜è¿°ã‹ã‚‰ã®å‹•ç”»ç”Ÿæˆãªã©ãŒç ”ç©¶ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>52. 3Dç”Ÿæˆ(3D Generation):<br>AIã‚’ç”¨ã„ã¦æ–°ã—ã„3Dãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚è£½å“ãƒ‡ã‚¶ã‚¤ãƒ³ã€ã‚²ãƒ¼ãƒ é–‹ç™ºã€å»ºç¯‰è¨­è¨ˆãªã©ã®åˆ†é‡ã§æ³¨ç›®ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>53. è§†é¢‘ç†è§£(Video Understanding):<br>å‹•ç”»ã®å†…å®¹ã‚’è§£æã—ã€ã‚·ãƒ¼ãƒ³ã®æ§‹é€ ã€ç‰©ä½“ã®é–¢ä¿‚ã€ã‚¤ãƒ™ãƒ³ãƒˆã®é€²è¡Œãªã©ã‚’ç†è§£ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ãƒ“ãƒ‡ã‚ªæ¤œç´¢ã‚„è‡ªå‹•è¦ç´„ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>54. è¡Œä¸ºè¯†åˆ«(Action Recognition):<br>å‹•ç”»å†…ã®äººç‰©ã®è¡Œå‹•ã‚’è­˜åˆ¥ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã€ã‚¹ãƒãƒ¼ãƒ„åˆ†æã€ãƒ’ãƒ¥ãƒ¼ãƒãƒ³ãƒ»ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ»ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãªã©ã§æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>55. è¡Œä¸ºæ£€æµ‹(Action Detection):<br>å‹•ç”»å†…ã®ç‰¹å®šã®è¡Œå‹•ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ¤œå‡ºã—ã€ãã®æ™‚é–“çš„ãƒ»ç©ºé–“çš„ä½ç½®ã‚’ç‰¹å®šã™ã‚‹æŠ€è¡“ã§ã™ã€‚ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚·ã‚¹ãƒ†ãƒ ã‚„ç•°å¸¸è¡Œå‹•ã®æ¤œçŸ¥ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>ã¯ã„ã€ç¶šãã‚’èª¬æ˜ã„ãŸã—ã¾ã™ã€‚<br><br>56. æ–‡æœ¬æ£€æµ‹(Text Detection):<br>ç”»åƒã‚„å‹•ç”»å†…ã®ãƒ†ã‚­ã‚¹ãƒˆã®ä½ç½®ã‚’æ¤œå‡ºã™ã‚‹æŠ€è¡“ã§ã™ã€‚OCRã‚·ã‚¹ãƒ†ãƒ ã®å‰å‡¦ç†ã¨ã—ã¦é‡è¦ã§ã€çœ‹æ¿ã®èªè­˜ã‚„æ–‡æ›¸åˆ†æãªã©ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚<br><br>57. çŸ¥è¯†è’¸é¦(Knowledge Distillation):<br>å¤§è¦æ¨¡ã§è¤‡é›‘ãªã€Œæ•™å¸«ã€ãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã‚’ã€ã‚ˆã‚Šå°ã•ãªã€Œç”Ÿå¾’ã€ãƒ¢ãƒ‡ãƒ«ã«è»¢ç§»ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®è»½é‡åŒ–ã¨æ€§èƒ½ç¶­æŒã®ä¸¡ç«‹ã‚’ç›®æŒ‡ã—ã¾ã™ã€‚<br><br>58. æ¨¡å‹å‰ªæ(Model Pruning):<br>å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰é‡è¦åº¦ã®ä½ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚„å±¤ã‚’å‰Šé™¤ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’è»½é‡åŒ–ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ãƒ¢ãƒã‚¤ãƒ«ãƒ‡ãƒã‚¤ã‚¹ã§ã®åŠ¹ç‡çš„ãªå®Ÿè¡Œãªã©ã«å½¹ç«‹ã¡ã¾ã™ã€‚<br><br>59. å›¾åƒå‹ç¼©(Image Compression):<br>ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’åŠ¹ç‡çš„ã«åœ§ç¸®ã—ã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å¸¯åŸŸå¹…ã‚’ç¯€ç´„ã™ã‚‹æŠ€è¡“ã§ã™ã€‚æœ€è¿‘ã§ã¯æ©Ÿæ¢°å­¦ç¿’ã‚’ç”¨ã„ãŸæ–°ã—ã„åœ§ç¸®æ‰‹æ³•ã‚‚ç ”ç©¶ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>60. ä¸‰ç»´é‡å»º(3D Reconstruction):<br>2Dç”»åƒã‚„å‹•ç”»ã‹ã‚‰3Dãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹æŠ€è¡“ã§ã™ã€‚å»ºç¯‰ã€è€ƒå¤å­¦ã€æ˜ ç”»åˆ¶ä½œãªã©ã€æ§˜ã€…ãªåˆ†é‡ã§æ´»ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>61. æ·±åº¦ä¼°è®¡(Depth Estimation):<br>å˜çœ¼ã¾ãŸã¯è¤‡çœ¼ã®2Dç”»åƒã‹ã‚‰ã€ã‚·ãƒ¼ãƒ³ã®å¥¥è¡Œãæƒ…å ±ã‚’æ¨å®šã™ã‚‹æŠ€è¡“ã§ã™ã€‚3Då†æ§‹æˆã‚„æ‹¡å¼µç¾å®Ÿãªã©ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚<br><br>62. è½¨è¿¹é¢„æµ‹(Trajectory Prediction):<br>ç‰©ä½“ã‚„äººã®éå»ã®å‹•ãã«åŸºã¥ã„ã¦ã€å°†æ¥ã®å‹•ãã‚’äºˆæ¸¬ã™ã‚‹æŠ€è¡“ã§ã™ã€‚è‡ªå‹•é‹è»¢ã€ç¾¤è¡†è¡Œå‹•åˆ†æã€ã‚¹ãƒãƒ¼ãƒ„æˆ¦ç•¥ç«‹æ¡ˆãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>63. è½¦é“çº¿æ£€æµ‹(Lane Detection):<br>é“è·¯ä¸Šã®è»Šç·šã‚’æ¤œå‡ºã—è¿½è·¡ã™ã‚‹æŠ€è¡“ã§ã™ã€‚è‡ªå‹•é‹è»¢ã‚·ã‚¹ãƒ†ãƒ ã‚„å…ˆé€²é‹è»¢æ”¯æ´ã‚·ã‚¹ãƒ†ãƒ ï¼ˆADASï¼‰ã«ãŠã„ã¦é‡è¦ãªè¦ç´ ã¨ãªã£ã¦ã„ã¾ã™ã€‚<br><br>64. å›¾åƒæè¿°(Image Captioning):<br>ç”»åƒã®å†…å®¹ã‚’è‡ªç„¶è¨€èªã§èª¬æ˜ã™ã‚‹æ–‡ç« ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚è¦–è¦šéšœå®³è€…æ”¯æ´ã‚„ç”»åƒæ¤œç´¢ã®é«˜åº¦åŒ–ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>65. è§†è§‰é—®ç­”(Visual Question Answering):<br>ç”»åƒã«é–¢ã™ã‚‹è‡ªç„¶è¨€èªã®è³ªå•ã«å¯¾ã—ã¦ã€é©åˆ‡ãªå›ç­”ã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚ç”»åƒç†è§£ã¨ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ä¸¡æ–¹ã®èƒ½åŠ›ãŒå¿…è¦ã¨ã•ã‚Œã¾ã™ã€‚<br><br>66. æ‰‹è¯­è¯†åˆ«(Sign Language Recognition):<br>æ‰‹è©±ã®ã‚¸ã‚§ã‚¹ãƒãƒ£ãƒ¼ã‚’èªè­˜ã—ã€ãã‚Œã‚’æ–‡å­—ã‚„éŸ³å£°ã«å¤‰æ›ã™ã‚‹æŠ€è¡“ã§ã™ã€‚è´è¦šéšœå®³è€…ã¨ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³æ”¯æ´ã«å½¹ç«‹ã¡ã¾ã™ã€‚<br><br>67. è§†é¢‘é¢„æµ‹(Video Prediction):<br>éå»ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã«åŸºã¥ã„ã¦ã€å‹•ç”»ã®å°†æ¥ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’äºˆæ¸¬ã™ã‚‹æŠ€è¡“ã§ã™ã€‚å‹•ç”»åœ§ç¸®ã€ç•°å¸¸æ¤œçŸ¥ã€è‡ªå‹•é‹è»¢ãªã©ã€æ§˜ã€…ãªå¿œç”¨ãŒè€ƒãˆã‚‰ã‚Œã¦ã„ã¾ã™ã€‚<br><br>68. æ–°è§†ç‚¹åˆæˆ(Novel View Synthesis):<br>æ—¢å­˜ã®ç”»åƒã‚„é™ã‚‰ã‚ŒãŸè¦–ç‚¹ã®æƒ…å ±ã‹ã‚‰ã€æ–°ã—ã„è¦–ç‚¹ã®ç”»åƒã‚’ç”Ÿæˆã™ã‚‹æŠ€è¡“ã§ã™ã€‚ä»®æƒ³ç¾å®Ÿã‚„è‡ªç”±è¦–ç‚¹æ˜ åƒãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>69. Zero-Shot Learning(é›¶æ ·æœ¬å­¦ä¹ ):<br>å­¦ç¿’æ™‚ã«è¦‹ãŸã“ã¨ã®ãªã„ã‚¯ãƒ©ã‚¹ã‚’èªè­˜ã™ã‚‹å­¦ç¿’æ‰‹æ³•ã§ã™ã€‚äº‹å‰ã«å­¦ç¿’ã—ã¦ã„ãªã„æ–°ã—ã„ã‚«ãƒ†ã‚´ãƒªã®ç‰©ä½“ã‚’è­˜åˆ¥ã™ã‚‹èƒ½åŠ›ã‚’æŒã¡ã¾ã™ã€‚<br><br>70. ç«‹ä½“åŒ¹é…(Stereo Matching):<br>ã‚¹ãƒ†ãƒ¬ã‚ªã‚«ãƒ¡ãƒ©ã§æ’®å½±ã•ã‚ŒãŸå·¦å³ã®ç”»åƒã‹ã‚‰å¯¾å¿œç‚¹ã‚’è¦‹ã¤ã‘ã€æ·±åº¦æƒ…å ±ã‚’æŠ½å‡ºã™ã‚‹æŠ€è¡“ã§ã™ã€‚3Då†æ§‹æˆã‚„æ·±åº¦æ¨å®šã®åŸºç¤ã¨ãªã‚Šã¾ã™ã€‚<br><br>71. ç‰¹å¾åŒ¹é…(Feature Matching):<br>ç•°ãªã‚‹ç”»åƒé–“ã§å¯¾å¿œã™ã‚‹ç‰¹å¾´ç‚¹ã‚’è¦‹ã¤ã‘ã‚‹æŠ€è¡“ã§ã™ã€‚ç”»åƒã®ã‚¹ãƒ†ã‚£ãƒƒãƒãƒ³ã‚°ã€ç‰©ä½“è¿½è·¡ã€SLAMãªã©ã€æ§˜ã€…ãªã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã®åŸºç¤ã¨ãªã‚Šã¾ã™ã€‚<br><br>72. åœºæ™¯å›¾ç”Ÿæˆ(Scene Graph Generation):<br>ç”»åƒå†…ã®ç‰©ä½“é–“ã®é–¢ä¿‚ã‚’æ§‹é€ åŒ–ã—ã¦è¡¨ç¾ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ã‚·ãƒ¼ãƒ³ã®æ„å‘³çš„ç†è§£ã‚„é«˜åº¦ãªç”»åƒæ¤œç´¢ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>73. è®¡æ•°(Counting):<br>ç”»åƒã‚„å‹•ç”»å†…ã®ç‰¹å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®æ•°ã‚’è‡ªå‹•çš„ã«æ•°ãˆã‚‹æŠ€è¡“ã§ã™ã€‚ç¾¤è¡†ã®æ¨å®šã€åœ¨åº«ç®¡ç†ã€ç”Ÿç‰©å­¦çš„ã‚µãƒ³ãƒ—ãƒ«ã®è¨ˆæ•°ãªã©ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>74. éšå¼ç¥ç»è¡¨ç¤º(Implicit Neural Representations):<br>3Dã‚·ãƒ¼ãƒ³ã‚„å½¢çŠ¶ã‚’é€£ç¶šé–¢æ•°ã¨ã—ã¦ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§è¡¨ç¾ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚NeRFã¯ã“ã®æ‰‹æ³•ã®ä¸€ä¾‹ã§ã€åŠ¹ç‡çš„ãªã‚·ãƒ¼ãƒ³è¡¨ç¾ãŒå¯èƒ½ã§ã™ã€‚<br><br>75. å›¾åƒè´¨é‡è¯„ä»·(Image Quality Assessment):<br>ç”»åƒã®å“è³ªã‚’å®¢è¦³çš„ã«è©•ä¾¡ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ç”»åƒå‡¦ç†ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æ€§èƒ½è©•ä¾¡ã‚„ã€å“è³ªã«åŸºã¥ã„ãŸç”»åƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãªã©ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚<br><br>76. è§†é¢‘è´¨é‡è¯„ä»·(Video Quality Assessment):<br>å‹•ç”»ã®å“è³ªã‚’å®¢è¦³çš„ã«è©•ä¾¡ã™ã‚‹æŠ€è¡“ã§ã™ã€‚ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚µãƒ¼ãƒ“ã‚¹ã®å“è³ªç›£è¦–ã‚„ã€ãƒ“ãƒ‡ã‚ªåœ§ç¸®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®æœ€é©åŒ–ãªã©ã«å¿œç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚<br><br>77. æ•°æ®é›†(Datasets):<br>æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚„è©•ä¾¡ã«ä½¿ç”¨ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã®é›†åˆã§ã™ã€‚ImageNetã€COCOã€KITTI<br><br>ãªã©ã€å„ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒé–‹ç™ºã•ã‚Œã€ç ”ç©¶ã‚„æŠ€è¡“é–‹ç™ºã‚’åŠ é€Ÿã—ã¦ã„ã¾ã™ã€‚<br><br>ã“ã‚Œã‚‰ã®æŠ€è¡“ã¯ç›¸äº’ã«é–¢é€£ã—åˆã„ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã¨æ©Ÿæ¢°å­¦ç¿’ã®åˆ†é‡ã‚’å½¢ä½œã£ã¦ã„ã¾ã™ã€‚å¤šãã®å¿œç”¨åˆ†é‡ã§é©æ–°çš„ãªã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ç”Ÿã¿å‡ºã™ã¨ã¨ã‚‚ã«ã€æ–°ãŸãªèª²é¡Œã«ã‚‚ç›´é¢ã—ã¦ãŠã‚Šã€ä»Šå¾Œã•ã‚‰ãªã‚‹ç™ºå±•ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹â€‹</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2024-09-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1426" target="_blank" rel="noopener noreferrer" class="title-link">Molmo, AI2, 2024.09</a>
<span class="snippet"><span>GPT Summary</span>- Molmoã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ãŸæœ€å…ˆç«¯ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«AIãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ç‰¹ã«å°å‹ãƒ¢ãƒ‡ãƒ«ãŒå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã€‚Molmoã¯ã€ç‰©ç†çš„ãŠã‚ˆã³ä»®æƒ³çš„ãªä¸–ç•Œã¨ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¯èƒ½ã«ã—ã€éŸ³å£°ãƒ™ãƒ¼ã‚¹ã®èª¬æ˜ã‚’ç”¨ã„ãŸæ–°ã—ã„ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å°å…¥ã€‚ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã€éè¨€èªçš„æ‰‹ãŒã‹ã‚Šã‚’æ´»ç”¨ã—ã¦è³ªå•ã«ç­”ãˆã‚‹èƒ½åŠ›ã‚’æŒã¤ã€‚Molmoãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚¦ã‚§ã‚¤ãƒˆã§ãƒ—ãƒ­ãƒ—ãƒ©ã‚¤ã‚¨ã‚¿ãƒªã‚·ã‚¹ãƒ†ãƒ ã«å¯¾æŠ—ã™ã‚‹æ€§èƒ½ã‚’ç™ºæ®ã—ã€ä»Šå¾Œã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã‚¦ã‚§ã‚¤ãƒˆã‚„ãƒ‡ãƒ¼ã‚¿ã‚’å…¬é–‹äºˆå®šã€‚</span>
<span class="snippet"><span>Comment</span><p>ä»¥ä¸‹ãŒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœï¼ˆVLMã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼‰ã€‚11 benchmarksã¨æ›¸ã‹ã‚Œã¦ã„ã‚‹ã®ã¯ã€VLMã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã‚ã‚‹ç‚¹ã«æ³¨æ„ã€‚<br><br><br><br>&lt;img width="981" alt="image" src="


&lt;a href="https://github.com/user-attachments/assets/510204e5-4cfb-4ba3-a6db-fff717a637bc"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/510204e5-4cfb-4ba3-a6db-fff717a637bc"&lt;/a&gt;


&gt;<br><br>&lt;img width="940" alt="image" src="


&lt;a href="https://github.com/user-attachments/assets/a4a77006-fcde-4c33-b6df-54dc5d8cbdfa"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/a4a77006-fcde-4c33-b6df-54dc5d8cbdfa"&lt;/a&gt;


&gt;<br><br></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<span class="issue_date">Issue Date: 2024-09-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1422" target="_blank" rel="noopener noreferrer" class="title-link">Llama 3.2: Revolutionizing edge AI and vision with open, customizable models, Meta, 2024.09</a>
<span class="snippet"><span>Comment</span><p>11Bã¨90Bã®VLMã¨ã€ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹å‘ã‘ã®1B, 3Bã®SLMã‚’ç™ºè¡¨ã€‚<br><img src="https://github.com/user-attachments/assets/13c4af37-19bd-4de7-b501-eb48f955af0c" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/d6b75b15-88cb-4d9e-9838-0da24308ccda" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/7475b30d-4619-4117-a911-d308291f86cb" alt="image" loading="lazy"></p>
<p>Llama3.2ã®VLMã§ã¯ã€äº‹å‰å­¦ç¿’ã•ã‚ŒãŸimage encoderã‚’äº‹å‰å­¦ç¿’ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦çµ„ã¿åˆã‚ã›ã‚‹ãŸã‚ã®Adapterã‚’è¤‡æ•°å­¦ç¿’ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦å®Ÿç¾ã€‚<br><br>å…·ä½“çš„ã«ã¯ã€Llama 3.1ï¼ˆtext only modelï¼‰ã«å¯¾ã—ã¦ã€image encoderã¨Adapterã‚’è¿½åŠ ã—ã€å¤§è¦æ¨¡ã§ãƒã‚¤ã‚¸ãƒ¼ãªï¼ˆimage,textï¼‰ãƒšã‚¢ã§äº‹å‰å­¦ç¿’ã€‚ç¶šã„ã¦ã€ä¸­è¦æ¨¡ã®ã‚µã‚¤ã‚ºã®é«˜å“è³ªãªin-domainï¼ˆi.e. æ§˜ã€…ãªãƒ‰ãƒ¡ã‚¤ãƒ³ã®ï¼‰ã®çŸ¥è­˜ã‚’é«˜ã‚ã‚‹ã‚ˆã†ãªï¼ˆimage,textï¼‰ãƒšã‚¢ã§å­¦ç¿’ã—ãŸã€‚<br><br>äº‹å¾Œå­¦ç¿’ã§ã¯ã€Llama3.1ã¨åŒæ§˜ã«SFT, Rejection Sampling, DPOã®ãƒ©ã‚¦ãƒ³ãƒ‰ã‚’è¤‡æ•°å›ç¹°ã‚Šè¿”ã—ãŸã€‚Llama3.1ã‚’ç”¨ã„ã¦ã€in-domainã®ç”»åƒã«å¯¾ã™ã‚‹QAã‚’Data Augmentationã—ã€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã€‚ã•ã‚‰ã«å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ã¦å…¨ã¦ã®å›ç­”å€™è£œã‚’ãƒ©ãƒ³ã‚¯ã¥ã‘ã—ã¦é«˜å“è³ªãªSFTãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã€‚ã¾ãŸã€ãƒ¢ãƒ‡ãƒ«ã®å®‰å…¨æ€§ãŒé«˜ã¾ã‚‹ã‚ˆã†ãªãƒ‡ãƒ¼ã‚¿ã‚‚è¿½åŠ ã—ãŸã€‚<br><br>Llama3.1ã®äº‹å¾Œå­¦ç¿’ã®ãƒ—ãƒ­ã‚»ã‚¹ã«ã¤ã„ã¦ã¯ <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1359" target="_blank" rel="noopener noreferrer">è«–æ–‡ç´¹ä»‹ / The Llama 3 Herd of Models, 2024.08</a>
 ã‚‚å‚ç…§ã®ã“ã¨ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<span class="issue_date">Issue Date: 2024-09-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1379" target="_blank" rel="noopener noreferrer" class="title-link">ml-engineering</a>
<span class="snippet"><span>Comment</span><p>LLMã‚„VLMã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã®ãƒ„ãƒ¼ãƒ«ã‚„ãƒã‚¦ãƒã‚¦ãŒã¾ã¨ã‚ã‚‰ã‚ŒãŸãƒªãƒã‚¸ãƒˆãƒª</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<span class="issue_date">Issue Date: 2024-04-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1281" target="_blank" rel="noopener noreferrer" class="title-link">Grok-1.5 Vision Preview, 2024</a>
<span class="snippet"><span>Comment</span><p><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/88dd70ce-5874-4786-8e66-7484984c7a72" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="articles/Alignment.html" target="_blank" rel="noopener noreferrer">#Alignment</a>
<a class="button" href="articles/TextualInversion.html" target="_blank" rel="noopener noreferrer">#TextualInversion</a>
<span class="issue_date">Issue Date: 2024-03-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1258" target="_blank" rel="noopener noreferrer" class="title-link">repeng</a>
<span class="snippet"><span>Comment</span><p>LLMã®å‡ºåŠ›ã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æ•°ç™¾å€‹ã®äº‹ä¾‹ã ã‘ã§å­¦ç¿’ã—ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚promptã§æŒ‡å®šã™ã‚‹ã®ã¨ã¯ç•°ãªã‚Šã€æ•°å€¤ã§ã‚¹ã‚¿ã‚¤ãƒ«ã®å¼·ã•ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ãŒå¯èƒ½ã‚‰ã—ã„ï¼ˆå…ƒãƒ„ã‚¤ãƒ¼ãƒˆï¼‰ã€‚ç”»åƒç”Ÿæˆåˆ†é‡ã«ãŠã‘ã‚‹Textual Inversionã¨åŒã˜æŠ€è¡“ã¨ã®ã“ã¨ã€‚<br><br>Textual Inversionã¨ã¯ã€å°‘é‡ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ç”¨ã„ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€éƒ¨åˆ†ã«æ–°ãŸãªã€Œå˜èªã€ã‚’è¿½åŠ ã—ã€å˜èªã¨å¯¾å¿œã™ã‚‹ç”»åƒã‚’ç”¨ã„ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹ã“ã¨ã§ã€promptä¸­ã§ã€Œå˜èªã€ã‚’åˆ©ç”¨ã—ãŸå ´åˆã«å­¦ç¿’ã—ãŸç”»åƒã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å­˜åœ¨ã—ãªãã¦ã‚‚å¯ï¼‰ã‚’ç”Ÿæˆã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹æŠ€è¡“ã€ã‚‰ã—ã„ã€‚<br><br>Huggiegface: 


<a href="https://huggingface.co/docs/diffusers/training/text_inversion" target="_blank" rel="noopener noreferrer">https://huggingface.co/docs/diffusers/training/text_inversion</a>


<br>ï¼ˆå‚è€ƒï¼‰GPTã«è³ªå•ã—ãŸéš›ã®ãƒ­ã‚°: 


<a href="https://chat.openai.com/share/e4558c44-ce09-417f-9c77-6f3855e583fa" target="_blank" rel="noopener noreferrer">https://chat.openai.com/share/e4558c44-ce09-417f-9c77-6f3855e583fa</a>


<br>å…ƒãƒ„ã‚¤ãƒ¼ãƒˆ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/webbigdata/status/1770272397184389211?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="articles/Prompting.html" target="_blank" rel="noopener noreferrer">#Prompting</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/AutomaticPromptEngineering.html" target="_blank" rel="noopener noreferrer">#AutomaticPromptEngineering</a>
<span class="issue_date">Issue Date: 2023-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1171" target="_blank" rel="noopener noreferrer" class="title-link">multimodal-maestro</a>
<span class="snippet"><span>Comment</span><p>Large Multimodal Model (LMM)ã«ãŠã„ã¦ã€é›‘ãªpromptã‚’ä¸ãˆã‚‹ã¦ã‚‚è‡ªå‹•çš„ã«è‰¯ã„æ„Ÿã˜outputã‚’ç”Ÿæˆã—ã¦ãã‚Œã‚‹ã£ã½ã„ï¼Ÿ<br><br><br><br>ä»¥ä¸‹ã®ä¾‹ã¯ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰ã®å¼•ç”¨ã§ã‚ã‚‹ãŒã€ã“ã®ä¾‹ã§ã¯ã€"Find dog." ã¨ã„ã†é›‘ãªpromptã‹ã‚‰ã€ç”»åƒä¸­å¤®ã«ä½ç½®ã™ã‚‹çŠ¬ã«[9]ã¨ã„ã†ãƒ©ãƒ™ãƒ«ã‚’ä¸ãˆã¾ã—ãŸã€ã¨ã„ã†responseã‚’å¾—ã‚‰ã‚Œã¦ã„ã‚‹ã€‚pipelineã¨ã—ã¦ã¯ã€Visual Promptã«å¯¾ã—ã¦ã¾ãšSAMã‚’ç”¨ã„ã¦ã‚¤ãƒ¡ãƒ¼ã‚¸ã®segmentationã‚’è¡Œã„ã€å„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«ãƒ©ãƒ™ãƒ«ã‚’æŒ¯ã‚‹ã€‚ã“ã®ãƒ©ãƒ™ãƒ«ãŒæŒ¯ã‚‰ã‚ŒãŸç”»åƒã¨ã€"Find dog." ã¨ã„ã†é›‘ãªpromptã‚’ä¸ãˆã‚‹ã ã‘ã§è‰¯ã„æ„Ÿã˜ã«å‡¦ç†ã‚’ã—ã¦ãã‚Œã‚‹ã‚ˆã†ã ã€‚    <br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/5220e62f-93f1-4eb9-b365-a9caaf933778" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/GenerativeAI.html" target="_blank" rel="noopener noreferrer">#GenerativeAI</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<span class="issue_date">Issue Date: 2023-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1170" target="_blank" rel="noopener noreferrer" class="title-link">LaVie: Text-to-Video generation, demo</a>
<span class="snippet"><span>Comment</span><p>ãƒ‡ãƒ¢ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§è©¦ã—ã¦ã¿ãŸã‚‰ã€3ç§’ã»ã©ã®prompté€šã‚Šã®å‹•ç”»ãŒç”Ÿæˆã•ã‚ŒãŸã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/4343fa52-698c-4a59-bad0-758fcd30d3ac" alt="image" loading="lazy"><br><br></p>
<p>FF14ã®èµ¤é­”å°å£«ã«å¤‰ãˆãŸã‚‰ã€ãã‚Œã£ã½ã„ã®å‡ºã¦ããŸ<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/07b6def8-01f2-4baf-9ba3-ab1ccc40c90e" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/TabularData.html" target="_blank" rel="noopener noreferrer">#TabularData</a>
<span class="issue_date">Issue Date: 2023-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1167" target="_blank" rel="noopener noreferrer" class="title-link">Table Transformer Demo</a>
<span class="snippet"><span>Comment</span><p>PDFä¸­ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ãã®æ§‹é€ ï¼ˆè¡Œåˆ—ã‚»ãƒ«ï¼‰ã‚’detectã™ã‚‹ãƒ¢ãƒ‡ãƒ«<br><br>Exampleã¯ä»¥ä¸‹ã®ã‚ˆã†ãªæ„Ÿã˜ï¼ˆæ—¥æœ¬èªã ã¨ã©ã‚Œãã‚‰ã„ã§ãã‚‹ã®ã‹ãª...ï¼‰<br><br><br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/7f62e16b-1ff8-46ad-b6df-7792981f8f58" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<span class="issue_date">Issue Date: 2023-11-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1156" target="_blank" rel="noopener noreferrer" class="title-link">ML Papers Explained</a>
<span class="snippet"><span>Comment</span><p>ä»¥ä¸‹ã®åˆ†é‡ã®ä»£è¡¨çš„ãªè«–æ–‡ãŒã¾ã¨ã‚ã‚‰ã‚Œã¦ã„ã‚‹ï¼ˆåŸºæœ¬çš„ã«ã¯Transformerç™»å ´å¾Œã®ã‚‚ã®ãŒå¤šã„ï¼‰<br><br>- è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆTransformer, Elmoãªã©ï¼‰<br>- Visionãƒ¢ãƒ‡ãƒ«ï¼ˆViTãªã©ï¼‰<br>- CNNï¼ˆAlexNetãªã©ï¼‰<br>- Single Stage Object Detectors<br>- Region-based Convolutional Neural Networks<br>- DocumentAIï¼ˆTableNetãªã©ï¼‰<br>- Layout Transformers<br>- Tabular Deeplearning</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/NaturalLanguageGeneration.html" target="_blank" rel="noopener noreferrer">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ImageCaptioning.html" target="_blank" rel="noopener noreferrer">#ImageCaptioning</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2023-11-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1114" target="_blank" rel="noopener noreferrer" class="title-link">Zero-shot Learningç¶²ç¾…çš„ã‚µãƒ¼ãƒ™ã‚¤: CLIPãŒåˆ‡ã‚Šé–‹ã„ãŸVision &amp; Languageã®æ–°ã—ã„ä¸–ç•Œ</a>
<span class="snippet"><span>Comment</span><p>ã“ã‚Œã¯ã™ã”ã„ã¾ã¨ã‚â€¦ã€‚ã¾ã é€”ä¸­ã¾ã§ã—ã‹èª­ã‚ã¦ã„ãªã„ã€‚CLIPã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆã—ã¦CLIPã‚’å¼•ç”¨ã—ã¦ã„ã‚‹è«–æ–‡ã‹ã‚‰é‡è¦ãªã‚‚ã®ã‚’æ¦‚è¦ä»˜ãã§ã¾ã¨ã‚ã¦ã„ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2023-10-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1107" target="_blank" rel="noopener noreferrer" class="title-link">StableDiffusion, LLMã®GPUãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã®ã‚ã‚Œã“ã‚Œ</a>
<span class="snippet"><span>Comment</span><p>Gradient Accumulation, Gradient Checkpointingã®èª¬æ˜ãŒä¸å¯§ã§ã‚ã‹ã‚Šã‚„ã™ã‹ã£ãŸã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ChatGPT.html" target="_blank" rel="noopener noreferrer">#ChatGPT</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<span class="issue_date">Issue Date: 2023-09-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1052" target="_blank" rel="noopener noreferrer" class="title-link">OpenAIã€ChatGPTãŒç”»åƒã‚’åˆ†æã™ã‚‹ã€GPT-4Vï¼ˆãƒ“ã‚¸ãƒ§ãƒ³ï¼‰ã€ã‚’ç™ºè¡¨ã€‚å®‰å…¨æ€§ã€å—œå¥½æ€§ã€ç¦ç¥‰æ©Ÿèƒ½ã‚’å¼·åŒ–, AIDB, 2023.09</a>
<span class="snippet"><span>Comment</span><p>ãŠã†â€¦ã‚„ã¹ãˆãªâ€¦<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/3ee7dc96-af6f-47f9-98c0-c6be5d9384f1" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NaturalLanguageGeneration.html" target="_blank" rel="noopener noreferrer">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2023-08-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1003" target="_blank" rel="noopener noreferrer" class="title-link">èµ°è¡Œå‹•ç”»ã‚’èª¬æ˜ã™ã‚‹LLMã‚’ä½œæˆã—ã€80å°ã®GPUã§åˆ†æ•£ä¸¦åˆ—å­¦ç¿’ã•ã›ãŸè©±</a>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/ImageCaptioning.html" target="_blank" rel="noopener noreferrer">#ImageCaptioning</a>
<span class="issue_date">Issue Date: 2023-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/871" target="_blank" rel="noopener noreferrer" class="title-link">Comparing captioning models</a>
<span class="snippet"><span>Comment</span><p>SoTAã®vision languageãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¢ã€‚BLIP, BLIP2,GIT,InstructBLIPã‚’è©¦ã›ã‚‹</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<a class="button" href="articles/InductiveBias.html" target="_blank" rel="noopener noreferrer">#InductiveBias</a>
<span class="issue_date">Issue Date: 2023-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/809" target="_blank" rel="noopener noreferrer" class="title-link">Objaverse-XL: A Universe of 10M+ 3D Objects</a>
<span class="snippet"><span>Comment</span><p>10Mã‚’è¶…ãˆã‚‹3D objectã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å…¬é–‹ã—ã€3D Modelã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦Zero123-XLã‚’è¨“ç·´ã€‚<br>å…ƒãƒ„ã‚¤ãƒ¼ãƒˆã®GifãŒã‚ã‹ã‚Šã‚„ã™ã„ã€‚<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/mattdeitke/status/1678855859089326080?s=46&t=8VBxVyng2U93usaVloHk7w"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>ãŸã¨ãˆã°inputã•ã‚ŒãŸã‚¤ãƒ¡ãƒ¼ã‚¸ã«å¯¾ã—ã¦ã€è‡ªç”±ã«ã‚«ãƒ¡ãƒ©ã®è¦–ç‚¹ã‚’è¨­å®šã—ã€ãã®è¦–ç‚¹ã‹ã‚‰ã®ç‰©ä½“ã®ç”»åƒã‚’å‡ºåŠ›ã§ãã‚‹ã€‚</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<span class="issue_date">Issue Date: 2023-07-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/784" target="_blank" rel="noopener noreferrer" class="title-link">Awesome Multimodal LLMs</a>
<span class="snippet"><span>Comment</span><p>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªLLMã®ãƒªã‚¹ãƒˆãŒã¾ã¨ã‚ã‚‰ã‚Œã¦ã„ã‚‹</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="articles/Explanation.html" target="_blank" rel="noopener noreferrer">#Explanation</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2022-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/499" target="_blank" rel="noopener noreferrer" class="title-link">Transformers Interpret, 2022</a>
<span class="snippet"><span>Comment</span><p>transformersã®ãƒ¢ãƒ‡ãƒ«ã‚’ãŸã£ãŸ2è¡Œè¿½åŠ ã™ã‚‹ã ã‘ã§ã€explainableã«ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª<br><br>åŸºæœ¬çš„ã«textã¨visionã®classificationã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹æ¨¡æ§˜<br>text classificationã®å ´åˆã€ãŸã¨ãˆã°input tokenã®å„ãƒˆãƒ¼ã‚¯ãƒ³ã®åˆ†é¡ã«å¯¾ã™ã‚‹å¯„ä¸åº¦ã‚’outputã—ã¦ãã‚Œã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<span class="issue_date">Issue Date: 2022-10-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/489" target="_blank" rel="noopener noreferrer" class="title-link">CNN vs. ViT, ç‰›ä¹…å…ˆç”Ÿ</a>
<span class="snippet"><span>Comment</span><p>ãƒ»Swin Transformer, Depth-wise conv, ConvNeXt, ViTã¨CNNã®ãƒ­ãƒã‚¹ãƒˆæ€§ã®é•ã„ã®è©±ãŒã‚ã‚Šå‹‰å¼·ã«ãªã‚‹<br><br>ãƒ»æœ€çµ‚çš„ãªçµè«–ãŒã€CNNã‚‚Transformerã‚‚å¤‰ã‚ã‚‰ãªã„ï¼ˆæ˜ç¢ºãªå‹è€…ã¯ã„ãªã„; ä»Šã®ã¨ã“ã‚å¼•ãåˆ†ã‘ï¼‰ã¨ã„ã†ã®ã¯ãŠã‚‚ã—ã‚ã‹ã£ãŸ</p>
<p>depth-wise conv, point-wise convã®è§£èª¬è¨˜äº‹ï¼š


<a href="https://agirobots.com/depthwise-pointwise-convolution/" target="_blank" rel="noopener noreferrer">https://agirobots.com/depthwise-pointwise-convolution/</a>


<br><br><br><br>é€šå¸¸ã®CNNã®ãƒ•ã‚£ãƒ«ã‚¿ã«ã‚ˆã‚‹feature mapè¨ˆç®—ã‚’ã€ç©ºé–“æ–¹å‘ï¼ˆdepth-wise convï¼‰ã¨ãƒãƒ£ãƒãƒ«æ–¹å‘ï¼ˆpoint-wise conv; 1x1 convï¼‰ã«åˆ†è§£ã™ã‚‹ã“ã¨ã§å¤§å¹…ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°å‰Šæ¸›</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2021-11-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/430" target="_blank" rel="noopener noreferrer" class="title-link">Deep Residual Learning for Image Recognition, He+, Microsoft Research, CVPRâ€™16</a>
<span class="snippet"><span>Comment</span><p>ResNetè«–æ–‡<br><br>ResNetã§ã¯ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®è¨ˆç®—ã™ã‚‹é–¢æ•°ã‚’ã€æ®‹å·®F(x)ã¨æ’ç­‰é–¢æ•°xã®å’Œã¨ã—ã¦å®šç¾©ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒå…¥åŠ›ã¨ã®å·®åˆ†ã ã‘ã‚’å­¦ç¿’ã™ã‚Œã°è‰¯ããªã‚Šã€ãƒ¢ãƒ‡ãƒ«ã‚’æ·±ãã—ã¦ã‚‚æœ€é©åŒ–ãŒã—ã‚„ã™ããªã‚‹åŠ¹æœãã‚ã‚‹ã€‚æ•°ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«Residual Connectionã‚’å°å…¥ã—ã€æ’ç­‰é–¢æ•°ã«ã‚ˆã‚‹ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆãŒã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã„ã‚‹ã€‚<br><br><br><br><img src="https://user-images.githubusercontent.com/12249301/140301726-1d2e89e1-1d69-43d9-8d2b-0adb272e577a.png" alt="image" loading="lazy"><br><br><br><br>ResNetãŒææ¡ˆã•ã‚Œã‚‹ä»¥å‰ã€ãƒ¢ãƒ‡ãƒ«ã‚’æ·±ãã™ã‚Œã°è¡¨ç¾åŠ›ãŒä¸ŠãŒã‚‹ã¯ãšãªã®ã«ã€å®Ÿéš›ã«ã¯ç²¾åº¦ãŒä¸‹ãŒã£ã¦ã—ã¾ã†ã“ã¨ã‹ã‚‰ã€ç†è«–ä¸Šãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒæ’ç­‰é–¢æ•°ã¨ãªã‚‹ã‚ˆã†ã«åˆæœŸåŒ–ã™ã‚Œã°ã€æ·±ã„ãƒ¢ãƒ‡ãƒ«ã§ã‚‚æµ…ã„ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã®è¡¨ç¾ãŒç²å¾—ã§ãã‚‹ã€ã¨è¨€ã†è€ƒãˆæ–¹ã‚’ç™ºå±•ã•ã›ãŸã€‚<br><br><br><br>ï¼ˆã‚¹ãƒ†ãƒ¼ãƒˆã‚ªãƒ–AIã‚¬ã‚¤ãƒ‰ã«åŸºã¥ãï¼‰</p>
<p>åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã§ã‚ˆã‚Šå±¤ã‚’æ·±ãã§ãã‚‹ï¼ˆPlainãªæ§‹é€ ã¨æ¯”ã¹ã‚‹ã¨å±¤ãŒ1ã¤å¢—ãˆã‚‹ï¼‰Bottleneckã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚‚ææ¡ˆã—ã¦ã„ã‚‹ã€‚<br><br><br><br><img src="https://user-images.githubusercontent.com/12249301/140302452-649b0ea7-cce4-44c1-9e7d-b509ef8bca52.png" alt="image" loading="lazy"><br><br></p>
<p>ä»Šã‚„å½“ãŸã‚Šå‰ã®ã‚ˆã†ã«ä½¿ã‚ã‚Œã¦ã„ã‚‹Residual Connectionã¯ã€å±¤ã®æ·±ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«å¿…é ˆã®æŠ€è¡“ãªã®ã ã¨å†èªè­˜ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/ImageClassification.html" target="_blank" rel="noopener noreferrer">#ImageClassification</a>
<span class="issue_date">Issue Date: 2021-05-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/346" target="_blank" rel="noopener noreferrer" class="title-link">EfficientNetè§£èª¬, omiita ï¼ˆã‚ªãƒŸãƒ¼ã‚¿ï¼‰, 2019</a>
<span class="snippet"><span>Comment</span><p>æ—¢å­˜ç”»åƒèªè­˜ãƒ¢ãƒ‡ãƒ«ã®æ§‹é€ ã¯å¤‰åŒ–ã•ã›ãšã€åºƒã•ã€æ·±ã•ã€è§£åƒåº¦ã‚’è¤‡åˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€å¾“æ¥ã‚ˆã‚Šã‚‚å°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€ã‹ã¤å­¦ç¿’é€Ÿåº¦ã§SoTAã‚’é”æˆã€‚åºƒã•ã€æ·±ã•ã€è§£åƒåº¦ã¯ãã‚Œãã‚Œæ€§èƒ½ã«äº’ã„ã«å½±éŸ¿ã—ã‚ã£ã¦ãŠã‚Šã€å¾“æ¥ã®ã‚ˆã†ã«åˆ¥ã€…ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã®ã§ã¯ãªãã€3ã¤ã®ãƒãƒ©ãƒ³ã‚¹ã‚’ã¨ã‚ŠãªãŒã‚‰ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã€‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹éš›ã¯ã€çµæœçš„ã«ã¯ãã‚Œãã‚Œã‚’ã‚ã‚‹å€¤ã§å®šæ•°å€ã™ã‚Œã°è‰¯ãã€ãã®ã‚ã‚‹å€¤ã¯æœ€å¤§ãƒ¡ãƒ¢ãƒªã‚„æœ€å¤§FLOPSæ•°ä»¥ä¸‹ï¼ˆãŠã‚ˆã³FLOPSãŒ2ã®Î¦ä¹—ã§å¢—åŠ ã™ã‚‹ã‚ˆã†ãªï¼‰ã¨ã„ã£ãŸåˆ¶ç´„ä¸‹ã§AccuracyãŒæœ€å¤§åŒ–ã•ã‚Œã‚‹å€¤ã‚’ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã§è¦‹ã¤ã‘ã‚‹ï¼ˆã‚‰ã—ã„ã€‚ã–ã£ãã‚Šã¨ã—ãŸç†è§£ï¼‰ã€‚<br>è»¢ç§»å­¦ç¿’ã—ã¦ã‚‚å¤šãã®ã‚¿ã‚¹ã‚¯ã§SoTAé”æˆã—ãŸã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<span class="issue_date">Issue Date: 2021-05-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/344" target="_blank" rel="noopener noreferrer" class="title-link">MLP-like Architecture</a>
<span class="snippet"><span>Comment</span><p>gMLP:å¤§è¦æ¨¡ãªself-attentionãŒç„¡ã„Spatial Gating Unitã‚’æ­è¼‰ã—ãŸã‚·ãƒ³ãƒ—ãƒ«ãªMLPã§ã‚‚ã€Transformerã®æ€§èƒ½ã«è¿‘ã¥ã‘ãŸã‚ˆï¼ˆç‰¹ã«CVï¼‰ã€‚ã¤ã¾ã‚Šã€self-attentionã¯essentialã¨ã„ã†ã‚ã‘ã§ã¯ãªã•ãã†ã ã‚ˆã€‚<br><br>NLPã®å ´åˆã¯gMLPã ã¨Transformerã¨perplexityã§comparableã€ä¸€éƒ¨downstreamã‚¿ã‚¹ã‚¯ã ã¨å‹ã¦ãªã‹ã£ãŸã‘ã©ã€single headã®tiny attentionã‚’è¿½åŠ ã—ãŸã‚‰ã€Transformerã‚’perplexityã¨GLUEã®ä¸€éƒ¨ã‚¿ã‚¹ã‚¯ã§outperformã—ãŸã‚ˆã€‚<br>ã¤ã¾ã‚Šã€Transformerã¿ãŸã„ã«å¤§è¦æ¨¡ãªself-attentionã¯å¿…é ˆã§ã¯ãªãã€å°è¦æ¨¡ã®attentionã§ï¼ˆcross sentenceã®é–¢ä¿‚æ€§ã‚’æ‰ãˆã‚‹ã«ã¯ï¼‰ååˆ†ã ã‚ˆã€‚<br>ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚‚Transformerã‚’ä¸Šå›ã£ãŸã‚ˆã€‚<br><br>ã£ã¦æ„Ÿã˜ï¼Ÿ<br><br>ã‚“ãƒ¼Transformerã«å‹ã£ãŸã¿ãŸã„ãªè¨€ã„æ–¹ã‚’SNSã ã¨è¦‹ã‹ã‘ã‚‹ã‘ã©ã€è©•ä¾¡ã—ã¦ã‚‹ã‚¿ã‚¹ã‚¯ãŒå°‘ãªã„ã—ã€ã©ã¡ã‚‰ã‹ã¨ã„ã†ã¨comparableãªdownstreamã‚¿ã‚¹ã‚¯ãŒå¤šã„ã—ã€ãã‚Œã¯è¨€ã„éãã§ã¯ï¼Ÿ<br>ã“ã®è«–æ–‡ãŒè¨€ã„ãŸã„ã®ã¯ã€å¤§è¦æ¨¡ãªself-attentionãŒæ€§èƒ½ã‚’å‡ºã™ä¸Šã§essentialãªã‚ã‘ã§ã¯ãªã„ã‚ˆã€ã£ã¦ã“ã¨ã§ã‚ã‚Šã€<br><br>ãƒ»CVã®å ´åˆã¯self-attentionã¯å¿…é ˆã§ã¯ãªã„<br>ãƒ»NLPã§ã¯ã€tiny attentionã§ã‚‚ååˆ†<br><br>ã¨ã„ã†æ„Ÿã˜ãªã®ã§ã¯ã€‚<br></p>
<p>ã¾ã‚ã§ã‚‚Transformerã¨comparableãªã‚‰ã€Transformerä¸€å¼·ã§ã¯ç„¡ããªã£ãŸã‚ˆã­</p>
<p>Spatial Gating Unitï¼ˆSGUï¼‰ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®é–¢ä¿‚æ€§ã‚’æ‰ãˆã‚‹ãŸã‚ã®ã‚²ãƒ¼ãƒˆã§ã€SGUãŒç„¡ã„ã¨gMLPãƒ–ãƒ­ãƒƒã‚¯ã¯ãŸã ã®äºŒå±¤ã®FFNã¨ãªã‚‹ã€‚<br><br>SGUã¯ã€å…¥åŠ›ã‚’spatial dimensionã«å¯¾ã—ã¦ç·šå½¢å¤‰æ›ã—ãŸå€¤ã¨ã€å…ƒã®å…¥åŠ›ã®element-wiseãªç©ã§è¡¨ç¾ã™ã‚‹ã€‚ã“ã®ç·šå½¢å¤‰æ›ã‚’ã™ã‚‹éš›ã¯ã€Wã®å€¤ã‚’0ã®è¿‘å‚ã§åˆæœŸåŒ–ã—ã€ãƒã‚¤ã‚¢ã‚¹é …ã‚’1ã«åˆæœŸåŒ–ã™ã‚‹ã“ã¨ãŒã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ã ã£ãŸã€‚ã“ã‚Œã¯ã€å­¦ç¿’ã®åˆã‚ã§ã¯ç·šå½¢å¤‰æ›ã¯identical mappingã«è¿‘ã„ã‚‚ã®ã¨ãªã‚‹ãŸã‚ã€gMLPãƒ–ãƒ­ãƒƒã‚¯ã¯FFNã«è¿‘ã„ã‚‚ã®ã¨ãªã‚‹ã€‚ã“ã‚ŒãŒå­¦ç¿’ãŒé€²ã‚€ã«ã¤ã‚ŒWã®é‡ã¿ãŒèª¿æ•´ã•ã‚Œã€cross tokenã®é–¢ä¿‚æ€§ã‚’æ‰ãˆãŸãƒ–ãƒ­ãƒƒã‚¯ã¸ã¨å¾ã€…ã«å¤‰åŒ–ã—ã¦ã„ãã“ã¨ã«ãªã‚‹ã€‚<br>ã¾ãŸã€SGUã¸ã®å…¥åŠ›ã¯GLUã®ã‚ˆã†ã«channel dimensionã«äºŒåˆ†å‰²ã—ã€ç‰‡æ–¹ã‚’element-wiseç©ã«ã€ã‚‚ã†ä¸€æ–¹ã‚’spatialãªç·šå½¢å¤‰æ›ã«åˆ©ç”¨ã™ã‚‹ï¼ˆ4ç¨®é¡è©¦ã—ãŸä¸­ã§ä¸€ç•ªæ€§èƒ½ãŒè‰¯ã‹ã£ãŸï¼‰ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/CommentGeneration.html" target="_blank" rel="noopener noreferrer">#CommentGeneration</a>
<span class="issue_date">Issue Date: 2019-09-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/326" target="_blank" rel="noopener noreferrer" class="title-link">Cross-domain personalized image captioning, Long+, 2019</a>
<button onclick="hideContent(0)" style="display: none;">hide</button>
</div>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<script>
  document.addEventListener('DOMContentLoaded', function() {
    const tweets = document.querySelectorAll('.tweet-embed[data-embed]');

    if ('IntersectionObserver' in window) {
      const observer = new IntersectionObserver((entries, obs) => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            const el = entry.target;
            const html = el.getAttribute('data-embed');
            if (html) {
              const placeholder = el.querySelector('.tweet-placeholder');
              if (placeholder) placeholder.remove();

              el.innerHTML = html.trim();

              if (window.twttr?.widgets?.load) {
                window.twttr.widgets.load(el);
              }
            }
            obs.unobserve(el); // å‡¦ç†æ¸ˆã¿ã¯ç›£è¦–è§£é™¤
          }
        });
      }, {
        rootMargin: '500px 0px', // ç”»é¢æ‰‹å‰200pxã§èª­ã¿è¾¼ã¿é–‹å§‹
        threshold: 0
      });

      tweets.forEach(tweet => observer.observe(tweet));

    } else {
      // IntersectionObserveræœªå¯¾å¿œãƒ–ãƒ©ã‚¦ã‚¶ç”¨ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
      function lazyLoadFallback() {
        tweets.forEach(el => {
          if (el.getAttribute('data-embed') && el.getBoundingClientRect().top < window.innerHeight) {
            const html = el.getAttribute('data-embed');
            const loadingImg = el.querySelector('.tweet-loading');
            if (loadingImg) loadingImg.remove();
            el.innerHTML = html.trim();
            el.removeAttribute('data-embed');
            if (window.twttr?.widgets?.load) {
              window.twttr.widgets.load(el);
            }
          }
        });
      }
      window.addEventListener('scroll', lazyLoadFallback);
      lazyLoadFallback();
    }
  });
</script>



    </div>

</article>
<div class="post-nav">
<a class="previous" href="/paper_notes/articles/ComputerUse.html" title="ComputerUseã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">ComputerUseã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§</a><a class="next" href="/paper_notes/articles/ConceptErasure.html" title="ConceptErasureã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">ConceptErasureã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§</a>
</div>
<div class="post-related">
      <div>Related Articles</div>
      <ul>
        <li class="">
          <a class="post-link" href="/paper_notes/articles/Pretraining.html" title="Pretrainingã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            Pretrainingã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/KnowledgeEditing.html" title="KnowledgeEditingã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            KnowledgeEditingã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/MultitaskLearning.html" title="MultitaskLearningã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            MultitaskLearningã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/IJCNLP.html" title="IJCNLPã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            IJCNLPã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
</ul>
    </div>
<div class="post-comments"></div></section>
</div>


  </section>
  <section class="sidebar" style="margin-left: 15px;">
    <!-- Get sidebar items --><style type="text/css" media="screen">
.post-menu ul {
  list-style: none;
  padding: 0;
  margin: 0;
}
</style>

<div class="post-menu">
  <div class="post-menu-title">TOC</div>
  <div class="post-menu-content"></div>
</div>

<script>
  function generateContent() {
    var menu = document.querySelector(".post-menu");
    var menuContent =  menu.querySelector(".post-menu-content");
    var headings = document.querySelector(".post-content").querySelectorAll("h2, h3, h4, h5, h6");

    // Hide menu when no headings
    if (headings.length === 0) {
      return menu.style.display = "none";
    }

    // Generate post menu
    var menuHTML = '';
    for (var i = 0; i < headings.length; i++) {
      var h = headings[i];
      menuHTML += (
        '<li class="h-' + h.tagName.toLowerCase() + '">'
        + '<a href="#h-' + h.getAttribute('id') + '">' + h.textContent + '</a></li>');
    }

    menuContent.innerHTML = '<ul>' + menuHTML + '</ul>';

    // The header element
    var header = document.querySelector('header.site-header');

    function doMenuCollapse(index, over_items) {
      var items = menuContent.firstChild.children;

      if (over_items == undefined) {
        over_items = 20;
      }

      if (items.length < over_items) {
        return;
      }

      var activeItem = items[index];
      var beginItem = activeItem
      var endItem = activeItem
      var beginIndex = index;
      var endIndex = index + 1;
      while (beginIndex >= 0
        && !items[beginIndex].classList.contains('h-h2')) {
        beginIndex -= 1;
      }
      while (endIndex < items.length
        && !items[endIndex].classList.contains('h-h2')) {
        endIndex += 1;
      }
      for (var i = 0; i < beginIndex; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
      for (var i = beginIndex + 1; i < endIndex; i++) {
        item = items[i]
        // if (!item.classList.contains('h-h2')) {
          item.style.display = '';
        // }
      }
      for (var i = endIndex; i < items.length; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
    }

    // Init menu collapsed
    doMenuCollapse(-1);

    // Active the menu item
    window.addEventListener('scroll', function (event) {
      var lastActive = menuContent.querySelector('.active');
      var changed = true;
      var activeIndex = -1;
      for (var i = headings.length - 1; i >= 0; i--) {
        var h = headings[i];
        var headingRect = h.getBoundingClientRect();
        var headerRect = header.getBoundingClientRect();
        var headerTop = Math.floor(headerRect.top);
        var headerHeight = Math.floor(headerRect.height);
        var headerHeight = headerTop + headerHeight + 20;
        if (headingRect.top <= headerHeight) {
          var id = 'h-' + h.getAttribute('id');
          var a = menuContent.querySelector('a[href="#' + id  + '"]');
          var curActive = a.parentNode;
          if (curActive) {
            curActive.classList.add('active');
            activeIndex = i;
          }
          if (lastActive == curActive) {
            changed = false;
          }
          break;
        }
      }
      if (changed) {
        if (lastActive) {
          lastActive.classList.remove('active');
        }
        doMenuCollapse(activeIndex);
      }
      event.preventDefault();
    });
  }
  generateContent();
</script>
</section>
</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/paper_notes/"></data>

  <div class="wrapper">
    <div class="site-footer-inner">
<div>Copyright Â© 2023-current AkihikoWATANABE. The header images and any thumbnail images for the posts were generated by ChatGPT's DALL-E3.</div>
      <div>Powered by <a title="Jekyll is a simple, blog-aware, static site
      generator." href="https://jekyllrb.com/">Jekyll</a> &amp; <a title="Yat, yet
      another theme." href="https://github.com/jeffreytse/jekyll-theme-yat">Yat Theme</a>.</div>
      <div class="footer-col rss-subscribe">Subscribe <a href="/paper_notes/feed.xml">via RSS</a>
</div>
    </div>
  </div>
</footer>
</body>
  </html>
