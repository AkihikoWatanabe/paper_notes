<!DOCTYPE html>
<html lang="ja">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-translate-customization" content="108d9124921d80c3-80e20d618ff053c8-g4f02ec6f3dba68b7-c">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Attentionã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§ | ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ</title>
<meta name="generator" content="Jekyll v3.10.0">
<meta property="og:title" content="Attentionã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
<meta name="author" content="AkihikoWATANABE">
<meta property="og:locale" content="ja">
<meta name="description" content="Attention #EfficiencyImprovement #Pocket #NLP #Transformer">
<meta property="og:description" content="Attention #EfficiencyImprovement #Pocket #NLP #Transformer">
<link rel="canonical" href="http://akihikowatanabe.github.io/paper_notes/articles/Attention.html">
<meta property="og:url" content="http://akihikowatanabe.github.io/paper_notes/articles/Attention.html">
<meta property="og:site_name" content="ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-12-08T00:51:36+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Attentionã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"AkihikoWATANABE"},"dateModified":"2025-12-08T00:51:36+00:00","datePublished":"2025-12-08T00:51:36+00:00","description":"Attention #EfficiencyImprovement #Pocket #NLP #Transformer","headline":"Attentionã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§","mainEntityOfPage":{"@type":"WebPage","@id":"http://akihikowatanabe.github.io/paper_notes/articles/Attention.html"},"url":"http://akihikowatanabe.github.io/paper_notes/articles/Attention.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="icon" href="">
  <link rel="canonical" href="http://akihikowatanabe.github.io">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/main.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/custom_button.css">
  <script src="/paper_notes/assets/js/main.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script><link type="application/atom+xml" rel="alternate" href="http://akihikowatanabe.github.io/paper_notes/feed.xml" title="ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ">
<script>
  function showMore(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "block";
          
      // ã“ã®ãƒœã‚¿ãƒ³ã®å‚ç…§ã‚’å–å¾—ã—ã¦éè¡¨ç¤ºã«ã—ã¾ã™
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "none";
      }
          
      // hideãƒœã‚¿ãƒ³ã®å‚ç…§ã‚’å–å¾—ã—ã¦è¡¨ç¤ºã—ã¾ã™
      const hideButton = contentDiv.querySelector('button[onclick^="hideContent"]');
      if (hideButton) {
        hideButton.style.display = "block";
      }
    }
  }

  function hideContent(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "none";
  
      // moreãƒœã‚¿ãƒ³ã®å‚ç…§ã‚’å–å¾—ã—ã¦è¡¨ç¤ºã—ã¾ã™
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "block";
      }
  
      // ã“ã®ãƒœã‚¿ãƒ³ã‚’éš ã—ã¾ã™
      const hideButtons = document.querySelectorAll('button[onclick^="hideContent"]');
      const hideButton = hideButtons[index];
      if (hideButton) {
        hideButton.style.display = "none";
      }
    }
  }
</script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script charset="UTF-8" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/go.min.js" async></script>



















<script>
// Init highlight js
document.addEventListener('DOMContentLoaded', function(event) {
  var els = document.querySelectorAll('pre code')

  function addLangData(block) {
    var outer = block.parentElement.parentElement.parentElement;
    var lang = block.getAttribute('data-lang');
    for (var i = 0; i < outer.classList.length; i++) {
      var cls = outer.classList[i];
      if (cls.startsWith('language-')) {
        lang = cls;
        break;
      }
    }
    if (!lang) {
      cls = block.getAttribute('class');
      lang = cls ? cls.replace('hljs ', '') : '';
    }
    if (lang.startsWith('language-')) {
      lang = lang.substr(9);
    }
    block.setAttribute('class', 'hljs ' + lang);
    block.parentNode.setAttribute('data-lang', lang);
  }

  function addBadge(block) {
    var enabled = ('true' || 'true').toLowerCase();
    if (enabled == 'true') {
      var pre = block.parentElement;
      pre.classList.add('badge');
    }
  }

  function handle(block) {
    addLangData(block);
    addBadge(block)
    hljs.highlightBlock(block);
  }

  for (var i = 0; i < els.length; i++) {
    var el = els[i];
    handle(el);
  }
});
</script>

<style>
  /* code language badge */
  pre.badge::before {
    content: attr(data-lang);
    color: #fff;
    background-color: #ff4e00;
    padding: 0 .5em;
    border-radius: 0 2px;
    text-transform: uppercase;
    text-align: center;
    min-width: 32px;
    display: inline-block;
    position: absolute;
    right: 0;
  }

  /* fix wrong badge display for firefox browser */
  code > table pre::before {
    display: none;
  }
</style>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe-lightbox.umd.min.js" async></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe.umd.min.js" async></script>
<link href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/photoswipe.min.css" rel="stylesheet">
<style>
  .pswp .pswp__container .pswp__img {
    background-color: white;
  }
</style>

<script>
  function initPhotoSwipe() {
    let customOptions = {};

    try {
      const data = `{"gallery"=>"section.main", "children"=>"a.photo-swipe", "bgOpacity"=>0.8, "padding"=>{"top"=>20, "bottom"=>40, "left"=>100, "right"=>100}}`.replaceAll("=>", ":");
      customOptions = JSON.parse(data);
    } catch (e) {
      console.info("Invalid custom photo previewer options! " + e.message);
    }

    // Define object and gallery options
    const options = Object.assign(
      {
        gallery: "section.main",
        children: "a.photo-swipe",
        photo_scale: 2,
        // dynamic import is not supported in UMD version
        pswpModule: PhotoSwipe,
      },
      customOptions
    );

    const galleryEl = document.querySelector(options.gallery);
    if (!galleryEl) {
      return;
    }

    const imgEls = [];
    const els = galleryEl.querySelectorAll("img:not(.emoji)");
    els.forEach((el) => {
      if (el.src.trim() == "") {
        return;
      }
      if (!imgEls.includes(el)) {
        imgEls.push(el);
      }
    });

    if (imgEls.length === 0) {
      return;
    }

    imgEls.forEach((imgEl) => {
      imgEl.outerHTML = `
      <a class="photo-swipe"
        href="${imgEl.src}"
        data-pswp-width="${
          Math.max(imgEl.naturalWidth, imgEl.width) * options.photo_scale
        }"
        data-pswp-height="${
          Math.max(imgEl.naturalHeight, imgEl.height) * options.photo_scale
        }"
        data-pswp-caption="${imgEl.getAttribute("caption") || imgEl.alt}"
        target="_blank">
        ${imgEl.outerHTML}
      </a>`;
    });

    // Initialize PhotoSwipe 5
    var lightbox = new PhotoSwipeLightbox(options);

    lightbox.init();
  }

  window.addEventListener("load", initPhotoSwipe);
</script>
<meta name="google-site-verification" content="u_DTTPcCZ806iq51zgirHyWq3556HUKGq8AQfH91iFI">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-P70KSB88WH"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-P70KSB88WH');
  </script>

<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>





































































































































<header class="site-header site-header-transparent" role="banner">

  <div class="wrapper">
    <div class="site-header-inner">
<span class="site-brand"><a class="site-brand-inner" rel="author" href="/paper_notes/">
  <img class="site-favicon" title="ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ" src="" onerror="this.style.display='none'">
  ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ
</a>
</span><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger">
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewbox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
              </svg>
            </span>
          </label>

          <div class="trigger">









<span class="page-link">



<div id="google_translate_element" style="display: none;">
</div>

<span class="ct-language">
  <ul class="list-unstyled ct-language-dropdown">
    
      <li>
        <a href="#" class="lang-select" data-lang="en">
          
          <img src="https://cdn.countryflags.com/thumbs/united-states-of-america/flag-400.png" title="English">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="fr">
          
          <img src="https://cdn.countryflags.com/thumbs/france/flag-400.png" title="French">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="zh-CN">
          
          <img src="https://cdn.countryflags.com/thumbs/china/flag-400.png" title="Chinese(Simple)">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ja">
          
          <img src="https://cdn.countryflags.com/thumbs/japan/flag-400.png" title="Japanese">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ko">
          
          <img src="https://cdn.countryflags.com/thumbs/south-korea/flag-400.png" title="Korean">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ru">
          
          <img src="https://cdn.countryflags.com/thumbs/russia/flag-400.png" title="Russian">
          
        </a>
      </li>
    
  </ul>
</span>

<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({
    pageLanguage: 'ja',
    autoDisplay: false,
    layout: google.translate.TranslateElement.InlineLayout.VERTICAL
  }, 'google_translate_element');

  // Links to cross-origin destinations are unsafe
  var gll = document.getElementsByClassName('goog-logo-link')[0];
  if (gll) {
    gll.setAttribute('rel', 'noopener');
  }

  function restoreLang() {
    var iframe = document.getElementsByClassName('goog-te-banner-frame')[0];
    if (!iframe) return;

    var innerDoc = iframe.contentDocument || iframe.contentWindow.document;
    var restore_el = innerDoc.getElementsByTagName("button");

    for (var i = 0; i < restore_el.length; i++) {
      if (restore_el[i].id.indexOf("restore") >= 0) {
        restore_el[i].click();
        var close_el = innerDoc.getElementsByClassName("goog-close-link");
        close_el[0].click();
        return;
      }
    }
  }

  function triggerHtmlEvent(element, eventName) {
    var event;
    if (document.createEvent) {
      event = document.createEvent('HTMLEvents');
      event.initEvent(eventName, true, true);
      element.dispatchEvent(event);
    } else {
      event = document.createEventObject();
      event.eventType = eventName;
      element.fireEvent('on' + event.eventType, event);
    }
  }

  var googleCombo = document.querySelector("select.goog-te-combo");
  var langSelect = document.querySelector('.ct-language');
  langSelect.addEventListener('click', function(event) {
    if (!event.target) {
      return;
    }

    var selected = document.querySelector('.ct-language .ct-language-selected');
    if (selected) {
      selected.classList.remove('ct-language-selected');
    }

    var target = event.target;
    while (target && target !== langSelect ) {
      if (target.matches('.lang-select')) {
        break;
      }
      target = target.parentElement;
    }

    if (target && target.matches('.lang-select')) {
      var lang = target.getAttribute('data-lang');
      if (googleCombo.value == lang) {
        restoreLang();
      } else {
        target.parentElement.classList.add('ct-language-selected');
        googleCombo.value = lang;
        triggerHtmlEvent(googleCombo, 'change');
      }
    }

    event.preventDefault();
  });
}
</script>

<script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit" async></script>
</span>
</div>
        </nav>
</div>
  </div>
</header>

<script>
  function initHeader() {
    var lastScrollY = getScrollPos().y;
    var documentElement = document.documentElement;

    function storeScrollData() {
      var y = getScrollPos().y;documentElement.setAttribute("data-header-transparent", "");var scrollStatus = "";

      if (y <= 0) {
        scrollStatus = "top";
      } else if ((window.innerHeight + y) >= document.body.offsetHeight) {
        scrollStatus = "bottom";
      } else {
        var isScrollDown = (y - lastScrollY > 0) ? true : false;
        scrollStatus = isScrollDown ? "down" : "up";
      }

      lastScrollY = y;
      documentElement.setAttribute("data-scroll-status", scrollStatus);
    }

    window.addEventListener('scroll', function(e) {
      storeScrollData();
    });

    storeScrollData();
  }
  document.addEventListener('DOMContentLoaded', initHeader);
</script>


























































































































































<style>
    html .page-banner .page-banner-img > *:first-child {
      opacity: 0.4;
    }

    html[data-theme="dark"] .page-banner .page-banner-img > *:first-child {
      opacity: 0.2872;
    }
  </style>
<section class="page-banner">
    <div class="page-banner-img">
<div style="background-image: url(/paper_notes/assets/images/banner.png)"></div>
        <img class="img-placeholder" src="/paper_notes/assets/images/banner.png">
</div>
    <div class="wrapper">
      <div class="page-banner-inner">
<header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ</h1>
  <h2 class="post-subtitle">å‹‰å¼·ã—ãŸè«–æ–‡ã‚„æŠ€è¡“ç­‰ã®æƒ…å ±ã‚’Githubã®Issueã«ãƒ¡ãƒ¢ã£ã¦ã„ã‚‹ã²ã¨ã®ãƒ–ãƒ­ã‚°ã€‚
ãã‚Œãªã‚Šã«ãƒ¡ãƒ¢ã®é‡ãŒè“„ç©ã•ã‚Œã¦ããŸã®ã§ã€ä¸€åº¦æ•´ç†ã—ãŸã„ãªã¨æ€ã„ãƒ–ãƒ­ã‚°ã¯ã˜ã‚ã¦ã¿ã¾ã—ãŸï¼
è‡ªç„¶è¨€èªå‡¦ç†(NLP), æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ (RecommenderSystem), Educational Data Mining (EDM), Learning Analytics (LA)ãªã©ã®åˆ†é‡ã®ãƒ¡ãƒ¢ãŒå¤šã„ã¨æ€ã„ã¾ã™ã€‚
æœ€è¿‘ã¯ç‰¹ã«LLMã®å‹‰å¼·ãŒå¤šã‚ã§ã™ :)</h2>

  <div class="post-meta">
    <time class="dt-published" datetime="2025-12-08T00:51:36+00:00" itemprop="datePublished"><i class="fa fa-calendar"></i> Dec 8, 2025
    </time><span class="post-author left-vsplit"><i class="fa fa-pencil"></i> AkihikoWATANABE</span>
    
































    <span class="post-reading-time left-vsplit"><i class="fa fa-clock-o"></i> About 2 hours 7 mins</span>
  </div></header>
</div>
    </div>
  </section><script>
  function hashLocate(hashValue) {
    hashValue = hashValue.replace(/^.*#h-/, '');
    hashValue = decodeURIComponent(hashValue);
    var element = document.getElementById(hashValue);

    if (!element) {
      return;
    }

    var header = document.querySelector('header.site-header');
    var headerRect = header.getBoundingClientRect();
    var headerTop = Math.floor(headerRect.top);
    var headerHeight = Math.floor(headerRect.height);
    var scrollPos = getScrollPos();
    var offsetY = element.offsetTop - (headerTop + headerHeight + 20);

    if (offsetY == scrollPos.y) {
      return;
    }

    if (headerTop == 0  && offsetY > scrollPos.y) {
      offsetY += headerHeight + 2;
    } else if (headerTop < 0  && offsetY < scrollPos.y) {
      offsetY -= headerHeight - 2;
    }

    smoothScrollTo(offsetY);
  }

  // The first event occurred
  window.addEventListener('load', function(event) {
    if (window.location.hash) {
      hashLocate(window.location.hash);
    }
  });

  // The first event occurred
  window.addEventListener('click', function(event) {
    if (event.target.tagName.toLowerCase() == 'a') {
      hashLocate(event.target.getAttribute('href'));
    }
  });
</script>
<div class="theme-toggle">
  <input type="checkbox" id="theme-switch">
  <label for="theme-switch">
    <div class="toggle"></div>
    <div class="names">
      <p class="light">Light</p>
      <p class="dark">Dark</p>
    </div>
  </label>
</div>




<script>
  (function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var nightModeOption = ('off' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
      storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
      var data = storage.getItem('theme');
      try {
        data = JSON.parse(data ? data : '');
      } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
      }
      return data;
    }

    function handleThemeToggle(nightShift) {
      themeData.nightShift = nightShift;
      saveThemeData(themeData);
      html.dataset.theme = nightShift ? 'dark' : 'light';
      setTimeout(function() {
        sw.checked = nightShift ? true : false;
      }, 50);
    }

    function autoThemeToggle() {
      // Next time point of theme toggle
      var now = new Date();
      var toggleAt = new Date();
      var hours = now.getHours();
      var nightShift = hours >= 19 || hours <=7;

      if (nightShift) {
        if (hours > 7) {
          toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
      } else {
        toggleAt.setHours(19);
      }

      toggleAt.setMinutes(0);
      toggleAt.setSeconds(0);
      toggleAt.setMilliseconds(0)

      var delay = toggleAt.getTime() - now.getTime();

      // auto toggle theme mode
      setTimeout(function() {
        handleThemeToggle(!nightShift);
      }, delay);

      return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
      };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
      handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
      var data = autoThemeToggle();

      // Toggle theme by local setting
      if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
      } else {
        handleThemeToggle(themeData.nightShift);
      }
    } else if (nightModeOption == 'manual') {
      handleThemeToggle(themeData.nightShift);
    } else {
      var nightShift = themeData.nightShift;
      if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
      }
      handleThemeToggle(nightShift);
    }
  })();
</script>
<div id="click-to-top" class="click-to-top">
  <i class="fa fa-arrow-up"></i>
</div>
<script>
  (function () {
    const clickToTop = document.getElementById('click-to-top');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 100) {
        clickToTop.classList.add('show')
      }else {
        clickToTop.classList.remove('show')
      }
    });
    clickToTop.addEventListener('click', () => {
      window.smoothScrollTo(0);
    });
  })();
</script>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="framework">
  <section class="main">

     <div class="post">
  <section>









<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="post-content e-content" itemprop="articleBody">

      <h2 id="Attention"> Attention</h2>
<div class="visible-content">
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>


<br>


<span class="issue_date">Issue Date: 2025-11-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3700" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Optimizing Mixture of Block Attention, Guangxuan Xiao+, arXiv'25, 2025.11</a>
<span class="snippet"><span>GPT Summary</span>- Mixture of Block Attention (MoBA)ã¯ã€LLMã«ãŠã‘ã‚‹é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚’åŠ¹ç‡åŒ–ã™ã‚‹ãŒã€ãã®è¨­è¨ˆåŸå‰‡ã‚„GPUå®Ÿè£…ãŒä¸ååˆ†ã§ã‚ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€MoBAã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’åˆ†æã—ã€ã‚¯ã‚¨ãƒªã¨ã‚­ãƒ¼ã®è¦ªå’Œæ€§ã«åŸºã¥ããƒ–ãƒ­ãƒƒã‚¯ã®è­˜åˆ¥èƒ½åŠ›ãŒæ€§èƒ½ã«å½±éŸ¿ã™ã‚‹ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ã€‚æ”¹å–„ç­–ã¨ã—ã¦ã€å°ã•ãªãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã®ä½¿ç”¨ã¨ã‚­ãƒ¼ã«å¯¾ã™ã‚‹çŸ­ã„ç•³ã¿è¾¼ã¿ã®é©ç”¨ã‚’ææ¡ˆã€‚ã“ã‚Œã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«ã€FlashMoBAã‚’å°å…¥ã—ã€åŠ¹ç‡çš„ãªMoBAå®Ÿè¡Œã‚’å¯èƒ½ã«ã™ã‚‹CUDAã‚«ãƒ¼ãƒãƒ«ã‚’é–‹ç™ºã€‚FlashMoBAã¯ã€æœ€å¤§14.7å€ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’é”æˆã—ã€ç†è«–ã«åŸºã¥ãæ”¹å–„ã‚’å®Ÿç”¨åŒ–ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/papers_anon/status/1990266669206536330?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>flash_attention2ã«å¯¾ã—ã¦æœ€å¤§ã§14.7å€ğŸ‘€ã©ã†ã„ã†æ¡ä»¶ã€å®Ÿé¨“ã ã‚ã†ã‹</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ObjectDetection.html" target="_blank" rel="noopener noreferrer">#ObjectDetection</a>


<br>


<span class="issue_date">Issue Date: 2025-11-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3583" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] YOLOv12: Attention-Centric Real-Time Object Detectors, Yunjie Tian+, NeurIPS'25, 2025.02</a>
<span class="snippet"><span>GPT Summary</span>- YOLOv12ã¯ã€æ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æ´»ç”¨ã—ãŸæ–°ã—ã„YOLOãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã€CNNãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã®é€Ÿåº¦ã‚’ç¶­æŒã—ã¤ã¤ã€ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ç‰¹ã«ã€YOLOv12-Nã¯T4 GPUä¸Šã§1.64 msã®æ¨è«–é…å»¶ã§40.6%ã®mAPã‚’é”æˆã—ã€YOLOv10-NãŠã‚ˆã³YOLOv11-Nã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã€‚ã¾ãŸã€YOLOv12ã¯RT-DETRã‚„RT-DETRv2ã‚ˆã‚Šã‚‚å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã—ã€è¨ˆç®—é‡ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ãªãŒã‚‰ã‚‚é«˜é€Ÿãªå®Ÿè¡Œã‚’å®Ÿç¾ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/prakashkagitha/status/1985824676216783193?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/Hybrid.html" target="_blank" rel="noopener noreferrer">#Hybrid</a>


<br>


<span class="issue_date">Issue Date: 2025-10-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3413" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Every Attention Matters: An Efficient Hybrid Architecture for  Long-Context Reasoning, Ling Team+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- Ring-linearãƒ¢ãƒ‡ãƒ«ã‚·ãƒªãƒ¼ã‚ºã€ç‰¹ã«Ring-mini-linear-2.0ï¼ˆ16Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã¨Ring-flash-linear-2.0ï¼ˆ104Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã‚’ç´¹ä»‹ã€‚ä¸¡ãƒ¢ãƒ‡ãƒ«ã¯ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¡ç”¨ã—ã€é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ¨è«–ã§I/Oã¨è¨ˆç®—ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å‰Šæ¸›ã€‚æ¨è«–ã‚³ã‚¹ãƒˆã¯32å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¯†ãªãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦1/10ã€å…ƒã®Ringã‚·ãƒªãƒ¼ã‚ºã¨æ¯”ã¹ã¦50%ä»¥ä¸Šå‰Šæ¸›ã€‚æœ€é©ãªãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚’ç‰¹å®šã—ã€é«˜æ€§èƒ½FP8ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€Œlingheã€ã«ã‚ˆã‚Šãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ãŒ50%å‘ä¸Šã€‚è¤‡æ•°ã®è¤‡é›‘æ¨è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§SOTAãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã€‚</span>
<span class="snippet"><span>Comment</span><p>HF:


<a href="https://huggingface.co/inclusionAI/Ring-flash-linear-2.0-128k" target="_blank" rel="noopener noreferrer">https://huggingface.co/inclusionAI/Ring-flash-linear-2.0-128k</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1981442875192987882?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/grad62304977/status/1981571978382500203?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
</div>
<button onclick="showMore(0)">more</button>
<div class="hidden-content">
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<span class="issue_date">Issue Date: 2025-10-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3304" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Frequency-Dynamic Attention Modulation for Dense Prediction, Linwei Chen+, ICCV'25, 2025.07</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€Vision Transformersï¼ˆViTsï¼‰ã®å‘¨æ³¢æ•°å¿œç­”ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«ã€Frequency-Dynamic Attention Modulationï¼ˆFDAMï¼‰ã‚’ææ¡ˆã€‚FDAMã¯ã€æ³¨æ„è¡Œåˆ—ã®ãƒ­ãƒ¼ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ã‚’åè»¢ã•ã›ã‚‹Attention Inversionï¼ˆAttInvï¼‰ã¨ã€ç•°ãªã‚‹å‘¨æ³¢æ•°æˆåˆ†ã«é‡ã¿ä»˜ã‘ã‚’è¡Œã†Frequency Dynamic Scalingï¼ˆFreqScaleï¼‰ã‹ã‚‰æˆã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¡¨ç¾ã®å´©å£Šã‚’å›é¿ã—ã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚„ç‰©ä½“æ¤œå‡ºãªã©ã®ã‚¿ã‚¹ã‚¯ã§ä¸€è²«ã—ãŸæ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ã€‚ãƒªãƒ¢ãƒ¼ãƒˆã‚»ãƒ³ã‚·ãƒ³ã‚°æ¤œå‡ºã§ã‚‚æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1979109880830267606?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/AttentionSinks.html" target="_blank" rel="noopener noreferrer">#AttentionSinks</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-10-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3270" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] StreamingVLM: Real-Time Understanding for Infinite Video Streams, Ruyi Xu+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- StreamingVLMã¯ã€ç„¡é™ã®ãƒ“ãƒ‡ã‚ªã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ç†è§£ã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã‚’çµ±ä¸€ã—ãŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æ¡ç”¨ã€‚ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚·ãƒ³ã‚¯ã®çŠ¶æ…‹ã‚’å†åˆ©ç”¨ã—ã€çŸ­ã„ãƒ“ã‚¸ãƒ§ãƒ³ãƒˆãƒ¼ã‚¯ãƒ³ã¨é•·ã„ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ä¿æŒã™ã‚‹ã“ã¨ã§ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’æŠ‘ãˆã¤ã¤é«˜ã„æ€§èƒ½ã‚’å®Ÿç¾ã€‚æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯Inf-Streams-Evalã§66.18%ã®å‹ç‡ã‚’é”æˆã—ã€ä¸€èˆ¬çš„ãªVQAèƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã«æˆåŠŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/teortaxestex/status/1978324546370343088?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã“ã‚Œã¯èˆˆå‘³æ·±ã„</p>
<p>ä¿æŒã™ã‚‹KV Cacheã®ä¸Šé™ã‚’æ±ºã‚ã€Sink Token[^1]ã¯ä¿æŒã—[^2]ï¼ˆ512ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã€textual tokenã¯é•·è·é›¢ã§ä¿æŒã€visual tokenã¯çŸ­è·é›¢ã§ä¿æŒã€ã¾ãŸpositional encodingã¨ã—ã¦ã¯RoPEã‚’æ¡ç”¨ã™ã‚‹ãŒã€å›ºå®šã•ã‚ŒãŸãƒ¬ãƒ³ã‚¸ã®ä¸­ã§å‹•çš„ã«indexã‚’æ›´æ–°ã™ã‚‹ã“ã¨ã§ã€ä½ç›¸ã‚’å­¦ç¿’æ™‚ã®rangeã«åã‚OODã«ãªã‚‰ãªã„ã‚ˆã†ãªå·¥å¤«ã‚’ã™ã‚‹ã“ã¨ã§ã€memoryã¨è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’ä¸€å®šã«ä¿ã¡ãªãŒã‚‰long contextã§ã®ä¸€è²«æ€§ã¨ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®latencyã‚’å®Ÿç¾ã™ã‚‹ã€ã¨ã„ã£ãŸè©±ã«ã¿ãˆã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/4d063c90-e10a-4d07-9095-f87ee85c33fb" alt="image" loading="lazy"><br><br>å­¦ç¿’æ™‚ã¯ãƒ•ãƒ¬ãƒ¼ãƒ ãŒoverlapã—ãŸè¤‡æ•°ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†ã‘ã¦ã€ãã‚Œãã‚Œã‚’full attentionã§å­¦ç¿’ã™ã‚‹ï¼ˆSink Tokenã¯ä¿æŒã™ã‚‹ï¼‰ã€‚ã“ã‚Œã¯ä¸Šè¿°ã®inferenceæ™‚ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨æ•´åˆã—ã¦ãŠã‚Šå­¦ç¿’æ™‚ã¨inferenceæ™‚ã®gapãŒæœ€å°é™ã«ãªã‚‹ã€‚ã¾ãŸã€ã‚ã–ã‚ã–long videoã§å­¦ç¿’ã™ã‚‹å¿…è¦ãŒãªã„ã€‚ï¼ˆç¾ã—ã„è§£æ±ºæ–¹æ³•ï¼‰<br><img src="https://github.com/user-attachments/assets/98b50d1b-b9c4-427a-93f5-d385b2bc35a1" alt="image" loading="lazy"><br><br>[^1]: decoder-only transformerã®ä½™å‰°ãªattention scoreã®æ¨ã¦å ´ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹sequenceå†’é ­ã®æ•°ãƒˆãƒ¼ã‚¯ãƒ³(3--4ãƒˆãƒ¼ã‚¯ãƒ³ç¨‹åº¦ï¼‰ã®ã“ã¨ã€‚æœ¬è«–æ–‡ã§ã¯512ãƒˆãƒ¼ã‚¯ãƒ³ã¨å¤§ãã‚ã®Sink Tokenã‚’ä¿æŒã—ã¦ã„ã‚‹ã€‚<br>[^2]: Attention Sinksã«ã‚ˆã£ã¦ã€long contextã®æ€§èƒ½ãŒæ”¹å–„ã•ã‚Œ <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1860" target="_blank" rel="noopener noreferrer">Why do LLMs attend to the first token?, Federico Barbero+, COLM'25</a>
 decoder-only transformerã®å±¤ãŒæ·±ã„éƒ¨åˆ†ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®è¡¨ç¾ãŒå‡ä¸€åŒ–ã•ã‚Œã¦ã—ã¾ã†over-mixingã‚’æŠ‘åˆ¶ã™ã‚‹ <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1861" target="_blank" rel="noopener noreferrer">Efficient Streaming Language Models with Attention Sinks, Guangxuan Xiao+, ICLR'24</a>
 ã“ã¨ãŒå ±å‘Šã•ã‚Œã¦ã„ã‚‹</p>
<p>AttentionSinké–¢é€£ãƒªãƒ³ã‚¯:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1861" target="_blank" rel="noopener noreferrer">Efficient Streaming Language Models with Attention Sinks, Guangxuan Xiao+, ICLR'24</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1860" target="_blank" rel="noopener noreferrer">Why do LLMs attend to the first token?, Federico Barbero+, COLM'25</a>
</p>
<p>â†‘ã“ã‚Œã¯å…ƒãƒã‚¹ãƒˆã‚’èª­ã‚“ã§ï¼ˆã¨è«–æ–‡æ–œã‚èª­ã¿ï¼‰ã®æ„Ÿæƒ³ã®ã‚ˆã†ãªã‚‚ã®ãªã®ã§ã€è©³ç´°ã¯å¾Œã§å…ƒè«–æ–‡ã‚’èª­ã‚€ã€‚</p>
<p>é–¢é€£:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/yukangchen_/status/1978653384539341287?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/AttentionSinks.html" target="_blank" rel="noopener noreferrer">#AttentionSinks</a>
<a class="button" href="articles/CompressionValleys.html" target="_blank" rel="noopener noreferrer">#CompressionValleys</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3194" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Attention Sinks and Compression Valleys in LLMs are Two Sides of the  Same Coin, Enrique Queipo-de-Llano+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æ³¨æ„ã®æ²ˆé™ã¨åœ§ç¸®ã®è°·ã®é–¢é€£æ€§ã‚’ç¤ºã—ã€å¤§è¦æ¨¡ãªæ´»æ€§åŒ–ãŒè¡¨ç¾ã®åœ§ç¸®ã¨ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®æ¸›å°‘ã‚’å¼•ãèµ·ã“ã™ã“ã¨ã‚’ç†è«–çš„ã«è¨¼æ˜ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ãŒä¸­é–“å±¤ã§æ¥µç«¯ãªæ´»æ€§åŒ–ã‚’ç”Ÿã‚€ã¨ã€åœ§ç¸®ã®è°·ã¨æ³¨æ„ã®æ²ˆé™ãŒåŒæ™‚ã«ç¾ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã€‚Transformerãƒ™ãƒ¼ã‚¹ã®LLMãŒãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¸‰ã¤ã®ãƒ•ã‚§ãƒ¼ã‚ºã§å‡¦ç†ã™ã‚‹ã€ŒMix-Compress-Refineã€ç†è«–ã‚’ææ¡ˆã—ã€ã‚¿ã‚¹ã‚¯ä¾å­˜ã®è¡¨ç¾ã®é•ã„ã‚’èª¬æ˜ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1976235853853909048?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/Sparse.html" target="_blank" rel="noopener noreferrer">#Sparse</a>
<a class="button" href="articles/SparseAttention.html" target="_blank" rel="noopener noreferrer">#SparseAttention</a>
<span class="issue_date">Issue Date: 2025-10-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3176" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] vAttention: Verified Sparse Attention, Aditya Desai+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- vAttentionã¯ã€ãƒˆãƒƒãƒ—-$k$ã¨ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’çµ±åˆã—ãŸæ–°ã—ã„ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®$(\epsilon, \delta)$ä¿è¨¼ã‚’æä¾›ã—ã€è¿‘ä¼¼ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿç”¨æ€§ã¨ä¿¡é ¼æ€§ãŒå‘ä¸Šã—ã€ãƒ•ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¨åŒç­‰ã®å“è³ªã‚’ä¿ã¡ãªãŒã‚‰ã€æœ€å¤§20å€ã®ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã‚’å®Ÿç¾ã€‚æ¨è«–ã‚·ãƒŠãƒªã‚ªã§ã‚‚è¿…é€Ÿãªãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒå¯èƒ½ã§ã€å®Ÿé¨“ã«ã‚ˆã‚Šæ€§èƒ½ã®å‘ä¸ŠãŒç¢ºèªã•ã‚ŒãŸã€‚ã‚³ãƒ¼ãƒ‰ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/f14bertolotti/status/1975872208858915012?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-10-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3142" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Compressed Convolutional Attention: Efficient Attention in a Compressed  Latent Space, Tomas Figliolia+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- Compressed Convolutional Attentionï¼ˆCCAï¼‰ã‚’ææ¡ˆã—ã€ã‚¯ã‚¨ãƒªã€ã‚­ãƒ¼ã€ãƒãƒªãƒ¥ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦å…¨ã¦ã®æ³¨æ„æ“ä½œã‚’å…±æœ‰ã•ã‚ŒãŸæ½œåœ¨ç©ºé–“å†…ã§å®Ÿè¡Œã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã€FLOPã‚’å¤§å¹…ã«å‰Šæ¸›ã€‚ã•ã‚‰ã«ã€CCAã¨ãƒ˜ãƒƒãƒ‰å…±æœ‰ã‚’çµ„ã¿åˆã‚ã›ãŸCompressed Convolutional Grouped Query Attentionï¼ˆCCGQAï¼‰ã¯ã€è¨ˆç®—ã¨å¸¯åŸŸå¹…ã®åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã€GQAã‚„MLAã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã€‚å®Ÿé¨“ã§ã¯ã€CCGQAãŒMoEãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ä»–ã®æ³¨æ„ãƒ¡ã‚½ãƒƒãƒ‰ã‚’åœ§å€’ã—ã€MHAã¨æ¯”è¼ƒã—ã¦ã‚‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã—ã¤ã¤KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’8å€åœ§ç¸®ã€‚H100 GPUä¸Šã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨äº‹å‰ãƒ•ã‚£ãƒ«ã®é€Ÿåº¦ã‚’å¤§å¹…ã«å‘ä¸Šã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/stochasticchasm/status/1975390382537252984?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Denseãƒ¢ãƒ‡ãƒ«ã¨MoEãƒ¢ãƒ‡ãƒ«ã§Attentionã®å„ç¨®variantã®æ€§èƒ½ãŒå¤§ããå¤‰åŒ–ã™ã‚‹æ¨¡æ§˜ã€‚ã‹ã¤ã€ææ¡ˆæ‰‹æ³•ã¯ã©ã¡ã‚‰ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚‚è‰¯ã„æ€§èƒ½ã‚’é”æˆã™ã‚‹æ¨¡æ§˜(Fig3,4)ã€‚</p>
<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1975427848371367982?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/teortaxestex/status/1975401062157652266?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/Sparse.html" target="_blank" rel="noopener noreferrer">#Sparse</a>
<a class="button" href="articles/SparseAttention.html" target="_blank" rel="noopener noreferrer">#SparseAttention</a>
<span class="issue_date">Issue Date: 2025-10-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3109" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] VideoNSA: Native Sparse Attention Scales Video Understanding, Enxin Song+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- VideoNSAã¯ã€ãƒ“ãƒ‡ã‚ªç†è§£ã®ãŸã‚ã«Native Sparse Attentionã‚’é©ç”¨ã—ã€é•·ã„æ™‚é–“ã‚¹ã‚±ãƒ¼ãƒ«ã§ã®ä¸€è²«æ€§ã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã€‚216Kã®ãƒ“ãƒ‡ã‚ªæŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§Qwen2.5-VLã‚’ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã«ã¯å¯†ãªæ³¨æ„ã€ãƒ“ãƒ‡ã‚ªã«ã¯NSAã‚’ä½¿ç”¨ã€‚ãƒˆãƒ¼ã‚¯ãƒ³åœ§ç¸®ã‚„å¾“æ¥ã®ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒã—ã¦ã€é•·ã„ãƒ“ãƒ‡ã‚ªç†è§£ã‚„æ™‚é–“çš„æ¨è«–ã§æ€§èƒ½ãŒå‘ä¸Šã€‚ã‚¢ãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åˆ†æã«ã‚ˆã‚Šã€ä¿¡é ¼æ€§ã®ã‚ã‚‹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚„æ³¨æ„ã®æœ€é©é…åˆ†ãªã©ã®é‡è¦ãªç™ºè¦‹ãŒå¾—ã‚‰ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenhaocha1/status/1974164887090684316?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ContextWindow.html" target="_blank" rel="noopener noreferrer">#ContextWindow</a>
<a class="button" href="articles/memory.html" target="_blank" rel="noopener noreferrer">#memory</a>
<span class="issue_date">Issue Date: 2025-09-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3043" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Short window attention enables long-term memorization, LoÃ¯c Cabannes+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- SWAXã¨ã„ã†ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¨xLSTMç·šå½¢RNNå±¤ã‚’çµ„ã¿åˆã‚ã›ã¦ãŠã‚Šã€çŸ­ã„ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãŒé•·æœŸçš„ãªè¨˜æ†¶ã‚’ã‚ˆã‚Šè‰¯ãè¨“ç·´ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚SWAXã¯ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚µã‚¤ã‚ºã‚’ç¢ºç‡çš„ã«å¤‰æ›´ã—ã€çŸ­ã„ãƒ»é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ä¸¡æ–¹ã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/arankomatsuzaki/status/1972874639333605540?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/Sparse.html" target="_blank" rel="noopener noreferrer">#Sparse</a>
<a class="button" href="articles/SparseAttention.html" target="_blank" rel="noopener noreferrer">#SparseAttention</a>
<span class="issue_date">Issue Date: 2025-09-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3003" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via   Semantic-Aware Permutation, Shuo Yang+, NeurIPS'25 Spotlight, 2025.05</a>
<span class="snippet"><span>GPT Summary</span>- Diffusion Transformersï¼ˆDiTsï¼‰ã®å‹•ç”»ç”Ÿæˆã«ãŠã‘ã‚‹ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã€é‡è¦ãƒˆãƒ¼ã‚¯ãƒ³ã®ç‰¹å®šç²¾åº¦ã‚’æœ€å¤§åŒ–ã—è¨ˆç®—ã®ç„¡é§„ã‚’æœ€å°åŒ–ã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸è¦ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯SVG2ã‚’ææ¡ˆã€‚SVG2ã¯æ„å‘³ã«åŸºã¥ããƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¨å†é…ç½®ã‚’è¡Œã„ã€è¨ˆç®—åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€HunyuanVideoãŠã‚ˆã³Wan 2.1ã§ãã‚Œãã‚Œæœ€å¤§2.30å€ãŠã‚ˆã³1.89å€ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’é”æˆã—ã€PSNRã‚’ç¶­æŒã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/chenfeng_x/status/1971343654662046184?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>pj page:


<a href="https://svg-project.github.io/v2/" target="_blank" rel="noopener noreferrer">https://svg-project.github.io/v2/</a>


</p>
<p>Q, Kãã‚Œãã‚Œã«ã¤ã„ã¦ç‹¬ç«‹ã—ã¦kmeansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿæ–½ã—ã€æ„å‘³çš„ã«é¡ä¼¼ã—ãŸQ, Kã‚’ã‚¯ãƒ©ã‚¹ã‚¿åŒ–ã—ã€mapä¸Šã§æ•£ã‚‰ã°ã£ã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®é…ç½®ã‚’æ•´é “ã—ã¦è¨ˆç®—æ©Ÿä¸Šã§åŠ¹ç‡çš„ã«æ‰±ãˆã‚‹ã‚ˆã†ã«ã—ã€å„ã‚¯ãƒ©ã‚¹ã‚¿ã®centroidã‚’attention scoreã®è¨ˆç®—ã«ç”¨ã„ã¦ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¹ã‚³ã‚¢ã‚’è¿‘ä¼¼ã™ã‚‹ã“ã¨ã§è¨ˆç®—ã‚’åŠ¹ç‡åŒ–ã—ã¾ã™ã€ã¨ã„ã£ãŸè©±ãªæ¨¡æ§˜ã€‚ã¾ãŸã€ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªã‚¯ãƒ©ã‚¹ã‚¿ã¨ãã†ã§ã¯ç„¡ã„ã‚‚ã®ãŒã‚ã‚‹ã®ã§ã€på€‹ã®ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªã‚¯ãƒ©ã‚¹ã‚¿ã‚’é¸æŠã—ã•ã‚‰ã«åŠ¹ç‡åŒ–ã‚’ã™ã‚‹æ¨¡æ§˜ã€‚<br><img src="https://github.com/user-attachments/assets/862cf5c8-5583-4f94-8b67-59177c444176" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<a class="button" href="articles/ContextEngineering.html" target="_blank" rel="noopener noreferrer">#ContextEngineering</a>
<span class="issue_date">Issue Date: 2025-09-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2995" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Massive Values in Self-Attention Modules are the Key to Contextual   Knowledge Understanding, Mingyu Jin+, ICML'25, 2025.02</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã¯æ–‡è„ˆçš„çŸ¥è­˜ã®ç†è§£ã«æˆåŠŸã—ã¦ãŠã‚Šã€ç‰¹ã«æ³¨æ„ã‚¯ã‚¨ãƒªï¼ˆQï¼‰ã¨ã‚­ãƒ¼ï¼ˆKï¼‰ã«ãŠã„ã¦é›†ä¸­ã—ãŸå¤§è¦æ¨¡ãªå€¤ãŒä¸€è²«ã—ã¦ç¾ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ã“ã‚Œã‚‰ã®å€¤ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ä¿å­˜ã•ã‚ŒãŸçŸ¥è­˜ã§ã¯ãªãã€ç¾åœ¨ã®æ–‡è„ˆã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹çŸ¥è­˜ã®è§£é‡ˆã«é‡è¦ã§ã‚ã‚‹ã€‚é‡å­åŒ–æˆ¦ç•¥ã®èª¿æŸ»ã«ã‚ˆã‚Šã€ã“ã‚Œã‚‰ã®å€¤ã‚’ç„¡è¦–ã™ã‚‹ã¨æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã€é›†ä¸­ã—ãŸå¤§è¦æ¨¡ãªå€¤ã®å‡ºç¾ãŒãƒ­ã‚¿ãƒªãƒ¼ãƒã‚¸ã‚·ãƒ§ãƒŠãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆRoPEï¼‰ã«ã‚ˆã£ã¦å¼•ãèµ·ã“ã•ã‚Œã‚‹ã“ã¨ã‚’ç™ºè¦‹ã—ãŸã€‚ã“ã‚Œã‚‰ã®çµæœã¯ã€LLMã®è¨­è¨ˆã¨æœ€é©åŒ–ã«é–¢ã™ã‚‹æ–°ãŸãªæ´å¯Ÿã‚’æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=1SMcxxQiSL&noteId=7BAXSETAwU" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=1SMcxxQiSL&noteId=7BAXSETAwU</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-09-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2977" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] UMoE: Unifying Attention and FFN with Shared Experts, Yuanhang Yang+, arXiv'25, 2025.05</a>
<span class="snippet"><span>GPT Summary</span>- Sparse Mixture of Experts (MoE) ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€Transformer ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ãŠã„ã¦æœ‰æœ›ãªæ‰‹æ³•ã§ã‚ã‚Šã€æ³¨æ„å±¤ã¸ã®æ‹¡å¼µãŒæ¢æ±‚ã•ã‚Œã¦ã„ã¾ã™ãŒã€æ—¢å­˜ã®æ³¨æ„ãƒ™ãƒ¼ã‚¹ã® MoE å±¤ã¯æœ€é©ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æœ¬è«–æ–‡ã§ã¯ã€æ³¨æ„å±¤ã¨ FFN å±¤ã® MoE è¨­è¨ˆã‚’çµ±ä¸€ã—ã€æ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®å†å®šå¼åŒ–ã‚’è¡Œã„ã€FFN æ§‹é€ ã‚’æ˜ã‚‰ã‹ã«ã—ã¾ã™ã€‚ææ¡ˆã™ã‚‹UMoEã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€æ³¨æ„ãƒ™ãƒ¼ã‚¹ã® MoE å±¤ã§å„ªã‚ŒãŸæ€§èƒ½ã‚’é”æˆã—ã€åŠ¹ç‡çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å…±æœ‰ã‚’å®Ÿç¾ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/nathancgy4/status/1970887450739281953?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Mixture of Attention Heads (MoA)ã¯ã“ã¡ã‚‰:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3110" target="_blank" rel="noopener noreferrer">[Paper Note] Mixture of Attention Heads: Selecting Attention Heads Per Token, Xiaofeng Zhang+, EMNLP'22, 2022.10</a>
</p>
<p>ã“ã®å›³ãŒã‚ã‹ã‚Šã‚„ã™ã„ã€‚å¾Œã»ã©èª¬æ˜ã‚’è¿½è¨˜ã™ã‚‹ã€‚ã–ã£ãã‚Šè¨€ã†ã¨ã€MoAã‚’å‰æã¨ã—ãŸã¨ãã«ã€æœ€å¾Œã®å‡ºåŠ›ã®å¤‰æ›éƒ¨åˆ†VW_oã‚’FFNã«ã‚ˆã‚‹å¤‰æ›ï¼ˆã¤ã¾ã‚ŠFFN Expertsã®ä¸€ã¤ï¼‰ã¨ã¿ãªã—ã¦ã€self-attentionã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ··ãœåˆã‚ã›ã‚‹ã¨ã„ã†è¶£æ—¨ã‚’å¤±ã‚ãªã„ç¯„å›²ã§è¨ˆç®—é †åºã‚’èª¿æ•´ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒŸãƒƒã‚¯ã‚¹ã™ã‚‹éƒ¨åˆ†ã‚’å…ˆã«æŒã£ã¦ãã‚‹ï¼‰ã™ã‚‹ã¨ã€FFNã®MoEã¨MoAã¯åŒã˜æ çµ„ã¿ã§æ‰±ãˆã‚‹ãŸã‚ã€expertsã‚’å…±æœ‰ã§ãã¦ãƒ¡ãƒ¢ãƒªã‚’å‰Šæ¸›ã§ãã€ã‹ã¤MoAã«ã‚ˆã£ã¦å¿…è¦ãªç®‡æ‰€ã®ã¿ã«attendã™ã‚‹èƒ½åŠ›ãŒé«˜ã¾ã‚Šæ€§èƒ½ã‚‚ä¸ŠãŒã‚Šã¾ã™ã€ã¿ãŸã„ãªè©±ã«è¦‹ãˆã‚‹ã€‚<br><br><img src="https://github.com/user-attachments/assets/44ba6bee-d1fa-4385-a4c6-2c937cc15ea5" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/248e1bc5-6c14-4b2d-9aed-c1d7359c605e" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<span class="issue_date">Issue Date: 2025-09-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2817" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Forgetting Transformer: Softmax Attention with a Forget Gate, Zhixuan Lin+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- å¿˜å´ã‚²ãƒ¼ãƒˆã‚’å–ã‚Šå…¥ã‚ŒãŸãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã€ŒFoXã€ã‚’ææ¡ˆã€‚FoXã¯é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚„ä¸‹æµã‚¿ã‚¹ã‚¯ã§ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’å¿…è¦ã¨ã—ãªã„ã€‚å†å¸°çš„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ã‚‚å„ªã‚ŒãŸèƒ½åŠ›ã‚’ä¿æŒã—ã€æ€§èƒ½å‘ä¸Šã®ãŸã‚ã®ã€ŒProã€ãƒ–ãƒ­ãƒƒã‚¯è¨­è¨ˆã‚’å°å…¥ã€‚ã‚³ãƒ¼ãƒ‰ã¯GitHubã§å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=q2Lnyegkr8" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=q2Lnyegkr8</a>


</p>
<p>code:


<a href="https://github.com/zhixuan-lin/forgetting-transformer" target="_blank" rel="noopener noreferrer">https://github.com/zhixuan-lin/forgetting-transformer</a>


</p>
<p>éå¸¸ã«ãŠã‚‚ã—ã‚ãã†</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Pruning.html" target="_blank" rel="noopener noreferrer">#Pruning</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-09-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2816" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Adaptive Computation Pruning for the Forgetting Transformer, Zhixuan Lin+, COLM'25</a>
<span class="snippet"><span>GPT Summary</span>- Forgeting Transformerï¼ˆFoXï¼‰ã¯ã€å¿˜å´ã‚²ãƒ¼ãƒˆã‚’ç”¨ã„ãŸã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ç‰¹å¾´ã¨ã—ã€å¾“æ¥ã®Transformerã¨æ¯”è¼ƒã—ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã€‚FoXã®ç‰¹æ€§ã‚’æ´»ã‹ã—ã€é©å¿œè¨ˆç®—ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆACPï¼‰ã‚’ææ¡ˆã—ã€è¨ˆç®—ã‚’å‹•çš„ã«ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€FLOPsã¨ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã‚’ç´„70%å‰Šæ¸›ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œæ™‚é–“ã‚’50%ã‹ã‚‰70%çŸ­ç¸®ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’10%ã‹ã‚‰40%å‘ä¸Šã•ã›ãŸã€‚æ€§èƒ½ã®åŠ£åŒ–ã¯ãªãã€é•·ã„æ–‡è„ˆé•·ã§ã¯ã•ã‚‰ãªã‚‹è¨ˆç®—ã‚³ã‚¹ãƒˆã®ç¯€ç´„ãŒå¯èƒ½ã§ã‚ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>code:


<a href="https://github.com/zhixuan-lin/forgetting-transformer" target="_blank" rel="noopener noreferrer">https://github.com/zhixuan-lin/forgetting-transformer</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/zhxlin/status/1967596994362220761?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>openreview:


<a href="https://openreview.net/forum?id=xNj14CY5S1#discussion" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=xNj14CY5S1#discussion</a>


</p>
<p>å…ˆè¡Œç ”ç©¶:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2817" target="_blank" rel="noopener noreferrer">[Paper Note] Forgetting Transformer: Softmax Attention with a Forget Gate, Zhixuan Lin+, ICLR'25</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/AttentionSinks.html" target="_blank" rel="noopener noreferrer">#AttentionSinks</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-09-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2760" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Gated Attention for Large Language Models: Non-linearity, Sparsity, and   Attention-Sink-Free, Zihan Qiu+, NeurIPS'25 Best Paper</a>
<span class="snippet"><span>GPT Summary</span>- ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®åŠ¹æœã‚’èª¿æŸ»ã™ã‚‹ãŸã‚ã€å¼·åŒ–ã•ã‚ŒãŸã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ãƒãƒªã‚¢ãƒ³ãƒˆã‚’å®Ÿé¨“ã€‚15B Mixture-of-Expertsãƒ¢ãƒ‡ãƒ«ã¨1.7Bå¯†ãªãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã—ã€ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰ã‚²ãƒ¼ãƒˆã®é©ç”¨ãŒæ€§èƒ½å‘ä¸Šã«å¯„ä¸ã™ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šè¨“ç·´ã®å®‰å®šæ€§ãŒå‘ä¸Šã—ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç‰¹æ€§ã‚‚æ”¹å–„ã€‚ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãŒã€Œã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚·ãƒ³ã‚¯ã€ã‚’è»½æ¸›ã—ã€é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®å¤–æŒ¿æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚é–¢é€£ã‚³ãƒ¼ãƒ‰ã¨ãƒ¢ãƒ‡ãƒ«ã‚‚å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1965895191929434513?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/nathancgy4/status/1971262170949091474?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>NeurIPS'25 Best Paper:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/alibaba_qwen/status/1993854188171006453?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<span class="issue_date">Issue Date: 2025-09-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2756" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Causal Attention with Lookahead Keys, Zhuoqing Song+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- CASTLEï¼ˆCAuSal aTtention with Lookahead kEysï¼‰ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚­ãƒ¼ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¿œã˜ã¦ç¶™ç¶šçš„ã«æ›´æ–°ã™ã‚‹æ–°ã—ã„å› æœæ³¨æ„æ©Ÿæ§‹ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å¾Œã«ç¾ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ã®æƒ…å ±ã‚’çµ±åˆã—ã¤ã¤è‡ªå·±å›å¸°çš„ç‰¹æ€§ã‚’ä¿æŒã€‚åŠ¹ç‡çš„ãªä¸¦åˆ—ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿç¾ã—ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æ¨™æº–çš„ãªå› æœæ³¨æ„æ©Ÿæ§‹ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1965630709479120897?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<span class="issue_date">Issue Date: 2025-08-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2428" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Less Is More: Training-Free Sparse Attention with Global Locality for  Efficient Reasoning, Lijie Yang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ã€ŒLessIsMoreã€ã¨ã„ã†æ–°ã—ã„ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ææ¡ˆã€‚ã“ã‚Œã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸è¦ã§ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ´»ç”¨ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³é¸æŠã‚’åŠ¹ç‡åŒ–ã€‚ç²¾åº¦ã‚’ç¶­æŒã—ã¤ã¤ã€ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é€Ÿåº¦ã‚’1.1å€å‘ä¸Šã•ã›ã€ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’2å€å‰Šæ¸›ã€‚æ—¢å­˜æ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦1.13å€ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’å®Ÿç¾ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/lijieyyang/status/1955139186530328633?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒªãƒ¼ã§1.1å€ã®ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é€Ÿåº¦ã§æ€§èƒ½ã‚‚Full Attentionã¨åŒç­‰ä»¥ä¸Šã®Sparse Attentionã‚‰ã—ã„</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-08-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2396" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Fast and Simplex: 2-Simplicial Attention in Triton, Aurko Roy+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- 2-ã‚·ãƒ³ãƒ—ãƒªã‚·ã‚¢ãƒ«ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã€æ¨™æº–çš„ãªãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚ˆã‚Šã‚‚å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚å›ºå®šã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³äºˆç®—å†…ã§ã€æ•°å­¦ã‚„æ¨è«–ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ãƒ‰ãƒƒãƒˆç©ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ä¸Šå›ã‚‹çµæœã‚’å¾—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/scaling01/status/1954682957798715669?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2325" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Efficient Attention Mechanisms for Large Language Models: A Survey, Yutao Sun+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è‡ªå·±æ³¨æ„ã®è¤‡é›‘ã•ãŒé•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®éšœå®³ã¨ãªã£ã¦ã„ã‚‹ã€‚ã“ã‚Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€ç·šå½¢æ³¨æ„æ‰‹æ³•ã¨ã‚¹ãƒ‘ãƒ¼ã‚¹æ³¨æ„æŠ€è¡“ãŒå°å…¥ã•ã‚Œã€è¨ˆç®—åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã¤ã¤ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’ä¿æŒã™ã‚‹ã€‚æœ¬ç ”ç©¶ã¯ã€ã“ã‚Œã‚‰ã®é€²å±•ã‚’ä½“ç³»çš„ã«ã¾ã¨ã‚ã€åŠ¹ç‡çš„ãªæ³¨æ„ã‚’å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«çµ„ã¿è¾¼ã‚€æ–¹æ³•ã‚’åˆ†æã—ã€ç†è«–ã¨å®Ÿè·µã‚’çµ±åˆã—ãŸã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªãƒ¢ãƒ‡ãƒ«è¨­è¨ˆã®åŸºç¤ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/1950287053046022286?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p><img src="https://github.com/user-attachments/assets/df56fa40-4206-4d12-9172-39f7b36f19c7" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-06-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2025" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Log-Linear Attention, Han Guo+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- å¯¾æ•°ç·šå½¢æ³¨æ„ã‚’ææ¡ˆã—ã€ç·šå½¢æ³¨æ„ã®åŠ¹ç‡æ€§ã¨ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹æ³¨æ„ã®è¡¨ç¾åŠ›ã‚’ä¸¡ç«‹ã€‚å›ºå®šã‚µã‚¤ã‚ºã®éš ã‚ŒçŠ¶æ…‹ã‚’å¯¾æ•°çš„ã«æˆé•·ã™ã‚‹éš ã‚ŒçŠ¶æ…‹ã«ç½®ãæ›ãˆã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å¯¾æ•°ç·šå½¢ã«æŠ‘ãˆã‚‹ã€‚Mamba-2ã¨Gated DeltaNetã®å¯¾æ•°ç·šå½¢ãƒãƒªã‚¢ãƒ³ãƒˆãŒç·šå½¢æ™‚é–“ã®ãƒãƒªã‚¢ãƒ³ãƒˆã¨æ¯”è¼ƒã—ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã“ã¨ã‚’ç¢ºèªã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1932194773559107911?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1931432543766847887?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<span class="issue_date">Issue Date: 2025-05-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1977" target="_blank" rel="noopener noreferrer" class="title-link">Insights into DeepSeek-V3: Scaling Challenges and Reflections on  Hardware for AI Architectures, Chenggang Zhao+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- DeepSeek-V3ã¯ã€2,048å°ã®NVIDIA H800 GPUã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢åˆ¶ç´„ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã®å…±åŒè¨­è¨ˆã‚’ç¤ºã™ã€‚ãƒ¡ãƒ¢ãƒªåŠ¹ç‡å‘ä¸Šã®ãŸã‚ã®ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰æ½œåœ¨æ³¨æ„ã‚„ã€è¨ˆç®—ã¨é€šä¿¡ã®æœ€é©åŒ–ã‚’å›³ã‚‹å°‚é–€å®¶ã®æ··åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€FP8æ··åˆç²¾åº¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãªã©ã®é©æ–°ã‚’å¼·èª¿ã€‚ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«åŸºã¥ãå°†æ¥ã®æ–¹å‘æ€§ã«ã¤ã„ã¦è­°è«–ã—ã€AIãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«å¿œãˆã‚‹ãŸã‚ã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã¨ãƒ¢ãƒ‡ãƒ«ã®å…±åŒè¨­è¨ˆã®é‡è¦æ€§ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/deedydas/status/1924512147947848039?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/AttentionSinks.html" target="_blank" rel="noopener noreferrer">#AttentionSinks</a>
<span class="issue_date">Issue Date: 2025-04-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1878" target="_blank" rel="noopener noreferrer" class="title-link">Using Attention Sinks to Identify and Evaluate Dormant Heads in  Pretrained LLMs, Pedro Sandoval-Segura+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã«ãŠã‘ã‚‹ã€Œä¼‘çœ ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã€ã‚’å®šç¾©ã—ã€ãã®å½±éŸ¿ã‚’èª¿æŸ»ã€‚6ã¤ã®ãƒ¢ãƒ‡ãƒ«ã¨5ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ãŸå®Ÿé¨“ã§ã€ä¼‘çœ ãƒ˜ãƒƒãƒ‰ã®å‡ºåŠ›ã‚’ã‚¼ãƒ­ã«ã—ã¦ã‚‚ç²¾åº¦ã‚’ç¶­æŒã§ãã‚‹ã“ã¨ã‚’ç¢ºèªã€‚ä¼‘çœ ãƒ˜ãƒƒãƒ‰ã¯äº‹å‰å­¦ç¿’ã®åˆæœŸã«å‡ºç¾ã—ã€å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã®ç‰¹æ€§ã«ä¾å­˜ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/psandovalsegura/status/1909652533334712691?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-04-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1871" target="_blank" rel="noopener noreferrer" class="title-link">KAA: Kolmogorov-Arnold Attention for Enhancing Attentive Graph Neural  Networks, Taoran Fang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æ³¨æ„GNNã«ãŠã‘ã‚‹ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã®ç†è§£ãŒä¸è¶³ã—ã¦ã„ã‚‹ä¸­ã€æœ¬ç ”ç©¶ã§ã¯ã‚³ãƒ«ãƒ¢ã‚´ãƒ­ãƒ•ãƒ»ã‚¢ãƒ«ãƒãƒ«ãƒ‰æ³¨æ„ï¼ˆKAAï¼‰ã‚’ææ¡ˆã—ã€ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°é–¢æ•°ã‚’çµ±ä¸€ã€‚KAAã¯KANã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’çµ±åˆã—ã€ã»ã¼ã™ã¹ã¦ã®æ³¨æ„GNNã«é©ç”¨å¯èƒ½ã§ã€è¡¨ç¾åŠ›ãŒå‘ä¸Šã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€KAAå¼·åŒ–ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°é–¢æ•°ãŒå…ƒã®ã‚‚ã®ã‚’ä¸€è²«ã—ã¦ä¸Šå›ã‚Šã€æœ€å¤§20%ä»¥ä¸Šã®æ€§èƒ½å‘ä¸Šã‚’é”æˆã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1908966571227398449?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-04-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1870" target="_blank" rel="noopener noreferrer" class="title-link">XAttention: Block Sparse Attention with Antidiagonal Scoring, Ruyi Xu+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- XAttentionã¯ã€Long-Context Transformer Modelsã«ãŠã‘ã‚‹é•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ¨è«–ã‚’åŠ é€Ÿã™ã‚‹ãƒ—ãƒ©ã‚°ã‚¢ãƒ³ãƒ‰ãƒ—ãƒ¬ã‚¤ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã€æ³¨æ„è¡Œåˆ—ã®åå¯¾å¯¾è§’ç·šã®å€¤ã‚’ç”¨ã„ã¦ãƒ–ãƒ­ãƒƒã‚¯ã®é‡è¦åº¦ã‚’è©•ä¾¡ã—ã€éæœ¬è³ªçš„ãªãƒ–ãƒ­ãƒƒã‚¯ã‚’å‰ªå®šã™ã‚‹ã“ã¨ã§é«˜ã„ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§ã‚’å®Ÿç¾ã€‚RULERã‚„LongBenchãªã©ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ãƒ•ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã«åŒ¹æ•µã™ã‚‹ç²¾åº¦ã‚’ä¿ã¡ãªãŒã‚‰ã€æœ€å¤§13.5å€ã®è¨ˆç®—åŠ é€Ÿã‚’é”æˆã€‚XAttentionã¯LCTMsã®åŠ¹ç‡çš„ãªå±•é–‹ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1908966571227398449?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-04-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1869" target="_blank" rel="noopener noreferrer" class="title-link">Slim attention: cut your context memory in half without loss of accuracy  -- K-cache is all you need for MHA, Nils Graef+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Slim attentionã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã®MHAã«ãŠã„ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒ¢ãƒªã‚’2å€ã«ç¸®å°ã—ã€æ¨è«–é€Ÿåº¦ã‚’æœ€å¤§2å€å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã§ã€ç²¾åº¦ã‚’æãªã†ã“ã¨ãªãå®Ÿè£…å¯èƒ½ã§ã™ã€‚ç‰¹ã«ã€Whisperãƒ¢ãƒ‡ãƒ«ã§ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒ¢ãƒªã‚’8å€å‰Šæ¸›ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆã‚’5å€é€Ÿãã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€ç¨€ãªã‚±ãƒ¼ã‚¹ã§ã¯T5-11Bãƒ¢ãƒ‡ãƒ«ã§ãƒ¡ãƒ¢ãƒªã‚’32å€å‰Šæ¸›ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1908966571227398449?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/AttentionSinks.html" target="_blank" rel="noopener noreferrer">#AttentionSinks</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-04-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1862" target="_blank" rel="noopener noreferrer" class="title-link">When Attention Sink Emerges in Language Models: An Empirical View, Xiangming Gu+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- è¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹ã€Œã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚·ãƒ³ã‚¯ã€ã¯ã€æ„å‘³çš„ã«é‡è¦ã§ãªã„ãƒˆãƒ¼ã‚¯ãƒ³ã«å¤§ããªæ³¨æ„ã‚’å‰²ã‚Šå½“ã¦ã‚‹ç¾è±¡ã§ã‚ã‚Šã€ã•ã¾ã–ã¾ãªå…¥åŠ›ã«å¯¾ã—ã¦å°ã•ãªãƒ¢ãƒ‡ãƒ«ã§ã‚‚æ™®éçš„ã«å­˜åœ¨ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚·ãƒ³ã‚¯ã¯äº‹å‰å­¦ç¿’ä¸­ã«å‡ºç¾ã—ã€æœ€é©åŒ–ã‚„ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒã€æå¤±é–¢æ•°ãŒãã®å‡ºç¾ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã€‚ç‰¹ã«ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚·ãƒ³ã‚¯ã¯ã‚­ãƒ¼ã®ãƒã‚¤ã‚¢ã‚¹ã®ã‚ˆã†ã«æ©Ÿèƒ½ã—ã€æƒ…å ±ã‚’æŒãŸãªã„è¿½åŠ ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¹ã‚³ã‚¢ã‚’ä¿å­˜ã™ã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸã€‚ã“ã®ç¾è±¡ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³ãŒã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹æ­£è¦åŒ–ã«ä¾å­˜ã—ã¦ã„ã‚‹ã“ã¨ã‹ã‚‰éƒ¨åˆ†çš„ã«ç”Ÿã˜ã¦ãŠã‚Šã€æ­£è¦åŒ–ãªã—ã®ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã«ç½®ãæ›ãˆã‚‹ã“ã¨ã§ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚·ãƒ³ã‚¯ã®å‡ºç¾ã‚’é˜²ãã“ã¨ãŒã§ãã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>Sink Rateã¨å‘¼ã°ã‚Œã‚‹ã€å…¨ã¦ã®headã®First Tokenã«å¯¾ã™ã‚‹attention scoreã®ã†ã¡ï¼ˆlayer l * head hå€‹å­˜åœ¨ã™ã‚‹ï¼‰ã€ã©ã®ç¨‹åº¦ã®å‰²åˆã®ã‚¹ã‚³ã‚¢ãŒé–¾å€¤ã‚’ä¸Šå›ã£ã¦ã„ã‚‹ã‹ã‚’è¡¨ã™æŒ‡æ¨™ã‚’ææ¡ˆ<br>ï¼ˆå¾Œã»ã©è©³ç´°ã‚’è¿½è¨˜ã™ã‚‹ï¼‰</p>
<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1860" target="_blank" rel="noopener noreferrer">Why do LLMs attend to the first token?, Federico Barbero+, COLM'25</a>
<br><br>ã®å…ˆè¡Œç ”ç©¶</p>
<p>è‘—è€…ãƒã‚¹ãƒˆï¼ˆopenai-gpt-120Bã‚’å—ã‘ã¦):<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gu_xiangming/status/1952811057673642227?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>openreview:


<a href="https://openreview.net/forum?id=78Nn4QJTEN" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=78Nn4QJTEN</a>


</p></span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/AttentionSinks.html" target="_blank" rel="noopener noreferrer">#AttentionSinks</a>
<a class="button" href="articles/COLM.html" target="_blank" rel="noopener noreferrer">#COLM</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-04-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1860" target="_blank" rel="noopener noreferrer" class="title-link">Why do LLMs attend to the first token?, Federico Barbero+, COLM'25</a>
<span class="snippet"><span>GPT Summary</span>- LLMsã¯æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã«å¼·ãæ³¨æ„ã‚’å‘ã‘ã‚‹ã€Œã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚·ãƒ³ã‚¯ã€ã‚’ç¤ºã—ã€ãã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãŒéå‰°æ··åˆã‚’é¿ã‘ã‚‹æ–¹æ³•ã‚’ç†è«–çš„ãƒ»å®Ÿè¨¼çš„ã«æ¢æ±‚ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•ã‚„ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒƒã‚­ãƒ³ã‚°ãŒã‚·ãƒ³ã‚¯ã®æŒ™å‹•ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’å®Ÿé¨“ã§ç¤ºã—ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç†è§£ã‚’æ·±ã‚ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/1908187563422261411?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Attention Sinkã«ã‚ˆã£ã¦ã€ãƒˆãƒ¼ã‚¯ãƒ³ã®æƒ…å ±ãŒover-mixingã•ã‚Œã‚‹ã“ã¨ãŒæŠ‘åˆ¶ã•ã‚Œã€Decoder-only LLMã®æ·±ã„å±¤ã®representationãŒå‡ä¸€åŒ–ã•ã‚Œã‚‹ã“ã¨ã‚’æŠ‘åˆ¶ã™ã‚‹ï¼ˆï¼promptã®æ‘‚å‹•ã«ãƒ­ãƒã‚¹ãƒˆã«ãªã‚‹ï¼‰ã“ã¨ãŒç¤ºã•ã‚ŒãŸæ¨¡æ§˜ã€‚<br><img src="https://github.com/user-attachments/assets/8a1223c0-5621-42a5-accc-31fa7f636856" alt="image" loading="lazy"><br>Gemma7Bã«ãŠã„ã¦ã€promptä¸­ã®ãƒˆãƒ¼ã‚¯ãƒ³ä¸€èªã‚’ç½®æ›ã—ãŸå¾Œã«ã€Attention Sinkï¼ˆ<bos>ï¼‰ã®æœ‰ç„¡ã«ã‚ˆã£ã¦ã€tokenãƒ¬ãƒ™ãƒ«ã®representationã«å¯¾ã—ã¦ã©ã®ã‚ˆã†ãªæ‘‚å‹•ãŒã‚ã‚‹ã‹ã‚’layerã”ã¨ã«ã¾ã¨ã‚ãŸå›³ãŒä¸‹è¨˜ã®æ¨¡æ§˜ã€‚Attention Sinkã«ã‚ˆã£ã¦ã€tokenã®æ‘‚å‹•ãŒä»–ã®token, layerã«å¯¾ã—ã¦mixingã•ã‚Œã‚‹ã®ãŒæŠ‘åˆ¶ã•ã‚Œã¦ã„ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/b1a4038a-d116-4bd1-b27b-c55eb861bee9" alt="image" loading="lazy"></bos></p>
<p>openreview:


<a href="https://openreview.net/forum?id=tu4dFUsW5z#discussion" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=tu4dFUsW5z#discussion</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-04-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1855" target="_blank" rel="noopener noreferrer" class="title-link">Multi-Token Attention, Olga Golovneva+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ãƒãƒ«ãƒãƒˆãƒ¼ã‚¯ãƒ³ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆMTAï¼‰ã‚’ææ¡ˆã—ã€è¤‡æ•°ã®ã‚¯ã‚¨ãƒªã¨ã‚­ãƒ¼ã®ãƒ™ã‚¯ãƒˆãƒ«ã«åŸºã¥ã„ã¦ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¦ã‚§ã‚¤ãƒˆã‚’æ¡ä»¶ä»˜ã‘ã‚‹ã“ã¨ã§ã€é–¢é€£ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚ˆã‚Šæ­£ç¢ºã«ç‰¹å®šã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚MTAã¯ç•³ã¿è¾¼ã¿æ“ä½œã‚’ç”¨ã„ã¦ã€è¿‘ãã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒäº’ã„ã«å½±éŸ¿ã‚’ä¸ãˆã€è±Šã‹ãªæƒ…å ±ã‚’æ´»ç”¨ã™ã‚‹ã€‚è©•ä¾¡çµæœã‹ã‚‰ã€MTAã¯Transformerãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚Šã€ç‰¹ã«é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã®æƒ…å ±æ¤œç´¢ã«ãŠã„ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jaseweston/status/1907260086017237207?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å¾“æ¥ã®Multi Head Attentionã§ã¯ã€å˜ä½“ã®QKã®ã¿ã‚’åˆ©ç”¨ã—ã¦ã„ãŸã‘ã©ã€è¤‡æ•°ã®QKã®æƒ…å ±ã‚’ç•³ã¿è¾¼ã‚“ã§æ´»ç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã€Headã‚‚ç•³ã¿è¾¼ã¿ã§é‡è¦ãªæƒ…å ±ãŒã‚ˆã‚Šä¼æ¬ã•ã‚Œã‚‹ã‚ˆã†ã«ã—ã¦ã€GroupNormalizationã‚’ã‹ã‘ãŸã‚‰Perplexityã®è¦³ç‚¹ã§Differential Transformerã‚’ä¸Šå›ã£ãŸã‚ˆã€ã¨ã„ã†è©±ãªæ¨¡æ§˜ã€‚<br><br><img src="https://github.com/user-attachments/assets/199e0794-a286-486d-9426-d86cfd208750" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/2997a61b-3367-4f43-b85a-ac8fa160391a" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/5ef8ddb0-538b-46e2-94b8-2ef495c938ec" alt="image" loading="lazy"><br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1856" target="_blank" rel="noopener noreferrer">Group Normalization, Yuxin Wu+, arXiv'18</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1466" target="_blank" rel="noopener noreferrer">Differential Transformer, Tianzhu Ye+, N/A, ICLR'25</a>
</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-03-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1775" target="_blank" rel="noopener noreferrer" class="title-link">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse   Attention, Jingyang Yuan+, ACL'25</a>
<span class="snippet"><span>GPT Summary</span>- é•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ãŸã‚ã«ã€è¨ˆç®—åŠ¹ç‡ã‚’æ”¹å–„ã™ã‚‹ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã€ŒNSAã€ã‚’ææ¡ˆã€‚NSAã¯å‹•çš„ãªéšå±¤ã‚¹ãƒ‘ãƒ¼ã‚¹æˆ¦ç•¥ã‚’ç”¨ã„ã€ãƒˆãƒ¼ã‚¯ãƒ³åœ§ç¸®ã¨é¸æŠã‚’çµ„ã¿åˆã‚ã›ã¦ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆèªè­˜ã¨ãƒ­ãƒ¼ã‚«ãƒ«ãªç²¾åº¦ã‚’ä¸¡ç«‹ã€‚å®Ÿè£…æœ€é©åŒ–ã«ã‚ˆã‚Šã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’å®Ÿç¾ã—ã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å¯èƒ½ã«ã™ã‚‹ã“ã¨ã§è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã€‚NSAã¯ãƒ•ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’ç¶­æŒã—ã¤ã¤ã€é•·ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã—ã¦å¤§å¹…ãªã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’é”æˆã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dair_ai/status/1893698286545969311?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ACL'25ã®Best Paperã®ä¸€ã¤:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1950644063952052643?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<span class="issue_date">Issue Date: 2025-04-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1864" target="_blank" rel="noopener noreferrer" class="title-link">Flex Attention: A Programming Model for Generating Optimized Attention  Kernels, Juechu Dong+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- FlexAttentionã¯ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®æ–°ã—ã„ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©é§†å‹•å‹ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã§ã€æ•°è¡Œã®PyTorchã‚³ãƒ¼ãƒ‰ã§å¤šãã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒªã‚¢ãƒ³ãƒˆã‚’å®Ÿè£…å¯èƒ½ã«ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ—¢å­˜ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒªã‚¢ãƒ³ãƒˆã‚’åŠ¹ç‡çš„ã«å®Ÿè£…ã—ã€ç«¶äº‰åŠ›ã®ã‚ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã€‚FlexAttentionã¯ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒªã‚¢ãƒ³ãƒˆã®çµ„ã¿åˆã‚ã›ã‚’å®¹æ˜“ã«ã—ã€çµ„ã¿åˆã‚ã›çˆ†ç™ºã®å•é¡Œã‚’è§£æ±ºã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1863" target="_blank" rel="noopener noreferrer">Llama 4 Series, Meta, 2025.04</a>
<br><br>ã§åˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹Attention</p>
<p>pytochã«ã‚ˆã‚‹è§£èª¬:


<a href="https://pytorch.org/blog/flexattention/" target="_blank" rel="noopener noreferrer">https://pytorch.org/blog/flexattention/</a>


<br><br>- Flex Attentionã¯ã‚ªãƒªã‚¸ãƒŠãƒ«ã®Attentionã®QK/sqrt(d_k)ã®è¨ˆç®—å¾Œã«ãƒ¦ãƒ¼ã‚¶ãŒå®šç¾©ã—ãŸé–¢æ•°score_modã‚’é©ç”¨ã™ã‚‹<br>- score_modã‚’å®šç¾©ã™ã‚‹ã“ã¨ã§ã€attention scoreã‚’softmaxã‚’ã‹ã‘ã‚‹ã¾ãˆã«é–¢æ•°ã«ã‚ˆã£ã¦èª¿æ•´ã§ãã‚‹<br>- å¤šãã®attentionã®äºœç¨®ã¯ã»ã¨ã‚“ã©ã®å ´åˆã“ã®æŠ½è±¡åŒ–ã§å¯¾å¿œã§ãã‚‹<br>- score_modã¯QK tokenã®å†…ç©ã«å¯¾å¿œã™ã‚‹ã®ã§ã€QKã®æƒ…å ±ã‚’å—ã‘å–ã‚Šã€ã‚¹ã‚«ãƒ©ãƒ¼å€¤ã‚’è¿”ã›ã°ãªã‚“ã§ã‚‚è‰¯ã„<br>  - score_modã®å®Ÿè£…ä¾‹ã¯å…ƒãƒªãƒ³ã‚¯å‚ç…§<br>- FA2ã¨æ¯”è¼ƒã—ã¦ï¼ˆç¾åœ¨ã®pytorchã§ã®å®Ÿè£…ä¸Šã¯ï¼‰Forward Passã¯90%, Backward Passã¯85%ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã§ã€å°‘ã—é…ã„ãŒä»Šå¾Œæ”¹å–„äºˆå®š</p>
<p>å…ƒè«–æ–‡ã‚ˆã‚Šå¼•ç”¨ã€‚éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ã§ã€æ•°å¼ä¸Šã¯ä¸‹è¨˜ã®ã‚ˆã†ã«è¡¨ã•ã‚Œã‚‹:<br><img src="https://github.com/user-attachments/assets/b4a393f0-46a9-46c6-9a47-0402ba58fb11" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/AttentionSinks.html" target="_blank" rel="noopener noreferrer">#AttentionSinks</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<a class="button" href="articles/Reference%20Collection.html" target="_blank" rel="noopener noreferrer">#Reference Collection</a>
<span class="issue_date">Issue Date: 2025-04-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1861" target="_blank" rel="noopener noreferrer" class="title-link">Efficient Streaming Language Models with Attention Sinks, Guangxuan Xiao+, ICLR'24</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã‚’ãƒãƒ«ãƒãƒ©ã‚¦ãƒ³ãƒ‰å¯¾è©±ã«å±•é–‹ã™ã‚‹éš›ã®èª²é¡Œã¨ã—ã¦ã€ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã¨é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã¸ã®ä¸€èˆ¬åŒ–ã®é›£ã—ã•ãŒã‚ã‚‹ã€‚ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºã‚’è¶…ãˆã‚‹ã¨å¤±æ•—ã™ã‚‹ãŒã€åˆæœŸãƒˆãƒ¼ã‚¯ãƒ³ã®KVã‚’ä¿æŒã™ã‚‹ã“ã¨ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå›å¾©ã™ã‚‹ã€Œã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚·ãƒ³ã‚¯ã€ã‚’ç™ºè¦‹ã€‚ã“ã‚Œã‚’åŸºã«ã€StreamingLLMã¨ã„ã†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€æœ‰é™ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸLLMãŒç„¡é™ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«ä¸€èˆ¬åŒ–å¯èƒ½ã«ãªã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚StreamingLLMã¯ã€æœ€å¤§400ä¸‡ãƒˆãƒ¼ã‚¯ãƒ³ã§å®‰å®šã—ãŸè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚’å®Ÿç¾ã—ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¨­å®šã§å¾“æ¥ã®æ‰‹æ³•ã‚’æœ€å¤§22.2å€ã®é€Ÿåº¦ã§ä¸Šå›ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>Attention Sinksã¨ã„ã†ç”¨èªã‚’æè¨€ã—ãŸç ”ç©¶<br><br>ä¸‹è¨˜ã®passageãŒAttention Sinksã®å®šç¾©ï¼ˆï¼æœ€åˆã®æ•°ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã¨ãã®æ°—æŒã¡ï¼ˆi.e., softmaxã«ã‚ˆã‚‹attention scoreã¯è¶³ã—åˆã‚ã›ã¦1ã«ãªã‚‰ãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚ã“ã‚ŒãŒéƒ½åˆã®æ‚ªã„ä¾‹ã¨ã—ã¦ã€ç¾åœ¨ã®tokenã®queryã«åŸºã¥ã„ã¦attention scoreã‚’è¨ˆç®—ã™ã‚‹éš›ã«éå»ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®å¤§åŠãŒirrelevantãªçŠ¶æ³ã‚’è€ƒãˆã‚‹ã€‚ã“ã®å ´åˆã€irrelevantãªãƒˆãƒ¼ã‚¯ãƒ³ã«attendã—ãŸãã¯ãªã„ã€‚ãã®ãŸã‚ã€auto-regressiveãªãƒ¢ãƒ‡ãƒ«ã§ã»ã¼å…¨ã¦ã®contextã§å¿…ãšå‡ºç¾ã™ã‚‹æœ€åˆã®æ•°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã€irrelevantãªãƒˆãƒ¼ã‚¯ãƒ³ã«attendã—ãªã„ãŸã‚ã®attention scoreã®æ¨ã¦å ´ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ã®ã†ã«å­¦ç¿’ãŒé€²ã‚€ï¼‰ã®ç†è§£ã«éå¸¸ã«é‡è¦<br>&gt; To understand the failure of window attention, we find an interesting phenomenon of autoregressive LLMs: a surprisingly large amount of attention score is allocated to the initial tokens, irrespective of their relevance to the language modeling task, as visualized in Figure 2. We term these tokens<br>â€œattention sinks". Despite their lack of semantic significance, they collect significant attention scores. We attribute the reason to the Softmax operation, which requires attention scores to sum up to one for all contextual tokens. Thus, even when the current query does not have a strong match in many previous tokens, the model still needs to allocate these unneeded attention values somewhere so it sums up to one. The reason behind initial tokens as sink tokens is intuitive: initial tokens are visible to almost all subsequent tokens because of the autoregressive language modeling nature, making them more readily trained to serve as attention sinks.</p>
<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1860" target="_blank" rel="noopener noreferrer">Why do LLMs attend to the first token?, Federico Barbero+, COLM'25</a>
<br><br>ã®å…ˆè¡Œç ”ç©¶ã€‚ã“ã¡ã‚‰ã§AttentionSinkãŒã©ã®ã‚ˆã†ã«ä½œç”¨ã—ã¦ã„ã‚‹ã®ã‹ï¼ŸãŒåˆ†æã•ã‚Œã¦ã„ã‚‹ã€‚</p>
<p>Figure1ãŒéå¸¸ã«ã‚ã‹ã‚Šã‚„ã™ã„ã€‚Initial Tokenï¼ˆå®Ÿéš›ã¯3--4ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã®KV Cacheã‚’ä¿æŒã™ã‚‹ã“ã¨ã§long contextã®æ€§èƒ½ãŒæ”¹å–„ã™ã‚‹ï¼ˆVanilla)ã€‚ã‚ã‚‹ã„ã¯ã€Softmaxã®åˆ†æ¯ã«1ã‚’è¿½åŠ ã—ãŸé–¢æ•°ã‚’ç”¨æ„ã—ï¼ˆæ•°å¼2)ã€å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®attention scoreã®åˆè¨ˆãŒ1ã«ãªã‚‰ãªãã¦ã‚‚è¨±ã•ã‚Œã‚‹ã‚ˆã†ãªå¤‰å½¢ã‚’ã™ã‚‹ã“ã¨ã§ã€ä½™å‰°ãªattention scoreãŒç”Ÿã˜ãªã„ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã§attention sinkã‚’é˜²ãï¼ˆZero Sink)ã€‚ã“ã‚Œã¯ã€ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ ã—ã€ãã“ã«attention scoreã‚’é€ƒãŒã›ã‚‹ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã«ç›¸å½“ã™ã‚‹ã€‚ã‚‚ã†ä¸€ã¤ã®æ–¹æ³•ã¯ã€globalã«åˆ©ç”¨å¯èƒ½ãªlearnableãªSink Tokenã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ä¸è¦ãªattention scoreã®æ¨ã¦å ´ã¨ã—ã¦æ©Ÿèƒ½ã•ã›ã‚‹ã€‚Table3ã‚’è¦‹ã‚‹ã¨ã€æœ€åˆã®4 tokenã‚’KV Cacheã«ä¿æŒã—ãŸå ´åˆã¯perplexityã¯å¤§ããå¤‰ã‚ã‚‰ãªã„ãŒã€Sink Tokenã‚’å°å…¥ã—ãŸæ–¹ãŒKV Cacheã§ä¿æŒã™ã‚‹Initial Tokenã®é‡ãŒå°‘ãªãã¦ã‚‚Zero Sinkã¨æ¯”ã¹ã‚‹ã¨æ€§èƒ½ãŒè‰¯ããªã‚‹ãŸã‚ã€ä»Šå¾Œãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹éš›ã¯Sink Tokenã‚’å°å…¥ã™ã‚‹ã“ã¨ã‚’è–¦ã‚ã¦ã„ã‚‹ã€‚æ—¢ã«å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦ã¯ã€Zero Sinkã«ã‚ˆã£ã¦long contextã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«å¯¾å‡¦å¯èƒ½ã¨æ€ã‚ã‚Œã‚‹ã€‚<br><br><img width="1122" height="639" alt="Image" src="&lt;a%20href=" https: target="_blank" rel="noopener noreferrer">https://github.com/user-attachments/assets/9d4714e5-02b9-45b5-affd-c6c34eb7c58f"


/&gt;</p>
<p>è‘—è€…ã«ã‚ˆã‚‹è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/guangxuan_xiao/status/1953656755109376040?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>openreview:


<a href="https://openreview.net/forum?id=NG7sS51zVF" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=NG7sS51zVF</a>


</p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2785" target="_blank" rel="noopener noreferrer">Attention ls Off By One, Evanmiller.org, 2023.07</a>
</p></span><br><br>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<span class="issue_date">Issue Date: 2024-11-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1523" target="_blank" rel="noopener noreferrer" class="title-link">Understanding LLMs: A Comprehensive Overview from Training to Inference, Yiheng Liu+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- ChatGPTã®æ™®åŠã«ä¼´ã„ã€LLMsã®ã‚³ã‚¹ãƒˆåŠ¹ç‡ã®è‰¯ã„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã¸ã®é–¢å¿ƒãŒé«˜ã¾ã£ã¦ã„ã‚‹ã€‚æœ¬è«–æ–‡ã§ã¯ã€LLMsã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æŠ€è¡“ã¨æ¨è«–ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæŠ€è¡“ã®é€²åŒ–ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã€ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚„ãƒ¢ãƒ‡ãƒ«åœ§ç¸®ãªã©ã®ã•ã¾ã–ã¾ãªå´é¢ã‚’è­°è«–ã™ã‚‹ã€‚ã¾ãŸã€LLMsã®åˆ©ç”¨æ–¹æ³•ã¨å°†æ¥ã®ç™ºå±•ã«ã¤ã„ã¦ã®æ´å¯Ÿã‚‚æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>[Perplexityï¼ˆå‚è€ƒ;Hallucinationã«æ³¨æ„ï¼‰](


<a href="https://www.perplexity.ai/search/yi-xia-nolun-wen-wodu-minei-ro-7vGwDK_AQX.HDO7j9H8iNA)" target="_blank" rel="noopener noreferrer">https://www.perplexity.ai/search/yi-xia-nolun-wen-wodu-minei-ro-7vGwDK_AQX.HDO7j9H8iNA)</a>


</p>
<p>å˜ãªã‚‹LLMã®ç†è«–çš„ãªèª¬æ˜ã«ã¨ã©ã¾ã‚‰ãšã€å®Ÿç”¨çš„ã«å¿…è¦ãªå„ç¨®ä¸¦åˆ—å‡¦ç†æŠ€è¡“ã€Mixed Precisionã€Offloadingãªã©ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚‚ã¾ã¨ã¾ã£ã¦ã„ã‚‹ã®ãŒã¨ã¦ã‚‚è‰¯ã„ã¨æ€ã†ã€‚</p>
<p>LLM Frameworkã®ã¨ã“ã‚ã«ã€ãƒ¡ã‚¸ãƒ£ãƒ¼ãªã‚‚ã®ãŒç¶²ç¾…ã•ã‚Œã¦ã„ãªã„ã‚ˆã†ã«æ„Ÿã˜ã‚‹ã€‚ãŸã¨ãˆã°ã€Unslothã‚„Liger-Kernelãªã©ã¯Transformersã®éƒ¨åˆ†ã§è¨€åŠã•ã‚Œã¦ã¦ã‚‚è‰¯ã„ã®ã§ã¯ã€ã¨æ„Ÿã˜ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<span class="issue_date">Issue Date: 2024-07-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1338" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] FlashAttention-3: Fast and Accurate Attention with Asynchrony and   Low-precision, Jay Shah+, NeurIPS'24</a>
<span class="snippet"><span>GPT Summary</span>- FlashAttention-3ã¯ã€Hopper GPUä¸Šã§Attentionã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã«ã€3ã¤ã®æŠ€è¡“ã‚’é–‹ç™ºã—ã€H100 GPUã§1.5-2.0å€ã®é€Ÿåº¦å‘ä¸Šã‚’å®Ÿç¾ã€‚FP16ã§740 TFLOPs/sã€FP8ã§ç´„1.2 PFLOPs/sã«é”ã—ã€FP8ã§ã¯æ•°å€¤èª¤å·®ãŒ2.6å€ä½ã„ã“ã¨ã‚’ç¢ºèªã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=tVConYid20&referrer=%5Bthe%20profile%20of%20Tri%20Dao%5D(%2Fprofile%3Fid%3D~Tri_Dao1)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=tVConYid20&referrer=%5Bthe%20profile%20of%20Tri%20Dao%5D(%2Fprofile%3Fid%3D~Tri_Dao1)</a>


</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1270" target="_blank" rel="noopener noreferrer" class="title-link">Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference, Piotr Nawrot+, N_A, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®ç”ŸæˆåŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€Dynamic Memory Compressionï¼ˆDMCï¼‰ãŒææ¡ˆã•ã‚ŒãŸã€‚DMCã¯ã€ç•°ãªã‚‹ãƒ˜ãƒƒãƒ‰ã¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ç•°ãªã‚‹åœ§ç¸®ç‡ã‚’é©ç”¨ã™ã‚‹æ–¹æ³•ã‚’å­¦ç¿’ã—ã€äº‹å‰å­¦ç¿’æ¸ˆã¿LLMsã«é©ç”¨ã•ã‚Œã‚‹ã€‚DMCã¯ã€å…ƒã®ä¸‹æµãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€å¤§4å€ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥åœ§ç¸®ã§ç¶­æŒã—ã¤ã¤ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚DMCã¯ã€GQAã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã•ã‚‰ãªã‚‹åˆ©ç›Šã‚’ã‚‚ãŸã‚‰ã™å¯èƒ½æ€§ãŒã‚ã‚Šã€é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨å¤§ããªãƒãƒƒãƒã‚’å‡¦ç†ã™ã‚‹éš›ã«æœ‰ç”¨ã§ã‚ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å‚è€ƒ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1776755029581676943?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è«–æ–‡ä¸­ã®Figure1ãŒéå¸¸ã«ã‚ã‹ã‚Šã‚„ã™ã„ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/d416547e-f9ca-4c6c-8ebb-7d164bef5283" alt="image" loading="lazy"><br><br></p>
<p>GQA <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1271" target="_blank" rel="noopener noreferrer">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head
  Checkpoints, Joshua Ainslie+, N/A, arXiv'23</a>
 ã¨æ¯”è¼ƒã—ã¦ã€2~4å€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åœ§ç¸®ã—ã¤ã¤ã€ã‚ˆã‚Šé«˜ã„æ€§èƒ½ã‚’å®Ÿç¾ã€‚70Bãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯ã€GQAã§8å€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åœ§ç¸®ã—ãŸä¸Šã§ã€DMCã§è¿½åŠ ã§2å€åœ§ç¸®ã‚’ã‹ã‘ãŸã¨ã“ã‚ã€åŒç­‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®Ÿç¾ã—ã¦ã„ã‚‹ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/7b131f07-5eab-4830-88cc-5f6fd0508958" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/python.html" target="_blank" rel="noopener noreferrer">#python</a>
<a class="button" href="articles/LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-08-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2474" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Efficient Memory Management for Large Language Model Serving with  PagedAttention, Woosuk Kwon+, SOSP'23</a>
<span class="snippet"><span>GPT Summary</span>- PagedAttentionã‚’ç”¨ã„ãŸvLLMã‚·ã‚¹ãƒ†ãƒ ã‚’ææ¡ˆã—ã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒªã®ç„¡é§„ã‚’å‰Šæ¸›ã—ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã§ã®æŸ”è»Ÿãªå…±æœ‰ã‚’å®Ÿç¾ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€åŒãƒ¬ãƒ™ãƒ«ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã§LLMã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’2-4å€å‘ä¸Šã€‚ç‰¹ã«é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚„å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§åŠ¹æœãŒé¡•è‘—ã€‚ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ä¸­ã€‚</span>
<span class="snippet"><span>Comment</span><p>ï¼ˆä»Šæ›´ãªãŒã‚‰ï¼‰vLLMã¯ã“ã¡ã‚‰:<br>


<a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">https://github.com/vllm-project/vllm</a>


<br><br>ç¾åœ¨ã®ä¸»è¦ãªLLM Inference/Serving Engineã®ã²ã¨ã¤ã€‚</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1271" target="_blank" rel="noopener noreferrer" class="title-link">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head  Checkpoints, Joshua Ainslie+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- Multi-query attentionï¼ˆMQAï¼‰ã¯ã€å˜ä¸€ã®key-value headã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãŠã‚Šã€ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®æ¨è«–ã‚’åŠ‡çš„ã«é«˜é€ŸåŒ–ã—ã¦ã„ã¾ã™ã€‚ãŸã ã—ã€MQAã¯å“è³ªã®ä½ä¸‹ã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚Šã€ã•ã‚‰ã«ã¯ã€ã‚ˆã‚Šé€Ÿã„æ¨è«–ã®ãŸã‚ã ã‘ã«åˆ¥å€‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ãŒæœ›ã¾ã—ããªã„å ´åˆã‚‚ã‚ã‚Šã¾ã™ã€‚æ—¢å­˜ã®ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨ˆé‡ã®5%ã‚’ä½¿ç”¨ã—ã¦MQAã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã«ã‚¢ãƒƒãƒ—ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®ãƒ¬ã‚·ãƒ”ã‚’ææ¡ˆã—ã€ã•ã‚‰ã«ã€è¤‡æ•°ã®key-value headã‚’ä½¿ç”¨ã™ã‚‹ãƒãƒ«ãƒã‚¯ã‚¨ãƒªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ä¸€èˆ¬åŒ–ã§ã‚ã‚‹ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã‚¯ã‚¨ãƒªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆGQAï¼‰ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ã‚¢ãƒƒãƒ—ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸGQAãŒã€MQAã¨åŒç­‰ã®é€Ÿåº¦ã§ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã«åŒ¹æ•µã™ã‚‹å“è³ªã‚’é”æˆã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>é€šå¸¸ã®Multi-Head AttentionãŒQKVãŒ1å¯¾1å¯¾å¿œãªã®ã«å¯¾ã—ã€Multi Query Attention (MQA) <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1272" target="_blank" rel="noopener noreferrer">Fast Transformer Decoding: One Write-Head is All You Need, Noam Shazeer, N/A, arXiv'19</a>
  ã¯å…¨ã¦ã®Qã«å¯¾ã—ã¦KVã‚’å…±æœ‰ã™ã‚‹ã€‚ä¸€æ–¹ã€GQAã¯ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«KVã‚’å…±æœ‰ã™ã‚‹ç‚¹ã§ç•°ãªã‚‹ã€‚MQAã¯å¤§å¹…ã«Infeerence` speedãŒæ”¹å–„ã™ã‚‹ãŒã€ç²¾åº¦ãŒåŠ£åŒ–ã™ã‚‹å•é¡ŒãŒã‚ã£ãŸã€‚ã“ã®ç ”ç©¶ã§ã¯é€šå¸¸ã®Multi-Head Attentionã«å¯¾ã—ã¦ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã®äº‹å‰å­¦ç¿’ã«å¯¾ã—ã¦è¿½åŠ ã®5%ã®è¨ˆç®—é‡ã§GQAãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/70ec2179-428c-47b8-af53-cb3cc0e4f022" alt="image" loading="lazy"><br><br></p>
<p>Main Result. Multi-Head Attentionã«å¯¾ã—ã¦ã€inference timeãŒå¤§å¹…ã«æ”¹å–„ã—ã¦ã„ã‚‹ãŒã€Multi-Query Attentionã‚ˆã‚Šã‚‚é«˜ã„æ€§èƒ½ã‚’ç¶­æŒã—ã¦ã„ã‚‹ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/3687aeb4-90b8-403d-853b-740121dd5f98" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-11-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1121" target="_blank" rel="noopener noreferrer" class="title-link">Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs, Qingru Zhang+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- PASTAã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã«ãŠã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæŒ‡å®šã—ãŸå¼·èª¿ãƒãƒ¼ã‚¯ã®ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã‚€ã“ã¨ã‚’å¯èƒ½ã«ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚PASTAã¯ã€æ³¨æ„ã®ä¸€éƒ¨ã‚’ç‰¹å®šã—ã€å†é‡ã¿ä»˜ã‘ã‚’é©ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®æ³¨æ„ã‚’ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæŒ‡å®šã—ãŸéƒ¨åˆ†ã«å‘ã‘ã¾ã™ã€‚å®Ÿé¨“ã§ã¯ã€PASTAãŒLLMã®æ€§èƒ½ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>ãƒ¦ãƒ¼ã‚¶ãŒpromptä¸­ã§å¼·èª¿ã—ãŸã„ã—ãŸéƒ¨åˆ†ãŒã‚ˆã‚Šè€ƒæ…®ã•ã‚Œã‚‹ã‚ˆã†ã«attention weightã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šå¿œç­”æ€§èƒ½ãŒå‘ä¸Šã—ã¾ã—ãŸã¨ã„ã†è©±ã£ã½ã„ã€‚ã‹ãªã‚Šé‡è¦ãªæŠ€è¡“ã ã¨æ€ã‚ã‚Œã‚‹ã€‚å¾Œã§ã—ã£ã‹ã‚Šèª­ã‚€ã€‚<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/a4d3714e-7279-495c-86f1-5ff4ed2cbeb8" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-08-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/923" target="_blank" rel="noopener noreferrer" class="title-link">The Hydra Effect: Emergent Self-repair in Language Model Computations, Thomas McGrath+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- ç§ãŸã¡ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®å†…éƒ¨æ§‹é€ ã‚’èª¿æŸ»ã—ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®è¨ˆç®—ã«ãŠã‘ã‚‹ç‰¹å®šã®åŠ¹æœã‚’ç¤ºã—ã¾ã—ãŸã€‚å…·ä½“çš„ã«ã¯ã€1ã¤ã®å±¤ã®å‰Šé™¤ãŒä»–ã®å±¤ã«ã‚ˆã£ã¦è£œå®Œã•ã‚Œã‚‹ã€ŒHydraåŠ¹æœã€ã¨ã€é…ã„MLPå±¤ãŒæœ€å¤§å°¤åº¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åˆ¶å¾¡ã™ã‚‹å½¹å‰²ã‚’æŒã¤ã“ã¨ã‚’ç¤ºã—ã¾ã—ãŸã€‚ã¾ãŸã€ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’ä½¿ç”¨ã—ãªã„è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã‚‚åŒæ§˜ã®åŠ¹æœãŒè¦‹ã‚‰ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã®åŠ¹æœã‚’äº‹å®Ÿã®å›æƒ³ã®æ–‡è„ˆã§åˆ†æã—ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®å›è·¯ãƒ¬ãƒ™ãƒ«ã®å±æ€§ä»˜ä¸ã«ã¤ã„ã¦è€ƒå¯Ÿã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>LLMã‹ã‚‰attention layerã‚’ä¸€ã¤å–ã‚Šé™¤ãã¨ã€å¾Œç¶šã®å±¤ãŒå–ã‚Šé™¤ã‹ã‚ŒãŸlayerã®æ©Ÿèƒ½ã‚’å¼•ãç¶™ãã‚ˆã†ãªåƒãã‚’ã™ã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸã€‚ã“ã‚Œã¯LLMã®è‡ªå·±ä¿®å¾©æ©Ÿèƒ½ã®ã‚ˆã†ãªã‚‚ã®ã§ã‚ã‚Šã€HydraEffectã¨å‘½åã•ã‚ŒãŸã€‚</p></span><br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/DataDistillation.html" target="_blank" rel="noopener noreferrer">#DataDistillation</a>
<a class="button" href="articles/Zero/FewShotLearning.html" target="_blank" rel="noopener noreferrer">#Zero/FewShotLearning</a>
<span class="issue_date">Issue Date: 2023-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/827" target="_blank" rel="noopener noreferrer" class="title-link">Dataset Distillation with Attention Labels for Fine-tuning BERT, ACL'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è’¸ç•™ã‚’ä½¿ç”¨ã—ã¦ã€å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ä¿æŒã—ãªãŒã‚‰ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’è¿…é€Ÿã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®å°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã™ã‚‹æ–¹æ³•ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’å¾®èª¿æ•´ã™ã‚‹ãŸã‚ã®è‡ªç„¶è¨€èªå‡¦ç†ã‚¿ã‚¹ã‚¯ã®è’¸ç•™ã•ã‚ŒãŸfew-shotãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹ç¯‰ã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚å®Ÿé¨“çµæœã§ã¯ã€æ³¨æ„ãƒ©ãƒ™ãƒ«ã‚’ä½¿ç”¨ã—ã¦few-shotãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ã€BERTã®å¾®èª¿æ•´ã«ãŠã„ã¦å°è±¡çš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®Ÿç¾ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã—ãŸã€‚ä¾‹ãˆã°ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†é¡ã‚¿ã‚¹ã‚¯ã§ã¯ã€ã‚ãšã‹1ã¤ã®ã‚µãƒ³ãƒ—ãƒ«ã¨ã‚ãšã‹1ã¤ã®å‹¾é…ã‚¹ãƒ†ãƒƒãƒ—ã®ã¿ã§ã€å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®98.5ï¼…ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>Datadistillationã—ãŸã‚‰ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã†ã¡1ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ã§ã€å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®98.5%ã®æ€§èƒ½ã‚’ç™ºæ®ã§ããŸã¨ã„ã†é©šç•°çš„ãªç ”ç©¶ï¼ˆã¾ãˆã‹ã‚å›ï¼‰</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/Inference.html" target="_blank" rel="noopener noreferrer">#Inference</a>
<span class="issue_date">Issue Date: 2023-04-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/601" target="_blank" rel="noopener noreferrer" class="title-link">Efficiently Scaling Transformer Inference, Reiner Pope+, N_A, MLSys'23</a>
<span class="snippet"><span>GPT Summary</span>- - å¤§è¦æ¨¡Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã®ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€æœ€é©ãªå¤šæ¬¡å…ƒåˆ†å‰²æŠ€è¡“ã‚’é¸æŠã™ã‚‹ãŸã‚ã®å˜ç´”ãªè§£æãƒ¢ãƒ‡ãƒ«ã‚’é–‹ç™º- ä½ãƒ¬ãƒ™ãƒ«ã®æœ€é©åŒ–ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€500B+ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«FLOPSåˆ©ç”¨ç‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã«ãŠã„ã¦ã€FasterTransformerãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆã‚’ä¸Šå›ã‚‹æ–°ã—ã„Paretoãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã‚’å®Ÿç¾- é©åˆ‡ãªåˆ†å‰²ã«ã‚ˆã‚Šã€ãƒãƒ«ãƒã‚¯ã‚¨ãƒªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ä½ã„ãƒ¡ãƒ¢ãƒªè¦ä»¶ã«ã‚ˆã‚Šã€32å€ã®å¤§ããªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯èƒ½- int8ã‚¦ã‚§ã‚¤ãƒˆé‡å­åŒ–ã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆä¸­ã®ä½ãƒãƒƒãƒã‚µã‚¤ã‚ºãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Š29msã§ã‚ã‚Šã€å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®å¤§ãƒãƒƒãƒã‚µã‚¤ã‚ºå‡¦ç†ã«ãŠã„ã¦76ï¼…ã®MFUã‚’å®Ÿç¾ã—ã€PaLM 540Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦2048ãƒˆãƒ¼ã‚¯ãƒ³ã®é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>ç‰¹ã«Multiquery Attentionã¨ã„ã†æŠ€è¡“ãŒTransformerã®inferenceã®ã‚³ã‚¹ãƒˆå‰Šæ¸›ã«æœ‰åŠ¹ã‚‰ã—ã„</p></span><br><br>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Sparse.html" target="_blank" rel="noopener noreferrer">#Sparse</a>
<a class="button" href="articles/SparseAttention.html" target="_blank" rel="noopener noreferrer">#SparseAttention</a>
<span class="issue_date">Issue Date: 2025-11-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3854" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Efficient Transformers: A Survey, Yi Tay+, ACM Computing Surveys'22, 2022.12</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€è¨ˆç®—åŠ¹ç‡ã‚„ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸã€ŒX-formerã€ãƒ¢ãƒ‡ãƒ«ï¼ˆReformerã€Linformerã€Performerã€Longformerãªã©ï¼‰ã®å¤§è¦æ¨¡ãªã‚»ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç´¹ä»‹ã—ã€æœ€è¿‘ã®ç ”ç©¶ã‚’ä½“ç³»çš„ã‹ã¤åŒ…æ‹¬çš„ã«ã¾ã¨ã‚ã‚‹ã€‚Transformersã¯è‡ªç„¶è¨€èªå‡¦ç†ã‚’å«ã‚€å¤šãã®åˆ†é‡ã§é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3854" target="_blank" rel="noopener noreferrer">[Paper Note] Efficient Transformers: A Survey, Yi Tay+, ACM Computing Surveys'22, 2022.12</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3855" target="_blank" rel="noopener noreferrer">[Paper Note] Big Bird: Transformers for Longer Sequences, Manzil Zaheer+, NIPS'20, 2020.07</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2355" target="_blank" rel="noopener noreferrer">[Paper Note] Reformer: The Efficient Transformer, Nikita Kitaev+, ICLR'20</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3853" target="_blank" rel="noopener noreferrer">[Paper Note] Generating Long Sequences with Sparse Transformers, Rewon Child+, arXiv'19, 2019.04</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2388" target="_blank" rel="noopener noreferrer">[Paper Note] Longformer: The Long-Document Transformer, Iz Beltagy+, arXiv'20</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-10-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3110" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Mixture of Attention Heads: Selecting Attention Heads Per Token, Xiaofeng Zhang+, EMNLP'22, 2022.10</a>
<span class="snippet"><span>GPT Summary</span>- Mixture of Attention Heads (MoA)ã¯ã€MoEãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’çµ„ã¿åˆã‚ã›ãŸæ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã€å‹•çš„ã«é¸æŠã•ã‚ŒãŸã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚²ãƒ¼ãƒˆåŒ–ã«ã‚ˆã‚Šè¨ˆç®—åŠ¹ç‡ã‚’ä¿ã¡ãªãŒã‚‰æ‹¡å¼µå¯èƒ½ã§ã€ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆå¯èƒ½æ€§ã«ã‚‚å¯„ä¸ã™ã‚‹ã€‚å®Ÿé¨“ã§ã¯ã€æ©Ÿæ¢°ç¿»è¨³ã‚„ãƒã‚¹ã‚¯ä»˜ãè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãªã©ã®ã‚¿ã‚¹ã‚¯ã§å¼·åŠ›ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚‹çµæœã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>FFNã«é©ç”¨ã•ã‚Œã‚‹ã“ã¨ãŒå¤šã‹ã£ãŸMoEã‚’multi-head attention (MHA) ã«é©ç”¨ã™ã‚‹ç ”ç©¶ã€‚ã“ã®ã‚ˆã†ãªattentionã‚’Mixture of Attention Heads (MoA)ã¨å‘¼ã¶ã€‚<br><br>å„MHAã¯è¤‡æ•°ã®attention expertsã‚’æŒã¡ã€ãã®ä¸­ã‹ã‚‰Kå€‹ã®ExpertsãŒç¾åœ¨ã®ã‚¯ã‚¨ãƒªq_tã«åŸºã¥ã„ã¦Routerã«ã‚ˆã£ã¦é¸å‡ºï¼ˆå¼7, 8)ã•ã‚Œã‚‹ã€‚ãã‚Œãã‚Œã®attention expertsã«å¯¾ã—ã¦q_tãŒæµã•ã‚Œã€é€šå¸¸ã®MHAã¨åŒã˜æµã‚Œã§outputãŒè¨ˆç®—ã•ã‚Œã€æœ€çµ‚çš„ã«é¸æŠã•ã‚ŒãŸéš›ã®ï¼ˆæ­£è¦åŒ–ã•ã‚ŒãŸï¼ˆå¼9ï¼‰ï¼‰probabilityã«ã‚ˆã‚‹åŠ é‡å¹³å‡ã«ã‚ˆã£ã¦å‡ºåŠ›ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆå¼6)ã€‚<br><br>æ³¨æ„ç‚¹ã¨ã—ã¦ã¯ã€å„attention expertsã¯ç‹¬ç«‹ã—ãŸprojection matrix W_q, W_oï¼ˆãã‚Œãã‚Œiç•ªç›®ã®expertsã«ãŠã‘ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³tã«ãŠã„ã¦ã€query q_tã‚’å¤‰æ›ã€output o_{i,t}ã‚’hidden spaceæ¬¡å…ƒã«æˆ»ã™å½¹å‰²ã‚’æŒã¤)ã‚’æŒã¤ãŒã€K, Vã«å¯¾ã™ã‚‹å¤‰æ›è¡Œåˆ—ã¯å…±æœ‰ã™ã‚‹ã¨è¨€ã†ç‚¹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ¬¡å…ƒã«å…¨ã¦ã®expertsã«å¯¾ã—ã¦k, vã«å¯¾ã™ã‚‹å¤‰æ›ã¯è¨ˆç®—ã—ã¦ãŠã‘ã‚‹ã®ã§ã€headã”ã¨ã«ç•°ãªã‚‹å¤‰æ›ã‚’å­¦ç¿’ã—ãªãŒã‚‰ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/3073c6b8-cdc7-4303-8881-0c07c502d0ec" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/d74ab1b7-e44c-461d-ad64-6f5ecacd8da2" alt="image" loading="lazy"><br><br>ã¾ãŸã€ç‰¹å®šã®expertsã«ã®ã¿ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãŒé›†ä¸­ã—ãªã„ã‚ˆã†ã«ã€lossã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§å­¦ç¿’ã®å®‰å®šã•ã›æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã¦ã„ã‚‹ï¼ˆ4.3ç¯€ï¼‰ã€‚</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<span class="issue_date">Issue Date: 2023-05-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/688" target="_blank" rel="noopener noreferrer" class="title-link">FlashAttention: Fast and Memory-Efficient Exact Attention with  IO-Awareness, Tri Dao+, N_A, arXiv'22</a>
<span class="snippet"><span>GPT Summary</span>- ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã—ã¦é…ãã€ãƒ¡ãƒ¢ãƒªã‚’å¤šãæ¶ˆè²»ã™ã‚‹ãŸã‚ã€æ³¨æ„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ”¹å–„ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚FlashAttentionã¯ã€ã‚¿ã‚¤ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦ã€GPUã®é«˜å¸¯åŸŸå¹…ãƒ¡ãƒ¢ãƒªï¼ˆHBMï¼‰ã¨GPUã®ã‚ªãƒ³ãƒãƒƒãƒ—SRAMé–“ã®ãƒ¡ãƒ¢ãƒªèª­ã¿å–ã‚Š/æ›¸ãè¾¼ã¿ã®æ•°ã‚’æ¸›ã‚‰ã—ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’é«˜é€Ÿã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã‚‹ã€‚FlashAttentionã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã§ã‚ˆã‚Šé•·ã„æ–‡è„ˆã‚’å¯èƒ½ã«ã—ã€ã‚ˆã‚Šé«˜å“è³ªãªãƒ¢ãƒ‡ãƒ«ã‚„ã€å®Œå…¨ã«æ–°ã—ã„æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>ã‚ˆã‚Šé«˜é€ŸãªGPUä¸Šã®SRAMä¸Šã§è¨ˆç®—ã§ãã‚‹ã‚ˆã†ã«QKVã‚’ãƒ–ãƒ­ãƒƒã‚¯å˜ä½ã«åˆ†å‰²ã—ã¦è¨ˆç®—ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šé«˜ã„è¨ˆç®—åŠ¹ç‡ã‚’å®Ÿç¾ã™ã‚‹FlashAttentionã‚’ææ¡ˆ[^1]<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/e3cb11b7-f413-4831-bea6-97886b683ff7" alt="image" loading="lazy"><br><br>[^1]: ï¼ˆ2025.05.24è¿½è¨˜)ä¸‹è¨˜æ—¥æœ¬èªãƒ–ãƒ­ã‚°ã‚’å‚è€ƒã«ä¸€éƒ¨æ–‡è¨€ã‚’è¨‚æ­£ã—ã¾ã—ãŸã€‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚</p>
<p>æ—¥æœ¬èªè§£èª¬:


<a href="https://zenn.dev/sinchir0/articles/21bb6e96c7b05b" target="_blank" rel="noopener noreferrer">https://zenn.dev/sinchir0/articles/21bb6e96c7b05b</a>


<br>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/sinchir0/status/1926199436406849786?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ—¥æœ¬èªè§£èª¬:


<a href="https://zenn.dev/uchiiii/articles/306d0bb7ef67a7" target="_blank" rel="noopener noreferrer">https://zenn.dev/uchiiii/articles/306d0bb7ef67a7</a>


<br>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/aquarobot0202/status/1957109068797018545?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Distillation.html" target="_blank" rel="noopener noreferrer">#Distillation</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="articles/Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<a class="button" href="articles/Findings.html" target="_blank" rel="noopener noreferrer">#Findings</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-10-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3345" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] MiniLMv2: Multi-Head Self-Attention Relation Distillation for  Compressing Pretrained Transformers, Wenhui Wang+, ACL'21 Findings, 2020.12</a>
<span class="snippet"><span>GPT Summary</span>- è‡ªå·±æ³¨æ„é–¢ä¿‚è’¸ç•™ã‚’ç”¨ã„ã¦ã€MiniLMã®æ·±å±¤è‡ªå·±æ³¨æ„è’¸ç•™ã‚’ä¸€èˆ¬åŒ–ã—ã€äº‹å‰å­¦ç¿’ã•ã‚ŒãŸãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®åœ§ç¸®ã‚’è¡Œã†æ‰‹æ³•ã‚’ææ¡ˆã€‚ã‚¯ã‚¨ãƒªã€ã‚­ãƒ¼ã€ãƒãƒªãƒ¥ãƒ¼ã®ãƒ™ã‚¯ãƒˆãƒ«é–“ã®é–¢ä¿‚ã‚’å®šç¾©ã—ã€ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã€‚æ³¨æ„ãƒ˜ãƒƒãƒ‰æ•°ã«åˆ¶é™ãŒãªãã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®å±¤é¸æŠæˆ¦ç•¥ã‚’æ¤œè¨ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€BERTã‚„RoBERTaã€XLM-Rã‹ã‚‰è’¸ç•™ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒæœ€å…ˆç«¯ã®æ€§èƒ½ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>æ•™å¸«ã¨ï¼ˆã‚ˆã‚Šå°è¦æ¨¡ãªï¼‰ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«é–“ã§ã€tokenã”ã¨ã®q-q/k-k/v-vã®dot productã«ã‚ˆã£ã¦å½¢æˆã•ã‚Œã‚‹relation mapï¼ˆãŸã¨ãˆã°q-qã®å ´åˆã¯relatiok mapã¯ãƒˆãƒ¼ã‚¯ãƒ³æ•°xãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®è¡Œåˆ—ã§å„è¦ç´ ãŒdot(qi, qj))ã§è¡¨ç¾ã•ã‚Œã‚‹é–¢ä¿‚æ€§ã‚’å†ç¾ã§ãã‚‹ã‚ˆã†ã«MHAã‚’è’¸ç•™ã™ã‚‹ã‚ˆã†ãªæ‰‹æ³•ã€‚å…·ä½“çš„ã«ã¯ã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®QKVã¨ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®QKVã«ã‚ˆã£ã¦æ§‹æˆã•ã‚Œã‚‹ãã‚Œãã‚Œã®relation mapé–“ã®KL Divergenceã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã†ã«è’¸ç•™ã™ã‚‹ã€‚ã“ã®ã¨ãæ•™å¸«ãƒ¢ãƒ‡ãƒ«ã¨ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®attention headsæ•°ãªã©ã¯ç•°ãªã£ã¦ã‚‚ã‚ˆã„ï¼ˆq-q/k-k/v-vãã‚Œãã‚Œã§å®šç¾©ã•ã‚Œã‚‹relation mapã¯ã¯ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«ä¾å­˜ã—ã¦ãŠã‚Šã€headæ•°ã«ã¯ä¾å­˜ã—ã¦ã„ãªã„ãŸã‚ï¼‰ã€‚</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-07-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2258" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, Ze Liu+, ICCV'21</a>
<span class="snippet"><span>GPT Summary</span>- Swin Transformerã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®æ–°ã—ã„ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹éšå±¤çš„ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ææ¡ˆã€‚ã‚·ãƒ•ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ–¹å¼ã«ã‚ˆã‚Šã€åŠ¹ç‡çš„ãªè‡ªå·±æ³¨æ„è¨ˆç®—ã‚’å®Ÿç¾ã—ã€ã•ã¾ã–ã¾ãªã‚¹ã‚±ãƒ¼ãƒ«ã§ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãŒå¯èƒ½ã€‚ç”»åƒåˆ†é¡ã‚„ç‰©ä½“æ¤œå‡ºã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãªã©ã§å¾“æ¥ã®æœ€å…ˆç«¯ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®ãƒ“ã‚¸ãƒ§ãƒ³ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã¦ã®å¯èƒ½æ€§ã‚’ç¤ºå”†ã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬:


<a href="https://qiita.com/m_sugimura/items/139b182ee7c19c83e70a" target="_blank" rel="noopener noreferrer">https://qiita.com/m_sugimura/items/139b182ee7c19c83e70a</a>


</p>
<p>ç”»åƒå‡¦ç†ã«ãŠã„ã¦ã€ç‰©ä½“ã®ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã‚„ã€è§£åƒåº¦ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€PatchMergeã¨å‘¼ã°ã‚Œã‚‹ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã®ã‚ˆã†ãªå‡¦ç†ã¨ã€å›ºå®šã‚µã‚¤ã‚ºã®ãƒ­ãƒ¼ã‚«ãƒ«ãªwindowã«åˆ†å‰²ã—ã¦Self-Attentionã‚’å®Ÿæ–½ã—ã€layerã”ã¨ã«é€šå¸¸ã®windowã¨ã‚·ãƒ•ãƒˆã•ã‚ŒãŸwindowã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§ã€windowé–“ã‚’è·¨ã„ã é–¢ä¿‚æ€§ã‚‚è€ƒæ…®ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹æ©Ÿæ§‹ã‚’å°å…¥ã—ãŸãƒ¢ãƒ‡ãƒ«ã€‚<br><img src="https://github.com/user-attachments/assets/a2d5f78c-27ec-4f18-bd7d-5475085cfa7b" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/92fb10e1-614e-44ef-9e65-3920cd863d46" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/2b8a543a-069e-468a-bc3c-1f288cdcf577" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/Sparse.html" target="_blank" rel="noopener noreferrer">#Sparse</a>
<a class="button" href="articles/SparseAttention.html" target="_blank" rel="noopener noreferrer">#SparseAttention</a>
<span class="issue_date">Issue Date: 2025-11-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3855" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Big Bird: Transformers for Longer Sequences, Manzil Zaheer+, NIPS'20, 2020.07</a>
<span class="snippet"><span>GPT Summary</span>- BigBirdã¯ã€Transformersãƒ¢ãƒ‡ãƒ«ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«å¯¾ã™ã‚‹äºŒæ¬¡çš„ä¾å­˜æ€§ã‚’ç·šå½¢ã«å‰Šæ¸›ã™ã‚‹ã‚¹ãƒ‘ãƒ¼ã‚¹æ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’æœ€å¤§8å€å‡¦ç†å¯èƒ½ã¨ãªã‚Šã€è³ªå•å¿œç­”ã‚„è¦ç´„ãªã©ã®NLPã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ãŒå‘ä¸Šã€‚ã•ã‚‰ã«ã€ã‚²ãƒãƒ ãƒ‡ãƒ¼ã‚¿ã¸ã®æ–°ãŸãªå¿œç”¨ã‚‚ç¤ºå”†ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬: 


<a href="https://www.docswell.com/s/DeepLearning2023/KVV8VP-dlvisual-grounding-of-learned-physical-models-238500048" target="_blank" rel="noopener noreferrer">https://www.docswell.com/s/DeepLearning2023/KVV8VP-dlvisual-grounding-of-learned-physical-models-238500048</a>


</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Sparse.html" target="_blank" rel="noopener noreferrer">#Sparse</a>
<a class="button" href="articles/SparseAttention.html" target="_blank" rel="noopener noreferrer">#SparseAttention</a>
<span class="issue_date">Issue Date: 2025-08-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2388" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Longformer: The Long-Document Transformer, Iz Beltagy+, arXiv'20</a>
<span class="snippet"><span>GPT Summary</span>- Longformerã¯ã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç·šå½¢ã«å‡¦ç†ã§ãã‚‹æ³¨æ„æ©Ÿæ§‹ã‚’æŒã¤Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã§ã€æ•°åƒãƒˆãƒ¼ã‚¯ãƒ³ã®æ–‡æ›¸ã‚’æ‰±ãˆã‚‹ã€‚å±€æ‰€çš„ãªã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ³¨æ„ã¨ã‚¿ã‚¹ã‚¯ã«åŸºã¥ãã‚°ãƒ­ãƒ¼ãƒãƒ«æ³¨æ„ã‚’çµ„ã¿åˆã‚ã›ã€æ–‡å­—ãƒ¬ãƒ™ãƒ«ã®è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã€‚äº‹å‰å­¦ç¿’ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã€é•·æ–‡ã‚¿ã‚¹ã‚¯ã§RoBERTaã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚ã¾ãŸã€Longformer-Encoder-Decoderï¼ˆLEDï¼‰ã‚’å°å…¥ã—ã€é•·æ–‡ç”Ÿæˆã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹åŠ¹æœã‚’ç¢ºèªã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>ï¼ˆå›ºå®šã•ã‚ŒãŸå°ã•ã‚ã®windowsã‚µã‚¤ã‚ºã®ä¸­ã§ã®ã¿attentionã‚’è¨ˆç®—ã™ã‚‹ï¼‰sliding window attentionã‚’ææ¡ˆã€‚Figure2ã‚’è¦‹ã‚‹ã¨ã€é€šå¸¸ã®Attentionã¨æ¯”è¼ƒã—ã¦ã€ç¾åœ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®å‘¨è¾ºã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ã—ã‹æ³¨ç›®ã—ãªã„ç‰¹æ€§ãŒå›³ç¤ºã•ã‚Œã¦ãŠã‚Šã€ã‚¤ãƒ¡ãƒ¼ã‚¸ãŒæ´ã¿ã‚„ã™ã„ã€‚<br><br><img width="795" height="231" alt="Image" src="&lt;a%20href=" https: target="_blank" rel="noopener noreferrer">https://github.com/user-attachments/assets/d1eccdaf-5b5b-4444-ad31-44c54c345d79"


/&gt;</p>
<p>OpenLLMã®æ–‡è„ˆã ã¨ã€Mistralã«æ¡ç”¨ã•ã‚Œã¦è©±é¡Œã«ãªã£ãŸã‹ã‚‚ï¼Ÿ<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1309" target="_blank" rel="noopener noreferrer">Mistral 7B, Albert Q. Jiang+, N/A, arXiv'23</a>
</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2356" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Transformers are RNNs: Fast Autoregressive Transformers with Linear  Attention, Angelos Katharopoulos+, ICML'20</a>
<span class="snippet"><span>GPT Summary</span>- è‡ªå·±æ³¨æ„ã‚’ã‚«ãƒ¼ãƒãƒ«ç‰¹å¾´ãƒãƒƒãƒ—ã®ç·šå½¢ãƒ‰ãƒƒãƒˆç©ã¨ã—ã¦è¡¨ç¾ã™ã‚‹ã“ã¨ã§ã€Transformersã®è¤‡é›‘æ€§ã‚’$\mathcal{O}\left(N^2\right)$ã‹ã‚‰$\mathcal{O}\left(N\right)$ã«å‰Šæ¸›ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è‡ªå·±å›å¸°å‹Transformersã®é€Ÿåº¦ãŒæœ€å¤§4000å€å‘ä¸Šã—ã€å¾“æ¥ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã€‚</span>
<span class="snippet"><span>Comment</span><p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1210" target="_blank" rel="noopener noreferrer">Transformers are Multi-State RNNs, Matanel Oren+, N/A, EMNLP'24</a>
 </p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/Sparse.html" target="_blank" rel="noopener noreferrer">#Sparse</a>
<a class="button" href="articles/SparseAttention.html" target="_blank" rel="noopener noreferrer">#SparseAttention</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2355" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Reformer: The Efficient Transformer, Nikita Kitaev+, ICLR'20</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€å±€æ‰€æ„Ÿåº¦ãƒãƒƒã‚·ãƒ¥ã‚’ç”¨ã„ãŸæ³¨æ„æ©Ÿæ§‹ã¨å¯é€†æ®‹å·®å±¤ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¨ˆç®—é‡ã‚’O($L^2$)ã‹ã‚‰O($L\log L$)ã«å‰Šæ¸›ã—ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨é€Ÿåº¦ã‚’å‘ä¸Šã•ã›ãŸReformerãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿç¾ã€‚ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¨åŒç­‰ã®æ€§èƒ½ã‚’ç¶­æŒã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview: 


<a href="https://openreview.net/forum?id=rkgNKkHtvB" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=rkgNKkHtvB</a>


</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2354" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Linformer: Self-Attention with Linear Complexity, Sinong Wang+, arXiv'20</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã¯è‡ªç„¶è¨€èªå‡¦ç†ã§æˆåŠŸã‚’åã‚ã¦ã„ã‚‹ãŒã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã—ã¦ã¯é«˜ã‚³ã‚¹ãƒˆã€‚è‡ªå·±æ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã§è¿‘ä¼¼ã—ã€è¤‡é›‘ã•ã‚’$O(n^2)$ã‹ã‚‰$O(n)$ã«å‰Šæ¸›ã™ã‚‹æ–°ã—ã„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¡ãƒ¢ãƒªã¨æ™‚é–“åŠ¹ç‡ãŒå‘ä¸Šã—ãŸç·šå½¢ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã€ŒLinformerã€ãŒæ¨™æº–ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã®æ€§èƒ½ã‚’ç¤ºã™ã€‚</span>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/PositionalEncoding.html" target="_blank" rel="noopener noreferrer">#PositionalEncoding</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2359" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context, Zihang Dai+, ACL'19</a>
<span class="snippet"><span>GPT Summary</span>- Transformer-XLã¯ã€å›ºå®šé•·ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¶…ãˆãŸé•·æœŸçš„ãªä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã™ã‚‹æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã®å†å¸°ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¨æ–°ã—ã„ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¡ç”¨ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€RNNã‚ˆã‚Š80%ã€å¾“æ¥ã®Transformersã‚ˆã‚Š450%é•·ã„ä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã—ã€è©•ä¾¡æ™‚ã«ã¯æœ€å¤§1,800å€ã®é€Ÿåº¦å‘ä¸Šã‚’å®Ÿç¾ã€‚enwiki8ã‚„WikiText-103ãªã©ã§æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã€æ•°åƒãƒˆãƒ¼ã‚¯ãƒ³ã®ä¸€è²«ã—ãŸãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚‚å¯èƒ½ã€‚ã‚³ãƒ¼ãƒ‰ã¨ãƒ¢ãƒ‡ãƒ«ã¯Tensorflowã¨PyTorchã§åˆ©ç”¨å¯èƒ½ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/329" target="_blank" rel="noopener noreferrer">äº‹å‰å­¦ç¿’è¨€èªãƒ¢ãƒ‡ãƒ«ã®å‹•å‘ / Survey of Pretrained Language Models, Kyosuke Nishida, 2019</a>
</p>
<p>3.2ç¯€ã®å®šå¼åŒ–ã‚’è¦‹ã‚‹ã¨ã€ä¸€ã¤å‰ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒ»layerã”ã¨ã®hidden stateã‚’ã€ç¾åœ¨ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å¯¾å¿œã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã¨layerã®hidden stateã«concatã—ï¼ˆéå»ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«å½±éŸ¿ã‚’ä¸ãˆãªã„ã‚ˆã†ã«å‹¾é…ã‚’ä¼æ¬ã•ã›ãªã„Stop-Gradientã‚’é©ç”¨ã™ã‚‹ï¼‰ã€QKVã®ã†ã¡ã€KVã®è¨ˆç®—ã«æ´»ç”¨ã—ã¦ã„ã‚‹ã€‚ã¾ãŸã€çµ¶å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’åˆ©ç”¨ã™ã‚‹ã¨ãƒ¢ãƒ‡ãƒ«ãŒã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–“ã®æ™‚ç³»åˆ—çš„ãªé–¢ä¿‚ã‚’èªè­˜ã§ããªããªã‚‹ãŸã‚ã€ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«ã¯ç›¸å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’åˆ©ç”¨ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç¾åœ¨ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®KVãŒä¸€ã¤å‰ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«ã‚ˆã£ã¦æ¡ä»¶ã¥ã‘ã‚‰ã‚Œã€contextã¨ã—ã¦è€ƒæ…®ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã¨ãªã‚Šã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–“ã‚’è·¨ã„ã ä¾å­˜é–¢ä¿‚ã®è€ƒæ…®ãŒå®Ÿç¾ã•ã‚Œã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1272" target="_blank" rel="noopener noreferrer" class="title-link">Fast Transformer Decoding: One Write-Head is All You Need, Noam Shazeer, N_A, arXiv'19</a>
<span class="snippet"><span>GPT Summary</span>- ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯é«˜é€Ÿã‹ã¤ç°¡å˜ã ãŒã€å¢—åˆ†æ¨è«–ã¯å¤§ããª"keys"ã¨"values"ãƒ†ãƒ³ã‚½ãƒ«ã‚’ç¹°ã‚Šè¿”ã—èª­ã¿è¾¼ã‚€ãŸã‚ã«é…ããªã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚ãã“ã§ã€ã‚­ãƒ¼ã¨å€¤ã‚’å…±æœ‰ã™ã‚‹ãƒãƒ«ãƒã‚¯ã‚¨ãƒªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆã—ã€ãƒ¡ãƒ¢ãƒªå¸¯åŸŸå¹…è¦ä»¶ã‚’ä½æ¸›ã™ã‚‹ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€é«˜é€Ÿãªãƒ‡ã‚³ãƒ¼ãƒ‰ãŒå¯èƒ½ã§ã€ã‚ãšã‹ãªå“è³ªã®ä½ä¸‹ã—ã‹ãªã„ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>Multi Query Attentionè«–æ–‡ã€‚KVã®setã«å¯¾ã—ã¦ã€å˜ä¸€ã®Queryã®ã¿ã§Multi-Head Attentionã‚’ä»£æ›¿ã™ã‚‹ã€‚åŠ‡çš„ã«Decoderã®InferenceãŒæ—©ããªã‚Šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒæ¸›ã‚‹ãŒã€è«–æ–‡ä¸­ã§ã¯è¨€åŠã•ã‚Œã¦ã„ãªã„ï¼Ÿã‚ˆã†ã ãŒã€æ€§èƒ½ã¨å­¦ç¿’ã®å®‰å®šæ€§ãŒèª²é¡Œã¨ãªã‚‹ã‚ˆã†ã§ã‚ã‚‹ã€‚<br><br><br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/e2d77b43-70c3-4922-a822-bf95d6b4704f" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/PositionalEncoding.html" target="_blank" rel="noopener noreferrer">#PositionalEncoding</a>
<span class="issue_date">Issue Date: 2025-08-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2386" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Self-Attention with Relative Position Representations, Peter Shaw+, NAACL'18</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€Transformerã®è‡ªå·±æ³¨æ„æ©Ÿæ§‹ã‚’æ‹¡å¼µã—ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹è¦ç´ é–“ã®ç›¸å¯¾çš„ãªä½ç½®ã‚’åŠ¹ç‡çš„ã«è€ƒæ…®ã™ã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã€‚WMT 2014ã®ç¿»è¨³ã‚¿ã‚¹ã‚¯ã§1.3 BLEUãŠã‚ˆã³0.3 BLEUã®æ”¹å–„ã‚’é”æˆã€‚ç›¸å¯¾ä½ç½®ã¨çµ¶å¯¾ä½ç½®ã®çµ„ã¿åˆã‚ã›ã§ã¯ã•ã‚‰ãªã‚‹æ”¹å–„ã¯è¦‹ã‚‰ã‚Œãªã‹ã£ãŸã€‚ææ¡ˆæ‰‹æ³•ã¯ã€ä»»æ„ã®ã‚°ãƒ©ãƒ•ãƒ©ãƒ™ãƒ«ä»˜ãå…¥åŠ›ã«ä¸€èˆ¬åŒ–å¯èƒ½ãªé–¢ä¿‚èªè­˜è‡ªå·±æ³¨æ„æ©Ÿæ§‹ã¨ã—ã¦ä½ç½®ä»˜ã‘ã‚‰ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>ç›¸å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ææ¡ˆã—ãŸç ”ç©¶</p>
<p>çµ¶å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¯<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/245" target="_blank" rel="noopener noreferrer">[Paper Note] Attention Is All You Need, Ashish Vaswani+, arXiv'17</a>
</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2353" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Efficient Attention: Attention with Linear Complexities, Zhuoran Shen+, arXiv'18</a>
<span class="snippet"><span>GPT Summary</span>- æ–°ã—ã„åŠ¹ç‡çš„ãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ææ¡ˆã—ã€ãƒ‰ãƒƒãƒˆç©ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¨åŒç­‰ã®æ€§èƒ½ã‚’ç¶­æŒã—ã¤ã¤ã€ãƒ¡ãƒ¢ãƒªã¨è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æŸ”è»Ÿãªçµ±åˆãŒå¯èƒ½ã¨ãªã‚Šã€ç²¾åº¦å‘ä¸Šã‚’å®Ÿç¾ã€‚å®Ÿé¨“çµæœã§ã¯ã€MS-COCO 2017ã§ã®ç‰©ä½“æ¤œå‡ºã‚„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®æ€§èƒ½å‘ä¸ŠãŒç¢ºèªã•ã‚Œã€Scene Flowãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯æœ€å…ˆç«¯ã®ç²¾åº¦ã‚’é”æˆã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>Figure1ã‚’è¦‹ã‚‹ã¨ã‚³ãƒ³ã‚»ãƒ—ãƒˆãŒä¸€ç›®ã§ã‚ã‹ã‚Šã€éå¸¸ã«ã‚ã‹ã‚Šã‚„ã™ã„<br><img width="1068" height="580" alt="Image" src="&lt;a%20href=" https: target="_blank" rel="noopener noreferrer">https://github.com/user-attachments/assets/18e6a7da-fc07-495f-bda6-bcef4acab321"


/&gt;</p></span><br><br>
<a class="button" href="articles/RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/SIGKDD.html" target="_blank" rel="noopener noreferrer">#SIGKDD</a>
<span class="issue_date">Issue Date: 2025-07-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2245" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Deep Interest Network for Click-Through Rate Prediction, Guorui Zhou+, KDD'18</a>
<span class="snippet"><span>GPT Summary</span>- ã‚¯ãƒªãƒƒã‚¯ç‡äºˆæ¸¬ã«ãŠã„ã¦ã€å›ºå®šé•·ã®è¡¨ç¾ãƒ™ã‚¯ãƒˆãƒ«ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¤šæ§˜ãªèˆˆå‘³ã‚’æ‰ãˆã‚‹ã®ã‚’å¦¨ã’ã‚‹å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€ãƒ­ãƒ¼ã‚«ãƒ«ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¦ãƒ‹ãƒƒãƒˆã‚’ç”¨ã„ãŸã€ŒDeep Interest Networkï¼ˆDINï¼‰ã€ã‚’ææ¡ˆã€‚DINã¯åºƒå‘Šã«å¿œã˜ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®èˆˆå‘³ã‚’é©å¿œçš„ã«å­¦ç¿’ã—ã€è¡¨ç¾åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ææ¡ˆæ‰‹æ³•ã¯æœ€å…ˆç«¯ã®æ‰‹æ³•ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€Alibabaã®åºƒå‘Šã‚·ã‚¹ãƒ†ãƒ ã«æˆåŠŸè£ã«å±•é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>ãƒ¦ãƒ¼ã‚¶ã®éå»ã®ã‚¢ã‚¤ãƒ†ãƒ ã¨ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’ã€å€™è£œã‚¢ã‚¤ãƒ†ãƒ ã«ã‚ˆã£ã¦æ¡ä»¶ã¥ã‘ãŸä¸Šã§attentionã«ã‚ˆã£ã¦é‡ã¿ã¥ã‘ã‚’ã™ã‚‹ã“ã¨ã§context vectorã‚’ä½œæˆã—æ´»ç”¨ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šå€™è£œã‚¢ã‚¤ãƒ†ãƒ ã”ã¨ã«ãƒ¦ãƒ¼ã‚¶ã®éå»ã®ã‚¢ã‚¤ãƒ†ãƒ ã¨ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã®ã†ã¡ã€ã©ã‚Œã‚’é‡è¦–ã™ã‚‹ã‹ã‚’å‹•çš„ã«å¤‰åŒ–ã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ã—ãŸç ”ç©¶ã€‚æœ€çµ‚çš„ã«ãƒ¦ãƒ¼ã‚¶ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸEmbeddingã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã®æƒ…å ±ãªã©ï¼‰ã®æƒ…å ±ã‚’ãƒ™ãƒ¼ã‚¹ã«ã—ãŸEmbeddingã¨ã€ä¸Šè¿°ã—ãŸcontext vectorã‚’concatã—ã€linearãªå¤‰æ›ã‚’å™›ã¾ã›ã¦ã‚¹ã‚³ã‚¢ã‚’å‡ºåŠ›ã™ã‚‹ã€‚å­¦ç¿’ã¯ã‚¯ãƒªãƒƒã‚¯ã‚¹ãƒ«ãƒ¼ãƒ­ã‚°ç­‰ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦NLL lossã‚’é©ç”¨ã™ã‚‹ã€‚é€šç§°DINã€‚<br><br><img src="https://github.com/user-attachments/assets/d88206a0-7eb0-4a78-8d2d-47460d66be61" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineTranslation.html" target="_blank" rel="noopener noreferrer">#MachineTranslation</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/PositionalEncoding.html" target="_blank" rel="noopener noreferrer">#PositionalEncoding</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2018-01-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/245" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Attention Is All You Need, Ashish Vaswani+, arXiv'17</a>
<span class="snippet"><span>GPT Summary</span>- Transformerã¯ã€å†å¸°ã‚„ç•³ã¿è¾¼ã¿ã‚’æ’é™¤ã—ã€æ³¨æ„æ©Ÿæ§‹ã®ã¿ã«åŸºã¥ã„ãŸæ–°ã—ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚ã‚‹ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€æ©Ÿæ¢°ç¿»è¨³ã‚¿ã‚¹ã‚¯ã§å„ªã‚ŒãŸå“è³ªã‚’ç¤ºã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“ã‚’å¤§å¹…ã«çŸ­ç¸®ã€‚WMT 2014ã®è‹±ç‹¬ç¿»è¨³ã§28.4 BLEUã€è‹±ä»ç¿»è¨³ã§41.8 BLEUã‚’é”æˆã—ã€æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚ã¾ãŸã€è‹±èªã®æ§‹æ–‡è§£æã«ã‚‚æˆåŠŸè£ã«é©ç”¨å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>Transformer (self-attentionã‚’åˆ©ç”¨) è«–æ–‡<br><br>è§£èª¬ã‚¹ãƒ©ã‚¤ãƒ‰ï¼š


<a href="https://www.slideshare.net/DeepLearningJP2016/dlattention-is-all-you-need" target="_blank" rel="noopener noreferrer">https://www.slideshare.net/DeepLearningJP2016/dlattention-is-all-you-need</a>


<br><br>è§£èª¬è¨˜äº‹ï¼š


<a href="https://qiita.com/nishiba/items/1c99bc7ddcb2d62667c6" target="_blank" rel="noopener noreferrer">https://qiita.com/nishiba/items/1c99bc7ddcb2d62667c6</a>


<br><br><br><br>* æ–°ã—ã„ç¿»è¨³ãƒ¢ãƒ‡ãƒ«(Transformer)ã‚’ææ¡ˆã€‚æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚ä¸¦åˆ—åŒ–ã«å¯¾å¿œã—ã¦ãŠã‚Šã€çŸ­æ™‚é–“ã®è¨“ç·´ã§ï¼ˆæ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã®1/4ä»¥ä¸‹ã®ã‚³ã‚¹ãƒˆï¼‰é«˜ã„BLEUã‚¹ã‚³ã‚¢ã‚’é”æˆã—ãŸã€‚<br><br>* Transformerã¯RNNã‚„CNNã‚’ä½¿ã‚ãšã€attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«åŸºã¥ã„ã¦ã„ã‚‹ã€‚<br><br><br><br>ï¼ˆè§£èª¬ã‚ˆã‚Šï¼‰</p>
<p>åˆ†ã‹ã‚Šã‚„ã™ã„:<br>


<a href="https://qiita.com/halhorn/items/c91497522be27bde17ce" target="_blank" rel="noopener noreferrer">https://qiita.com/halhorn/items/c91497522be27bde17ce</a>


</p>
<p>Transformerã®å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã®outputã®shapeã‚„ã€attention_maskã®å½¢çŠ¶ã€å®Ÿè£…ã«ã¤ã„ã¦è¨˜è¿°ã•ã‚Œã¦ãŠã‚Šæœ‰ç”¨:<br>


<a href="https://qiita.com/FuwaraMiyasaki/items/239f3528053889847825" target="_blank" rel="noopener noreferrer">https://qiita.com/FuwaraMiyasaki/items/239f3528053889847825</a>


</p>
<p>é›†åˆçŸ¥</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineTranslation.html" target="_blank" rel="noopener noreferrer">#MachineTranslation</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-05-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1954" target="_blank" rel="noopener noreferrer" class="title-link">Neural Machine Translation by Jointly Learning to Align and Translate, Dzmitry Bahdanau+, ICLR'15</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«æ©Ÿæ¢°ç¿»è¨³ã¯ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼-ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç”¨ã„ã¦ç¿»è¨³æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã‚ã‚‹ã€‚æœ¬è«–æ–‡ã§ã¯ã€å›ºå®šé•·ã®ãƒ™ã‚¯ãƒˆãƒ«ã®ä½¿ç”¨ãŒæ€§èƒ½å‘ä¸Šã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã§ã‚ã‚‹ã¨ã—ã€ãƒ¢ãƒ‡ãƒ«ãŒé–¢é€£ã™ã‚‹ã‚½ãƒ¼ã‚¹æ–‡ã®éƒ¨åˆ†ã‚’è‡ªå‹•çš„ã«æ¤œç´¢ã§ãã‚‹ã‚ˆã†ã«æ‹¡å¼µã™ã‚‹ã“ã¨ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è‹±èªã‹ã‚‰ãƒ•ãƒ©ãƒ³ã‚¹èªã¸ã®ç¿»è¨³ã‚¿ã‚¹ã‚¯ã§æœ€å…ˆç«¯ã®ãƒ•ãƒ¬ãƒ¼ã‚ºãƒ™ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ ã¨åŒç­‰ã®æ€§èƒ½ã‚’é”æˆã—ã€ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆãŒç›´æ„Ÿã¨ä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>(Cross-)Attentionã‚’åˆã‚ã¦ææ¡ˆã—ãŸç ”ç©¶ã€‚ãƒ¡ãƒ¢ã£ã¦ãªã‹ã£ãŸã®ã§ä»Šæ›´ãªãŒã‚‰è¿½åŠ ã€‚Attentionã¯ã“ã“ã‹ã‚‰ã¯ã˜ã¾ã£ãŸï¼ˆã¨èªè­˜ã—ã¦ã„ã‚‹ï¼‰</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Hybrid.html" target="_blank" rel="noopener noreferrer">#Hybrid</a>
<span class="issue_date">Issue Date: 2025-10-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3526" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Notes] KIMI LINEAR: AN EXPRESSIVE, EFFICIENT ATTENTION ARCHITECTURE, Kimi Team, 2025.10</a>
<span class="snippet"><span>Comment</span><p>HF:


<a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct" target="_blank" rel="noopener noreferrer">https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/kimi_moonshot/status/1983937694360322136?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/nrehiew_/status/1983891931823505518?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1983992979153985676?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rasbt/status/1984617030356451642?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-09-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3042" target="_blank" rel="noopener noreferrer" class="title-link">LLM ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¨å¤–æŒ¿, ä½è—¤ç«œé¦¬, 2025.09</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/joisino_/status/1972580573341470811?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/Reference%20Collection.html" target="_blank" rel="noopener noreferrer">#Reference Collection</a>
<a class="button" href="articles/Sparse.html" target="_blank" rel="noopener noreferrer">#Sparse</a>
<a class="button" href="articles/SparseAttention.html" target="_blank" rel="noopener noreferrer">#SparseAttention</a>
<span class="issue_date">Issue Date: 2025-09-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3033" target="_blank" rel="noopener noreferrer" class="title-link">DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention, DeepSeek-AI, 2025.09</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/danielhanchen/status/1972613546119991791?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>DeepSeek Sparse Attentionãƒã‚¤ãƒ³ãƒˆè§£èª¬:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/vllm_project/status/1972617272901644345?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1972802544863678832?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>DSAå›³è§£:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/awnihannun/status/1972763521185436088?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/scaling01/status/1972650237266465214?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å…¬å¼ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/deepseek_ai/status/1972604768309871061?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-09-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3010" target="_blank" rel="noopener noreferrer" class="title-link">We reverse-engineered Flash Attention 4, Modal Blog, 2025.09</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iwashi86/status/1972085451055157725?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Flash Attention4ã¯æ•°å­¦çš„ãªãƒˆãƒªãƒƒã‚¯ã‚ˆã‚Šã‚‚éåŒæœŸå‡¦ç†ã®è¤‡é›‘ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€Blackwellã«æœ€é©åŒ–ã€ã¨ã®ã“ã¨</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2025-09-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2994" target="_blank" rel="noopener noreferrer" class="title-link">æ§˜ã€…ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã«ãŠã‘ã‚‹ LLM ã® Self-Attention ã® Query ã¨ Key ã®åˆ†æ, ABEJA Tech Blog, 2025.09</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/abeja_tech/status/1971073813279621253?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ä»¥ä¸‹ã®ç ”ç©¶ã‚’å‚è€ƒã«åˆ†æã—ã¦ã„ã‚‹:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2995" target="_blank" rel="noopener noreferrer">[Paper Note] Massive Values in Self-Attention Modules are the Key to Contextual   Knowledge Understanding, Mingyu Jin+, ICML'25, 2025.02</a>
</p>
<p>RoPEã¯ä»¥ä¸‹:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1310" target="_blank" rel="noopener noreferrer">RoFormer: Enhanced Transformer with Rotary Position Embedding, Jianlin Su+, N/A, Neurocomputing, 2024</a>
</p>
<p>Massive Valueã¯transformerã®Q,Kã®æ´»æ€§å€¤ã«ç¾ã‚Œã‚‹æ¥µç«¯ã«å¤§ããªå€¤ã®ã“ã¨ã§ã€Massive Valueã¯æ–‡è„ˆçš„ãªçŸ¥è­˜ã®ç†è§£ã«ãŠã„ã¦é‡è¦ã¨ã®ã“ã¨ï¼ˆMassive Valueã‚’ç ´å£Šã™ã‚‹ã¨æ–‡è„ˆç†è§£ãŒé‡è¦ãªã‚¿ã‚¹ã‚¯ã®ã‚¹ã‚³ã‚¢ã¯è‘—ã—ãä½ä¸‹ã—ãŸãŒã€ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãªçŸ¥è­˜ãŒé‡è¦ãªã‚¿ã‚¹ã‚¯ã¯æ€§èƒ½ãŒå°‘ã—ä½ä¸‹ã™ã‚‹ã®ã¿ã€ã‹ã¤éMassive Valueã‚’ç ´å£Šã—ã¦ã‚‚å¤§ããªå¤‰åŒ–ã¯ç„¡ã‹ã£ãŸãŸã‚ï¼‰ã€‚ã¾ãŸMassive Valueã¯RoPEã‚’ä½¿ã£ãŸãƒ¢ãƒ‡ãƒ«ã®ã¿Q, Kã®ç‰¹å®šã®æ¬¡å…ƒã«ã®ã¿é›†ä¸­ã—ã¦å‡ºç¾ã™ã‚‹ã€‚ã“ã‚Œã¯RoPEã§ã¯å›è»¢è¡Œåˆ—ã‚’Q, Kã«ã®ã¿é©ç”¨ã—ã¦ã„ã‚‹ã“ã¨ã«èµ·å› ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŒã€å›è»¢è¡Œåˆ—ã®ç©ã®å‰å¾Œã§ã‚‚Massive ValueãŒå‡ºç¾ã™ã‚‹ã“ã¨ã¯å¤‰ã‚ã‚‰ãªã„ã“ã¨ã‹ã‚‰ã€å›è»¢è¡Œåˆ—ãã®ã‚‚ã®ã«èµ·å› ã™ã‚‹ã‚‚ã®ã¨ã„ã†ã‚ˆã‚Šã€å›è»¢è¡Œåˆ—ãŒã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«çµ„ã¿è¾¼ã¾ã‚Œã‚‹ã“ã¨ã§çµæœçš„ã«å­¦ç¿’ã•ã‚Œã‚‹ã‚‚ã®ãªã®ã§ã¯ãªã„ã‹ã€ã¨ã„ã†æ„Ÿã˜ã‚‰ã—ã„ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2025-09-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2785" target="_blank" rel="noopener noreferrer" class="title-link">Attention ls Off By One, Evanmiller.org, 2023.07</a>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2025-08-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2550" target="_blank" rel="noopener noreferrer" class="title-link">Why Stacking Sliding Windows Can't See Very Far, Guangxuan Xiao , 2025.08</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/guangxuan_xiao/status/1960103495081541921?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/python.html" target="_blank" rel="noopener noreferrer">#python</a>
<a class="button" href="articles/Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/MinimalCode.html" target="_blank" rel="noopener noreferrer">#MinimalCode</a>
<span class="issue_date">Issue Date: 2025-08-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2472" target="_blank" rel="noopener noreferrer" class="title-link">simple-paged-attention, torotoki, 2025.06</a>
<span class="snippet"><span>Comment</span><p>CUDA + C++ã«ã‚ˆã‚‹ãƒŸãƒ‹ãƒãƒ«ãªpaged-attentionã®å®Ÿè£…ã€‚ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç†è§£+å®Ÿè£…ç†è§£ã®å‚è€ƒã«éå¸¸ã«è‰¯ã•ãã†ã€‚</p>
<p>PagedAttentionã¯ ç¾åœ¨ã®ä¸»è¦ãªLLM Inference/Serving Engineã®ã²ã¨ã¤ã§ã‚ã‚‹vLLM ã§ï¼ˆææ¡ˆ|å®Ÿè£…ï¼‰ã•ã‚ŒãŸæŠ€è¡“ã§ã‚ã‚Šã€å…ƒè«–æ–‡ã¯ä¸‹è¨˜:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2474" target="_blank" rel="noopener noreferrer">[Paper Note] Efficient Memory Management for Large Language Model Serving with  PagedAttention, Woosuk Kwon+, SOSP'23</a>
</p>
<p>ã“ã®è¾ºã‚‚ã‚ã‚ã›ã¦èª­ã‚€ã¨ãŠã‚‚ã—ã‚ã„ã‹ã‚‚ã—ã‚Œãªã„:<br>


<a href="https://nttdocomo-developers.jp/entry/2024/12/19/090000_6" target="_blank" rel="noopener noreferrer">https://nttdocomo-developers.jp/entry/2024/12/19/090000_6</a>


</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/Scaling%20Laws.html" target="_blank" rel="noopener noreferrer">#Scaling Laws</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-05-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2001" target="_blank" rel="noopener noreferrer" class="title-link">2025å¹´åº¦äººå·¥çŸ¥èƒ½å­¦ä¼šå…¨å›½å¤§ä¼šãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«è¬›æ¼”ã€Œæ·±å±¤åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®æ•°ç†ã€, Taiji Suzuki, 2025.05</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/btreetaiji/status/1927678122817921442?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2025-03-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1812" target="_blank" rel="noopener noreferrer" class="title-link">15 types of attention mechanisms, Kseniase, 2025.03</a>
<span class="snippet"><span>Comment</span><p>Luongã‚‰ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚„soft, globalã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãªã©ã€å¤ãã‹ã‚‰ã‚ã‚‹attentionã‚‚å«ã¾ã‚Œã¦ã„ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2024-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1621" target="_blank" rel="noopener noreferrer" class="title-link">MHA vs MQA vs GQA vs MLA, Zain ul Abideen, 2024.07</a>
<span class="snippet"><span>Comment</span><p>DeepSeekã§ä½¿ã‚ã‚Œã¦ã„ã‚‹Multi Head Latent Attentionï¼ˆMLAï¼‰ã£ã¦ãªã‚“ã ï¼Ÿã¨æ€ã„èª­ã‚“ã ã€‚ç«¯çš„ã«è¨€ã†ã¨ã€GQAã‚„MQAã¯ã€KVã®ãƒ˜ãƒƒãƒ‰ã‚’ãã‚‚ãã‚‚æ¸›ã‚‰ã—ã¦KV Cacheã‚’æŠ‘ãˆã‚ˆã†ã€ã¨ã„ã†æ‰‹æ³•ã ã£ãŸãŒã€MLAã¯KVã‚’ä½ãƒ©ãƒ³ã‚¯ãªãƒ™ã‚¯ãƒˆãƒ«ã«åœ§ç¸®ã—ã¦ä¿æŒã—ã€ä½¿ã†æ™‚ã«å¾©å…ƒã™ã‚‹ã¨ã„ã£ãŸæ“ä½œã‚’ã™ã‚‹ã“ã¨ã§ã€MHAã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è½ã¨ã™ã“ã¨ãªãï¼ˆã‚€ã—ã‚ä¸ŠãŒã‚‹ã‚‰ã—ã„ï¼Ÿï¼‰ã€åˆ©ç”¨ã™ã‚‹KV Cacheã§åˆ©ç”¨ã™ã‚‹ãƒ¡ãƒ¢ãƒªã‚’å¤§å¹…ã«æ¸›ã‚‰ã›ã‚‹ã¨ã„ã†æ‰‹æ³•ã‚‰ã—ã„ã€‚</p>
<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1271" target="_blank" rel="noopener noreferrer">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head
  Checkpoints, Joshua Ainslie+, N/A, arXiv'23</a>
<br><br>MQA, GQAã®æ¦‚è¦ã«ã¤ã„ã¦ã¯ä¸Šè¨˜å‚ç…§ã®ã“ã¨ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-12-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1187" target="_blank" rel="noopener noreferrer" class="title-link">ã€ç¶šã€‘Flash Attentionã‚’ä½¿ã£ã¦LLMã®æ¨è«–ã‚’é«˜é€Ÿãƒ»è»½é‡åŒ–ã§ãã‚‹ã‹ï¼Ÿ</a>
<span class="snippet"><span>Comment</span><p>use_cacheãŒTrue/Falseã®å ´åˆã®FlashAttention2ã®inference timeã¨VRAMä½¿ç”¨é‡ã®å‚¾å‘ã‚’sequence_lengthã”ã¨ã«è€ƒå¯Ÿã—ã¦ã„ã‚‹ã€‚<br><br>use_cacheã¯Key Value cacheã®ã‚ªãƒ³ã‚ªãƒ•ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã‚ã‚‹ã€‚autoregressiveãªãƒ¢ãƒ‡ãƒ«ã®inferenceæ™‚ã«ã¯ã€ä½•åº¦ã‚‚åŒã˜input tokenã«å¯¾ã™ã‚‹KVã®è¨ˆç®—ãŒç”Ÿã˜ã‚‹ãŸã‚ï¼ˆMç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã—ãŸå¾Œã€M+1ç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ç”Ÿæˆã‚’ã™ã‚‹å ´åˆã€M-1ç•ªç›®ã¾ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®KVã‚’å†è¨ˆç®—ã›ã­ã°ãªã‚‰ãªã„ï¼‰ã€cacheã‚’ã™ã‚‹ã“ã¨ã§å¤§å¹…ã«è¨ˆç®—é€Ÿåº¦ãŒæ”¹å–„ã•ã‚Œã‚‹ã€‚<br><br>use_cacheã‚’Trueã«ã§ãã‚‹ãªã‚‰FlashAttention2ã®æ©æµã¯å°ã•ã„ï¼ˆinference timeãŒå°‘ã—æ—©ããªã‚‹ã®ã¿ï¼‰ãŸã‚ã€æ½¤æ²¢ãªVRAMãŒã‚ã‚‹ãªã‚‰å¾—ã‚‰ã‚Œã‚‹æ©æµã¯å°ã•ã„ã€‚<br>é€†ã«VRAMç¯€ç´„ã—ã¦use_cacheã‚’Falseã«ã›ã–ã‚‹ã‚’å¾—ãªã„ã®ã§ã‚ã‚Œã°ã€FlashAttention2ã«ã‚ˆã‚ŠVRAMä½¿ç”¨é‡ã‚’sequence_legthã®ç·šå½¢ã«æŠ‘ãˆã‚‹ã“ã¨ãŒã§ãã€ã‹ã¤inference timeã‚‚çŸ­ããªã‚‹ã€‚<br><br>â†‘ä¸Šè¨˜ã¯ã‚ãã¾ã§inferenceã‚’ã™ã‚‹å ´åˆã®ã¿ã®è©±ã§ã‚ã‚Šï¼ˆtrainæ™‚ã¯autoregressive modelã§ã¯causal maskã‚’ç”¨ã„ã€teacher forcingã§ä¸¦åˆ—ã«ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ãã‚‚ãã‚‚KV-cacheã™ã‚‹æ„å‘³ãŒãªã„ï¼‰ã€trainingã‚’ã™ã‚‹å ´åˆFlashAttention2ã§å¤§å¹…ã«VRAMä½¿ç”¨é‡ã‚’æ¸›ã‚‰ã›ã‚‹ã®ã§ã€ãã“ã¯åˆ†ã‘ã¦è€ƒãˆã‚‹ã“ã¨ã€‚<br>


<a href="https://qiita.com/jovyan/items/ff3d0a49163c7afa33ce" target="_blank" rel="noopener noreferrer">https://qiita.com/jovyan/items/ff3d0a49163c7afa33ce</a>


</p>
<p>Flash Attentionã‚’ä½¿ã£ã¦LLMã®æ¨è«–ã‚’é«˜é€Ÿãƒ»è»½é‡åŒ–ã§ãã‚‹ã‹ï¼Ÿ<br>


<a href="https://qiita.com/jovyan/items/11deb9d4601e4705a60d" target="_blank" rel="noopener noreferrer">https://qiita.com/jovyan/items/11deb9d4601e4705a60d</a>


<br><br>ã“ã¡ã‚‰ã®è¨˜äº‹ã‚‚éå¸¸ã«å‹‰å¼·ã«ãªã‚‹</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<span class="issue_date">Issue Date: 2023-07-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/899" target="_blank" rel="noopener noreferrer" class="title-link">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning, 2023</a>
<span class="snippet"><span>GPT Summary</span>- FlashAttention-2ã¯ã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«ãŠã‘ã‚‹Transformerã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ææ¡ˆã•ã‚ŒãŸæ‰‹æ³•ã§ã™ã€‚FlashAttention-2ã¯ã€éå¯¾ç§°ãªGPUãƒ¡ãƒ¢ãƒªéšå±¤ã‚’åˆ©ç”¨ã—ã¦ãƒ¡ãƒ¢ãƒªã®ç¯€ç´„ã¨ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã—ã€æœ€é©åŒ–ã•ã‚ŒãŸè¡Œåˆ—ä¹—ç®—ã«æ¯”ã¹ã¦ç´„2å€ã®é«˜é€ŸåŒ–ã‚’é”æˆã—ã¾ã™ã€‚ã¾ãŸã€FlashAttention-2ã¯GPTã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦ã‚‚é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã—ã€æœ€å¤§225 TFLOPs/sã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã«é”ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>Flash Attention1ã‚ˆã‚Šã‚‚2å€é«˜é€ŸãªFlash Attention 2</p>
<p>Flash Attention1ã¯ã“ã¡ã‚‰ã‚’å‚ç…§<br>


<a href="https://arxiv.org/pdf/2205.14135.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2205.14135.pdf</a>


<br><br>QK Matrixã®è¨ˆç®—ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†ã‘ã¦SRAMã«é€ã£ã¦å‡¦ç†ã™ã‚‹ã“ã¨ã§ã€3å€é«˜é€ŸåŒ–ã—ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’10-20å€ã‚’é”æˆã€‚<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/935f61f3-97ce-4e76-826b-040f92ca567c" alt="image" loading="lazy"></p></span><br><br>
<button onclick="hideContent(0)" style="display: none;">hide</button>
</div>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<script>
  document.addEventListener('DOMContentLoaded', function() {
    const tweets = document.querySelectorAll('.tweet-embed[data-embed]');

    if ('IntersectionObserver' in window) {
      const observer = new IntersectionObserver((entries, obs) => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            const el = entry.target;
            const html = el.getAttribute('data-embed');
            if (html) {
              const placeholder = el.querySelector('.tweet-placeholder');
              if (placeholder) placeholder.remove();

              el.innerHTML = html.trim();

              if (window.twttr?.widgets?.load) {
                window.twttr.widgets.load(el);
              }
            }
            obs.unobserve(el); // å‡¦ç†æ¸ˆã¿ã¯ç›£è¦–è§£é™¤
          }
        });
      }, {
        rootMargin: '500px 0px', // ç”»é¢æ‰‹å‰200pxã§èª­ã¿è¾¼ã¿é–‹å§‹
        threshold: 0
      });

      tweets.forEach(tweet => observer.observe(tweet));

    } else {
      // IntersectionObserveræœªå¯¾å¿œãƒ–ãƒ©ã‚¦ã‚¶ç”¨ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
      function lazyLoadFallback() {
        tweets.forEach(el => {
          if (el.getAttribute('data-embed') && el.getBoundingClientRect().top < window.innerHeight) {
            const html = el.getAttribute('data-embed');
            const loadingImg = el.querySelector('.tweet-loading');
            if (loadingImg) loadingImg.remove();
            el.innerHTML = html.trim();
            el.removeAttribute('data-embed');
            if (window.twttr?.widgets?.load) {
              window.twttr.widgets.load(el);
            }
          }
        });
      }
      window.addEventListener('scroll', lazyLoadFallback);
      lazyLoadFallback();
    }
  });
</script>


    </div>

</article>
<div class="post-nav">
<a class="previous" href="/paper_notes/articles/Attack.html" title="Attackã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">Attackã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§</a><a class="next" href="/paper_notes/articles/AttentionSinks.html" title="AttentionSinksã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">AttentionSinksã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§</a>
</div>
<div class="post-related">
      <div>Related Articles</div>
      <ul>
        <li class="">
          <a class="post-link" href="/paper_notes/articles/TechnologyEnhancedLearning.html" title="TechnologyEnhancedLearningã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            TechnologyEnhancedLearningã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/RecommenderSystems.html" title="RecommenderSystemsã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            RecommenderSystemsã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/Contamination-free.html" title="Contamination-freeã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            Contamination-freeã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/ExplicitFeedback.html" title="ExplicitFeedbackã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            ExplicitFeedbackã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
</ul>
    </div>
<div class="post-comments"></div></section>
</div>


  </section>
  <section class="sidebar" style="margin-left: 15px;">
    <!-- Get sidebar items --><style type="text/css" media="screen">
.post-menu ul {
  list-style: none;
  padding: 0;
  margin: 0;
}
</style>

<div class="post-menu">
  <div class="post-menu-title">TOC</div>
  <div class="post-menu-content"></div>
</div>

<script>
  function generateContent() {
    var menu = document.querySelector(".post-menu");
    var menuContent =  menu.querySelector(".post-menu-content");
    var headings = document.querySelector(".post-content").querySelectorAll("h2, h3, h4, h5, h6");

    // Hide menu when no headings
    if (headings.length === 0) {
      return menu.style.display = "none";
    }

    // Generate post menu
    var menuHTML = '';
    for (var i = 0; i < headings.length; i++) {
      var h = headings[i];
      menuHTML += (
        '<li class="h-' + h.tagName.toLowerCase() + '">'
        + '<a href="#h-' + h.getAttribute('id') + '">' + h.textContent + '</a></li>');
    }

    menuContent.innerHTML = '<ul>' + menuHTML + '</ul>';

    // The header element
    var header = document.querySelector('header.site-header');

    function doMenuCollapse(index, over_items) {
      var items = menuContent.firstChild.children;

      if (over_items == undefined) {
        over_items = 20;
      }

      if (items.length < over_items) {
        return;
      }

      var activeItem = items[index];
      var beginItem = activeItem
      var endItem = activeItem
      var beginIndex = index;
      var endIndex = index + 1;
      while (beginIndex >= 0
        && !items[beginIndex].classList.contains('h-h2')) {
        beginIndex -= 1;
      }
      while (endIndex < items.length
        && !items[endIndex].classList.contains('h-h2')) {
        endIndex += 1;
      }
      for (var i = 0; i < beginIndex; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
      for (var i = beginIndex + 1; i < endIndex; i++) {
        item = items[i]
        // if (!item.classList.contains('h-h2')) {
          item.style.display = '';
        // }
      }
      for (var i = endIndex; i < items.length; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
    }

    // Init menu collapsed
    doMenuCollapse(-1);

    // Active the menu item
    window.addEventListener('scroll', function (event) {
      var lastActive = menuContent.querySelector('.active');
      var changed = true;
      var activeIndex = -1;
      for (var i = headings.length - 1; i >= 0; i--) {
        var h = headings[i];
        var headingRect = h.getBoundingClientRect();
        var headerRect = header.getBoundingClientRect();
        var headerTop = Math.floor(headerRect.top);
        var headerHeight = Math.floor(headerRect.height);
        var headerHeight = headerTop + headerHeight + 20;
        if (headingRect.top <= headerHeight) {
          var id = 'h-' + h.getAttribute('id');
          var a = menuContent.querySelector('a[href="#' + id  + '"]');
          var curActive = a.parentNode;
          if (curActive) {
            curActive.classList.add('active');
            activeIndex = i;
          }
          if (lastActive == curActive) {
            changed = false;
          }
          break;
        }
      }
      if (changed) {
        if (lastActive) {
          lastActive.classList.remove('active');
        }
        doMenuCollapse(activeIndex);
      }
      event.preventDefault();
    });
  }
  generateContent();
</script>
</section>
</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/paper_notes/"></data>

  <div class="wrapper">
    <div class="site-footer-inner">
<div>Copyright Â© 2023-current AkihikoWATANABE. The header images and any thumbnail images for the posts were generated by ChatGPT's DALL-E3.</div>
      <div>Powered by <a title="Jekyll is a simple, blog-aware, static site
      generator." href="https://jekyllrb.com/">Jekyll</a> &amp; <a title="Yat, yet
      another theme." href="https://github.com/jeffreytse/jekyll-theme-yat">Yat Theme</a>.</div>
      <div class="footer-col rss-subscribe">Subscribe <a href="/paper_notes/feed.xml">via RSS</a>
</div>
    </div>
  </div>
</footer>
</body>
  </html>
