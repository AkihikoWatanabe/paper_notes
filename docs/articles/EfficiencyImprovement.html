<!DOCTYPE html>
<html lang="ja">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-translate-customization" content="108d9124921d80c3-80e20d618ff053c8-g4f02ec6f3dba68b7-c">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>EfficiencyImprovementã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§ | ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ</title>
<meta name="generator" content="Jekyll v3.10.0">
<meta property="og:title" content="EfficiencyImprovementã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
<meta name="author" content="AkihikoWATANABE">
<meta property="og:locale" content="ja">
<meta name="description" content="EfficiencyImprovement #GraphBased #Pocket #NLP #RAG(RetrievalAugmentedGeneration) #EMNLP">
<meta property="og:description" content="EfficiencyImprovement #GraphBased #Pocket #NLP #RAG(RetrievalAugmentedGeneration) #EMNLP">
<link rel="canonical" href="http://akihikowatanabe.github.io/paper_notes/articles/EfficiencyImprovement.html">
<meta property="og:url" content="http://akihikowatanabe.github.io/paper_notes/articles/EfficiencyImprovement.html">
<meta property="og:site_name" content="ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-11-19T00:48:58+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="EfficiencyImprovementã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"AkihikoWATANABE"},"dateModified":"2025-11-19T00:48:58+00:00","datePublished":"2025-11-19T00:48:58+00:00","description":"EfficiencyImprovement #GraphBased #Pocket #NLP #RAG(RetrievalAugmentedGeneration) #EMNLP","headline":"EfficiencyImprovementã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§","mainEntityOfPage":{"@type":"WebPage","@id":"http://akihikowatanabe.github.io/paper_notes/articles/EfficiencyImprovement.html"},"url":"http://akihikowatanabe.github.io/paper_notes/articles/EfficiencyImprovement.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="icon" href="">
  <link rel="canonical" href="http://akihikowatanabe.github.io">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/main.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/custom_button.css">
  <script src="/paper_notes/assets/js/main.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script><link type="application/atom+xml" rel="alternate" href="http://akihikowatanabe.github.io/paper_notes/feed.xml" title="ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ">
<script>
  function showMore(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "block";
          
      // ã“ã®ãƒœã‚¿ãƒ³ã®å‚ç…§ã‚’å–å¾—ã—ã¦éè¡¨ç¤ºã«ã—ã¾ã™
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "none";
      }
          
      // hideãƒœã‚¿ãƒ³ã®å‚ç…§ã‚’å–å¾—ã—ã¦è¡¨ç¤ºã—ã¾ã™
      const hideButton = contentDiv.querySelector('button[onclick^="hideContent"]');
      if (hideButton) {
        hideButton.style.display = "block";
      }
    }
  }

  function hideContent(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "none";
  
      // moreãƒœã‚¿ãƒ³ã®å‚ç…§ã‚’å–å¾—ã—ã¦è¡¨ç¤ºã—ã¾ã™
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "block";
      }
  
      // ã“ã®ãƒœã‚¿ãƒ³ã‚’éš ã—ã¾ã™
      const hideButtons = document.querySelectorAll('button[onclick^="hideContent"]');
      const hideButton = hideButtons[index];
      if (hideButton) {
        hideButton.style.display = "none";
      }
    }
  }
</script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script charset="UTF-8" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/go.min.js" async></script>



















<script>
// Init highlight js
document.addEventListener('DOMContentLoaded', function(event) {
  var els = document.querySelectorAll('pre code')

  function addLangData(block) {
    var outer = block.parentElement.parentElement.parentElement;
    var lang = block.getAttribute('data-lang');
    for (var i = 0; i < outer.classList.length; i++) {
      var cls = outer.classList[i];
      if (cls.startsWith('language-')) {
        lang = cls;
        break;
      }
    }
    if (!lang) {
      cls = block.getAttribute('class');
      lang = cls ? cls.replace('hljs ', '') : '';
    }
    if (lang.startsWith('language-')) {
      lang = lang.substr(9);
    }
    block.setAttribute('class', 'hljs ' + lang);
    block.parentNode.setAttribute('data-lang', lang);
  }

  function addBadge(block) {
    var enabled = ('true' || 'true').toLowerCase();
    if (enabled == 'true') {
      var pre = block.parentElement;
      pre.classList.add('badge');
    }
  }

  function handle(block) {
    addLangData(block);
    addBadge(block)
    hljs.highlightBlock(block);
  }

  for (var i = 0; i < els.length; i++) {
    var el = els[i];
    handle(el);
  }
});
</script>

<style>
  /* code language badge */
  pre.badge::before {
    content: attr(data-lang);
    color: #fff;
    background-color: #ff4e00;
    padding: 0 .5em;
    border-radius: 0 2px;
    text-transform: uppercase;
    text-align: center;
    min-width: 32px;
    display: inline-block;
    position: absolute;
    right: 0;
  }

  /* fix wrong badge display for firefox browser */
  code > table pre::before {
    display: none;
  }
</style>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe-lightbox.umd.min.js" async></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe.umd.min.js" async></script>
<link href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/photoswipe.min.css" rel="stylesheet">
<style>
  .pswp .pswp__container .pswp__img {
    background-color: white;
  }
</style>

<script>
  function initPhotoSwipe() {
    let customOptions = {};

    try {
      const data = `{"gallery"=>"section.main", "children"=>"a.photo-swipe", "bgOpacity"=>0.8, "padding"=>{"top"=>20, "bottom"=>40, "left"=>100, "right"=>100}}`.replaceAll("=>", ":");
      customOptions = JSON.parse(data);
    } catch (e) {
      console.info("Invalid custom photo previewer options! " + e.message);
    }

    // Define object and gallery options
    const options = Object.assign(
      {
        gallery: "section.main",
        children: "a.photo-swipe",
        photo_scale: 2,
        // dynamic import is not supported in UMD version
        pswpModule: PhotoSwipe,
      },
      customOptions
    );

    const galleryEl = document.querySelector(options.gallery);
    if (!galleryEl) {
      return;
    }

    const imgEls = [];
    const els = galleryEl.querySelectorAll("img:not(.emoji)");
    els.forEach((el) => {
      if (el.src.trim() == "") {
        return;
      }
      if (!imgEls.includes(el)) {
        imgEls.push(el);
      }
    });

    if (imgEls.length === 0) {
      return;
    }

    imgEls.forEach((imgEl) => {
      imgEl.outerHTML = `
      <a class="photo-swipe"
        href="${imgEl.src}"
        data-pswp-width="${
          Math.max(imgEl.naturalWidth, imgEl.width) * options.photo_scale
        }"
        data-pswp-height="${
          Math.max(imgEl.naturalHeight, imgEl.height) * options.photo_scale
        }"
        data-pswp-caption="${imgEl.getAttribute("caption") || imgEl.alt}"
        target="_blank">
        ${imgEl.outerHTML}
      </a>`;
    });

    // Initialize PhotoSwipe 5
    var lightbox = new PhotoSwipeLightbox(options);

    lightbox.init();
  }

  window.addEventListener("load", initPhotoSwipe);
</script>
<meta name="google-site-verification" content="u_DTTPcCZ806iq51zgirHyWq3556HUKGq8AQfH91iFI">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-P70KSB88WH"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-P70KSB88WH');
  </script>

<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>





































































































































<header class="site-header site-header-transparent" role="banner">

  <div class="wrapper">
    <div class="site-header-inner">
<span class="site-brand"><a class="site-brand-inner" rel="author" href="/paper_notes/">
  <img class="site-favicon" title="ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ" src="" onerror="this.style.display='none'">
  ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ
</a>
</span><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger">
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewbox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
              </svg>
            </span>
          </label>

          <div class="trigger">









<span class="page-link">



<div id="google_translate_element" style="display: none;">
</div>

<span class="ct-language">
  <ul class="list-unstyled ct-language-dropdown">
    
      <li>
        <a href="#" class="lang-select" data-lang="en">
          
          <img src="https://cdn.countryflags.com/thumbs/united-states-of-america/flag-400.png" title="English">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="fr">
          
          <img src="https://cdn.countryflags.com/thumbs/france/flag-400.png" title="French">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="zh-CN">
          
          <img src="https://cdn.countryflags.com/thumbs/china/flag-400.png" title="Chinese(Simple)">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ja">
          
          <img src="https://cdn.countryflags.com/thumbs/japan/flag-400.png" title="Japanese">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ko">
          
          <img src="https://cdn.countryflags.com/thumbs/south-korea/flag-400.png" title="Korean">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ru">
          
          <img src="https://cdn.countryflags.com/thumbs/russia/flag-400.png" title="Russian">
          
        </a>
      </li>
    
  </ul>
</span>

<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({
    pageLanguage: 'ja',
    autoDisplay: false,
    layout: google.translate.TranslateElement.InlineLayout.VERTICAL
  }, 'google_translate_element');

  // Links to cross-origin destinations are unsafe
  var gll = document.getElementsByClassName('goog-logo-link')[0];
  if (gll) {
    gll.setAttribute('rel', 'noopener');
  }

  function restoreLang() {
    var iframe = document.getElementsByClassName('goog-te-banner-frame')[0];
    if (!iframe) return;

    var innerDoc = iframe.contentDocument || iframe.contentWindow.document;
    var restore_el = innerDoc.getElementsByTagName("button");

    for (var i = 0; i < restore_el.length; i++) {
      if (restore_el[i].id.indexOf("restore") >= 0) {
        restore_el[i].click();
        var close_el = innerDoc.getElementsByClassName("goog-close-link");
        close_el[0].click();
        return;
      }
    }
  }

  function triggerHtmlEvent(element, eventName) {
    var event;
    if (document.createEvent) {
      event = document.createEvent('HTMLEvents');
      event.initEvent(eventName, true, true);
      element.dispatchEvent(event);
    } else {
      event = document.createEventObject();
      event.eventType = eventName;
      element.fireEvent('on' + event.eventType, event);
    }
  }

  var googleCombo = document.querySelector("select.goog-te-combo");
  var langSelect = document.querySelector('.ct-language');
  langSelect.addEventListener('click', function(event) {
    if (!event.target) {
      return;
    }

    var selected = document.querySelector('.ct-language .ct-language-selected');
    if (selected) {
      selected.classList.remove('ct-language-selected');
    }

    var target = event.target;
    while (target && target !== langSelect ) {
      if (target.matches('.lang-select')) {
        break;
      }
      target = target.parentElement;
    }

    if (target && target.matches('.lang-select')) {
      var lang = target.getAttribute('data-lang');
      if (googleCombo.value == lang) {
        restoreLang();
      } else {
        target.parentElement.classList.add('ct-language-selected');
        googleCombo.value = lang;
        triggerHtmlEvent(googleCombo, 'change');
      }
    }

    event.preventDefault();
  });
}
</script>

<script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit" async></script>
</span>
</div>
        </nav>
</div>
  </div>
</header>

<script>
  function initHeader() {
    var lastScrollY = getScrollPos().y;
    var documentElement = document.documentElement;

    function storeScrollData() {
      var y = getScrollPos().y;documentElement.setAttribute("data-header-transparent", "");var scrollStatus = "";

      if (y <= 0) {
        scrollStatus = "top";
      } else if ((window.innerHeight + y) >= document.body.offsetHeight) {
        scrollStatus = "bottom";
      } else {
        var isScrollDown = (y - lastScrollY > 0) ? true : false;
        scrollStatus = isScrollDown ? "down" : "up";
      }

      lastScrollY = y;
      documentElement.setAttribute("data-scroll-status", scrollStatus);
    }

    window.addEventListener('scroll', function(e) {
      storeScrollData();
    });

    storeScrollData();
  }
  document.addEventListener('DOMContentLoaded', initHeader);
</script>


























































































































































<style>
    html .page-banner .page-banner-img > *:first-child {
      opacity: 0.4;
    }

    html[data-theme="dark"] .page-banner .page-banner-img > *:first-child {
      opacity: 0.2872;
    }
  </style>
<section class="page-banner">
    <div class="page-banner-img">
<div style="background-image: url(/paper_notes/assets/images/banner.png)"></div>
        <img class="img-placeholder" src="/paper_notes/assets/images/banner.png">
</div>
    <div class="wrapper">
      <div class="page-banner-inner">
<header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ</h1>
  <h2 class="post-subtitle">å‹‰å¼·ã—ãŸè«–æ–‡ã‚„æŠ€è¡“ç­‰ã®æƒ…å ±ã‚’Githubã®Issueã«ãƒ¡ãƒ¢ã£ã¦ã„ã‚‹ã²ã¨ã®ãƒ–ãƒ­ã‚°ã€‚
ãã‚Œãªã‚Šã«ãƒ¡ãƒ¢ã®é‡ãŒè“„ç©ã•ã‚Œã¦ããŸã®ã§ã€ä¸€åº¦æ•´ç†ã—ãŸã„ãªã¨æ€ã„ãƒ–ãƒ­ã‚°ã¯ã˜ã‚ã¦ã¿ã¾ã—ãŸï¼
è‡ªç„¶è¨€èªå‡¦ç†(NLP), æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ (RecommenderSystem), Educational Data Mining (EDM), Learning Analytics (LA)ãªã©ã®åˆ†é‡ã®ãƒ¡ãƒ¢ãŒå¤šã„ã¨æ€ã„ã¾ã™ã€‚
æœ€è¿‘ã¯ç‰¹ã«LLMã®å‹‰å¼·ãŒå¤šã‚ã§ã™ :)</h2>

  <div class="post-meta">
    <time class="dt-published" datetime="2025-11-19T00:48:58+00:00" itemprop="datePublished"><i class="fa fa-calendar"></i> Nov 19, 2025
    </time><span class="post-author left-vsplit"><i class="fa fa-pencil"></i> AkihikoWATANABE</span>
    
































    <span class="post-reading-time left-vsplit"><i class="fa fa-clock-o"></i> About 6 hours 32 mins</span>
  </div></header>
</div>
    </div>
  </section><script>
  function hashLocate(hashValue) {
    hashValue = hashValue.replace(/^.*#h-/, '');
    hashValue = decodeURIComponent(hashValue);
    var element = document.getElementById(hashValue);

    if (!element) {
      return;
    }

    var header = document.querySelector('header.site-header');
    var headerRect = header.getBoundingClientRect();
    var headerTop = Math.floor(headerRect.top);
    var headerHeight = Math.floor(headerRect.height);
    var scrollPos = getScrollPos();
    var offsetY = element.offsetTop - (headerTop + headerHeight + 20);

    if (offsetY == scrollPos.y) {
      return;
    }

    if (headerTop == 0  && offsetY > scrollPos.y) {
      offsetY += headerHeight + 2;
    } else if (headerTop < 0  && offsetY < scrollPos.y) {
      offsetY -= headerHeight - 2;
    }

    smoothScrollTo(offsetY);
  }

  // The first event occurred
  window.addEventListener('load', function(event) {
    if (window.location.hash) {
      hashLocate(window.location.hash);
    }
  });

  // The first event occurred
  window.addEventListener('click', function(event) {
    if (event.target.tagName.toLowerCase() == 'a') {
      hashLocate(event.target.getAttribute('href'));
    }
  });
</script>
<div class="theme-toggle">
  <input type="checkbox" id="theme-switch">
  <label for="theme-switch">
    <div class="toggle"></div>
    <div class="names">
      <p class="light">Light</p>
      <p class="dark">Dark</p>
    </div>
  </label>
</div>




<script>
  (function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var nightModeOption = ('off' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
      storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
      var data = storage.getItem('theme');
      try {
        data = JSON.parse(data ? data : '');
      } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
      }
      return data;
    }

    function handleThemeToggle(nightShift) {
      themeData.nightShift = nightShift;
      saveThemeData(themeData);
      html.dataset.theme = nightShift ? 'dark' : 'light';
      setTimeout(function() {
        sw.checked = nightShift ? true : false;
      }, 50);
    }

    function autoThemeToggle() {
      // Next time point of theme toggle
      var now = new Date();
      var toggleAt = new Date();
      var hours = now.getHours();
      var nightShift = hours >= 19 || hours <=7;

      if (nightShift) {
        if (hours > 7) {
          toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
      } else {
        toggleAt.setHours(19);
      }

      toggleAt.setMinutes(0);
      toggleAt.setSeconds(0);
      toggleAt.setMilliseconds(0)

      var delay = toggleAt.getTime() - now.getTime();

      // auto toggle theme mode
      setTimeout(function() {
        handleThemeToggle(!nightShift);
      }, delay);

      return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
      };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
      handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
      var data = autoThemeToggle();

      // Toggle theme by local setting
      if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
      } else {
        handleThemeToggle(themeData.nightShift);
      }
    } else if (nightModeOption == 'manual') {
      handleThemeToggle(themeData.nightShift);
    } else {
      var nightShift = themeData.nightShift;
      if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
      }
      handleThemeToggle(nightShift);
    }
  })();
</script>
<div id="click-to-top" class="click-to-top">
  <i class="fa fa-arrow-up"></i>
</div>
<script>
  (function () {
    const clickToTop = document.getElementById('click-to-top');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 100) {
        clickToTop.classList.add('show')
      }else {
        clickToTop.classList.remove('show')
      }
    });
    clickToTop.addEventListener('click', () => {
      window.smoothScrollTo(0);
    });
  })();
</script>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="framework">
  <section class="main">

     <div class="post">
  <section>









<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="post-content e-content" itemprop="articleBody">

      <h2 id="EfficiencyImprovement"> EfficiencyImprovement</h2>
<div class="visible-content">
<a class="button" href="articles/GraphBased.html" target="_blank" rel="noopener noreferrer">#GraphBased</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/RAG(RetrievalAugmentedGeneration).html" target="_blank" rel="noopener noreferrer">#RAG(RetrievalAugmentedGeneration)</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>


<br>


<span class="issue_date">Issue Date: 2025-11-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3713" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] LightRAG: Simple and Fast Retrieval-Augmented Generation, Zirui Guo+, EMNLP'25, 2024.10</a>
<span class="snippet"><span>GPT Summary</span>- LightRAGã¯ã€ã‚°ãƒ©ãƒ•æ§‹é€ ã‚’å–ã‚Šå…¥ã‚ŒãŸRetrieval-Augmented Generation (RAG)ã‚·ã‚¹ãƒ†ãƒ ã§ã€æ–‡è„ˆã«é–¢é€£ã—ãŸå¿œç­”ã‚’æä¾›ã—ã¾ã™ã€‚äºŒé‡ãƒ¬ãƒ™ãƒ«ã®æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚Šã€çŸ¥è­˜ç™ºè¦‹ã‚’å¼·åŒ–ã—ã€é–¢é€£ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®åŠ¹ç‡çš„ãªæ¤œç´¢ã‚’å®Ÿç¾ã€‚å¢—åˆ†æ›´æ–°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚Šã€æ€¥é€Ÿã«å¤‰åŒ–ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ç’°å¢ƒã§ã‚‚å¿œç­”æ€§ã‚’ç¶­æŒã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€æ—¢å­˜ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨æ¯”è¼ƒã—ã¦ç²¾åº¦ã¨åŠ¹ç‡ãŒå¤§å¹…ã«æ”¹å–„ã•ã‚ŒãŸã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚LightRAGã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>github:


<a href="https://github.com/HKUDS/LightRAG" target="_blank" rel="noopener noreferrer">https://github.com/HKUDS/LightRAG</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huang_chao4969/status/1990277067431424465?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>


<br>


<span class="issue_date">Issue Date: 2025-11-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3700" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Optimizing Mixture of Block Attention, Guangxuan Xiao+, arXiv'25, 2025.11</a>
<span class="snippet"><span>GPT Summary</span>- Mixture of Block Attention (MoBA)ã¯ã€LLMã«ãŠã‘ã‚‹é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚’åŠ¹ç‡åŒ–ã™ã‚‹ãŒã€ãã®è¨­è¨ˆåŸå‰‡ã‚„GPUå®Ÿè£…ãŒä¸ååˆ†ã§ã‚ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€MoBAã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’åˆ†æã—ã€ã‚¯ã‚¨ãƒªã¨ã‚­ãƒ¼ã®è¦ªå’Œæ€§ã«åŸºã¥ããƒ–ãƒ­ãƒƒã‚¯ã®è­˜åˆ¥èƒ½åŠ›ãŒæ€§èƒ½ã«å½±éŸ¿ã™ã‚‹ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ã€‚æ”¹å–„ç­–ã¨ã—ã¦ã€å°ã•ãªãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚ºã®ä½¿ç”¨ã¨ã‚­ãƒ¼ã«å¯¾ã™ã‚‹çŸ­ã„ç•³ã¿è¾¼ã¿ã®é©ç”¨ã‚’ææ¡ˆã€‚ã“ã‚Œã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«ã€FlashMoBAã‚’å°å…¥ã—ã€åŠ¹ç‡çš„ãªMoBAå®Ÿè¡Œã‚’å¯èƒ½ã«ã™ã‚‹CUDAã‚«ãƒ¼ãƒãƒ«ã‚’é–‹ç™ºã€‚FlashMoBAã¯ã€æœ€å¤§14.7å€ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’é”æˆã—ã€ç†è«–ã«åŸºã¥ãæ”¹å–„ã‚’å®Ÿç”¨åŒ–ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/papers_anon/status/1990266669206536330?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>flash_attention2ã«å¯¾ã—ã¦æœ€å¤§ã§14.7å€ğŸ‘€ã©ã†ã„ã†æ¡ä»¶ã€å®Ÿé¨“ã ã‚ã†ã‹</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>


<br>


<span class="issue_date">Issue Date: 2025-11-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3699" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Virtual Width Networks, Seed+, arXiv'25, 2025.11</a>
<span class="snippet"><span>GPT Summary</span>- Virtual Width Networks (VWN)ã¯ã€éš ã‚Œå±¤ã®ã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã™ã“ã¨ãªãã€ã‚ˆã‚Šåºƒã„è¡¨ç¾ã‚’å¯èƒ½ã«ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚‹ã€‚VWNã¯ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã®è¨ˆç®—ã‚’ã»ã¼ä¸€å®šã«ä¿ã¡ãªãŒã‚‰åŸ‹ã‚è¾¼ã¿ç©ºé–“ã‚’æ‹¡å¼µã—ã€8å€ã®æ‹¡å¼µã§ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã®æœ€é©åŒ–ã‚’åŠ é€Ÿã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé€²ã‚€ã«ã¤ã‚Œã¦ã“ã®åˆ©ç‚¹ã¯å¢—å¹…ã•ã‚Œã€ä»®æƒ³å¹…ã¨æå¤±å‰Šæ¸›ã®é–“ã«ã¯å¯¾æ•°ç·šå½¢ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é–¢ä¿‚ãŒã‚ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1990270269873864796?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/papers_anon/status/1990269413195743671?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é‡è¦è«–æ–‡ã«è¦‹ãˆã‚‹ã€‚transformerã®ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã®æ¬¡å…ƒã¯å¤‰ãˆãªã„ã§ãƒ™ã‚¯ãƒˆãƒ«ã®widthã‚’åºƒã’ã‚‹ã“ã¨ã¨åŒç­‰ã®åŠ¹åŠ›ã‚’å¾—ã‚‹ãŸã‚ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ææ¡ˆã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚<br><br>ã–ã£ãã‚Šè¨€ã†ã¨embeddingã‚’Nå€ï¼ˆover-width)ã—ã€ææ¡ˆæ‰‹æ³•ã§ã‚ã‚‹GHCã‚’ç”¨ã„ã¦ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã«æµã›ã‚‹ã‚µã‚¤ã‚ºã«ãƒ™ã‚¯ãƒˆãƒ«ã‚’åœ§ç¸®ã—transformerãƒ–ãƒ­ãƒƒã‚¯ã§å‡¦ç†ã—over-widthã—ãŸæ¬¡å…ƒã«æˆ»ã™å‡¦ç†ã‚’ã™ã‚‹æ©Ÿæ§‹ã¨ã€over-widthã—ãŸembeddingã‚’æ¬¡å…ƒæ•°ã¯å¤‰ãˆãšã«å¤‰æ›ã™ã‚‹linearã‚’å™›ã¾ã›ãŸçµæœã‚’è¶³ã—åˆã‚ã›ã‚‹ã‚ˆã†ãªæ©Ÿæ§‹ã‚’ç”¨æ„ã—ã¦æœ€å¤§ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã§ã‚ã‚‹transformerãƒ–ãƒ­ãƒƒã‚¯ã®è¨ˆç®—é‡ã¯å¤‰ãˆãšã«è¡¨ç¾åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ã€ã¨ã„ã£ãŸæ„Ÿã˜ã®æ‰‹æ³•ãªæ¨¡æ§˜<br><br><img src="https://github.com/user-attachments/assets/8c2fc967-54ff-43fa-a469-ece0cdaa0131" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/8d9a123f-747f-48f7-b04b-167e98ed2350" alt="image" loading="lazy"></p></span><br><br>
</div>
<p><button onclick="showMore(0)">more</button></p>
<div class="hidden-content">
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/LatentReasoning.html" target="_blank" rel="noopener noreferrer">#LatentReasoning</a>
<a class="button" href="articles/RecurrentModels.html" target="_blank" rel="noopener noreferrer">#RecurrentModels</a>
<a class="button" href="articles/RecursiveModels.html" target="_blank" rel="noopener noreferrer">#RecursiveModels</a>
<span class="issue_date">Issue Date: 2025-11-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3655" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence, Sean McLeish+, arXiv'25, 2025.11</a>
<span class="snippet"><span>GPT Summary</span>- æ·±å±¤å†å¸°è¨€èªãƒ¢ãƒ‡ãƒ«ã®é€²å±•ã«ã‚ˆã‚Šã€å†å¸°ã®è¨ˆç®—é‡ã‚’è¨“ç·´æ™‚ã¨ãƒ†ã‚¹ãƒˆæ™‚ã§åˆ‡ã‚Šé›¢ã™ã“ã¨ãŒå¯èƒ½ã«ã€‚æœ¬ç ”ç©¶ã§ã¯ã€éå†å¸°è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’æ·±å±¤å†å¸°ãƒ¢ãƒ‡ãƒ«ã«å¤‰æ›ã™ã‚‹æ–¹æ³•ã‚’ææ¡ˆã—ã€å†å¸°ã®ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ã‚’ç”¨ã„ã‚‹ã“ã¨ã§æ€§èƒ½ã‚’ç¶­æŒã—ã¤ã¤è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚æ•°å­¦å®Ÿé¨“ã§ã¯ã€å†å¸°ãƒ¢ãƒ‡ãƒ«ã¸ã®å¤‰æ›ãŒãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚ˆã‚Šã‚‚å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/seanmcleish/status/1988268805479690364?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/micahgoldblum/status/1988265009508655528?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/LLM-as-a-Judge.html" target="_blank" rel="noopener noreferrer">#LLM-as-a-Judge</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Stability.html" target="_blank" rel="noopener noreferrer">#Stability</a>
<span class="issue_date">Issue Date: 2025-11-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3636" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with   Conformal Prediction, Huanxin Sheng+, EMNLP'25 SAC Highlights, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- LLMã‚’ç”¨ã„ãŸè‡ªç„¶è¨€èªç”Ÿæˆã®è©•ä¾¡ã«ãŠã‘ã‚‹ä¸ç¢ºå®Ÿæ€§ã‚’åˆ†æã™ã‚‹ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã€‚é©åˆäºˆæ¸¬ã‚’é€šã˜ã¦äºˆæ¸¬åŒºé–“ã‚’æ§‹ç¯‰ã—ã€ä¸­å¤®å€¤ã«åŸºã¥ãã‚¹ã‚³ã‚¢ã‚’ä½ãƒã‚¤ã‚¢ã‚¹ã®ä»£æ›¿æ‰‹æ®µã¨ã—ã¦æç¤ºã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€é©åˆäºˆæ¸¬ãŒæœ‰åŠ¹ãªäºˆæ¸¬åŒºé–“ã‚’æä¾›ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã€åˆ¤æ–­ã®å‘ä¸Šã«å‘ã‘ãŸä¸­å¤®å€¤ã‚„å†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœ‰ç”¨æ€§ã‚‚æ¢æ±‚ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huanxinshe5254/status/1987186857545969698?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å®Ÿç”¨ä¸Šéå¸¸ã«é‡è¦ãªè©±ã«è¦‹ãˆã‚‹</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Search.html" target="_blank" rel="noopener noreferrer">#Search</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Contamination-free.html" target="_blank" rel="noopener noreferrer">#Contamination-free</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-11-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3633" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Infini-gram mini: Exact n-gram Search at the Internet Scale with   FM-Index, Hao Xu+, EMNLP'25 Best Paper, 2025.06</a>
<span class="snippet"><span>GPT Summary</span>- ã€Œinfini-gram miniã€ã¯ã€ãƒšã‚¿ãƒã‚¤ãƒˆãƒ¬ãƒ™ãƒ«ã®ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ¼ãƒ‘ã‚¹ã‚’åŠ¹ç‡çš„ã«æ¤œç´¢å¯èƒ½ã«ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã§ã€FM-indexãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ç”¨ã„ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆã—ã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’44%ã«å‰Šæ¸›ã€‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆé€Ÿåº¦ã‚„ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å¤§å¹…ã«æ”¹å–„ã—ã€83TBã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆãƒ†ã‚­ã‚¹ãƒˆã‚’99æ—¥ã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ã€‚å¤§è¦æ¨¡ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ±šæŸ“ã®åˆ†æã‚’è¡Œã„ã€ä¸»è¦ãªLMè©•ä¾¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãŒã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆã‚¯ãƒ­ãƒ¼ãƒªãƒ³ã‚°ã§æ±šæŸ“ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã€‚æ±šæŸ“ç‡ã‚’å…±æœ‰ã™ã‚‹å…¬å ±ã‚’ãƒ›ã‚¹ãƒˆã—ã€æ¤œç´¢ã‚¯ã‚¨ãƒªç”¨ã®ã‚¦ã‚§ãƒ–ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¨APIã‚‚æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/liujc1998/status/1986821544405045497?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>pj page:


<a href="https://infini-gram-mini.io" target="_blank" rel="noopener noreferrer">https://infini-gram-mini.io</a>


</p>
<p>benchmarmk contamination monitoring system: 


<a href="https://huggingface.co/spaces/infini-gram-mini/Benchmark-Contamination-Monitoring-System" target="_blank" rel="noopener noreferrer">https://huggingface.co/spaces/infini-gram-mini/Benchmark-Contamination-Monitoring-System</a>


</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ImageSegmentation.html" target="_blank" rel="noopener noreferrer">#ImageSegmentation</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/Video.html" target="_blank" rel="noopener noreferrer">#Video</a>
<a class="button" href="articles/2D%20(Image).html" target="_blank" rel="noopener noreferrer">#2D (Image)</a>
<span class="issue_date">Issue Date: 2025-11-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3631" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] EdgeTAM: On-Device Track Anything Model, Chong Zhou+, arXiv'25, 2025.01</a>
<span class="snippet"><span>GPT Summary</span>- SAM 2ã¯å‹•ç”»ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ãƒ¡ãƒ¢ãƒªãƒãƒ³ã‚¯ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’é€šã˜ã¦æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã¦ã„ã¾ã™ã€‚æœ¬ç ”ç©¶ã§ã¯ã€ãƒ¢ãƒã‚¤ãƒ«ãƒ‡ãƒã‚¤ã‚¹ä¸Šã§ã®åŠ¹ç‡ã‚’é«˜ã‚ã‚‹ãŸã‚ã«ã€EdgeTAMã‚’ææ¡ˆã—ã€2Dç©ºé–“ãƒ‘ãƒ¼ã‚»ãƒ—ã‚¿ãƒ¼ã‚’ç”¨ã„ã¦è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¡ãƒ¢ãƒªã®ç©ºé–“æ§‹é€ ã‚’ä¿æŒã—ã¤ã¤ã€æ¨è«–ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãªã—ã§æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹è’¸ç•™ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚‚å°å…¥ã€‚EdgeTAMã¯è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§é«˜ã„J&amp;Fã‚¹ã‚³ã‚¢ã‚’é”æˆã—ã€iPhone 15 Pro Maxã§16 FPSã§å‹•ä½œã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/mervenoyann/status/1986785795424788812?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>SAM2ã‚ˆã‚Šæ€§èƒ½ã¯å°‘ã—åŠ£ã‚‹ãŒã€edge-deviceã¦ã‚å‹•ä½œå¯èƒ½ã§éå¸¸ã«é«˜é€Ÿãªãƒ¢ãƒ‡ãƒ«ï¼ˆpromptã«ã‚ˆã£ã¦åˆ¶å¾¡å¯èƒ½ãªsegmentation)ã¨ã®ã“ã¨<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3632" target="_blank" rel="noopener noreferrer">[Paper Note] SAM 2: Segment Anything in Images and Videos, Nikhila Ravi+, ICLR'25, 2024.08</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<span class="issue_date">Issue Date: 2025-11-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3617" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Scaling Agent Learning via Experience Synthesis, Zhaorun Chen+, arXiv'25, 2025.11</a>
<span class="snippet"><span>GPT Summary</span>- DreamGymã¯ã€å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’åŠ¹ç‡åŒ–ã™ã‚‹ãŸã‚ã®çµ±ä¸€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚Šã€é«˜ã‚³ã‚¹ãƒˆã®ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚„ä¸å®‰å®šãªå ±é…¬ä¿¡å·ã®èª²é¡Œã«å¯¾å‡¦ã—ã¾ã™ã€‚ç’°å¢ƒã®ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’æ¨è«–ã«åŸºã¥ãçµŒé¨“ãƒ¢ãƒ‡ãƒ«ã«è’¸ç•™ã—ã€å®‰å®šã—ãŸçŠ¶æ…‹é·ç§»ã¨ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã—ã¾ã™ã€‚ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ãŸçµŒé¨“ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡ã«ã‚ˆã‚Šã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å¼·åŒ–ã—ã€æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’é©å¿œçš„ã«ç”Ÿæˆã™ã‚‹ã“ã¨ã§ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ å­¦ç¿’ã‚’å®Ÿç¾ã—ã¾ã™ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€DreamGymã¯åˆæˆè¨­å®šã¨ãƒªã‚¢ãƒ«ãªã‚·ãƒŠãƒªã‚ªã§RLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å¤§å¹…ã«æ”¹å–„ã—ã€éRLæº–å‚™ã‚¿ã‚¹ã‚¯ã§ã¯30ï¼…ä»¥ä¸Šã®æ€§èƒ½å‘ä¸Šã‚’ç¤ºã—ã¾ã—ãŸã€‚åˆæˆçµŒé¨“ã®ã¿ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒãƒªã‚·ãƒ¼ã¯ã€å®Ÿç’°å¢ƒRLã«ãŠã„ã¦ã‚‚å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªã‚¦ã‚©ãƒ¼ãƒ ã‚¹ã‚¿ãƒ¼ãƒˆæˆ¦ç•¥ã‚’æä¾›ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jaseweston/status/1986613046047846569?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-11-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3615" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence  Generation, Alexandre PichÃ©+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ã‚’ç”¨ã„ã¦å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®æ¨è«–èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã€PipelineRLã‚’ææ¡ˆã€‚PipelineRLã¯éåŒæœŸãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã¨ãƒ¢ãƒ‡ãƒ«æ›´æ–°ã‚’åŒæ™‚ã«è¡Œã„ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®æ–°é®®ã•ã‚’ä¿ã¡ãªãŒã‚‰ã€GPUã®åˆ©ç”¨ç‡ã‚’æœ€å¤§åŒ–ã€‚å®Ÿé¨“ã§ã¯ã€å¾“æ¥ã®RLæ‰‹æ³•ã«æ¯”ã¹ã¦ç´„2å€ã®å­¦ç¿’é€Ÿåº¦ã‚’é”æˆã€‚PipelineRLã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹å®Ÿè£…ã‚‚å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/finbarrtimbers/status/1986481494823739581?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/DPO.html" target="_blank" rel="noopener noreferrer">#DPO</a>
<a class="button" href="articles/Cultural.html" target="_blank" rel="noopener noreferrer">#Cultural</a>
<span class="issue_date">Issue Date: 2025-11-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3605" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Culture Cartography: Mapping the Landscape of Cultural Knowledge, Caleb Ziems+, EMNLP'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- LLMã¯æ–‡åŒ–ç‰¹æœ‰ã®çŸ¥è­˜ã‚’å¿…è¦ã¨ã—ã€CultureCartographyã¨ã„ã†æ··åˆã‚¤ãƒ‹ã‚·ã‚¢ãƒ†ã‚£ãƒ–ã‚’ææ¡ˆã€‚LLMãŒè‡ªä¿¡ã®ä½ã„è³ªå•ã‚’ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã—ã€äººé–“ãŒãã®ã‚®ãƒ£ãƒƒãƒ—ã‚’åŸ‹ã‚ã‚‹ã“ã¨ã§é‡è¦ãªãƒˆãƒ”ãƒƒã‚¯ã«å°ãã€‚CultureExplorerãƒ„ãƒ¼ãƒ«ã‚’ç”¨ã„ãŸå®Ÿé¨“ã§ã€å¾“æ¥ã®ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚åŠ¹æœçš„ã«çŸ¥è­˜ã‚’ç”Ÿæˆã—ã€Llama-3.1-8Bã®ç²¾åº¦ã‚’æœ€å¤§19.2%å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/diyi_yang/status/1985826293053866234?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>åŠ¹ç‡çš„ã«LLMã«ã¨ã£ã¦æœªçŸ¥ã€ã‹ã¤é‡è¦ãªæ–‡åŒ–çš„ãªçŸ¥è­˜ãƒãƒ³ã‚¯ã‚’ä½œæˆã™ã‚‹è©±ãªæ¨¡æ§˜ã€‚ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä¼¼ãŸã‚ˆã†ãªæ€æƒ³ã«è¦‹ãˆã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<span class="issue_date">Issue Date: 2025-11-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3576" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Opportunistic Expert Activation: Batch-Aware Expert Routing for Faster  Decode Without Retraining, Costin-Andrei Oncescu+, arXiv'25, 2025.11</a>
<span class="snippet"><span>GPT Summary</span>- MoEã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç”¨ã„ãŸLLMã®ãƒ‡ã‚³ãƒ¼ãƒ‰ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’ä½ä¸‹ã•ã›ã‚‹ãŸã‚ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰å°‚é–€å®¶ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’å‹•çš„ã«å†ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã€‚ãƒãƒƒãƒèªè­˜ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’æ´»ç”¨ã—ã€ãƒ¡ãƒ¢ãƒªã«æ—¢ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹å°‚é–€å®¶ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€ç²¾åº¦ã‚’ç¶­æŒã—ã¤ã¤ã€Qwen3-30BãŠã‚ˆã³Qwen3-235Bãƒ¢ãƒ‡ãƒ«ã§ãã‚Œãã‚Œ39%ã¨15%ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å‰Šæ¸›ã‚’é”æˆã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1985955439029227927?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Stability.html" target="_blank" rel="noopener noreferrer">#Stability</a>
<a class="button" href="articles/Reference%20Collection.html" target="_blank" rel="noopener noreferrer">#Reference Collection</a>
<a class="button" href="articles/train-inference-gap.html" target="_blank" rel="noopener noreferrer">#train-inference-gap</a>
<span class="issue_date">Issue Date: 2025-11-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3532" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Defeating the Training-Inference Mismatch via FP16, Penghui Qi+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã‘ã‚‹ä¸å®‰å®šæ€§ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒãƒªã‚·ãƒ¼ã¨æ¨è«–ãƒãƒªã‚·ãƒ¼ã®æ•°å€¤çš„ä¸ä¸€è‡´ã«èµ·å› ã™ã‚‹ã€‚å¾“æ¥ã®å¯¾ç­–ã¯åŠ¹æœãŒè–„ã‹ã£ãŸãŒã€æœ¬ç ”ç©¶ã§ã¯FP16ã«æˆ»ã™ã“ã¨ã§ã“ã®å•é¡Œã‚’è§£æ±ºã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚ã“ã®å¤‰æ›´ã¯ç°¡å˜ã§ã€ãƒ¢ãƒ‡ãƒ«ã‚„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ä¿®æ­£ã‚’å¿…è¦ã¨ã›ãšã€å®‰å®šã—ãŸæœ€é©åŒ–ã¨é€Ÿã„åæŸã‚’å®Ÿç¾ã—ã€å¤šæ§˜ãªã‚¿ã‚¹ã‚¯ã§å¼·åŠ›ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/mavenlin/status/1984130875307782257?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>RLå­¦ç¿’æ™‚ã®æµ®å‹•å°æ•°ç‚¹æ•°è¡¨ç¾ã‚’bf16ã‹ã‚‰fp16ã«å¤‰æ›´ã™ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªå¤‰æ›´ã§ã€è¨“ç·´-æ¨è«–æ™‚ã®gapãŒå°ã•ããªã‚Šå­¦ç¿’ãŒæ”¹å–„ã™ã‚‹ã€ã¨ã„ã†è©±ã‚‰ã—ã„ã€‚</p>
<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dimitrispapail/status/1984286681022050373?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenhuchen/status/1984470768688759000?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1984395743696994736?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1984750121255313462?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>verlã¯FP16ã§ã®å­¦ç¿’ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ã®ã§è‘—è€…ãŒãƒ‘ãƒƒãƒã‚’å‡ºã—ãŸæ¨¡æ§˜:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/qphutu/status/1984268030558519737?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="articles/Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<a class="button" href="articles/Inference.html" target="_blank" rel="noopener noreferrer">#Inference</a>
<a class="button" href="articles/Entropy.html" target="_blank" rel="noopener noreferrer">#Entropy</a>
<span class="issue_date">Issue Date: 2025-10-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3507" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM  Reasoning, Aman Sharma+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã«åŸºã¥ãæ–°ã—ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€æ¨è«–ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡ã‚’å‘ä¸Šã€‚ã‚·ãƒ£ãƒãƒ³ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’ä¿¡é ¼åº¦ä¿¡å·ã¨ã—ã¦åˆ©ç”¨ã—ã€æ—©æœŸåœæ­¢ã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã§ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’25-50%å‰Šæ¸›ã€‚ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ç•°ãªã‚‹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼é–¾å€¤ã‚’ç”¨ã„ã¦ã€æ­£ã—ã„ç­”ãˆã‚’æ—©æœŸã«å¾—ã‚‹ã“ã¨ã‚’èªè­˜ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³ç¯€ç´„ã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·å‰Šæ¸›ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚ç²¾åº¦ã‚’ç¶­æŒã—ã¤ã¤ä¸€è²«ã—ãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã€ç¾ä»£ã®æ¨è«–ã‚·ã‚¹ãƒ†ãƒ ã®ç‰¹å¾´ã‚’æ˜ã‚‰ã‹ã«ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/lossfunk/status/1983509644355268908?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒ‡ã‚³ãƒ¼ãƒ‰æ™‚ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã«å¿œã˜ã¦ã€reasoningã‚’æ‰“ã¡åˆ‡ã‚‹ã‹å¦ã‹åˆ¤å®šã—ã¦ã‚³ã‚¹ãƒˆå‰Šæ¸›ã—ã¤ã¤æ¨è«–ã™ã‚‹è©±ãªæ¨¡æ§˜</p>
<p>vLLMã¨ã‹ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆä¸ŠãŒã£ãŸã‚‰å¬‰ã—ã„ãªã‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Personalization.html" target="_blank" rel="noopener noreferrer">#Personalization</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-10-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3505" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Memory-Efficient Backpropagation for Fine-Tuning LLMs on  Resource-Constrained Mobile Devices, Congzheng Song+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ¢ãƒã‚¤ãƒ«ãƒ‡ãƒã‚¤ã‚¹å‘ã‘ã«ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®è‰¯ã„ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè£…ï¼ˆMeBPï¼‰ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨è¨ˆç®—æ™‚é–“ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æ”¹å–„ã—ã€ã‚¼ãƒ­æ¬¡æœ€é©åŒ–ã‚ˆã‚Šã‚‚é€ŸãåæŸã—ã€å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®Ÿç¾ã€‚iPhone 15 Pro Maxã§ã®æ¤œè¨¼ã«ã‚ˆã‚Šã€0.5Bã‹ã‚‰4Bã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤LLMãŒ1GBæœªæº€ã®ãƒ¡ãƒ¢ãƒªã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚å®Ÿè£…ä¾‹ã¯å…¬é–‹æ¸ˆã¿ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1983377112208986329?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>iPhoneä¸Šã§4Bãƒ¢ãƒ‡ãƒ«ã¾ã§FinetuningãŒã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸæ¨¡æ§˜ã€‚</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/NormalizingFlow.html" target="_blank" rel="noopener noreferrer">#NormalizingFlow</a>
<a class="button" href="articles/Compression.html" target="_blank" rel="noopener noreferrer">#Compression</a>
<span class="issue_date">Issue Date: 2025-10-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3488" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] FARMER: Flow AutoRegressive Transformer over Pixels, Guangting Zheng+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- FARMERã¨ã„ã†æ–°ã—ã„ç”Ÿæˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€æ­£è¦åŒ–ãƒ•ãƒ­ãƒ¼ã¨è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ã¦é«˜å“è³ªãªç”»åƒåˆæˆã¨å°¤åº¦æ¨å®šã‚’å®Ÿç¾ã€‚æ½œåœ¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¸ã®å¤‰æ›ã‚„è‡ªå·±æ•™å¸«ã‚ã‚Šæ¬¡å…ƒå‰Šæ¸›ã«ã‚ˆã‚Šã€ARãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®åŠ¹ç‡ã‚’å‘ä¸Šã€‚æ¨è«–é€Ÿåº¦ã‚’åŠ é€Ÿã™ã‚‹è’¸ç•™ã‚¹ã‚­ãƒ¼ãƒ ã¨ç”»åƒç”Ÿæˆå“è³ªã‚’å‘ä¸Šã•ã›ã‚‹åˆ†é¡å™¨ãƒ•ãƒªãƒ¼ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚’å°å…¥ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€FARMERã¯æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦ç«¶äº‰åŠ›ã®ã‚ã‚‹æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1983015650139222334?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1983034143580795131?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã“ã‚Œã¯...ğŸ‘€ğŸ‘€ğŸ‘€</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<span class="issue_date">Issue Date: 2025-10-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3458" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per  Token via Reinforcement Learning, Shih-Yang Liu+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æ¨è«–è¨€èªãƒ¢ãƒ‡ãƒ«ã¯é•·ã„å‡ºåŠ›ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒå¤šãã€å¿œç­”ã®é•·ã•ã«å¯¾ã™ã‚‹ç²¾åº¦å‘ä¸ŠãŒèª²é¡Œã§ã‚ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€åˆ‡ã‚Šæ¨ã¦ã‚’ç”¨ã„ãŸå¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ã®å†è€ƒã‚’è¡Œã„ã€ç²¾åº¦ä½ä¸‹ã®åŸå› ã¯ä¸ååˆ†ãªRLæœ€é©åŒ–ã«ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚3ã¤ã®èª²é¡Œï¼ˆãƒã‚¤ã‚¢ã‚¹ã€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®å´©å£Šã€ã‚¹ãƒ‘ãƒ¼ã‚¹ãªå ±é…¬ä¿¡å·ï¼‰ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€DLERã¨ã„ã†ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã‚’ææ¡ˆã—ã€å‡ºåŠ›ã®é•·ã•ã‚’70ï¼…ä»¥ä¸Šå‰Šæ¸›ã—ã¤ã¤ç²¾åº¦ã‚’å‘ä¸Šã•ã›ãŸã€‚ã•ã‚‰ã«ã€Difficulty-Aware DLERã‚’å°å…¥ã—ã€ç°¡å˜ãªè³ªå•ã«å¯¾ã—ã¦é©å¿œçš„ã«åˆ‡ã‚Šæ¨ã¦ã‚’å³ã—ãã™ã‚‹ã“ã¨ã§åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã‚‚ææ¡ˆã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://nvlabs.github.io/DLER/" target="_blank" rel="noopener noreferrer">https://nvlabs.github.io/DLER/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/simonxindong/status/1982217071728435519?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>reasoningã‚’ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®è¦³ç‚¹ã§åŠ¹ç‡åŒ–ã™ã‚‹è©±</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<a class="button" href="articles/mid-training.html" target="_blank" rel="noopener noreferrer">#mid-training</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/Parallelism.html" target="_blank" rel="noopener noreferrer">#Parallelism</a>
<span class="issue_date">Issue Date: 2025-10-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3425" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for  Scalable LLM Training, Huawei Bai+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- éåŒæœŸéšå±¤ã‚¼ãƒ­ä¸¦åˆ—å‡¦ç†ï¼ˆAsyncHZPï¼‰ã‚’ææ¡ˆã—ã€ã‚·ãƒ³ãƒ—ãƒ«ã•ã¨ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’ä¿ã¡ãªãŒã‚‰ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚’å‘ä¸Šã€‚å¾“æ¥ã®ZeROã®é€šä¿¡ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å‰Šæ¸›ã—ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚„å‹¾é…ã®å†ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’é©å¿œçš„ã«è¡Œã†ã€‚ãƒãƒ«ãƒã‚¹ãƒˆãƒªãƒ¼ãƒ éåŒæœŸã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã«ã‚ˆã‚Šé€šä¿¡ã¨è¨ˆç®—ã‚’é‡ã­åˆã‚ã›ã€ãƒ¡ãƒ¢ãƒªã®æ–­ç‰‡åŒ–ã‚’æœ€å°é™ã«æŠ‘ãˆã‚‹ã€‚DenseãŠã‚ˆã³Mixture-of-Expertsãƒ¢ãƒ‡ãƒ«ã§ã®è©•ä¾¡ã«ã‚ˆã‚Šã€AsyncHZPãŒå¾“æ¥ã®NDä¸¦åˆ—å‡¦ç†ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1981756482128671166?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/Hybrid.html" target="_blank" rel="noopener noreferrer">#Hybrid</a>
<span class="issue_date">Issue Date: 2025-10-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3413" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Every Attention Matters: An Efficient Hybrid Architecture for  Long-Context Reasoning, Ling Team+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- Ring-linearãƒ¢ãƒ‡ãƒ«ã‚·ãƒªãƒ¼ã‚ºã€ç‰¹ã«Ring-mini-linear-2.0ï¼ˆ16Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã¨Ring-flash-linear-2.0ï¼ˆ104Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã‚’ç´¹ä»‹ã€‚ä¸¡ãƒ¢ãƒ‡ãƒ«ã¯ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¡ç”¨ã—ã€é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ¨è«–ã§I/Oã¨è¨ˆç®—ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å‰Šæ¸›ã€‚æ¨è«–ã‚³ã‚¹ãƒˆã¯32å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¯†ãªãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦1/10ã€å…ƒã®Ringã‚·ãƒªãƒ¼ã‚ºã¨æ¯”ã¹ã¦50%ä»¥ä¸Šå‰Šæ¸›ã€‚æœ€é©ãªãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚’ç‰¹å®šã—ã€é«˜æ€§èƒ½FP8ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€Œlingheã€ã«ã‚ˆã‚Šãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ãŒ50%å‘ä¸Šã€‚è¤‡æ•°ã®è¤‡é›‘æ¨è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§SOTAãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã€‚</span>
<span class="snippet"><span>Comment</span><p>HF:


<a href="https://huggingface.co/inclusionAI/Ring-flash-linear-2.0-128k" target="_blank" rel="noopener noreferrer">https://huggingface.co/inclusionAI/Ring-flash-linear-2.0-128k</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1981442875192987882?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/grad62304977/status/1981571978382500203?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Pixel-based.html" target="_blank" rel="noopener noreferrer">#Pixel-based</a>
<span class="issue_date">Issue Date: 2025-10-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3375" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text  Inputs in Multimodal LLMs, Yanhong Li+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”»åƒã¨ã—ã¦æä¾›ã™ã‚‹ã“ã¨ã§ã€LLMã®ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’å‰Šæ¸›ã—ã¤ã¤æ€§èƒ½ã‚’ç¶­æŒã§ãã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚é•·ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”»åƒã«ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã—ã€ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã«ç›´æ¥å…¥åŠ›ã™ã‚‹ã“ã¨ã§ã€å¿…è¦ãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å¤§å¹…ã«æ¸›å°‘ã•ã›ã‚‹ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€RULERã¨CNN/DailyMailã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æ€§èƒ½ã‚’æãªã†ã“ã¨ãªãã€ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¯€ç´„ãŒå®Ÿç¾ã§ãã‚‹ã“ã¨ã‚’ç¢ºèªã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1980942325573648703?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<a class="button" href="articles/AutomaticPromptOptimization.html" target="_blank" rel="noopener noreferrer">#AutomaticPromptOptimization</a>
<span class="issue_date">Issue Date: 2025-10-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3362" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Prompt-MII: Meta-Learning Instruction Induction for LLMs, Emily Xiao+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- PROMPT-MIIã¨ã„ã†æ–°ã—ã„æŒ‡ç¤ºèª˜å°ãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¾‹ã‚’ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ç¸®å°ã™ã‚‹ã“ã¨ã§ã€ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ï¼ˆICLï¼‰ã¨åŒç­‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®Ÿç¾ã€‚3,000ä»¥ä¸Šã®åˆ†é¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€90ã®æœªè¦‹ã‚¿ã‚¹ã‚¯ã§è©•ä¾¡ã—ãŸçµæœã€ä¸‹æµãƒ¢ãƒ‡ãƒ«ã®å“è³ªã‚’4-9 F1ãƒã‚¤ãƒ³ãƒˆå‘ä¸Šã•ã›ã€å¿…è¦ãªãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’3-13å€å‰Šæ¸›ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gneubig/status/1980644772902789603?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã‚¿ã‚¹ã‚¯ã®examplar/demonstrationã‹ã‚‰ã‚¿ã‚¹ã‚¯ã«é–¢ã™ã‚‹descriptionï¼‰ï¼instruction)ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€ç”Ÿæˆã•ã‚ŒãŸinstructionã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€manyshotã§ICLã™ã‚‹ã‚ˆã‚Šã‚‚ã€å°‘ãªã„ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’é”æˆã™ã‚‹ã¨ã„ã£ãŸè©±ã«è¦‹ãˆã‚‹ã€‚ã©ã†ã„ã†instructionã«ãªã‚‹ã®ã‹ãŒéå¸¸ã«èˆˆå‘³ãŒã‚ã‚‹ã€‚A.6å‚ç…§ã®ã“ã¨ã€‚ç´°ã‹ãå…·ä½“çš„ã ãŒã‚³ãƒ³ãƒ‘ã‚¯ãƒˆãªæŒ‡ç¤ºãŒè¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ãªinstructionã¨ãªã£ã¦ã„ã‚‹ã€‚<br><br><img src="https://github.com/user-attachments/assets/ee02024a-725d-47a1-9b8c-46fed1dbcb8f" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/363dd369-79d3-4b37-b4de-f880e2fb746d" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ContextWindow.html" target="_blank" rel="noopener noreferrer">#ContextWindow</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-10-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3357" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Glyph: Scaling Context Windows via Visual-Text Compression, Jiale Cheng+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒã¤å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®å®Ÿç”¨æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã€Glyphã¨ã„ã†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”»åƒã«å¤‰æ›ã—ã¦è¦–è¦šã¨è¨€èªã®ãƒ¢ãƒ‡ãƒ«ï¼ˆVLMsï¼‰ã§å‡¦ç†ã—ã¾ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚Šã€3-4å€ã®ãƒˆãƒ¼ã‚¯ãƒ³åœ§ç¸®ã‚’å®Ÿç¾ã—ã€ç²¾åº¦ã‚’ç¶­æŒã—ã¤ã¤å‡¦ç†é€Ÿåº¦ã‚’ç´„4å€å‘ä¸Šã•ã›ã¾ã™ã€‚ã•ã‚‰ã«ã€128Kã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®VLMãŒ1Mãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ†ã‚­ã‚¹ãƒˆã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†å¯èƒ½ã«ãªã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1980474912168112471?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/arankomatsuzaki/status/1980722682246398069?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”»åƒã«ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã—ã¦VLMã«å…¥åŠ›ã™ã‚‹ã“ã¨ã§textã¨æ¯”è¼ƒã—ã¦3.2å€KV Cache (context)ã‚’åœ§ç¸®ã—ã€prefillingã¨ãƒ‡ã‚³ãƒ¼ãƒ‰é€Ÿåº¦ã‚‚4.8, 4.4å€é«˜é€ŸåŒ–ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚‰ã—ã„<br><br><img src="https://github.com/user-attachments/assets/e65f880d-0d04-434f-9a51-accc84d44a6f" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="articles/Diversity.html" target="_blank" rel="noopener noreferrer">#Diversity</a>
<a class="button" href="articles/Verification.html" target="_blank" rel="noopener noreferrer">#Verification</a>
<a class="button" href="articles/DeepResearch.html" target="_blank" rel="noopener noreferrer">#DeepResearch</a>
<a class="button" href="articles/LongHorizon.html" target="_blank" rel="noopener noreferrer">#LongHorizon</a>
<span class="issue_date">Issue Date: 2025-10-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3348" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Synthesizing Agentic Data for Web Agents with Progressive Difficulty  Enhancement Mechanisms, Shrey Pandit+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- Webãƒ™ãƒ¼ã‚¹ã®ã€Œãƒ‡ã‚£ãƒ¼ãƒ—ãƒªã‚µãƒ¼ãƒã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€é•·æœŸçš„ãªã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’é€šã˜ã¦è¤‡é›‘ãªè³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯ã‚’è§£æ±ºã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ãŒã€å¾“æ¥ã®æ–¹æ³•ã¯æ¨è«–ã®è¤‡é›‘ã•ã‚’æ‰ãˆãã‚Œãªã„ã€‚ãã“ã§ã€ã‚¿ã‚¹ã‚¯ã®è¤‡é›‘ã•ã‚’æ®µéšçš„ã«å¢—åŠ ã•ã›ã‚‹äºŒæ®µéšã®ãƒ‡ãƒ¼ã‚¿åˆæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å°å…¥ã—ã€ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒè³ªå•ã«æŒ‘æˆ¦ã—ã€äº‹å®Ÿç¢ºèªã‚’è¡Œã†ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ææ¡ˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒæ—¢å­˜ã®ã‚‚ã®ã‚ˆã‚Šã‚‚åŠ¹æœçš„ãªè¨“ç·´ã‚’å¯èƒ½ã«ã—ã€ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å¤šæ§˜æ€§ãŒ2å€ã§ã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/caimingxiong/status/1980302240163488057?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/3D%20(Scene).html" target="_blank" rel="noopener noreferrer">#3D (Scene)</a>
<a class="button" href="articles/Robotics.html" target="_blank" rel="noopener noreferrer">#Robotics</a>
<a class="button" href="articles/VisionLanguageActionModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageActionModel</a>
<a class="button" href="articles/SpatialUnderstanding.html" target="_blank" rel="noopener noreferrer">#SpatialUnderstanding</a>
<span class="issue_date">Issue Date: 2025-10-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3328" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Spatial Forcing: Implicit Spatial Representation Alignment for  Vision-language-action Model, Fuhao Li+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- Spatial Forcing (SF)ã¨ã„ã†æ–°ã—ã„æ•´åˆæˆ¦ç•¥ã‚’ææ¡ˆã—ã€VLAãƒ¢ãƒ‡ãƒ«ãŒ3Dç©ºé–“ç†è§£èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ä¿ƒé€²ã€‚SFã¯3Då…¥åŠ›ã‚„æ·±åº¦æ¨å®šå™¨ã«ä¾å­˜ã›ãšã€VLAã®ä¸­é–“è¦–è¦šåŸ‹ã‚è¾¼ã¿ã‚’3DåŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®å¹¾ä½•å­¦çš„è¡¨ç¾ã¨æ•´åˆã•ã›ã‚‹ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€SFã¯æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æœ€å¤§3.8å€åŠ é€Ÿã€ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã‚’æ”¹å–„ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1979911820401086613?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<a class="button" href="articles/KV%20Cache.html" target="_blank" rel="noopener noreferrer">#KV Cache</a>
<span class="issue_date">Issue Date: 2025-10-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3319" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Attention Is All You Need for KV Cache in Diffusion LLMs, Quan Nguyen-Tri+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€æ‹¡æ•£å‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆDLMsï¼‰ã®ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å¾…æ©Ÿæ™‚é–“ã‚’æœ€å°åŒ–ã—ã¤ã¤äºˆæ¸¬ç²¾åº¦ã‚’æœ€å¤§åŒ–ã™ã‚‹ãŸã‚ã«ã€é©å¿œçš„ãªKVã‚­ãƒ£ãƒƒã‚·ãƒ¥å†è¨ˆç®—æ‰‹æ³•ã€ŒElastic-Cacheã€ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æµ…ã„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å†—é•·æ€§ã‚’å‰Šæ¸›ã—ã€é‡è¦ãªãƒˆãƒ¼ã‚¯ãƒ³ã«åŸºã¥ã„ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ã‚’å‹•çš„ã«è¡Œã†ã€‚å®Ÿé¨“ã§ã¯ã€GSM8Kã‚„HumanEvalã§ã®é€Ÿåº¦å‘ä¸Šã‚’ç¤ºã—ã€ç”Ÿæˆå“è³ªã‚’ç¶­æŒã—ãªãŒã‚‰é«˜ã„ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’é”æˆã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/1979180865520570615?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>DLMã«ãŠã„ã¦ã€denoisingã®å„ã‚¹ãƒ†ãƒƒãƒ—ã«ãŠã„ã¦å…¨ã¦ã®KVã‚’å†è¨ˆç®—ã™ã‚‹ã®ã§ã¯ãªãã€attention scoreãŒå¤§ãããƒ‰ãƒªãƒ•ãƒˆã—ã¦ã„ãªã„éƒ¨åˆ†ã«ã¤ã„ã¦ã¯KV Cacheã‚’å†åˆ©ç”¨ã—ã€å¤§ãããƒ‰ãƒªãƒ•ãƒˆã—ãŸéƒ¨åˆ†ã ã‘å†è¨ˆç®—ã™ã‚‹ã‚ˆã†ãªä»•çµ„ã¿ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€å“è³ªã‚’æãªã†ã“ã¨ãªãæ¨è«–é€Ÿåº¦ã‚’é«˜é€ŸåŒ–ã—ãŸæ¨¡æ§˜</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/DynamicNetworks.html" target="_blank" rel="noopener noreferrer">#DynamicNetworks</a>
<a class="button" href="articles/Routing.html" target="_blank" rel="noopener noreferrer">#Routing</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-10-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3298" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Dr.LLM: Dynamic Layer Routing in LLMs, Ahmed Heakl+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- Dr.LLMã¯ã€LLMsã«å‹•çš„ãªå±¤ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’å°å…¥ã—ã€è¨ˆç®—åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€‚ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æœ¨æ¢ç´¢ã‚’ç”¨ã„ã¦é«˜å“è³ªãªå±¤æ§‹æˆã‚’å°å‡ºã—ã€ARCã‚„DARTã§ç²¾åº¦ã‚’æœ€å¤§+3.4%å‘ä¸Šã•ã›ã€å¹³å‡5å±¤ã‚’ç¯€ç´„ã€‚ãƒ‰ãƒ¡ã‚¤ãƒ³å¤–ã‚¿ã‚¹ã‚¯ã§ã‚‚ã‚ãšã‹0.85%ã®ç²¾åº¦ä½ä¸‹ã§å¾“æ¥æ‰‹æ³•ã‚’ä¸Šå›ã‚‹ã€‚æ˜ç¤ºçš„ãªç›£è¦–ä¸‹ã§ã®ãƒ«ãƒ¼ã‚¿ãƒ¼ãŒLLMsã‚’åŠ¹ç‡çš„ã«æ´»ç”¨ã§ãã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>Layerã”ã¨ã«MLPã®routerã‚’ç”¨æ„ã—ã€ï¼ˆå…ƒã®LLMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯freezeã—ã¦ï¼‰Layerã‚’skip, execute, repeatã™ã‚‹ã‹ã‚’è¿½åŠ ã§å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€ã‚¯ã‚¨ãƒªã«å¿œã˜ã¦å‹•çš„ã«è¨ˆç®—ã‚³ã‚¹ãƒˆã¨pathã‚’èª¿æ•´ã™ã‚‹èƒ½åŠ›ã‚’èº«ã«ã¤ã‘ã•ã›ã€æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã¤ã¤ã‚‚è¨ˆç®—é‡ã‚‚å‰Šæ¸›ã§ãã¾ã™ã€ã¨ã„ã£ãŸè©±ãªæ¨¡æ§˜ã€‚routerãŒå­¦ç¿’ã•ã‚Œã¦ã„ã‚‹ã®ã§inferenceæ™‚ã«searchã¯ä¸è¦ã€‚</p></span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Test-Time%20Scaling.html" target="_blank" rel="noopener noreferrer">#Test-Time Scaling</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/Diversity.html" target="_blank" rel="noopener noreferrer">#Diversity</a>
<span class="issue_date">Issue Date: 2025-10-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3280" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Representation-Based Exploration for Language Models: From Test-Time to  Post-Training, Jens Tuyls+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ãŒè¨€èªãƒ¢ãƒ‡ãƒ«ã®è¡Œå‹•ç™ºè¦‹ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¿æŸ»ã€‚äº‹å‰å­¦ç¿’ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®éš ã‚ŒçŠ¶æ…‹ã‚’åŸºã«ã—ãŸè¡¨ç¾ãƒ™ãƒ¼ã‚¹ã®ãƒœãƒ¼ãƒŠã‚¹ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€å¤šæ§˜æ€§ã¨pass@kç‡ãŒå¤§å¹…ã«æ”¹å–„ã•ã‚Œã‚‹ã“ã¨ã‚’ç™ºè¦‹ã€‚æ¨è«–æ™‚ã«ãŠã‘ã‚‹æ¢ç´¢ãŒåŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã€ãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦ã‚‚RLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ã®çµ±åˆã«ã‚ˆã‚Šæ€§èƒ½ãŒå‘ä¸Šã€‚æ„å›³çš„ãªæ¢ç´¢ãŒæ–°ã—ã„è¡Œå‹•ã®ç™ºè¦‹ã«å¯„ä¸ã™ã‚‹å¯èƒ½æ€§ã‚’ç¤ºå”†ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/canondetortugas/status/1978245046366319048?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ¢ç´¢ã®å¤šæ§˜æ€§ã‚’ã‚ã’ã¦RLã“å­¦ç¿’åŠ¹ç‡ã€test time scalingã®åŠ¹ç‡ã‚’ä¸Šã’ã‚‹ã¨ã„ã†è©±</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<span class="issue_date">Issue Date: 2025-10-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3277" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Expert-as-a-Service: Towards Efficient, Scalable, and Robust Large-scale  MoE Serving, Ziming Liu+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- EaaSã¨ã„ã†æ–°ã—ã„ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’ææ¡ˆã—ã€Mixture-of-Experts (MoE)ãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡çš„ã§ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªå±•é–‹ã‚’å®Ÿç¾ã€‚MoEãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ç‹¬ç«‹ã—ãŸã‚¹ãƒ†ãƒ¼ãƒˆãƒ¬ã‚¹ã‚µãƒ¼ãƒ“ã‚¹ã«åˆ†è§£ã—ã€ãƒªã‚½ãƒ¼ã‚¹ã®ç´°ã‹ã„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ã‚¹ã‚’æä¾›ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€EaaSã¯ãƒ¢ãƒãƒªã‚·ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ã¨åŒç­‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã—ã¤ã¤ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®æ¸›å°‘ã‚’2%æœªæº€ã«æŠ‘ãˆã€æœ€å¤§37.5%ã®è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’ç¯€ç´„ã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1978286917159624888?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/AttentionSinks.html" target="_blank" rel="noopener noreferrer">#AttentionSinks</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-10-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3270" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] StreamingVLM: Real-Time Understanding for Infinite Video Streams, Ruyi Xu+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- StreamingVLMã¯ã€ç„¡é™ã®ãƒ“ãƒ‡ã‚ªã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ç†è§£ã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã‚’çµ±ä¸€ã—ãŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æ¡ç”¨ã€‚ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚·ãƒ³ã‚¯ã®çŠ¶æ…‹ã‚’å†åˆ©ç”¨ã—ã€çŸ­ã„ãƒ“ã‚¸ãƒ§ãƒ³ãƒˆãƒ¼ã‚¯ãƒ³ã¨é•·ã„ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’ä¿æŒã™ã‚‹ã“ã¨ã§ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’æŠ‘ãˆã¤ã¤é«˜ã„æ€§èƒ½ã‚’å®Ÿç¾ã€‚æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯Inf-Streams-Evalã§66.18%ã®å‹ç‡ã‚’é”æˆã—ã€ä¸€èˆ¬çš„ãªVQAèƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã«æˆåŠŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/teortaxestex/status/1978324546370343088?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã“ã‚Œã¯èˆˆå‘³æ·±ã„</p>
<p>ä¿æŒã™ã‚‹KV Cacheã®ä¸Šé™ã‚’æ±ºã‚ã€Sink Token[^1]ã¯ä¿æŒã—[^2]ï¼ˆ512ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã€textual tokenã¯é•·è·é›¢ã§ä¿æŒã€visual tokenã¯çŸ­è·é›¢ã§ä¿æŒã€ã¾ãŸpositional encodingã¨ã—ã¦ã¯RoPEã‚’æ¡ç”¨ã™ã‚‹ãŒã€å›ºå®šã•ã‚ŒãŸãƒ¬ãƒ³ã‚¸ã®ä¸­ã§å‹•çš„ã«indexã‚’æ›´æ–°ã™ã‚‹ã“ã¨ã§ã€ä½ç›¸ã‚’å­¦ç¿’æ™‚ã®rangeã«åã‚OODã«ãªã‚‰ãªã„ã‚ˆã†ãªå·¥å¤«ã‚’ã™ã‚‹ã“ã¨ã§ã€memoryã¨è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’ä¸€å®šã«ä¿ã¡ãªãŒã‚‰long contextã§ã®ä¸€è²«æ€§ã¨ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®latencyã‚’å®Ÿç¾ã™ã‚‹ã€ã¨ã„ã£ãŸè©±ã«ã¿ãˆã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/4d063c90-e10a-4d07-9095-f87ee85c33fb" alt="image" loading="lazy"><br><br>å­¦ç¿’æ™‚ã¯ãƒ•ãƒ¬ãƒ¼ãƒ ãŒoverlapã—ãŸè¤‡æ•°ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†ã‘ã¦ã€ãã‚Œãã‚Œã‚’full attentionã§å­¦ç¿’ã™ã‚‹ï¼ˆSink Tokenã¯ä¿æŒã™ã‚‹ï¼‰ã€‚ã“ã‚Œã¯ä¸Šè¿°ã®inferenceæ™‚ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨æ•´åˆã—ã¦ãŠã‚Šå­¦ç¿’æ™‚ã¨inferenceæ™‚ã®gapãŒæœ€å°é™ã«ãªã‚‹ã€‚ã¾ãŸã€ã‚ã–ã‚ã–long videoã§å­¦ç¿’ã™ã‚‹å¿…è¦ãŒãªã„ã€‚ï¼ˆç¾ã—ã„è§£æ±ºæ–¹æ³•ï¼‰<br><img src="https://github.com/user-attachments/assets/98b50d1b-b9c4-427a-93f5-d385b2bc35a1" alt="image" loading="lazy"><br><br>[^1]: decoder-only transformerã®ä½™å‰°ãªattention scoreã®æ¨ã¦å ´ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹sequenceå†’é ­ã®æ•°ãƒˆãƒ¼ã‚¯ãƒ³(3--4ãƒˆãƒ¼ã‚¯ãƒ³ç¨‹åº¦ï¼‰ã®ã“ã¨ã€‚æœ¬è«–æ–‡ã§ã¯512ãƒˆãƒ¼ã‚¯ãƒ³ã¨å¤§ãã‚ã®Sink Tokenã‚’ä¿æŒã—ã¦ã„ã‚‹ã€‚<br>[^2]: Attention Sinksã«ã‚ˆã£ã¦ã€long contextã®æ€§èƒ½ãŒæ”¹å–„ã•ã‚Œ <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1860" target="_blank" rel="noopener noreferrer">Why do LLMs attend to the first token?, Federico Barbero+, COLM'25</a>
 decoder-only transformerã®å±¤ãŒæ·±ã„éƒ¨åˆ†ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®è¡¨ç¾ãŒå‡ä¸€åŒ–ã•ã‚Œã¦ã—ã¾ã†over-mixingã‚’æŠ‘åˆ¶ã™ã‚‹ <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1861" target="_blank" rel="noopener noreferrer">Efficient Streaming Language Models with Attention Sinks, Guangxuan Xiao+, ICLR'24</a>
 ã“ã¨ãŒå ±å‘Šã•ã‚Œã¦ã„ã‚‹</p>
<p>AttentionSinké–¢é€£ãƒªãƒ³ã‚¯:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1861" target="_blank" rel="noopener noreferrer">Efficient Streaming Language Models with Attention Sinks, Guangxuan Xiao+, ICLR'24</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1860" target="_blank" rel="noopener noreferrer">Why do LLMs attend to the first token?, Federico Barbero+, COLM'25</a>
</p>
<p>â†‘ã“ã‚Œã¯å…ƒãƒã‚¹ãƒˆã‚’èª­ã‚“ã§ï¼ˆã¨è«–æ–‡æ–œã‚èª­ã¿ï¼‰ã®æ„Ÿæƒ³ã®ã‚ˆã†ãªã‚‚ã®ãªã®ã§ã€è©³ç´°ã¯å¾Œã§å…ƒè«–æ–‡ã‚’èª­ã‚€ã€‚</p>
<p>é–¢é€£:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/yukangchen_/status/1978653384539341287?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Quantization.html" target="_blank" rel="noopener noreferrer">#Quantization</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/Entropy.html" target="_blank" rel="noopener noreferrer">#Entropy</a>
<span class="issue_date">Issue Date: 2025-10-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3259" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning  for LLMs, Wei Huang+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- QeRLã¯ã€LLMså‘ã‘ã®é‡å­åŒ–å¼·åŒ–å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã€NVFP4é‡å­åŒ–ã¨LoRAã‚’çµ„ã¿åˆã‚ã›ã¦RLã®ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚’åŠ é€Ÿã—ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã—ã¾ã™ã€‚é‡å­åŒ–ãƒã‚¤ã‚ºãŒãƒãƒªã‚·ãƒ¼ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã‚’å¢—åŠ ã•ã›ã€æ¢ç´¢ã‚’å¼·åŒ–ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã€AQNãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã§ãƒã‚¤ã‚ºã‚’å‹•çš„ã«èª¿æ•´ã—ã¾ã™ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆãƒ•ã‚§ãƒ¼ã‚ºã§1.5å€ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’é”æˆã—ã€32B LLMã®RLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å˜ä¸€ã®H100 80GB GPUã§å¯èƒ½ã«ã—ã¾ã—ãŸã€‚QeRLã¯ã€å ±é…¬ã®æˆé•·ã¨æœ€çµ‚ç²¾åº¦ã§å„ªã‚ŒãŸçµæœã‚’ç¤ºã—ã€LLMsã«ãŠã‘ã‚‹RLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®åŠ¹ç‡çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨ã—ã¦ã®åœ°ä½ã‚’ç¢ºç«‹ã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://github.com/NVlabs/QeRL" target="_blank" rel="noopener noreferrer">https://github.com/NVlabs/QeRL</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1977949560149315847?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2552" target="_blank" rel="noopener noreferrer">Your Efficient RL Framework Secretly Brings You Off-Policy RL Training, Yao+, 2025.08</a>
<br><br>ã®ã‚ˆã†ãªãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã™ã‚‹éš›ã®ã‚¨ãƒ³ã‚¸ãƒ³ã¨å­¦ç¿’ã®ã‚¨ãƒ³ã‚¸ãƒ³ã®gapã«ã‚ˆã‚‹å•é¡Œã¯ç”Ÿã˜ãŸã‚Šã—ãªã„ã®ã ã‚ã†ã‹ã€‚</p>
<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1979325188581007627?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-10-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3258" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Diffusion Transformers with Representation Autoencoders, Boyang Zheng+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€å¾“æ¥ã®VAEã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’äº‹å‰å­¦ç¿’ã•ã‚ŒãŸè¡¨ç¾ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã«ç½®ãæ›ãˆãŸRepresentation Autoencodersï¼ˆRAEï¼‰ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é«˜å“è³ªãªå†æ§‹æˆã¨è±Šã‹ãªæ½œåœ¨ç©ºé–“ã‚’å®Ÿç¾ã—ã€æ‹¡æ•£ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®æ€§èƒ½å‘ä¸Šã‚’å›³ã‚‹ã€‚RAEã¯ã€è£œåŠ©çš„ãªè¡¨ç¾æ•´åˆæå¤±ãªã—ã§æ—©ã„åæŸã‚’é”æˆã—ã€ImageNetã§å„ªã‚ŒãŸç”»åƒç”Ÿæˆçµæœã‚’ç¤ºã—ãŸã€‚RAEã¯ã€æ‹¡æ•£ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®æ–°ã—ã„ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã—ã¦ã®åˆ©ç‚¹ã‚’æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://rae-dit.github.io" target="_blank" rel="noopener noreferrer">https://rae-dit.github.io</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1978001535717216751?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>U-Netã‚’Backboneã¨ã—ãŸVAEã®ä»£ã‚ã‚Šã«ViTã«åŸºã¥ãï¼ˆdown, up- scalingç„¡ã—ã®ï¼‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šå°‘ãªã„è¨ˆç®—é‡ã§é«˜ã„æ€§èƒ½ã‚’é”æˆã—ã¾ã—ãŸã€ã¨ã„ã£ãŸè©±ã«è¦‹ãˆã‚‹ã€‚</p>
<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1977967098736549990?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1978018195953848384?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/RLVR.html" target="_blank" rel="noopener noreferrer">#RLVR</a>
<span class="issue_date">Issue Date: 2025-10-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3254" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Part II: ROLL Flash -- Accelerating RLVR and Agentic Training with  Asynchrony, Han Lu+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- éåŒæœŸRLå¾Œå‡¦ç†ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã€ŒROLL Flashã€ã‚’ææ¡ˆã€‚ç´°ç²’åº¦ã®ä¸¦åˆ—æ€§ã¨ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆãƒ»ãƒˆãƒ¬ã‚¤ãƒ³ã®ãƒ‡ã‚«ãƒƒãƒ—ãƒªãƒ³ã‚°ã«åŸºã¥ãã€åŠ¹ç‡çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å®Ÿç¾ã€‚ROLL Flashã¯ãƒªã‚½ãƒ¼ã‚¹åˆ©ç”¨åŠ¹ç‡ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’å¤§å¹…ã«æ”¹å–„ã—ã€RLVRã‚¿ã‚¹ã‚¯ã§æœ€å¤§2.24å€ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¿ã‚¹ã‚¯ã§æœ€å¤§2.72å€ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’é”æˆã€‚éåŒæœŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒåŒæœŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨åŒç­‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã™ã“ã¨ã‚’ç¢ºèªã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1977959866699513889?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>RLã®ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆä¸­ã®GPUã®ã‚¢ã‚¤ãƒ‰ãƒ«ã‚¿ã‚¤ãƒ ã‚’å‰Šæ¸›ã—ã¾ã™ç³»ã®è©±ã‚‚æœ€è¿‘çµæ§‹è¦‹ã‚‹ã‚ˆã†ãª<br>ãŸã¨ãˆã°<br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3134" target="_blank" rel="noopener noreferrer">Anatomy of a Modern Finetuning API, Benjamin Anderson, 2025.10</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-10-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3248" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] dInfer: An Efficient Inference Framework for Diffusion Language Models, Yuxin Ma+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- dLLMã®æ¨è«–ã‚’åŠ¹ç‡åŒ–ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯dInferã‚’ææ¡ˆã€‚dInferã¯4ã¤ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«åˆ†è§£ã•ã‚Œã€æ–°ã—ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨æœ€é©åŒ–ã‚’çµ±åˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å‡ºåŠ›å“è³ªã‚’ç¶­æŒã—ã¤ã¤ã€æ¨è«–é€Ÿåº¦ã‚’å¤§å¹…ã«å‘ä¸Šã€‚HumanEvalã§1ç§’ã‚ãŸã‚Š1,100ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¶…ãˆã€å¾“æ¥ã®ã‚·ã‚¹ãƒ†ãƒ ã«æ¯”ã¹ã¦10å€ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’å®Ÿç¾ã€‚dInferã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>code:


<a href="https://github.com/inclusionAI/dInfer" target="_blank" rel="noopener noreferrer">https://github.com/inclusionAI/dInfer</a>


</p>
<p>ã¨ã†ã¨ã†dLLMã‚’é«˜é€Ÿã§inferenceã§ãã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãŒå‡ºãŸæ¨¡æ§˜ã€‚inclusionAIã‚ˆã‚Šã€‚</p>
<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1978662709773373856?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Pruning.html" target="_blank" rel="noopener noreferrer">#Pruning</a>
<a class="button" href="articles/Test-Time%20Scaling.html" target="_blank" rel="noopener noreferrer">#Test-Time Scaling</a>
<a class="button" href="articles/Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<a class="button" href="articles/Parallel.html" target="_blank" rel="noopener noreferrer">#Parallel</a>
<span class="issue_date">Issue Date: 2025-10-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3232" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] DeepPrune: Parallel Scaling without Inter-trace Redundancy, Shangqing Tu+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- DeepPruneã¨ã„ã†æ–°ã—ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€ä¸¦åˆ—ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®è¨ˆç®—éåŠ¹ç‡ã‚’è§£æ±ºã€‚80%ä»¥ä¸Šã®æ¨è«–ãƒˆãƒ¬ãƒ¼ã‚¹ãŒåŒä¸€ã®å›ç­”ã‚’ç”Ÿæˆã™ã‚‹å•é¡Œã«å¯¾å‡¦ã—ã€ç„¦ç‚¹æå¤±ã¨ã‚ªãƒ¼ãƒãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æŠ€è¡“ã‚’ç”¨ã„ãŸåˆ¤å®šãƒ¢ãƒ‡ãƒ«ã§åŒç­‰æ€§ã‚’äºˆæ¸¬ã€‚ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã®è²ªæ¬²ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã§å†—é•·ãªçµŒè·¯ã‚’ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€80%ä»¥ä¸Šã®ãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›ã‚’é”æˆã—ã¤ã¤ã€ç²¾åº¦ã‚’ç¶­æŒã€‚åŠ¹ç‡çš„ãªä¸¦åˆ—æ¨è«–ã®æ–°åŸºæº–ã‚’ç¢ºç«‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://deepprune.github.io" target="_blank" rel="noopener noreferrer">https://deepprune.github.io</a>


</p>
<p>HF:


<a href="https://huggingface.co/collections/THU-KEG/deepprune-68e5c1ea71f789a6719b2c1c" target="_blank" rel="noopener noreferrer">https://huggingface.co/collections/THU-KEG/deepprune-68e5c1ea71f789a6719b2c1c</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1977134276966789446?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/memory.html" target="_blank" rel="noopener noreferrer">#memory</a>
<a class="button" href="articles/RecurrentModels.html" target="_blank" rel="noopener noreferrer">#RecurrentModels</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3195" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Artificial Hippocampus Networks for Efficient Long-Context Modeling, Yunhao Fang+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- é•·å¤§ãªã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ãŠã‘ã‚‹ãƒ¡ãƒ¢ãƒªã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è§£æ±ºã™ã‚‹ãŸã‚ã€äººå·¥æµ·é¦¬ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆAHNï¼‰ã‚’ææ¡ˆã€‚AHNã¯çŸ­æœŸãƒ¡ãƒ¢ãƒªã‚’ç¶­æŒã—ã¤ã¤ã€é•·æœŸãƒ¡ãƒ¢ãƒªã‚’åœ§ç¸®ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€AHNã‚’ç”¨ã„ãŸãƒ¢ãƒ‡ãƒ«ãŒå¾“æ¥ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚Šã€è¨ˆç®—ã¨ãƒ¡ãƒ¢ãƒªè¦ä»¶ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã¤ã¤ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1976107974939816308?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/teortaxestex/status/1976451334951063854?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-10-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3191" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] The Markovian Thinker, Milad Aghajohari+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- å¼·åŒ–å­¦ç¿’ã‚’ç”¨ã„ã¦é•·ã„æ€è€ƒã®é€£é–ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã€Œãƒãƒ«ã‚³ãƒ•çš„æ€è€ƒã€ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€çŠ¶æ…‹ã‚’ä¸€å®šã®ã‚µã‚¤ã‚ºã«åˆ¶é™ã—ã€æ€è€ƒã®é•·ã•ã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã‚µã‚¤ã‚ºã‹ã‚‰åˆ‡ã‚Šé›¢ã™ã“ã¨ã§ã€ç·šå½¢è¨ˆç®—ã‚’å®Ÿç¾ã€‚æ–°ã—ã„RLç’°å¢ƒã€ŒDelethinkã€ã‚’æ§‹ç¯‰ã—ã€ãƒ¢ãƒ‡ãƒ«ã¯çŸ­ã„æŒã¡è¶Šã—ã§æ¨è«–ã‚’ç¶™ç¶šã™ã‚‹ã“ã¨ã‚’å­¦ç¿’ã€‚è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã€é•·ã„æ¨è«–ã‚’åŠ¹ç‡çš„ã«è¡Œã„ã€ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›ã€‚æ€è€ƒç’°å¢ƒã®å†è¨­è¨ˆãŒã€åŠ¹ç‡çš„ã§ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªæ¨è«–LLMã®å®Ÿç¾ã«å¯„ä¸ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/maghajohari/status/1976296195438887012?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1976466786565656986?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1976798665038758377?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/RepresentationLearning.html" target="_blank" rel="noopener noreferrer">#RepresentationLearning</a>
<a class="button" href="articles/RAG(RetrievalAugmentedGeneration).html" target="_blank" rel="noopener noreferrer">#RAG(RetrievalAugmentedGeneration)</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-10-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3178" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Generative Representational Instruction Tuning, Niklas Muennighoff+, ICLR'25, 2024.02</a>
<span class="snippet"><span>GPT Summary</span>- ç”Ÿæˆçš„è¡¨ç¾æŒ‡ç¤ºãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆGRITï¼‰ã‚’ç”¨ã„ã¦ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ãŒç”Ÿæˆã‚¿ã‚¹ã‚¯ã¨åŸ‹ã‚è¾¼ã¿ã‚¿ã‚¹ã‚¯ã‚’åŒæ™‚ã«å‡¦ç†ã§ãã‚‹æ‰‹æ³•ã‚’ææ¡ˆã€‚GritLM 7Bã¯MTEBã§æ–°ãŸãªæœ€å…ˆç«¯ã‚’é”æˆã—ã€GritLM 8x7Bã¯ã™ã¹ã¦ã®ã‚ªãƒ¼ãƒ—ãƒ³ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã€‚GRITã¯ç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã¨åŸ‹ã‚è¾¼ã¿ãƒ‡ãƒ¼ã‚¿ã®çµ±åˆã«ã‚ˆã‚‹æ€§èƒ½æå¤±ãŒãªãã€RAGã‚’60%ä»¥ä¸Šé«˜é€ŸåŒ–ã™ã‚‹åˆ©ç‚¹ã‚‚ã‚ã‚‹ã€‚ãƒ¢ãƒ‡ãƒ«ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=BC4lIvfSzv" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=BC4lIvfSzv</a>


</p>
<p>å¾“æ¥ã¯gemerativeã‚¿ã‚¹ã‚¯ã¨embeddingã‚¿ã‚¹ã‚¯ã¯åˆ¥ã€…ã«ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã•ã‚Œã¦ã„ãŸãŒã€ãã‚Œã‚’çµ±ä¸€çš„ãªæ çµ„ã¿ã§å®Ÿæ–½ã—ã€ä¸¡æ–¹ã®ã‚¿ã‚¹ã‚¯ã§åŒç­‰ã®ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®ä»–ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦é«˜ã„æ€§èƒ½ã‚’é”æˆã—ãŸç ”ç©¶ã€‚å¾“æ¥ã®generativeã‚¿ã‚¹ã‚¯ç”¨ã®next-token-prediction lossã¨embeddingã‚¿ã‚¹ã‚¯ç”¨ã®constastive lossã‚’çµ„ã¿åˆã‚ã›ã¦å­¦ç¿’ã™ã‚‹ï¼ˆå¼3ï¼‰ã€‚ã‚¿ã‚¹ã‚¯ã®åŒºåˆ¥ã¯instructionã«ã‚ˆã‚Šå®Ÿæ–½ã—ã€embeddingã‚¿ã‚¹ã‚¯ã®å ´åˆã¯ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®last hidden stateã®mean poolingã§representationã‚’å–å¾—ã™ã‚‹ã€‚ã¾ãŸã€embeddingã®æ™‚ã¯bi-directional attention / generativeã‚¿ã‚¹ã‚¯ã®æ™‚ã¯causal maskãŒé©ç”¨ã•ã‚Œã‚‹ã€‚ã“ã‚Œã‚‰ã®attentionã®é©ç”¨ã®ã•ã‚Œæ–¹ã®é•ã„ãŒã€ã©ã®ã‚ˆã†ã«ç®¡ç†ã•ã‚Œã‚‹ã‹ã¯ã¾ã ã—ã£ã‹ã‚Šèª­ã‚ã¦ã„ãªã„ã®ã§ã‚ˆãã‚ã‹ã£ã¦ã„ãªã„ãŒã€éå¸¸ã«èˆˆå‘³æ·±ã„ç ”ç©¶ã§ã‚ã‚‹ã€‚<br><br>&lt;img width="603" height="349" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/acb2cbcd-364d-43c7-b51a-6c5ea9866415"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/acb2cbcd-364d-43c7-b51a-6c5ea9866415"&lt;/a&gt;


/&gt;</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Tokenizer.html" target="_blank" rel="noopener noreferrer">#Tokenizer</a>
<a class="button" href="articles/Decoder.html" target="_blank" rel="noopener noreferrer">#Decoder</a>
<span class="issue_date">Issue Date: 2025-10-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3177" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization, ThÃ©ophane Vallaeys+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æ–°ã—ã„ãƒ”ã‚¯ã‚»ãƒ«æ‹¡æ•£ãƒ‡ã‚³ãƒ¼ãƒ€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆSSDDï¼‰ã‚’ææ¡ˆã—ã€KL-VAEã«ä¾å­˜ã›ãšã«é«˜å“è³ªãªç”»åƒå†æ§‹æˆã‚’å®Ÿç¾ã€‚SSDDã¯æ•µå¯¾çš„æå¤±ãªã—ã§è¨“ç·´ã•ã‚Œã€å†æ§‹æˆFIDã‚’æ”¹å–„ã—ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é€Ÿåº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€KL-VAEã®ä»£æ›¿ã¨ã—ã¦è¿…é€Ÿã‹ã¤é«˜å“è³ªãªç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ãŒå¯èƒ½ã¨ãªã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1975484440475505039?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<span class="issue_date">Issue Date: 2025-10-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3132" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Free Draft-and-Verification: Toward Lossless Parallel Decoding for  Diffusion Large Language Models, Shutong Wu+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- Diffusion Large Language Models (DLLMs)ã¯ã€åŒæ–¹å‘ã®æ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«ã‚ˆã‚Šæ–‡è„ˆã‚’æ‰ãˆã‚‹èƒ½åŠ›ãŒé«˜ã„ãŒã€æ¨è«–åŠ¹ç‡ãŒè‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã«åŠ£ã‚‹ã€‚æ—¢å­˜ã®ä¸¦åˆ—ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯æ€§èƒ½ä½ä¸‹ã‚’ä¼´ã†ã€‚ã“ã‚Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€æå¤±ã®ãªã„ä¸¦åˆ—ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Ÿç¾ã™ã‚‹æ–°ã—ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã€ŒFree Draft-and-Verificationï¼ˆFreedaveï¼‰ã€ã‚’ææ¡ˆã€‚Freedaveã«ã‚ˆã‚Šã€DLLMsã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã¯æ•°å­¦çš„æ¨è«–ã‚¿ã‚¹ã‚¯ã§æœ€å¤§2.8å€å‘ä¸Šã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/lingyang_pu/status/1974754204473536620?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Alignment.html" target="_blank" rel="noopener noreferrer">#Alignment</a>
<a class="button" href="articles/SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="articles/VariationalAutoEncoder.html" target="_blank" rel="noopener noreferrer">#VariationalAutoEncoder</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/RewardModel.html" target="_blank" rel="noopener noreferrer">#RewardModel</a>
<span class="issue_date">Issue Date: 2025-10-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3131" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Limited Preference Data? Learning Better Reward Model with Latent Space  Synthesis, Leitian Tao+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- å ±é…¬ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ãŸã‚ã«ã€LLMã®æ½œåœ¨åŸ‹ã‚è¾¼ã¿ç©ºé–“ã§å¥½ã¿ãƒ‡ãƒ¼ã‚¿ã‚’åˆæˆã™ã‚‹æ–°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯LENSã‚’ææ¡ˆã€‚VAEã‚’ç”¨ã„ã¦åŸ‹ã‚è¾¼ã¿ã®æ§‹é€ åŒ–ã•ã‚ŒãŸè¡¨ç¾ã‚’å­¦ç¿’ã—ã€ã‚³ã‚¹ãƒˆã®ã‹ã‹ã‚‹ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’å›é¿ã—ã¤ã¤ã€å¤šæ§˜ã§ä¸€è²«ã—ãŸåˆæˆå¥½ã¿ãƒšã‚¢ã‚’ç”Ÿæˆã€‚å®Ÿé¨“ã§ã¯ã€åˆæˆãƒšã‚¢ãŒå…ƒã®å¥½ã¿ã®é †åºã‚’ä¿æŒã—ã€å ±é…¬ãƒ¢ãƒ‡ãƒ«ã®ä¸€èˆ¬åŒ–ã‚’æ”¹å–„ã€‚ç”Ÿæˆé€Ÿåº¦ã¯18å€é€Ÿãã€16,000å€å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã§å„ªã‚ŒãŸçµæœã‚’é”æˆã€‚åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’é€šã˜ã¦å ±é…¬ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚’å¼·åŒ–ã™ã‚‹åŠ¹æœçš„ãªæ‰‹æ³•ã‚’æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/sharonyixuanli/status/1974871491117187099?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2025-10-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3128" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning, Aayush Mishra+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ï¼ˆICLï¼‰ã®æ´»æ€§åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ©ç”¨ã—ã¦ã€ç›£è¦–ä»˜ããƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆSFTï¼‰ã®å“è³ªã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã€‚ICLã¨SFTã®ç•°ãªã‚‹é©å¿œãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ç¤ºã—ã€ICLæ´»æ€§åŒ–ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆï¼ˆIA2ï¼‰ã¨ã„ã†è‡ªå·±è’¸ç•™æŠ€è¡“ã‚’å°å…¥ã€‚IA2ã‚’SFTã®å‰ã«å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ç²¾åº¦ã¨ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’12ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å®Ÿè¨¼ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«é©å¿œã®å†…éƒ¨ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«å¯¾ã™ã‚‹æ–°ãŸãªè¦–ç‚¹ã‚‚æä¾›ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/danielkhashabi/status/1974119053728919790?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/ImageCaptioning.html" target="_blank" rel="noopener noreferrer">#ImageCaptioning</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/LLM-as-a-Judge.html" target="_blank" rel="noopener noreferrer">#LLM-as-a-Judge</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/MultiDimensional.html" target="_blank" rel="noopener noreferrer">#MultiDimensional</a>
<span class="issue_date">Issue Date: 2025-10-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3059" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] VELA: An LLM-Hybrid-as-a-Judge Approach for Evaluating Long Image   Captions, Kazuki Matsuda+, EMNLP'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€é•·ã„ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã®è‡ªå‹•è©•ä¾¡ã«ç‰¹åŒ–ã—ãŸæ–°ã—ã„æŒ‡æ¨™VELAã‚’ææ¡ˆã—ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆMLLMsï¼‰ã‚’æ´»ç”¨ã—ãŸè©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã€‚ã•ã‚‰ã«ã€è©•ä¾¡æŒ‡æ¨™ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã®LongCap-Arenaãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å°å…¥ã—ã€7,805æšã®ç”»åƒã¨32,246ä»¶ã®äººé–“ã®åˆ¤æ–­ã‚’ç”¨ã„ã¦ã€VELAãŒæ—¢å­˜ã®æŒ‡æ¨™ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/yuigawada/status/1973309026546098355?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Quantization.html" target="_blank" rel="noopener noreferrer">#Quantization</a>
<span class="issue_date">Issue Date: 2025-09-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3047" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Pretraining Large Language Models with NVFP4, NVIDIA+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€NVFP4ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç”¨ã„ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®å®‰å®šã‹ã¤æ­£ç¢ºãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã‚’ææ¡ˆã€‚ãƒ©ãƒ³ãƒ€ãƒ ãƒãƒ€ãƒãƒ¼ãƒ‰å¤‰æ›ã‚„äºŒæ¬¡å…ƒé‡å­åŒ–ã‚¹ã‚­ãƒ¼ãƒ ã‚’å–ã‚Šå…¥ã‚Œã€åã‚Šã®ãªã„å‹¾é…æ¨å®šã‚’å®Ÿç¾ã€‚10å…†ãƒˆãƒ¼ã‚¯ãƒ³ã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚Šã€FP8ã¨åŒç­‰ã®æ€§èƒ½ã‚’é”æˆã—ã€ç‹­ã„ç²¾åº¦ã®LLMãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã‘ã‚‹é€²å±•ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/arankomatsuzaki/status/1972869149102858441?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1975344045754097685?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/Sparse.html" target="_blank" rel="noopener noreferrer">#Sparse</a>
<span class="issue_date">Issue Date: 2025-09-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3003" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via   Semantic-Aware Permutation, Shuo Yang+, NeurIPS'25 Spotlight, 2025.05</a>
<span class="snippet"><span>GPT Summary</span>- Diffusion Transformersï¼ˆDiTsï¼‰ã®å‹•ç”»ç”Ÿæˆã«ãŠã‘ã‚‹ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã€é‡è¦ãƒˆãƒ¼ã‚¯ãƒ³ã®ç‰¹å®šç²¾åº¦ã‚’æœ€å¤§åŒ–ã—è¨ˆç®—ã®ç„¡é§„ã‚’æœ€å°åŒ–ã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸è¦ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯SVG2ã‚’ææ¡ˆã€‚SVG2ã¯æ„å‘³ã«åŸºã¥ããƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¨å†é…ç½®ã‚’è¡Œã„ã€è¨ˆç®—åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€HunyuanVideoãŠã‚ˆã³Wan 2.1ã§ãã‚Œãã‚Œæœ€å¤§2.30å€ãŠã‚ˆã³1.89å€ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’é”æˆã—ã€PSNRã‚’ç¶­æŒã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/chenfeng_x/status/1971343654662046184?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>pj page:


<a href="https://svg-project.github.io/v2/" target="_blank" rel="noopener noreferrer">https://svg-project.github.io/v2/</a>


</p>
<p>Q, Kãã‚Œãã‚Œã«ã¤ã„ã¦ç‹¬ç«‹ã—ã¦kmeansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿæ–½ã—ã€æ„å‘³çš„ã«é¡ä¼¼ã—ãŸQ, Kã‚’ã‚¯ãƒ©ã‚¹ã‚¿åŒ–ã—ã€mapä¸Šã§æ•£ã‚‰ã°ã£ã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®é…ç½®ã‚’æ•´é “ã—ã¦è¨ˆç®—æ©Ÿä¸Šã§åŠ¹ç‡çš„ã«æ‰±ãˆã‚‹ã‚ˆã†ã«ã—ã€å„ã‚¯ãƒ©ã‚¹ã‚¿ã®centroidã‚’attention scoreã®è¨ˆç®—ã«ç”¨ã„ã¦ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¹ã‚³ã‚¢ã‚’è¿‘ä¼¼ã™ã‚‹ã“ã¨ã§è¨ˆç®—ã‚’åŠ¹ç‡åŒ–ã—ã¾ã™ã€ã¨ã„ã£ãŸè©±ãªæ¨¡æ§˜ã€‚ã¾ãŸã€ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªã‚¯ãƒ©ã‚¹ã‚¿ã¨ãã†ã§ã¯ç„¡ã„ã‚‚ã®ãŒã‚ã‚‹ã®ã§ã€på€‹ã®ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªã‚¯ãƒ©ã‚¹ã‚¿ã‚’é¸æŠã—ã•ã‚‰ã«åŠ¹ç‡åŒ–ã‚’ã™ã‚‹æ¨¡æ§˜ã€‚<br><img src="https://github.com/user-attachments/assets/862cf5c8-5583-4f94-8b67-59177c444176" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<span class="issue_date">Issue Date: 2025-09-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3002" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's   Own Signals, Qinsi Wang+, NeurIPS'25 Spotlight, 2025.06</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®å¼·åŒ–å­¦ç¿’å¾®èª¿æ•´ï¼ˆRFTï¼‰ã«ãŠã‘ã‚‹ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã®ä½ä¸‹ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®ä¿¡å·ã€Œè§’åº¦é›†ä¸­ã€ã‚’ç‰¹å®šã€‚ã“ã‚Œã«åŸºã¥ãã€å‹¾é…é§†å‹•å‹è§’åº¦æƒ…å ±ãƒŠãƒ“ã‚²ãƒ¼ãƒˆå¼·åŒ–å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼ˆGAIN-RLï¼‰ã‚’ææ¡ˆã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’å‹•çš„ã«é¸æŠã™ã‚‹ã“ã¨ã§åŠ¹ç‡ã‚’å‘ä¸Šã€‚å®Ÿè¨¼è©•ä¾¡ã§ã¯ã€GAIN-RLãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚’2.5å€ä»¥ä¸Šå‘ä¸Šã•ã›ã€å…ƒã®ãƒ‡ãƒ¼ã‚¿ã®åŠåˆ†ã§ã‚ˆã‚Šè‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ãŸã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/chenfeng_x/status/1971343654662046184?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã‚„ç‰¹å®šã®é›£æ˜“åº¦ã«åŸºã¥ããƒ©ãƒ™ãƒ«ã‹ã‚‰RLã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã®ã§ã¯ãªãã€ãƒ¢ãƒ‡ãƒ«è‡ªèº«ã®ç¾åœ¨ã®å­¦ç¿’ã®çŠ¶æ…‹ã«åŸºã¥ã„ã¦å‹•çš„ã«é¸æŠã—å­¦ç¿’åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãªæ¨¡æ§˜ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/mid-training.html" target="_blank" rel="noopener noreferrer">#mid-training</a>
<span class="issue_date">Issue Date: 2025-09-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2997" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Expanding Reasoning Potential in Foundation Model by Learning Diverse  Chains of Thought Patterns, Xuemiao Zhang+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡æ¨è«–ãƒ¢ãƒ‡ãƒ«ã®é€²å±•ã¯å¼·åŒ–å­¦ç¿’ã«ã‚ˆã£ã¦ä¿ƒé€²ã•ã‚Œã€CoTãƒ‡ãƒ¼ã‚¿ã®åˆ©ç”¨ãŒæ¨è«–ã®æ·±ã•ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚ã—ã‹ã—ã€ã©ã®ãƒ‡ãƒ¼ã‚¿ã‚¿ã‚¤ãƒ—ãŒæœ€ã‚‚åŠ¹æœçš„ã‹ã¯æœªè§£æ±ºã®å•é¡Œã§ã‚ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€æ¨è«–ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ã‚’ç‹¬ç«‹ã—ãŸè©¦è¡Œã®æ•°ã®é€†æ•°ã¨ã—ã¦å®šç¾©ã—ã€ã“ã‚Œã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã«é«˜ä¾¡å€¤ã®æ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç”¨ã„ãŸå¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿ã®åˆ©ç”¨ã‚’ææ¡ˆã€‚å…·ä½“çš„ã«ã¯ã€CoTã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰åŸå­çš„ãªæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½è±¡åŒ–ã—ã€ã‚³ã‚¢ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã€‚äºŒé‡ç²’åº¦ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç”¨ã„ã¦é«˜ä¾¡å€¤ã®CoTãƒ‡ãƒ¼ã‚¿ã‚’åŠ¹ç‡çš„ã«é¸æŠã—ã€ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚10Bãƒˆãƒ¼ã‚¯ãƒ³ã®CoTPãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚Šã€85A6B Mixture-of-Expertsãƒ¢ãƒ‡ãƒ«ã¯AIME 2024ãŠã‚ˆã³2025ã§9.58%ã®æ”¹å–„ã‚’é”æˆã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1971461991039631691?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ç´°ã‹ã„ã¨ã“ã‚ã¯èª­ã‚ã¦ã„ãªã„ã®ã ãŒã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ä¸­ã‹ã‚‰é«˜å“è³ªãªæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã¤ã‚‚ã®ã‚’é¸ã‚“ã§å­¦ç¿’ã«ä½¿ã„ãŸã„ã¨ã„ã†ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã€‚ãã®ãŸã‚ã«ã¾ãšä¾¡å€¤ã®é«˜ã„æ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å«ã‚€ã‚³ã‚¢ã‚»ãƒƒãƒˆã‚’ä½œã‚Šã€ã‚³ã‚¢ã‚»ãƒƒãƒˆã¨é¡ä¼¼ã—ãŸæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚„ã€æ¨è«–ä¸­ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ—ã‚’æŒã¤ã‚µãƒ³ãƒ—ãƒ«ã‚’å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰åé›†ã™ã‚‹ã¿ãŸã„ãªè©±ãªæ¨¡æ§˜ã€‚é¡ä¼¼åº¦ã¯é‡ã¿ã¤ãDynamic Time Warping (DTW)ã§ã€åŸå§‹çš„ãªæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç³»åˆ—ã¨ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ç³»åˆ—ã®DTWã®ç·šå‹çµåˆã«ã‚ˆã£ã‚æ±‚ã‚ã‚‹ã€‚åŸå§‹çš„ãªæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚„ã€CoT sequenceä¸­ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼åˆ—ã¯DeepSeek-V3ã«ã‚ˆã£ã¦ç”Ÿæˆã™ã‚‹ã€‚<br><br>ã‚³ã‚¢ã‚»ãƒƒãƒˆã‚’ä½œã‚‹ãŸã‚ã«ã¯ã€å•é¡Œã‚¿ã‚¤ãƒ—ã‚„å•é¡Œã®é›£æ˜“åº¦ã«åŸºã¥ã„ã¦äººæ‰‹ã§å•é¡Œã‚’é¸ã³ã€ãã‚Œã‚‰ã«å¯¾ã—ã¦strong reasoning modelã§CoTã‚’ç”Ÿæˆã€‚å„CoTã«å¯¾ã—ã¦ï¼ˆãŠãã‚‰ãï¼‰DeepSeek-V3ã§reasoningã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã¯åŸå§‹çš„ãªCoTãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç³»åˆ—ã§æ§‹æˆã•ã‚Œã‚‹ï¼‰ã‚’ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã—ã€å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾ã—ã¦TF-IDFã«ã‚ˆã£ã¦é‡è¦åº¦ã‚’æ±ºå®šã™ã‚‹ã€‚æœ€çµ‚çš„ã«ã€å•é¡Œã«æ­£ç­”ã—ã¦ã„ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã«ã¤ã„ã¦ã€äººæ‰‹ã§é«˜å“è³ªã§discriminativeãªCoTãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒã¤ã‚‚ã®ã‚’é¸æŠã—ã€å„CoTãƒ‘ã‚¿ãƒ¼ãƒ³ã«é‡ã¿ã‚’ã¤ã‘ãŸä¸Šã§ã‚³ã‚¢ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ãŸã€ã¿ãŸã„ãªæ„Ÿã˜ã«è¦‹ãˆã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<span class="issue_date">Issue Date: 2025-09-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2938" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] LIMI: Less is More for Agency, Yang Xiao+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- AIã‚·ã‚¹ãƒ†ãƒ ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ã‚·ãƒ¼ã‚’ã€è‡ªå¾‹çš„ã«å•é¡Œã‚’ç™ºè¦‹ã—è§£æ±ºç­–ã‚’å®Ÿè¡Œã™ã‚‹èƒ½åŠ›ã¨å®šç¾©ã€‚æ€¥é€Ÿã«å¤‰åŒ–ã™ã‚‹æ¥­ç•Œã®ãƒ‹ãƒ¼ã‚ºã«å¿œã˜ã¦ã€å˜ãªã‚‹æ¨è«–ã‚’è¶…ãˆãŸè‡ªå¾‹çš„ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒæ±‚ã‚ã‚‰ã‚Œã¦ã„ã‚‹ã€‚LIMIï¼ˆLess Is More for Intelligent Agencyï¼‰ã¯ã€æœ€å°é™ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«ã§é«˜ã„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ã‚·ãƒ¼ã‚’å®Ÿç¾ã™ã‚‹æ–°ãŸãªåŸå‰‡ã‚’ææ¡ˆã—ã€78ã‚µãƒ³ãƒ—ãƒ«ã§73.5%ã®æˆæœã‚’é”æˆã€‚ã“ã‚Œã¯ã€å¾“æ¥ã®ãƒ‡ãƒ¼ã‚¿é‡ã«ä¾å­˜ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«å¯¾ã™ã‚‹æŒ‘æˆ¦ã§ã‚ã‚Šã€é«˜å“è³ªãªãƒ‡ãƒ¢ã®æˆ¦ç•¥çš„ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒé‡è¦ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/arankomatsuzaki/status/1970328242688246160?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>LLM Agentã®SFTã«ãŠã‘ã‚‹Less is more<br><br>å‚è€ƒ:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/700" target="_blank" rel="noopener noreferrer">LIMA: Less Is More for Alignment, Chunting Zhou+, N/A, NeurIPS'23</a>
</p>
<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1971777010658955436?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<span class="issue_date">Issue Date: 2025-09-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2898" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] BREAD: Branched Rollouts from Expert Anchors Bridge SFT &amp; RL for   Reasoning, Xuechen Zhang+, NeurIPS'25</a>
<span class="snippet"><span>GPT Summary</span>- å°å‹è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆSLMsï¼‰ã¯ã€ãƒˆãƒ¬ãƒ¼ã‚¹ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã«è¤‡é›‘ãªæ¨è«–ã‚’å­¦ã¶ã®ãŒé›£ã—ã„ã€‚æœ¬ç ”ç©¶ã§ã¯ã€SFT + RLã®é™ç•Œã‚’èª¿æŸ»ã—ã€BREADã¨ã„ã†æ–°ã—ã„æ‰‹æ³•ã‚’ææ¡ˆã€‚BREADã¯ã€å°‚é–€å®¶ã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚’ç”¨ã„ã¦SFTã¨RLã‚’çµ±åˆã—ã€å¤±æ•—ã—ãŸãƒˆãƒ¬ãƒ¼ã‚¹ã«å¯¾ã—ã¦çŸ­ã„ãƒ’ãƒ³ãƒˆã‚’æŒ¿å…¥ã™ã‚‹ã“ã¨ã§æˆåŠŸã‚’ä¿ƒé€²ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒç´„3å€é€Ÿããªã‚Šã€æ¨™æº–çš„ãªGRPOã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã€‚BREADã¯ã€SLMã®æ¨è«–èƒ½åŠ›ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/sametoymac/status/1968892463382200391?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<span class="issue_date">Issue Date: 2025-09-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2855" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] WebSailor: Navigating Super-human Reasoning for Web Agent, Kuan Li+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- WebSailorã¯ã€LLMã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦äººé–“ã®èªçŸ¥çš„é™ç•Œã‚’è¶…ãˆã‚‹ãŸã‚ã®ãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã§ã‚ã‚Šã€è¤‡é›‘ãªæƒ…å ±æ¢ç´¢ã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚æ§‹é€ åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚„æƒ…å ±ã®é›£èª­åŒ–ã€DUPOã‚’ç”¨ã„ã¦é«˜ä¸ç¢ºå®Ÿæ€§ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆã—ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®èƒ½åŠ›ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€‚</span>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/OpenSource.html" target="_blank" rel="noopener noreferrer">#OpenSource</a>
<a class="button" href="articles/Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-09-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2820" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] OpenVision 2: A Family of Generative Pretrained Visual Encoders for  Multimodal Learning, Yanqing Liu+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€OpenVisionã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç°¡ç´ åŒ–ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹æ–¹æ³•ã‚’ææ¡ˆã€‚ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã¨å¯¾ç…§æå¤±ã‚’å‰Šé™¤ã—ã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ‹ãƒ³ã‚°æå¤±ã®ã¿ã‚’ä½¿ç”¨ã—ãŸOpenVision 2ã‚’å°å…¥ã€‚åˆæœŸçµæœã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“ã‚’ç´„1.5å€çŸ­ç¸®ã—ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç´„1.8å€å‰Šæ¸›ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã€10å„„ä»¥ä¸Šã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’å¼·èª¿ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1967865921399296286?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>äº‹å‰å­¦ç¿’æ™‚ã«text, image encoderã®contrastive lossã§å­¦ç¿’ã—ã¦ã„ãŸãŒã€text encoderã‚’ç„¡ãã—image encoderã«å…¥åŠ›ã•ã‚ŒãŸimageã‹ã‚‰captionã‚’ç”Ÿæˆã™ã‚‹caption lossã®ã¿ã«ã™ã‚‹ã“ã¨ã§æ€§èƒ½ã‚’è½ã¨ã™ã“ã¨ãªãåŠ¹ç‡ã‚’æ”¹å–„</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Pruning.html" target="_blank" rel="noopener noreferrer">#Pruning</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-09-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2816" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Adaptive Computation Pruning for the Forgetting Transformer, Zhixuan Lin+, COLM'25</a>
<span class="snippet"><span>GPT Summary</span>- Forgeting Transformerï¼ˆFoXï¼‰ã¯ã€å¿˜å´ã‚²ãƒ¼ãƒˆã‚’ç”¨ã„ãŸã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ç‰¹å¾´ã¨ã—ã€å¾“æ¥ã®Transformerã¨æ¯”è¼ƒã—ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã€‚FoXã®ç‰¹æ€§ã‚’æ´»ã‹ã—ã€é©å¿œè¨ˆç®—ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆACPï¼‰ã‚’ææ¡ˆã—ã€è¨ˆç®—ã‚’å‹•çš„ã«ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€FLOPsã¨ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã‚’ç´„70%å‰Šæ¸›ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œæ™‚é–“ã‚’50%ã‹ã‚‰70%çŸ­ç¸®ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’10%ã‹ã‚‰40%å‘ä¸Šã•ã›ãŸã€‚æ€§èƒ½ã®åŠ£åŒ–ã¯ãªãã€é•·ã„æ–‡è„ˆé•·ã§ã¯ã•ã‚‰ãªã‚‹è¨ˆç®—ã‚³ã‚¹ãƒˆã®ç¯€ç´„ãŒå¯èƒ½ã§ã‚ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>code:


<a href="https://github.com/zhixuan-lin/forgetting-transformer" target="_blank" rel="noopener noreferrer">https://github.com/zhixuan-lin/forgetting-transformer</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/zhxlin/status/1967596994362220761?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>openreview:


<a href="https://openreview.net/forum?id=xNj14CY5S1#discussion" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=xNj14CY5S1#discussion</a>


</p>
<p>å…ˆè¡Œç ”ç©¶:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2817" target="_blank" rel="noopener noreferrer">[Paper Note] Forgetting Transformer: Softmax Attention with a Forget Gate, Zhixuan Lin+, ICLR'25</a>
</p></span><br><br>
<a class="button" href="articles/InformationRetrieval.html" target="_blank" rel="noopener noreferrer">#InformationRetrieval</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ContextWindow.html" target="_blank" rel="noopener noreferrer">#ContextWindow</a>
<a class="button" href="articles/RAG(RetrievalAugmentedGeneration).html" target="_blank" rel="noopener noreferrer">#RAG(RetrievalAugmentedGeneration)</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-09-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2748" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Efficient Context Selection for Long-Context QA: No Tuning, No  Iteration, Just Adaptive-$k$, Chihiro Taguchi+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Adaptive-$k$ retrievalã‚’ææ¡ˆã—ã€ã‚¯ã‚¨ãƒªã¨å€™è£œãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã®é¡ä¼¼åº¦ã«åŸºã¥ã„ã¦é©å¿œçš„ã«ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸æ•°ã‚’é¸æŠã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å›ºå®šã‚µã‚¤ã‚ºã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’ç™ºæ®ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡ã‚’æœ€å¤§10å€å‰Šæ¸›ã—ã¤ã¤70%ã®é–¢é€£ãƒ‘ãƒƒã‚»ãƒ¼ã‚¸ã‚’å–å¾—ã€‚LCLMsã¨åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã§ç²¾åº¦å‘ä¸Šã‚’å®Ÿç¾ã—ã€å‹•çš„ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºèª¿æ•´ãŒåŠ¹ç‡çš„ãªQAã«å¯„ä¸ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/megagonlabs/status/1965430004667613305?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å®Ÿå‹™ä¸Šã‚³ã‚¹ãƒˆã‚’æŠ‘ãˆã‚‰ã‚Œã‚‹ã®ã¯éå¸¸ã«å¬‰ã—ã„ã€‚ã‚ã¨ã§èª­ã‚€ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-09-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2726" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] SpikingBrain Technical Report: Spiking Brain-inspired Large Models, Yuqi Pan+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- SpikingBrainã¯ã€é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®åŠ¹ç‡çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã®ãŸã‚ã«è¨­è¨ˆã•ã‚ŒãŸè„³ã«ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã€MetaX GPUã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’æ´»ç”¨ã€‚ç·šå½¢ãŠã‚ˆã³ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ç·šå½¢ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¡ç”¨ã—ã€éNVIDIAãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ä¸Šã§ã®å¤§è¦æ¨¡LLMé–‹ç™ºã‚’å®Ÿç¾ã€‚SpikingBrain-7Bã¨SpikingBrain-76Bã‚’é–‹ç™ºã—ã€ç´„150Bãƒˆãƒ¼ã‚¯ãƒ³ã§ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®Transformerã¨åŒç­‰ã®æ€§èƒ½ã‚’é”æˆã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚’å¤§å¹…ã«æ”¹å–„ã—ã€ä½æ¶ˆè²»é›»åŠ›ã§ã®é‹ç”¨ã‚’å¯èƒ½ã«ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/f14bertolotti/status/1964949822429069761?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>TTFTãŒ4Mã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ™‚ã«Qwen2.5ã¨æ¯”ã¹ã¦100å€é«˜é€ŸåŒ–â€¦ï¼Ÿ</p>
<p>ä¸­å›½ã®MetaXç¤¾ã®GPUãŒåˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚<br><br>


<a href="https://www.metax-tech.com/en/goods/prod.html?cid=3" target="_blank" rel="noopener noreferrer">https://www.metax-tech.com/en/goods/prod.html?cid=3</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/RAG(RetrievalAugmentedGeneration).html" target="_blank" rel="noopener noreferrer">#RAG(RetrievalAugmentedGeneration)</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/SpeculativeDecoding.html" target="_blank" rel="noopener noreferrer">#SpeculativeDecoding</a>
<span class="issue_date">Issue Date: 2025-09-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2716" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] REFRAG: Rethinking RAG based Decoding, Xiaoqiang Lin+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- REFRAGã¯ã€RAGã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ãŠã‘ã‚‹é…å»¶ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã®åŠ¹ç‡çš„ãªãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚Šã€ã‚¹ãƒ‘ãƒ¼ã‚¹æ§‹é€ ã‚’åˆ©ç”¨ã—ã¦åˆå›ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§ã®æ™‚é–“ã‚’30.85å€åŠ é€Ÿã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€LLMsã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºã‚’16ã¾ã§æ‹¡å¼µå¯èƒ½ã«ã—ã€ã•ã¾ã–ã¾ãªé•·ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¿ã‚¹ã‚¯ã§ç²¾åº¦ã‚’æãªã†ã“ã¨ãªãã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’å®Ÿç¾ã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dr_singularity/status/1964453705430036982?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>èˆˆå‘³æ·±ã„ã€‚Speculative Decodingã®æ–°æ‰‹æ³•ã¨ã‚‚ã¿ãªã›ãã†ã€‚</p>
<p>åŒæ™‚æœŸã«å‡ºãŸä¸‹è¨˜ç ”ç©¶ã¨æ¯”è¼ƒã—ã¦ã©ã®ã‚ˆã†ãªpros/consãŒã‚ã‚‹ã ã‚ã†ã‹ï¼Ÿ<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2699" target="_blank" rel="noopener noreferrer">[Paper Note] Set Block Decoding is a Language Model Inference Accelerator, Itai Gat+, arXiv'25</a>
</p>
<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1966292095242744186?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-09-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2699" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Set Block Decoding is a Language Model Inference Accelerator, Itai Gat+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Set Block Decodingï¼ˆSBDï¼‰ã‚’ææ¡ˆã—ã€æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã¨ãƒã‚¹ã‚¯ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã‚’çµ±åˆã—ã¦ç”Ÿæˆã‚’åŠ é€Ÿã€‚SBDã¯è¤‡æ•°ã®æœªæ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¸¦è¡Œã—ã¦ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¯èƒ½ã§ã€å¾“æ¥ã®æ‰‹æ³•ã‚ˆã‚Šã‚‚é€Ÿåº¦å‘ä¸Šã‚’å®Ÿç¾ã€‚ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å¤‰æ›´ãªã—ã§æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã®æ•°ã‚’3-5å€å‰Šæ¸›ã—ã¤ã¤åŒç­‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/arankomatsuzaki/status/1963817987506643350?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/Coding.html" target="_blank" rel="noopener noreferrer">#Coding</a>
<a class="button" href="articles/SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<span class="issue_date">Issue Date: 2025-09-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2676" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents, Manish Shetty+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- é«˜æ€§èƒ½ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã«ãŠã‘ã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯GSOã‚’ææ¡ˆã€‚102ã®æœ€é©åŒ–ã‚¿ã‚¹ã‚¯ã‚’ç‰¹å®šã™ã‚‹è‡ªå‹•åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é–‹ç™ºã—ã€ä¸»è¦ãªã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æˆåŠŸç‡ã¯5%æœªæº€ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚å®šæ€§çš„åˆ†æã«ã‚ˆã‚Šã€ä½ãƒ¬ãƒ™ãƒ«è¨€èªã‚„æœ€é©åŒ–æˆ¦ç•¥ã®èª²é¡ŒãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚ç ”ç©¶ã®é€²å±•ã®ãŸã‚ã«ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®ã‚³ãƒ¼ãƒ‰ã¨ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã‚’å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://gso-bench.github.io" target="_blank" rel="noopener noreferrer">https://gso-bench.github.io</a>


</p>
<p>ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã®é«˜é€ŸåŒ–ã«é–¢ã™ã‚‹ãƒ™ãƒ³ãƒ</p>
<p>å…ƒãƒã‚¹ãƒˆã«æ²è¼‰ã•ã‚Œã¦ã„ã‚‹ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ã¯ã©ã“ã«ã‚ã‚‹ã®ã ã‚ã†ã€‚ã–ã£ã¨è¦‹ãŸæ„Ÿã˜è¦‹å½“ãŸã‚‰ãªã„ã€‚</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/GRPO.html" target="_blank" rel="noopener noreferrer">#GRPO</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-09-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2645" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs  via Bi-Mode Annealing and Reinforce Learning, Jie Jiang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- R-4Bã¯ã€å•é¡Œã®è¤‡é›‘ã•ã«å¿œã˜ã¦æ€è€ƒã‚’è¡Œã†ã‹ã©ã†ã‹ã‚’é©å¿œçš„ã«åˆ¤æ–­ã™ã‚‹è‡ªå‹•æ€è€ƒå‹ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆMLLMï¼‰ã§ã‚ã‚‹ã€‚æ€è€ƒèƒ½åŠ›ã¨éæ€è€ƒèƒ½åŠ›ã‚’æŒãŸã›ã€ãƒã‚¤ãƒ¢ãƒ¼ãƒ‰ãƒãƒªã‚·ãƒ¼æœ€é©åŒ–ï¼ˆBPOï¼‰ã‚’ç”¨ã„ã¦æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã®èµ·å‹•ã‚’ç²¾åº¦è‰¯ãåˆ¤æ–­ã™ã‚‹ã€‚è¨“ç·´ã«ã¯å¤šæ§˜ãªãƒˆãƒ”ãƒƒã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã€å®Ÿé¨“çµæœã¯R-4BãŒ25ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã€ç‰¹ã«æ¨è«–é›†ç´„å‹ã‚¿ã‚¹ã‚¯ã§ä½ã‚³ã‚¹ãƒˆã§é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ãŸã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1962445854654288036?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>VLMã«thinking, non-thinkingã‚’å…¥åŠ›ã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘ã•ã›ã‚‹æ‰‹æ³•</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<span class="issue_date">Issue Date: 2025-08-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2630" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] AWorld: Orchestrating the Training Recipe for Agentic AI, Chengyue Yu+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- AWorldã¨ã„ã†ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚·ã‚¹ãƒ†ãƒ ã‚’å°å…¥ã—ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ç’°å¢ƒã®ç›¸äº’ä½œç”¨ã‚’åŠ¹ç‡åŒ–ã€‚çµŒé¨“åé›†ã‚’14.6å€åŠ é€Ÿã—ã€Qwen3-32Bãƒ™ãƒ¼ã‚¹ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¨“ç·´ã—ã¦GAIAã®ç²¾åº¦ã‚’21.59%ã‹ã‚‰32.23%ã«å‘ä¸Šã€‚æœ€é›£é–¢ãƒ¬ãƒ™ãƒ«ã§å•†ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’è¶…ãˆã‚‹æ€§èƒ½ã‚’é”æˆã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/aicia_solid/status/1961999098032328902?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1963005182817808721?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-08-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2622" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation   Experts, Peng Jin+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€Mixture-of-Expertsï¼ˆMoEï¼‰æ‰‹æ³•ã®åŠ¹æœã¨åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€MoE++ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã€‚ã‚¼ãƒ­è¨ˆç®—ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã‚’å°å…¥ã—ã€ä½è¨ˆç®—ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã€é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã®å®¹æ˜“ã•ã‚’å®Ÿç¾ã€‚å®Ÿé¨“çµæœã«ã‚ˆã‚Šã€MoE++ã¯å¾“æ¥ã®MoEãƒ¢ãƒ‡ãƒ«ã«æ¯”ã¹ã¦1.1-2.1å€ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’æä¾›ã—ã€å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=t7P5BUKcYv" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=t7P5BUKcYv</a>


</p>
<p>å¾“æ¥ã®MoEã¨æ¯”ã¹ã¦ã€å°‚é–€å®¶ã¨ã—ã¦zero computation expertsã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€æ€§èƒ½ã‚’ç¶­æŒã—ãªãŒã‚‰åŠ¹ç‡çš„ã«inferenceã‚’ã™ã‚‹æ‰‹æ³•(MoEã«ãŠã„ã¦å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‡ä¸€ã«æ‰±ã‚ãªã„ï¼‰ã‚’ææ¡ˆã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚<br><br><img src="https://github.com/user-attachments/assets/b71d01ed-92b1-4c4f-ab90-bf9b01c461be" alt="image" loading="lazy"><br><br>zero computation expertsã¯3ç¨®é¡ã§<br>- Zero Experts: å…¥åŠ›ã‚’ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã«è½ã¨ã™<br>- Copy Experts: å…¥åŠ›xã‚’ãã®ã¾ã¾ã‚³ãƒ”ãƒ¼ã™ã‚‹<br>- Constant Experts: learnableãªå®šæ•°ãƒ™ã‚¯ãƒˆãƒ«vã‚’å­¦ç¿’ã—ã€xã¨ç·šå½¢çµåˆã—ã¦å‡ºåŠ›ã™ã‚‹ã€‚W_cã«ã‚ˆã£ã¦å…¥åŠ›xã‚’å¤‰æ›ã™ã‚‹ã“ã¨ã§ç·šå½¢è£œã€€çµåˆã®ä¿‚æ•°a1,a2ã‚’å…¥åŠ›ã«å¿œã˜ã¦å‹•çš„ã«æ±ºå®šã™ã‚‹ã€‚<br><br><img src="https://github.com/user-attachments/assets/8c2f8f4c-d8d2-44ad-b3f0-4951f9fb2cfb" alt="image" loading="lazy"><br><br>Routingã®æ‰‹æ³•ã‚„gating residualã€å­¦ç¿’æ‰‹æ³•ã®å·¥å¤«ã‚‚ãªã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ãªã®ã§ã€å¾Œã§èª­ã‚€ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<span class="issue_date">Issue Date: 2025-08-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2621" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Shortcut-connected Expert Parallelism for Accelerating   Mixture-of-Experts, Weilin Cai+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- ScMoEã¯ã€ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚²ãƒ¼ãƒˆæ··åˆå°‚é–€å®¶ãƒ¢ãƒ‡ãƒ«ã®è¨ˆç®—è² è·ã‚’åˆ†æ•£ã•ã›ã‚‹æ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã€é€šä¿¡ã¨è¨ˆç®—ã®é‡è¤‡ã‚’æœ€å¤§100%å¯èƒ½ã«ã—ã€å…¨å¯¾å…¨é€šä¿¡ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’è§£æ¶ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§1.49å€ã€æ¨è«–ã§1.82å€ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’å®Ÿç¾ã—ã€ãƒ¢ãƒ‡ãƒ«å“è³ªã‚‚æ—¢å­˜æ‰‹æ³•ã¨åŒç­‰ã¾ãŸã¯ãã‚Œä»¥ä¸Šã‚’é”æˆã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=GKly3FkxN4&noteId=4tfWewv7R2" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=GKly3FkxN4&noteId=4tfWewv7R2</a>


</p></span><br><br>
<a class="button" href="articles/Controllable.html" target="_blank" rel="noopener noreferrer">#Controllable</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Search.html" target="_blank" rel="noopener noreferrer">#Search</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Test-Time%20Scaling.html" target="_blank" rel="noopener noreferrer">#Test-Time Scaling</a>
<a class="button" href="articles/Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<span class="issue_date">Issue Date: 2025-08-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2610" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs, Ziyue Li+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- äº‹å‰å­¦ç¿’æ¸ˆã¿ã®LLMã®å±¤ã‚’ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¨ã—ã¦æ“ä½œã—ã€å„ã‚µãƒ³ãƒ—ãƒ«ã«æœ€é©ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ§‹ç¯‰ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã€‚ãƒ¢ãƒ³ãƒ†ã‚«ãƒ«ãƒ­æœ¨æ¢ç´¢ã‚’ç”¨ã„ã¦ã€æ•°å­¦ãŠã‚ˆã³å¸¸è­˜æ¨è«–ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€é©ãªå±¤ã®é€£é–ï¼ˆCoLaï¼‰ã‚’ç‰¹å®šã€‚CoLaã¯æŸ”è»Ÿã§å‹•çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æä¾›ã—ã€æ¨è«–åŠ¹ç‡ã‚’æ”¹å–„ã™ã‚‹å¯èƒ½æ€§ã‚’ç¤ºå”†ã€‚75%ä»¥ä¸Šã®æ­£ã—ã„äºˆæ¸¬ã«å¯¾ã—ã¦çŸ­ã„CoLaã‚’è¦‹ã¤ã‘ã€60%ä»¥ä¸Šã®ä¸æ­£ç¢ºãªäºˆæ¸¬ã‚’æ­£ã™ã“ã¨ãŒã§ãã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ã€‚å›ºå®šã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é™ç•Œã‚’å…‹æœã™ã‚‹é“ã‚’é–‹ãã€‚</span>
<span class="snippet"><span>Comment</span><p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1961749826028347602?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>äº‹å‰å­¦ç¿’æ¸ˆã¿è¨€èªãƒ¢ãƒ‡ãƒ«ã®forward pathã«ãŠã‘ã‚‹å„layerã‚’building blocksã¨ã¿ãªã—ã¦ã€å…¥åŠ›ã«å¿œã˜ã¦ã‚¹ã‚­ãƒƒãƒ—ã€ã‚ã‚‹ã„ã¯å†å¸°çš„ãªåˆ©ç”¨ã‚’MCTSã«ã‚ˆã£ã¦é¸æŠã™ã‚‹ã“ã¨ã§ã€test timeæ™‚ã®ãƒ¢ãƒ‡ãƒ«ã®æ·±ã•ã‚„ã€ãƒ¢ãƒ‡ãƒ«ã®å‡¡åŒ–æ€§èƒ½ã‚’ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦é©ç”¨ã•ã›ã‚‹ã‚ˆã†ãªæ‰‹æ³•ã‚’ææ¡ˆã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ›´æ–°ã¯ä¸è¦ã€‚k, r âˆˆ {1,2,3,4} ã®ç¯„å›²ã§ã€"kå€‹ã®layerã‚’skip"ã€ã‚ã‚‹ã„ã¯kå€‹ã®layerã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’rå›å†å¸°ã™ã‚‹ã€ã¨ã™ã‚‹ã“ã¨ã§æ¢ç´¢ç¯„å›²ã‚’é™å®šçš„ã«ã—testæ™‚ã®éå‰°ãªè¨ˆç®—ã‚’æŠ‘æ­¢ã—ã¦ã„ã‚‹ã€‚ã¾ãŸã€MCTSã«ãŠã‘ã‚‹simulationã®å›æ•°ã¯200å›ã€‚length penaltyã‚’å¤§ããã™ã‚‹ã“ã¨ã§compactãªforward pathã«ãªã‚‹ã‚ˆã†ã«èª¿æ•´ã€10%ã®ç¢ºç‡ã§ã¾ã æ¢ç´¢ã—ã¦ã„ãªã„å­ãƒãƒ¼ãƒ‰ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠã™ã‚‹ã“ã¨ã§æ¢ç´¢ã‚’ä¿ƒã™ã‚ˆã†ã«ã—ã¦ã„ã‚‹ã€‚ã‚ªãƒªã‚¸ãƒŠãƒ«ã¨æ¯”è¼ƒã—ã¦å®Ÿè¡Œæ™‚é–“ãŒã©ã®ç¨‹åº¦å¢—ãˆã¦ã—ã¾ã†ã®ã‹ï¼Ÿã«èˆˆå‘³ãŒã‚ã£ãŸãŒã€ãƒ¢ãƒ‡ãƒ«ã®æ·±ã•ã¨ã„ã†è¦³ç‚¹ã§æ¨è«–åŠ¹ç‡ã¯è€ƒå¯Ÿã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆãŸãŒã€å®Ÿè¡Œæ™‚é–“ã¨ã„ã†è¦³ç‚¹ã§ã¯ã–ã£ã¨è¦‹ãŸæ„Ÿã˜è¨˜è¼‰ãŒãªã„ã‚ˆã†ã«è¦‹ãˆãŸã€‚<br><br>&lt;img width="948" height="301" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/0a03cdc2-141b-40a1-a11e-9560187ff7b6"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/0a03cdc2-141b-40a1-a11e-9560187ff7b6"&lt;/a&gt;


/&gt;<br><br>ä»¥ä¸‹ã®åºƒç¯„ãªQAã€å¹…åºƒã„é›£æ˜“åº¦ã‚’æŒã¤æ•°å­¦ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ï¼ˆAppendix Bã«å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã”ã¨ã«500 sampleã‚’åˆ©ç”¨ã¨è¨˜è¼‰ãŒã‚ã‚‹ï¼‰ã‚’ã—ãŸã¨ã“ã‚ã€å¤§å¹…ã«æ€§èƒ½ãŒå‘ä¸Šã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚ãŸã ã—ã€8Bç¨‹åº¦ã®ã‚µã‚¤ã‚ºã®ãƒ¢ãƒ‡ãƒ«ã§ã—ã‹å®Ÿé¨“ã¯ã•ã‚Œã¦ã„ãªã„ã€‚<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2613" target="_blank" rel="noopener noreferrer">[Paper Note] Think you have Solved Question Answering? Try ARC, the AI2 Reasoning
  Challenge, Peter Clark+, arXiv'18</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2615" target="_blank" rel="noopener noreferrer">[Paper Note] DART-Math: Difficulty-Aware Rejection Tuning for Mathematical  Problem-Solving, Yuxuan Tong+, NeurIPS'24</a>
<br>&lt;img width="986" height="682" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/c6d88c0a-4ae0-41b7-8526-17d041692f49"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/c6d88c0a-4ae0-41b7-8526-17d041692f49"&lt;/a&gt;


/&gt;</p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2611" target="_blank" rel="noopener noreferrer">[Paper Note] Looped Transformers are Better at Learning Learning Algorithms, Liu Yang+, ICLR'24</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2612" target="_blank" rel="noopener noreferrer">[Paper Note] Looped Transformers for Length Generalization, Ying Fan+, ICLR'25</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2605" target="_blank" rel="noopener noreferrer">[Paper Note] Universal Transformers, Mostafa Dehghani+, ICLR'19</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2241" target="_blank" rel="noopener noreferrer">[Paper Note] Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive   Token-Level Computation, Sangmin Bae+, NeurIPS'25</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/memory.html" target="_blank" rel="noopener noreferrer">#memory</a>
<span class="issue_date">Issue Date: 2025-08-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2585" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Ultra-Sparse Memory Network, Zihao Huang+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- UltraMemã¯ã€å¤§è¦æ¨¡ã§è¶…ã‚¹ãƒ‘ãƒ¼ã‚¹ãªãƒ¡ãƒ¢ãƒªå±¤ã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€Transformerãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’å‰Šæ¸›ã—ã¤ã¤æ€§èƒ½ã‚’ç¶­æŒã™ã‚‹æ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ææ¡ˆã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€UltraMemã¯MoEã‚’ä¸Šå›ã‚‹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç‰¹æ€§ã‚’ç¤ºã—ã€æœ€å¤§2000ä¸‡ã®ãƒ¡ãƒ¢ãƒªã‚¹ãƒ­ãƒƒãƒˆã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ãŒæœ€å…ˆç«¯ã®æ¨è«–é€Ÿåº¦ã¨æ€§èƒ½ã‚’é”æˆã™ã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/NeuralArchitectureSearch.html" target="_blank" rel="noopener noreferrer">#NeuralArchitectureSearch</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/Reference%20Collection.html" target="_blank" rel="noopener noreferrer">#Reference Collection</a>
<span class="issue_date">Issue Date: 2025-08-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2548" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Jet-Nemotron: Efficient Language Model with Post Neural Architecture  Search, Yuxian Gu+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Jet-Nemotronã¯æ–°ã—ã„ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã€ãƒ•ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ä»¥ä¸Šã®ç²¾åº¦ã‚’æŒã¡ãªãŒã‚‰ç”Ÿæˆã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’å¤§å¹…ã«æ”¹å–„ã—ã¾ã™ã€‚Post Neural Architecture Searchï¼ˆPostNASï¼‰ã‚’ç”¨ã„ã¦é–‹ç™ºã•ã‚Œã€äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‹ã‚‰åŠ¹ç‡çš„ã«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ–ãƒ­ãƒƒã‚¯ã‚’æ¢ç´¢ã—ã¾ã™ã€‚Jet-Nemotron-2Bãƒ¢ãƒ‡ãƒ«ã¯ã€ä»–ã®å…ˆé€²ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦é«˜ã„ç²¾åº¦ã‚’é”æˆã—ã€ç”Ÿæˆã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’æœ€å¤§53.6å€å‘ä¸Šã•ã›ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1959832287073403137?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è‘—è€…ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hancai_hm/status/1960000017235902722?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jacksonatkinsx/status/1960090774122483783?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/webbigdata/status/1960392071384326349?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/1960724749790929009?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ç¶šå ±:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hancai_hm/status/1972794734747033985?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>ã‚³ãƒ¼ãƒ‰ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒãƒªãƒªãƒ¼ã‚¹<br><br>code:


<a href="https://github.com/NVlabs/Jet-Nemotron" target="_blank" rel="noopener noreferrer">https://github.com/NVlabs/Jet-Nemotron</a>


<br>HF:


<a href="https://huggingface.co/collections/jet-ai/jet-nemotron-68ac76e8356b5399ef83ac9c" target="_blank" rel="noopener noreferrer">https://huggingface.co/collections/jet-ai/jet-nemotron-68ac76e8356b5399ef83ac9c</a>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/Length.html" target="_blank" rel="noopener noreferrer">#Length</a>
<a class="button" href="articles/Inference.html" target="_blank" rel="noopener noreferrer">#Inference</a>
<span class="issue_date">Issue Date: 2025-08-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2536" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] TokenSkip: Controllable Chain-of-Thought Compression in LLMs, Heming Xia+, EMNLP'25</a>
<span class="snippet"><span>GPT Summary</span>- Chain-of-Thought (CoT)ã¯LLMã®æ¨è«–èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ãŒã€é•·ã„CoTå‡ºåŠ›ã¯æ¨è«–é…å»¶ã‚’å¢—åŠ ã•ã›ã‚‹ã€‚ã“ã‚Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€é‡è¦åº¦ã®ä½ã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é¸æŠçš„ã«ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹TokenSkipã‚’ææ¡ˆã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€TokenSkipã¯CoTãƒˆãƒ¼ã‚¯ãƒ³ã®ä½¿ç”¨ã‚’å‰Šæ¸›ã—ã¤ã¤æ¨è«–æ€§èƒ½ã‚’ç¶­æŒã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚ç‰¹ã«ã€Qwen2.5-14B-Instructã§GSM8Kã«ãŠã„ã¦æ¨è«–ãƒˆãƒ¼ã‚¯ãƒ³ã‚’40%å‰Šæ¸›ã—ã€æ€§èƒ½ä½ä¸‹ã¯0.4%æœªæº€ã§ã‚ã£ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hemingkx/status/1891873475545137245?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Inference.html" target="_blank" rel="noopener noreferrer">#Inference</a>
<span class="issue_date">Issue Date: 2025-08-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2535" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Pushing the Envelope of LLM Inference on AI-PC, Evangelos Georganas+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- è¶…ä½ãƒ“ãƒƒãƒˆLLMãƒ¢ãƒ‡ãƒ«ã®ç™»å ´ã«ã‚ˆã‚Šã€ãƒªã‚½ãƒ¼ã‚¹åˆ¶ç´„ã®ã‚ã‚‹ç’°å¢ƒã§ã®LLMæ¨è«–ãŒå¯èƒ½ã«ã€‚1ãƒ“ãƒƒãƒˆãŠã‚ˆã³2ãƒ“ãƒƒãƒˆã®ãƒã‚¤ã‚¯ãƒ­ã‚«ãƒ¼ãƒãƒ«ã‚’è¨­è¨ˆã—ã€PyTorch-TPPã«çµ±åˆã™ã‚‹ã“ã¨ã§ã€æ¨è«–åŠ¹ç‡ã‚’æœ€å¤§2.2å€å‘ä¸Šã€‚ã“ã‚Œã«ã‚ˆã‚Šã€AI PCã‚„ã‚¨ãƒƒã‚¸ãƒ‡ãƒã‚¤ã‚¹ã§ã®è¶…ä½ãƒ“ãƒƒãƒˆLLMãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡çš„ãªå±•é–‹ãŒæœŸå¾…ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1959379120577826935?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/GRPO.html" target="_blank" rel="noopener noreferrer">#GRPO</a>
<span class="issue_date">Issue Date: 2025-08-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2530" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Hard Examples Are All You Need: Maximizing GRPO Post-Training Under  Annotation Budgets, Benjamin Pikus+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ãƒªã‚½ãƒ¼ã‚¹ãŒåˆ¶ç´„ã•ã‚ŒãŸçŠ¶æ³ã§ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦ã€é›£æ˜“åº¦ã®ç•°ãªã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¾‹ã®å„ªå…ˆé †ä½ã‚’æ¤œè¨ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€æœ€ã‚‚é›£ã—ã„ä¾‹ã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒæœ€å¤§47%ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã‚’ã‚‚ãŸã‚‰ã™ã“ã¨ãŒç¤ºã•ã‚Œã€é›£ã—ã„ä¾‹ãŒå­¦ç¿’æ©Ÿä¼šã‚’å¤šãæä¾›ã™ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€äºˆç®—åˆ¶ç´„ä¸‹ã§ã®åŠ¹æœçš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã¨ã—ã¦ã€é›£ã—ã„ä¾‹ã‚’å„ªå…ˆã™ã‚‹ã“ã¨ãŒæ¨å¥¨ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®pass@kãŒä½ã„hardestãªã‚µãƒ³ãƒ—ãƒ«ã§GRPOã‚’å­¦ç¿’ã™ã‚‹ã®ãŒãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ãŒè‰¯ãã€OODã«å¯¾ã™ã‚‹æ±åŒ–æ€§èƒ½ã‚‚ç™ºæ®ã•ã‚Œã¾ã™ã€ã¨ã„ã†ã®ã‚’Qwen3-4B, 14B, Phi4ã§å®Ÿé¨“ã—ã¦ç¤ºã—ã¾ã—ãŸã€ã¨ã„ã†è©±ã£ã½ã„ï¼Ÿ<br><br>å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã€ãŠã‚ˆã³GSM8Kã€BIG Bench hardã§ã®ã€Tracking Shuffled Objectã®ã¿ã§ã®å®Ÿé¨“ãªæ¨¡æ§˜ï¼Ÿå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚„ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã©ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã‚‚ã†ã¾ãã„ãã‹ã¯ã‚ˆãåˆ†ã‹ã‚‰ãªã„ã€‚OODã®å®Ÿé¨“ã‚‚AIME2025ã§ã®ã¿ã®å®Ÿé¨“ã—ã¦ã„ã‚‹ã‚ˆã†ãªã®ã§ãã“ã¯ç•™æ„ã—ãŸæ–¹ãŒè‰¯ã„ã‹ã‚‚ã€‚<br>rewardã¨ã—ã¦ä½•ã‚’ä½¿ã£ãŸã®ã‹ãªã©ã®ç´°ã‹ã„å†…å®¹ã‚’è¿½ãˆã¦ã„ãªã„ã€‚</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/pratyushrt/status/1958947577216524352?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2527" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency  Optimized Routing, Yiqun Zhang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- LLMã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨åŠ¹ç‡ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ãŸã‚ã«ã€ãƒ†ã‚¹ãƒˆæ™‚ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€ŒAvengers-Proã€ã‚’ææ¡ˆã€‚ã‚¯ã‚¨ãƒªã‚’åŸ‹ã‚è¾¼ã¿ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã—ã€æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€6ã¤ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã€‚æœ€å¼·ã®å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã‚’å¹³å‡ç²¾åº¦ã§+7%ä¸Šå›ã‚Šã€ã‚³ã‚¹ãƒˆã‚’27%å‰Šæ¸›ã—ã¤ã¤ç´„90%ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®Ÿç¾ã€‚ã™ã¹ã¦ã®å˜ä¸€ãƒ¢ãƒ‡ãƒ«ã®ä¸­ã§æœ€é«˜ã®ç²¾åº¦ã¨æœ€ä½ã®ã‚³ã‚¹ãƒˆã‚’æä¾›ã™ã‚‹ãƒ‘ãƒ¬ãƒ¼ãƒˆãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã‚’é”æˆã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ä¸­ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/1958897458408563069?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã‚¯ã‚¨ãƒªã‚’kmeansã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã—ã€å„ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã«ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®performanceã¨costã‚’äº‹å‰ã«ç®—å‡ºã—ã¦ãŠãã€‚ãã—ã¦æ–°ãŸãªã‚¯ã‚¨ãƒªãŒæ¥ãŸæ™‚ã«ã‚¯ã‚¨ãƒªãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã‚‹top pã®ã‚¯ãƒ©ã‚¹ã‚¿ã®performanae-cost efficiencyã‚’åˆè¨ˆã—ã€ã‚¹ã‚³ã‚¢ãŒé«˜ã„ä¸€ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠï¼ˆï¼routing)ã—inferenceã‚’å®Ÿæ–½ã™ã‚‹ã€‚ã‚¯ã‚¨ãƒªã¯Qwenã§embeddingåŒ–ã—ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«æ´»ç”¨ã™ã‚‹ã€‚ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿Î±âˆˆ[0,1]ã«ã‚ˆã£ã¦ã€performance, costã©ã¡ã‚‰ã‚’é‡è¦–ã™ã‚‹ã‹ã®ãƒãƒ©ãƒ³ã‚¹ã‚’èª¿æ•´ã™ã‚‹ã€‚<br><br>ã‚·ãƒ³ãƒ—ãƒ«ãªæ‰‹æ³•ã ãŒã€GPT-5 mediumã¨åŒç­‰ã®ã‚³ã‚¹ãƒˆ/æ€§èƒ½ã€€ã§ã‚ˆã‚Šé«˜ã„ã€€æ€§èƒ½/ã‚³ã‚¹ãƒˆã€€ã‚’å®Ÿç¾ã€‚<br><img src="https://github.com/user-attachments/assets/203f99a3-79b3-4465-985b-2bbd124d3972" alt="image" loading="lazy"></p>
<p>æ€§èƒ½å‘ä¸Šã€ã‚³ã‚¹ãƒˆå‰Šæ¸›ã§ãƒ€ãƒ¡æŠ¼ã—ã—ãŸã„æ™‚ã«ä½¿ãˆãã†ã ãŒã€ç™ºè¡Œã™ã‚‹ã‚¯ã‚¨ãƒªãŒãƒ—ãƒ­ãƒ—ãƒ©ã‚¤ã‚¨ã‚¿ãƒªãƒ‡ãƒ¼ã‚¿ã€ã‚ã‚‹ã„ã¯ãã‚‚ãã‚‚å…¨ç„¶ãƒ‡ãƒ¼ã‚¿ãªã„ã‚“ã§ã™ã€ã¿ãŸã„ãªçŠ¶æ³ã®å ´åˆã€ã‚¯ã‚¨ãƒªã®å‰²å½“å…ˆã¨ãªã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ã‚’é©åˆ‡ã«ç¢ºä¿ã™ã‚‹ï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«ç”¨ã„ã‚‹ååˆ†ãªé‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã™ã‚‹ï¼‰ã®ãŒå¤§å¤‰ãªå ´é¢ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€‚</p>
<p>ï¼ˆå…¨ç„¶æœ¬ç­‹ã¨é–¢ä¿‚ãªã„ãŒã€æœ€è¿‘è«–æ–‡ã®ã‚¿ã‚¤ãƒˆãƒ«ã«Beyondã¤ã‘ã‚‹ã®æµè¡Œã£ã¦ã‚‹â€¦ï¼Ÿï¼‰</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/AutomaticSpeechRecognition(ASR).html" target="_blank" rel="noopener noreferrer">#AutomaticSpeechRecognition(ASR)</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<span class="issue_date">Issue Date: 2025-08-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2525" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] LiteASR: Efficient Automatic Speech Recognition with Low-Rank  Approximation, Keisuke Kamahori+, EMNLP'25</a>
<span class="snippet"><span>GPT Summary</span>- LiteASRã¯ã€ç¾ä»£ã®è‡ªå‹•éŸ³å£°èªè­˜ãƒ¢ãƒ‡ãƒ«ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’ä½ãƒ©ãƒ³ã‚¯åœ§ç¸®ã™ã‚‹æ‰‹æ³•ã§ã€æ¨è«–ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã¤ã¤è»¢å†™ç²¾åº¦ã‚’ç¶­æŒã—ã¾ã™ã€‚ä¸»æˆåˆ†åˆ†æã‚’ç”¨ã„ã¦ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã®ä¹—ç®—ã‚’è¿‘ä¼¼ã—ã€è‡ªå·±æ³¨æ„æ©Ÿæ§‹ã‚’æœ€é©åŒ–ã™ã‚‹ã“ã¨ã§ã€Whisper large-v3ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚µã‚¤ã‚ºã‚’50%ä»¥ä¸Šåœ§ç¸®ã—ã€Whisper mediumã¨åŒç­‰ã®ã‚µã‚¤ã‚ºã§ã‚ˆã‚Šè‰¯ã„è»¢å†™ç²¾åº¦ã‚’å®Ÿç¾ã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/keisukekamahori/status/1958695752810864754?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ç¾ä»£ã®ASRãƒ¢ãƒ‡ãƒ«ã¯encoderãŒè¨ˆç®—åŠ¹ç‡ã®ä¸Šã§ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¨ãªã£ã¦ã„ãŸãŒã€Forward Passã«ãŠã‘ã‚‹ activatrion Y ã‚’ PCA ï¼ˆå¼2, 3ï¼‰ã«åŸºã¥ã„ã¦2ã¤ã®ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã®ç©ï¼ˆã¨ãƒã‚¤ã‚¢ã‚¹é …ã®åŠ ç®—; å¼5ï¼‰ã«ã‚ˆã£ã¦è¿‘ä¼¼ã—è¨ˆç®—åŠ¹ç‡ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ãŸã€ã¨ã„ã†è©±ãªæ¨¡æ§˜ã€‚weightã‚’ä½ãƒ©ãƒ³ã‚¯ã«å†™åƒã™ã‚‹V_kã¨ãƒã‚¤ã‚¢ã‚¹é …ã®Y_Mï¼ˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«å¯¾ã™ã‚‹activation Yã®å¹³å‡ï¼‰ã¯calibrfationãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã£ã¦äº‹å‰ã«è¨ˆç®—å¯èƒ½ã¨ã®ã“ã¨ã€‚ã¾ãŸã€PCAã®rank kãŒattention headã®æ¬¡å…ƒæ•°ã‚ˆã‚Šå°ã•ã„å ´åˆã€self-attentionã®è¨ˆç®—ã‚‚ã‚ˆã‚Šï¼ˆQWKã¸å†™åƒã™ã‚‹Wã‚’ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã§è¿‘ä¼¼ã™ã‚‹ã“ã¨ã§ï¼‰åŠ¹ç‡çš„ãªæ‰‹æ³•ã‚’æ¡ç”¨ã§ãã€ãã¡ã‚‰ã«ã¤ã„ã¦ã‚‚ææ¡ˆã•ã‚Œã¦ã„ã‚‹æ¨¡æ§˜ã€‚ï¼ˆã–ã£ãã‚Šã—ã‹èª­ã‚ã¦ã„ãªã„ã®ã§èª¤ã‚ŠãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€‚ï¼‰<br><br>&lt;img width="592" height="449" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/38c8aa6a-cad3-42d1-af6a-9102ed1df3f5"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/38c8aa6a-cad3-42d1-af6a-9102ed1df3f5"&lt;/a&gt;


/&gt;<br><br>&lt;img width="484" height="415" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/f8fa8cd1-2b6a-405a-88ec-3bfd2158dffb"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/f8fa8cd1-2b6a-405a-88ec-3bfd2158dffb"&lt;/a&gt;


/&gt;</p></span><br><br>
<a class="button" href="articles/Single.html" target="_blank" rel="noopener noreferrer">#Single</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-08-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2503" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent  Distillation and Agentic RL, Weizhen Li+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Chain-of-Agentsï¼ˆCoAï¼‰ã¨ã„ã†æ–°ã—ã„LLMæ¨è«–ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’ææ¡ˆã—ã€ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®å”åŠ›ã‚’å˜ä¸€ãƒ¢ãƒ‡ãƒ«å†…ã§ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã«å®Ÿç¾ã€‚ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆè’¸ç•™ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ã¦ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçš„ãªæ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã€å¼·åŒ–å­¦ç¿’ã§èƒ½åŠ›ã‚’å‘ä¸Šã€‚å¾—ã‚‰ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆåŸºç›¤ãƒ¢ãƒ‡ãƒ«ï¼ˆAFMsï¼‰ã¯ã€ã‚¦ã‚§ãƒ–ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚„ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¨­å®šã§æ–°ãŸãªæœ€å…ˆç«¯æ€§èƒ½ã‚’ç¤ºã™ã€‚ç ”ç©¶æˆæœã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã•ã‚Œã€ä»Šå¾Œã®ç ”ç©¶ã®åŸºç›¤ã‚’æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/1958186531161853995?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ã‚ˆã†ã«æŒ¯ã‚‹èˆã†ã‚·ãƒ³ã‚°ãƒ«ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ã€ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‹ã‚‰å¾—ã‚‰ã‚ŒãŸtrajectoryã‚’é€šã˜ã¦è’¸ç•™ã™ã‚‹ã“ã¨ã‚å®Ÿç¾ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã€‚SFTã§cold startã«å¯¾ã—ã¦è¨“ç·´ã—ãŸå¾Œã€verifiable reward (ã‚¿ã‚¹ã‚¯ã‚’æ­£å¸¸ã«å®Œäº†ã§ããŸã‹å¦ã‹)ã§RLã™ã‚‹æ¨¡æ§˜ã€‚<br><br><img src="https://github.com/user-attachments/assets/b4cafaba-488e-4d8b-a6d3-faf98733d134" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/80a934e9-db47-401b-809e-394ab5e20585" alt="image" loading="lazy"></p>
<p>ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚å…¬é–‹ã•ã‚Œã¦ã„ã‚‹æ¨¡æ§˜</p>
<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dongxi_nlp/status/1958604404338147417?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1959877518972137667?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2025-08-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2428" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Less Is More: Training-Free Sparse Attention with Global Locality for  Efficient Reasoning, Lijie Yang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ã€ŒLessIsMoreã€ã¨ã„ã†æ–°ã—ã„ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ææ¡ˆã€‚ã“ã‚Œã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸è¦ã§ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ´»ç”¨ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³é¸æŠã‚’åŠ¹ç‡åŒ–ã€‚ç²¾åº¦ã‚’ç¶­æŒã—ã¤ã¤ã€ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é€Ÿåº¦ã‚’1.1å€å‘ä¸Šã•ã›ã€ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’2å€å‰Šæ¸›ã€‚æ—¢å­˜æ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦1.13å€ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’å®Ÿç¾ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/lijieyyang/status/1955139186530328633?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒªãƒ¼ã§1.1å€ã®ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é€Ÿåº¦ã§æ€§èƒ½ã‚‚Full Attentionã¨åŒç­‰ä»¥ä¸Šã®Sparse Attentionã‚‰ã—ã„</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Search.html" target="_blank" rel="noopener noreferrer">#Search</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<a class="button" href="articles/Reference%20Collection.html" target="_blank" rel="noopener noreferrer">#Reference Collection</a>
<span class="issue_date">Issue Date: 2025-08-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2426" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale  Asynchronous RL, Jiaxuan Gao+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ASearcherã¯ã€LLMãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å¤§è¦æ¨¡ãªRLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿç¾ã™ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã‚ã‚Šã€é«˜åŠ¹ç‡ãªéåŒæœŸRLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨è‡ªå¾‹çš„ã«åˆæˆã•ã‚ŒãŸé«˜å“è³ªãªQ&amp;Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ã€æ¤œç´¢èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ææ¡ˆã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã€xBenchã§46.7%ã€GAIAã§20.8%ã®æ”¹å–„ã‚’é”æˆã—ã€é•·æœŸçš„ãªæ¤œç´¢èƒ½åŠ›ã‚’ç¤ºã—ãŸã€‚ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§æä¾›ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1955603041518035358?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è‘—è€…ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jxwuyi/status/1955487396344238486?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬ãƒã‚¹ãƒˆ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/1955266026498855354?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2466" target="_blank" rel="noopener noreferrer">[Paper Note] xbench: Tracking Agents Productivity Scaling with Profession-Aligned
  Real-World Evaluations, Kaiyuan Chen+, arXiv'25</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1158" target="_blank" rel="noopener noreferrer">GAIA: a benchmark for General AI Assistants, GrÃ©goire Mialon+, N/A, arXiv'23</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1461" target="_blank" rel="noopener noreferrer">[Paper Note] Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented  Generation, Satyapriya Krishna+, N/A, NAACL'25</a>
</p>
<p>æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã¯ &lt;= 10 turnsã®ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã•ã‚Œã¦ãŠã‚Šã€å¤§è¦æ¨¡ã§é«˜å“è³ªãªQAãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹å•é¡ŒãŒã‚ã£ãŸãŒã€ã‚·ãƒ¼ãƒ‰QAã«åŸºã¥ã„ã¦QAã‚’åˆæˆã™ã‚‹æ‰‹æ³•ã«ã‚ˆã£ã¦1.4ä¸‡ã‚·ãƒ¼ãƒ‰QAã‹ã‚‰134kã®é«˜å“è³ªãªQAã‚’åˆæˆã—ãŸï¼ˆã†ã¡25.6kã¯ãƒ„ãƒ¼ãƒ«åˆ©ç”¨ãŒå¿…è¦ï¼‰ã€‚å…·ä½“çš„ã«ã¯ã€ã‚·ãƒ¼ãƒ‰ã®QAã‚’åˆæˆã—ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒQAã®è¤‡é›‘åº¦ã‚’iterationã‚’ã—ãªãŒã‚‰å‘ä¸Šã•ã›ã¦ã„ãæ‰‹æ³•ã‚’ææ¡ˆã€‚äº‹å®Ÿæƒ…å ±ã¯å¸¸ã«verificationã‚’ã•ã‚Œã€åˆæˆãƒ—ãƒ­ã‚»ã‚¹ã®iterationã®ä¸­ã§ä¿æŒã•ã‚Œç¶šã‘ã‚‹ã€‚å€‹ã€…ã®iterationã«ãŠã„ã¦ã€ç¾åœ¨ã®QAã¨äº‹å®Ÿæƒ…å ±ã«åŸºã¥ã„ã¦ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯<br>- Injection: äº‹å®Ÿæƒ…å ±ã‚’æ–°ãŸã«æ³¨å…¥ã—QAã‚’ã‚ˆã‚Šãƒªãƒƒãƒã«ã™ã‚‹ã“ã¨ã§è¤‡é›‘åº¦ã‚’ä¸Šã’ã‚‹<br>- Fuzz: QAä¸­ã®ä¸€éƒ¨ã®è©³ç´°ãªæƒ…å ±ã‚’ã¼ã‹ã™ã“ã¨ã§ã€ä¸ç¢ºå®Ÿæ€§ã®ãƒ¬ãƒ™ãƒ«ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚<br>ã®2ç¨®é¡ã®æ“ä½œã‚’å®Ÿæ–½ã™ã‚‹ã€‚ãã®ä¸Šã§ã€QAã«å¯¾ã—ã¦Quality verificationã‚’å®Ÿæ–½ã™ã‚‹:<br>- Basic Quality: LLMã§qualityã‚’è©•ä¾¡ã™ã‚‹<br>- Difficulty Measurement: LRMã«ã‚ˆã£ã¦ã€è¤‡æ•°ã®å›ç­”å€™è£œã‚’ç”Ÿæˆã™ã‚‹<br>- Answer Uniqueness: Difficulty Measurementã§ç”Ÿæˆã•ã‚ŒãŸè¤‡æ•°ã®è§£ç­”æƒ…å ±ã«åŸºã¥ã„ã¦ã€mismatched answersãŒvalid answerã¨ãªã‚‹ã‹å¦ã‹ã‚’æ¤œè¨¼ã—ã€æ­£è§£ãŒå˜ä¸€ã§ã‚ã‚‹ã“ã¨ã‚’æ‹…ä¿ã™ã‚‹<br><br>&lt;img width="907" height="561" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/d020fc8f-b1da-4425-981a-6759cba5824b"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/d020fc8f-b1da-4425-981a-6759cba5824b"&lt;/a&gt;


/&gt;<br><br>ã¾ãŸã€è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã€ç‰¹ã«tool callsãŒéå¸¸ã«å¤šã„ã‚¿ã‚¹ã‚¯ã«ã¤ã„ã¦ã¯ã€å¤šãã®ã‚¿ãƒ¼ãƒ³æ•°ï¼ˆlong trajectoriesï¼‰ãŒå¿…è¦ã¨ãªã‚‹ãŒã€æ—¢å­˜ã®ãƒãƒƒãƒã«åŸºã¥ã„ãŸå­¦ç¿’æ‰‹æ³•ã§ã¯long trajectoriesã®ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚’ã—ã¦ã„ã‚‹é–“ã€ä»–ã®ã‚µãƒ³ãƒ—ãƒ«ã®å­¦ç¿’ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã—ã¾ã„å­¦ç¿’åŠ¹ç‡ãŒéå¸¸ã«æ‚ªã„ã®ã§ã€ãƒãƒƒãƒå†…ã®trajectoryã®ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã¨ãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°ã‚’åˆ†é›¢ï¼ˆãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒåˆ¥ã‚µãƒ¼ãƒã«é€ä¿¡ã•ã‚Œã‚µãƒ¼ãƒä¸Šã®Inference Engineã§éåŒæœŸã«å®Ÿè¡Œã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã™ã‚‹å´ã¯ååˆ†ãªtrajectoryãŒãƒãƒƒãƒå†…ã§æƒã£ãŸã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹ã€ã¿ãŸã„ãªæŒ™å‹•ï¼Ÿï¼‰ã™ã‚‹ã“ã¨ã§Idleã‚¿ã‚¤ãƒ ã‚’ç„¡ãã™ã‚ˆã†ãªæ‰‹æ³•ã‚’ææ¡ˆã—ãŸæ¨¡æ§˜ã€‚<br><br>&lt;img width="873" height="466" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/65d7e7b1-25fb-4288-a85e-07ae7a5eea2f"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/65d7e7b1-25fb-4288-a85e-07ae7a5eea2f"&lt;/a&gt;


/&gt;</p>
<p>æ—¢å­˜ã®æ‰‹æ³•ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®æ€§èƒ½ã¯å‘ä¸Šã—ã¦ã„ã‚‹ã€‚å­¦ç¿’ãŒé€²ã‚€ã«ã¤ã‚Œã¦ã€trajectoryä¸­ã®URLå‚ç…§å›æ•°ã‚„search queryæ•°ãªã©ãŒå¢—å¤§ã—ã¦ã„ãæ›²ç·šã¯è€ƒå¯Ÿã•ã‚Œã¦ã„ã‚‹ã€‚ä»–ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦ã€ã‚ˆã‚Šå¤šã„ã‚¿ãƒ¼ãƒ³æ•°ã‚’ã‚ˆã‚Šé«˜ã„æ­£ç¢ºæ€§ã‚’ä»¥ã£ã¦å®Ÿè¡Œã§ãã‚‹ã¨ã„ã£ãŸå®šé‡çš„ãªãƒ‡ãƒ¼ã‚¿ã¯ã¾ã å­˜åœ¨ã—ãªã„ã‚ˆã†ã«è¦‹ãˆãŸã€‚<br><br>&lt;img width="891" height="778" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/70644da8-b862-4bcb-bb05-d915c815b885"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/70644da8-b862-4bcb-bb05-d915c815b885"&lt;/a&gt;


/&gt;</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Alignment.html" target="_blank" rel="noopener noreferrer">#Alignment</a>
<a class="button" href="articles/DPO.html" target="_blank" rel="noopener noreferrer">#DPO</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-08-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2405" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap, Xuan Qi+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- LLMã®å¥½ã¿ã‚’äººé–“ã«åˆã‚ã›ã‚‹ãŸã‚ã®æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿é¸æŠæˆ¦ç•¥ã‚’ææ¡ˆã€‚DPOã®æš—é»™çš„å ±é…¬ã‚®ãƒ£ãƒƒãƒ—ãŒå°ã•ã„ãƒ‡ãƒ¼ã‚¿ã‚’é¸ã¶ã“ã¨ã§ã€ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã¨ãƒ¢ãƒ‡ãƒ«ã®æ•´åˆæ€§ã‚’å‘ä¸Šã€‚å…ƒã®ãƒ‡ãƒ¼ã‚¿ã®10ï¼…ã§5ã¤ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã€‚é™ã‚‰ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ã§ã®LLMæ•´åˆæ€§å‘ä¸Šã«å¯„ä¸ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/zhijingjin/status/1954535751489667173?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>preference pair dataã‚’å­¦ç¿’åŠ¹ç‡ã®è‰¯ã„ã‚µãƒ³ãƒ—ãƒ«ã®ã¿ã«åœ§ç¸®ã™ã‚‹ã“ã¨ã§å­¦ç¿’åŠ¹ç‡ã‚’ä¸Šã’ãŸã„ç³»ã®è©±ã§ã€chosen, rejectedãªã‚µãƒ³ãƒ—ãƒ«ã®ãã‚Œãã‚Œã«ã¤ã„ã¦ã€Â¥frac{ç¾åœ¨ã®ãƒãƒªã‚·ãƒ¼ã®å°¤åº¦}{å‚ç…§ãƒãƒªã‚·ãƒ¼ã®å°¤åº¦}ã«ã‚ˆã£ã¦reward rã‚’å®šç¾©ã—ï¼ˆãŠãã‚‰ãå‚ç…§ãƒãƒªã‚·ãƒ¼ã®å°¤åº¦ã«ã‚ˆã£ã¦ã‚µãƒ³ãƒ—ãƒ«ã®é‡è¦åº¦ã‚’é‡ã¿ã¥ã‘ã—ã¦ã„ã‚‹ï¼‰ã€r_chosenã¨r_rejectedã®å·®ã‚’reward gapã¨å®šç¾©ã—ã€gapãŒå¤§ãã„ã‚‚ã®ã¯é›£æ˜“åº¦ãŒä½ã„ã¨åˆ¤æ–­ã—ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã€ã¨ã„ã£ãŸè©±ã«è¦‹ãˆã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/1b930f5e-8db4-4c20-b7ca-59fb452f9056" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-08-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2396" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Fast and Simplex: 2-Simplicial Attention in Triton, Aurko Roy+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- 2-ã‚·ãƒ³ãƒ—ãƒªã‚·ã‚¢ãƒ«ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã€æ¨™æº–çš„ãªãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚ˆã‚Šã‚‚å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚å›ºå®šã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³äºˆç®—å†…ã§ã€æ•°å­¦ã‚„æ¨è«–ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ãƒ‰ãƒƒãƒˆç©ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ä¸Šå›ã‚‹çµæœã‚’å¾—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/scaling01/status/1954682957798715669?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2352" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] On the Expressiveness of Softmax Attention: A Recurrent Neural Network  Perspective, Gabriel Mongaras+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å†å¸°çš„ãªå½¢å¼ã‚’å°å‡ºã—ã€ç·šå½¢ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãŒãã®è¿‘ä¼¼ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å„éƒ¨åˆ†ã‚’RNNã®è¨€èªã§èª¬æ˜ã—ã€æ§‹æˆè¦ç´ ã®é‡è¦æ€§ã¨ç›¸äº’ä½œç”¨ã‚’ç†è§£ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãŒä»–ã®æ‰‹æ³•ã‚ˆã‚Šã‚‚è¡¨ç¾åŠ›ãŒé«˜ã„ç†ç”±ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1952485214162407644?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>LinearAttentioné–¢é€£ã®ç ”ç©¶ã¯ä¸‹è¨˜ã‚ãŸã‚ŠãŒã‚ã‚Šãã†ï¼Ÿ<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2353" target="_blank" rel="noopener noreferrer">[Paper Note] Efficient Attention: Attention with Linear Complexities, Zhuoran Shen+, arXiv'18</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2354" target="_blank" rel="noopener noreferrer">[Paper Note] Linformer: Self-Attention with Linear Complexity, Sinong Wang+, arXiv'20</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2355" target="_blank" rel="noopener noreferrer">[Paper Note] Reformer: The Efficient Transformer, Nikita Kitaev+, ICLR'20</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2356" target="_blank" rel="noopener noreferrer">[Paper Note] Transformers are RNNs: Fast Autoregressive Transformers with Linear  Attention, Angelos Katharopoulos+, ICML'20</a>
</p>
<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1271" target="_blank" rel="noopener noreferrer">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head
  Checkpoints, Joshua Ainslie+, N/A, arXiv'23</a>
<br><br>ãŸã¨ãˆã°GQAã¯Qwen3ã§åˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹ãŒã€æœ¬ç ”ç©¶ã®çŸ¥è¦‹ã‚’æ´»ç”¨ã—ã¦scaled-dot product attentionè¨ˆç®—æ™‚ã®Softmaxè¨ˆç®—ã®è¨ˆç®—é‡ãŒå‰Šæ¸›ã§ããŸã‚‰ã€ã•ã‚‰ã«è¨ˆç®—é‡ãŒå‰Šæ¸›ã§ããã†ï¼Ÿ</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<a class="button" href="articles/CrossDomain.html" target="_blank" rel="noopener noreferrer">#CrossDomain</a>
<span class="issue_date">Issue Date: 2025-08-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2341" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement  Learning on LLM, Xiaojiang Zhang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- äºŒæ®µéšå±¥æ­´å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒãƒªã‚·ãƒ¼æœ€é©åŒ–ï¼ˆSRPOï¼‰ã‚’ææ¡ˆã—ã€DeepSeek-R1-Zero-32Bã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’AIME24ãŠã‚ˆã³LiveCodeBenchã§é”æˆã€‚SRPOã¯ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—ã‚’ç´„1/10ã«å‰Šæ¸›ã—ã€åŠ¹ç‡æ€§ã‚’ç¤ºã™ã€‚äºŒã¤ã®é©æ–°ã¨ã—ã¦ã€ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¨å±¥æ­´å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æŠ€è¡“ã‚’å°å…¥ã—ã€LLMã®æ¨è«–èƒ½åŠ›ã‚’æ‹¡å¼µã™ã‚‹ãŸã‚ã®å®Ÿé¨“ã‚’è¡Œã£ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1914920300359377232?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>GRPOã‚ˆã‚Šã‚‚ã‚ˆã‚ŠåŠ¹ç‡çš„ãªæ‰‹æ³•ãªæ¨¡æ§˜ã€‚æœ€åˆã«æ•°å­¦ã®ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã‚’ã—Reasoning Capabilityã‚’èº«ã«ã¤ã‘ã•ã›ã€ãã®å¾Œåˆ¥ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã•ã›ã‚‹ã“ã¨ã§ã€ãã®èƒ½åŠ›ã‚’ç™ºæ®ã•ã›ã‚‹ã‚ˆã†ãªäºŒæ®µéšã®æ‰‹æ³•ã‚‰ã—ã„ã€‚<br><br>Datamixingã‚ˆã‚Šã‚‚é«˜ã„æ€§èƒ½ï¼ˆãŸã ã—ã€ã“ã‚Œã¯æ•°å­¦ã¨ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®CoT Lengthã®ãƒ‰ãƒ¡ã‚¤ãƒ³é–“ã®é•ã„ã«èµ·å› ã—ã¦ã“ã®ã‚ˆã†ãª2 stageãªæ‰‹æ³•ã«ã—ã¦ã„ã‚‹ã‚ˆã†ãªã®ã§ãã®ç‚¹ã«ã¯æ³¨æ„ãŒå¿…è¦ãã†ï¼‰ï¼Ÿã—ã£ã‹ã‚Šã¨èª­ã‚ã¦ã„ãªã„ã®ã§ã€èª­ã¿é•ã„ã®å¯èƒ½æ€§ã‚‚ã‚ã‚‹ã®ã§æ³¨æ„ã€‚<br><img src="https://github.com/user-attachments/assets/cf00de8b-1923-4f23-b575-0a889517ec9e" alt="image" loading="lazy"></p>
<p>ãªã‚“ãŸã‚‰RPOå¤šã™ãå•é¡Œ</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Optimizer.html" target="_blank" rel="noopener noreferrer">#Optimizer</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ModelMerge.html" target="_blank" rel="noopener noreferrer">#ModelMerge</a>
<a class="button" href="articles/Stability.html" target="_blank" rel="noopener noreferrer">#Stability</a>
<span class="issue_date">Issue Date: 2025-08-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2340" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM  Pre-training, Changxin Tian+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã®æ–°ãŸãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ã—ã¦ã€Warmup-Stable and Mergeï¼ˆWSMï¼‰ã‚’ææ¡ˆã€‚WSMã¯ã€å­¦ç¿’ç‡ã®æ¸›è¡°ã¨ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ã®é–¢ä¿‚ã‚’ç¢ºç«‹ã—ã€ã•ã¾ã–ã¾ãªæ¸›è¡°æˆ¦ç•¥ã‚’çµ±ä¸€çš„ã«æ‰±ã†ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ãƒãƒ¼ã‚¸æœŸé–“ãŒãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã«ãŠã„ã¦é‡è¦ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã€å¾“æ¥ã®WSDã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä¸Šå›ã‚‹æ€§èƒ½å‘ä¸Šã‚’é”æˆã€‚ç‰¹ã«ã€MATHã§+3.5%ã€HumanEvalã§+2.9%ã€MMLU-Proã§+5.5%ã®æ”¹å–„ã‚’è¨˜éŒ²ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/stochasticchasm/status/1951427541803106714?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Weight Decayã‚’ç„¡ãã›ã‚‹ã‚‰ã—ã„</p>
<p>ã‚¨ãƒƒã‚»ãƒ³ã‚¹ã®è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenhaocha1/status/1951790366900019376?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã•ãˆä¿å­˜ã—ã¦ãŠã„ã¦äº‹å¾Œçš„ã«æ´»ç”¨ã™ã‚‹ã“ã¨ã ã§ã€ç´°ã‹ãªãƒã‚¤ãƒ‘ãƒ©èª¿æ•´ã®ãŸã‚ã®è©¦è¡ŒéŒ¯èª¤ã™ã‚‹æ‰‹é–“ã¨è†¨å¤§ãªè¨ˆç®—ã‚³ã‚¹ãƒˆãŒãªããªã‚‹ã®ã§ã‚ã‚Œã°ç›¸å½“ç´ æ™´ã‚‰ã—ã„ã®ã§ã¯â€¦ï¼Ÿ<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1965893163152793728?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2025-07-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2325" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Efficient Attention Mechanisms for Large Language Models: A Survey, Yutao Sun+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è‡ªå·±æ³¨æ„ã®è¤‡é›‘ã•ãŒé•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®éšœå®³ã¨ãªã£ã¦ã„ã‚‹ã€‚ã“ã‚Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€ç·šå½¢æ³¨æ„æ‰‹æ³•ã¨ã‚¹ãƒ‘ãƒ¼ã‚¹æ³¨æ„æŠ€è¡“ãŒå°å…¥ã•ã‚Œã€è¨ˆç®—åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã¤ã¤ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã‚«ãƒãƒ¬ãƒƒã‚¸ã‚’ä¿æŒã™ã‚‹ã€‚æœ¬ç ”ç©¶ã¯ã€ã“ã‚Œã‚‰ã®é€²å±•ã‚’ä½“ç³»çš„ã«ã¾ã¨ã‚ã€åŠ¹ç‡çš„ãªæ³¨æ„ã‚’å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«çµ„ã¿è¾¼ã‚€æ–¹æ³•ã‚’åˆ†æã—ã€ç†è«–ã¨å®Ÿè·µã‚’çµ±åˆã—ãŸã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªãƒ¢ãƒ‡ãƒ«è¨­è¨ˆã®åŸºç¤ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/1950287053046022286?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p><img src="https://github.com/user-attachments/assets/df56fa40-4206-4d12-9172-39f7b36f19c7" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<a class="button" href="articles/Stability.html" target="_blank" rel="noopener noreferrer">#Stability</a>
<span class="issue_date">Issue Date: 2025-07-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2299" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Group Sequence Policy Optimization, Chujie Zheng+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Group Sequence Policy Optimization (GSPO)ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã®æ–°ã—ã„å¼·åŒ–å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å°¤åº¦ã«åŸºã¥ãé‡è¦åº¦æ¯”ã‚’ç”¨ã„ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†ã€‚GSPOã¯ã€å¾“æ¥ã®GRPOã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚ˆã‚Šã‚‚åŠ¹ç‡çš„ã§é«˜æ€§èƒ½ã§ã‚ã‚Šã€Mixture-of-Experts (MoE) ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®‰å®šåŒ–ã•ã›ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æœ€æ–°ã®Qwen3ãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦é¡•è‘—ãªæ”¹å–„ãŒè¦‹ã‚‰ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1948904443749302785?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å…¬å¼ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/alibaba_qwen/status/1949412072942612873?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>GRPOã¨GSPOã®é•ã„ã®GIF:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1953976551424634930?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<span class="issue_date">Issue Date: 2025-07-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2251" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] SWE-Perf: Can Language Models Optimize Code Performance on Real-World  Repositories?, Xinyi He+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ã‚³ãƒ¼ãƒ‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã¯é‡è¦ã§ã‚ã‚Šã€LLMsã®ãƒªãƒã‚¸ãƒˆãƒªãƒ¬ãƒ™ãƒ«ã§ã®èƒ½åŠ›ã¯æœªæ¢æ±‚ã€‚ã“ã‚Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€SWE-Perfã¨ã„ã†åˆã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å°å…¥ã€‚140ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç”¨ã„ã¦ã€LLMsã¨å°‚é–€å®¶ã®æœ€é©åŒ–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’è©•ä¾¡ã—ã€ç ”ç©¶æ©Ÿä¼šã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/sivil_taram/status/1945855374336446577?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã“ã‚Œã¾ã§ã®SWEç³»ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯Bug Fixãªã©ã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã•ã‚Œã¦ããŸãŒã€ã“ã¡ã‚‰ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆi.e., å®Ÿè¡Œæ™‚é–“ï¼‰ã‚’æ”¹å–„ã•ã›ã‚‰ã‚Œã‚‹ã‹ã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã—ã¦ã„ã‚‹ã¨ã®ã“ã¨ã€‚<br>å®Ÿéš›ã«ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰PRã‚’åé›†ã—ã€ãƒ‘ãƒƒãƒå‰å¾Œã®å®Ÿè¡Œæ™‚é–“ã‚’æ¯”è¼ƒã€‚20å›ã®runã‚’é€šã˜ã¦çµ±è¨ˆçš„ã«æœ‰æ„ãªå®Ÿè¡Œæ™‚é–“ã®å·®ãŒã‚ã‚‹ã‚‚ã®ã®ã¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ã—ã¦ã„ã‚‹ã¨ã®ã“ã¨ã€‚<br><br>Human Expertsã¯å¹³å‡10.9%ã®gainã‚’å¾—ãŸãŒã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯2.3%ã«ã¨ã©ã¾ã£ã¦ãŠã‚Šã€ã‚®ãƒ£ãƒƒãƒ—ãŒã‚ã‚‹ã¨ã®ã“ã¨ã€‚<br><br>å‚¾å‘ã¨ã—ã¦ã€LLMã¯low levelãªã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ï¼ˆç’°å¢ƒæ§‹ç¯‰, ä¾å­˜é–¢ä¿‚ã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°, importã®ãƒ­ã‚¸ãƒƒã‚¯ï¼‰ã‚’æ”¹å–„ã™ã‚‹ãŒã€Human Expertsã¯high levelãªãƒ­ã‚¸ãƒƒã‚¯ã‚„ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’æ”¹å–„ã™ã‚‹ï¼ˆe.g., ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚„ã€ãƒ‡ãƒ¼ã‚¿ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼‰ã€‚</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/memory.html" target="_blank" rel="noopener noreferrer">#memory</a>
<a class="button" href="articles/RecurrentModels.html" target="_blank" rel="noopener noreferrer">#RecurrentModels</a>
<a class="button" href="articles/RecursiveModels.html" target="_blank" rel="noopener noreferrer">#RecursiveModels</a>
<span class="issue_date">Issue Date: 2025-07-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2241" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive   Token-Level Computation, Sangmin Bae+, NeurIPS'25</a>
<span class="snippet"><span>GPT Summary</span>- Mixture-of-Recursionsï¼ˆMoRï¼‰ã¨ã„ã†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€å†å¸°å‹ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼å†…ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å…±æœ‰ã¨é©å¿œè¨ˆç®—ã‚’åŒæ™‚ã«å®Ÿç¾ã€‚MoRã¯ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å†åˆ©ç”¨ã¨ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®å†å¸°æ·±ã•ã®å‹•çš„å‰²ã‚Šå½“ã¦ã«ã‚ˆã‚Šã€ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚135Mã‹ã‚‰1.7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°FLOPsã‚’ç¶­æŒã—ã¤ã¤ã€å›°æƒ‘åº¦ã‚’ä½ä¸‹ã•ã›ã€å°‘æ•°ã‚·ãƒ§ãƒƒãƒˆç²¾åº¦ã‚’å‘ä¸Šã€‚MoRã¯å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®ã‚³ã‚¹ãƒˆã‚’æŠ‘ãˆã¤ã¤ã€å“è³ªå‘ä¸Šã«å¯„ä¸ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1945632764650533048?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1961593983114907806?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2605" target="_blank" rel="noopener noreferrer">[Paper Note] Universal Transformers, Mostafa Dehghani+, ICLR'19</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2612" target="_blank" rel="noopener noreferrer">[Paper Note] Looped Transformers for Length Generalization, Ying Fan+, ICLR'25</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2611" target="_blank" rel="noopener noreferrer">[Paper Note] Looped Transformers are Better at Learning Learning Algorithms, Liu Yang+, ICLR'24</a>
 </p>
<p>è‘—è€…ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/reza_byt/status/1980335836542677210?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/Stability.html" target="_blank" rel="noopener noreferrer">#Stability</a>
<span class="issue_date">Issue Date: 2025-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2194" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] SingLoRA: Low Rank Adaptation Using a Single Matrix, David BensaÃ¯d+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- SingLoRAã¯ã€LoRAã®ä½ãƒ©ãƒ³ã‚¯é©å¿œã‚’å†å®šå¼åŒ–ã—ã€å˜ä¸€ã®ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã¨ãã®è»¢ç½®ã®ç©ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®‰å®šæ€§ã‚’å‘ä¸Šã•ã›ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’ã»ã¼åŠæ¸›ã•ã›ã‚‹æ‰‹æ³•ã§ã™ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€å¸¸è­˜æ¨è«–ã‚¿ã‚¹ã‚¯ã§LLama 7Bã‚’ç”¨ã„ãŸãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§91.3%ã®ç²¾åº¦ã‚’é”æˆã—ã€LoRAã‚„LoRA+ã‚’ä¸Šå›ã‚‹çµæœã‚’ç¤ºã—ã¾ã—ãŸã€‚ã¾ãŸã€ç”»åƒç”Ÿæˆã«ãŠã„ã¦ã‚‚Stable Diffusionã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§é«˜ã„å¿ å®Ÿåº¦ã‚’å®Ÿç¾ã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1943701154497732765?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>LoRAã¯ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—BAã®ç©ã‚’è¨ˆç®—ã™ã‚‹ãŒã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜æŒ™å‹•ã‹ã‚‰å­¦ç¿’ã‚’ã‚¹ã‚¿ãƒ¼ãƒˆã™ã‚‹ãŸã‚ã«ã€Bã‚’zeroã§åˆæœŸåŒ–ã—ã€Aã¯ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã™ã‚‹ã€‚ã“ã®Aã¨Bã®ä¸å‡è¡¡ã•ãŒã€å‹¾é…æ¶ˆå¤±ã€çˆ†ç™ºã€ã‚ã‚‹ã„ã¯sub-optimalãªåæŸã®è¦å› ã¨ãªã£ã¦ã—ã¾ã£ã¦ã„ãŸï¼ˆinter-matrix scale conflicts)ã€‚ç‰¹ã«ã€LoRAã¯ãƒ¢ãƒ‡ãƒ«ã®widthãŒå¤§ãããªã‚‹ã¨ä¸å®‰å®šã«ãªã‚‹ã¨ã„ã†èª²é¡ŒãŒã‚ã£ãŸã€‚ã“ã®ãŸã‚ã€ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã‚’2ã¤ä½¿ã†ã®ã§ã¯ãªãã€1ã¤ã®ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ï¼ˆã¨ãã®è»¢ç½®ï¼‰ãŠã‚ˆã³optimizationã®step tã”ã¨ã«trainableãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã©ã®ç¨‹åº¦å½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ã‚’èª¿æ•´ã™ã‚‹åº¦åˆã„ã‚’æ±ºã‚ã‚‹scalar function u(t)ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—é–“ã®ä¸å‡è¡¡ã‚’è§£æ¶ˆã—ã¤ã¤ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’åŠæ¸›ã—ã€å­¦ç¿’ã®å®‰å®šæ€§ã¨æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ãŸã¨ãˆã°u(t)ã‚’å­¦ç¿’é–‹å§‹æ™‚ã«zeroã«ã™ã‚Œã°ã€å…ƒã®LoRAã«ãŠã„ã¦Bã‚’zeroã«åˆæœŸåŒ–ã™ã‚‹ã®ã¨åŒã˜æŒ™å‹•ï¼ˆã¤ã¾ã‚Šå…ƒã®ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜æŒ™å‹•ã‹ã‚‰å­¦ç¿’ã‚¹ã‚¿ãƒ¼ãƒˆãŒã§ããŸã‚Šã™ã‚‹ã€‚ã¿ãŸã„ãªæ„Ÿã˜ã ã‚ã†ã‹ï¼Ÿ<br><br><img src="https://github.com/user-attachments/assets/2dcd4ec1-59d3-43c0-ab8d-5c1c37e5ec3d" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/c73b8715-e0c8-45c8-a7fa-ea55ac8ca3ce" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/cf034dcd-37c4-48f1-a0a3-1d836db37820" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/82999835-ac1e-4380-8bd0-00d14022abf5" alt="image" loading="lazy"></p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1956" target="_blank" rel="noopener noreferrer">LoRA: Low-Rank Adaptation of Large Language Models, Edward J. Hu+, ICLR'22</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1245" target="_blank" rel="noopener noreferrer">LoRA+: Efficient Low Rank Adaptation of Large Models, Soufiane Hayou+, N/A, ICML'24</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/RLVR.html" target="_blank" rel="noopener noreferrer">#RLVR</a>
<span class="issue_date">Issue Date: 2025-07-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2184" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] First Return, Entropy-Eliciting Explore, Tianyu Zheng+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- FR3Eï¼ˆFirst Return, Entropy-Eliciting Exploreï¼‰ã¯ã€å¼·åŒ–å­¦ç¿’ã«ãŠã‘ã‚‹ä¸å®‰å®šãªæ¢ç´¢ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã®æ§‹é€ åŒ–ã•ã‚ŒãŸæ¢ç´¢ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚Šã€é«˜ä¸ç¢ºå®Ÿæ€§ã®æ„æ€æ±ºå®šãƒã‚¤ãƒ³ãƒˆã‚’ç‰¹å®šã—ã€ä¸­é–“ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã—ã¾ã™ã€‚å®Ÿé¨“çµæœã¯ã€FR3EãŒå®‰å®šã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ä¿ƒé€²ã—ã€ä¸€è²«ã—ãŸå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/f14bertolotti/status/1943201406271328524?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>RLVRã®ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã«ãŠã„ã¦ã€reasoning traceã«ãŠã‘ã‚‹å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‡ºåŠ›ã™ã‚‹éš›ã«ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒé«˜ã„éƒ¨åˆ†ã‚’ç‰¹å®šã—ï¼ˆã¤ã¾ã‚Šã€è¤‡æ•°ã®å€™è£œãŒã‚ã‚Šãƒ¢ãƒ‡ãƒ«ãŒè¿·ã£ã¦ã„ã‚‹ï¼‰ã€ãã®éƒ¨åˆ†ã«ã¤ã„ã¦ç•°ãªã‚‹æ„å›³çš„ã«ç•°ãªã‚‹ç”Ÿæˆãƒ‘ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹ã“ã¨ã§æ¢ç´¢ã‚’ä¿ƒã™ã‚ˆã†ã«ã™ã‚‹ã¨RLVRãŒã‚ˆã‚Šreliableã«ãªã‚‹ã¨ã„ã£ãŸè©±ã®ã‚ˆã†ã§ã‚ã‚‹<br><img src="https://github.com/user-attachments/assets/fc8adfcf-f6fc-4631-ba0a-04fa1401e96a" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/fabf56a8-20f3-4782-a07b-3c854f01dfd5" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/Distillation.html" target="_blank" rel="noopener noreferrer">#Distillation</a>
<span class="issue_date">Issue Date: 2025-07-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2129" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] NaturalThoughts: Selecting and Distilling Reasoning Traces for General  Reasoning Tasks, Yang Li+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®æ¨è«–ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ç”¨ã„ã¦ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹æ–¹æ³•ã‚’ä½“ç³»çš„ã«ç ”ç©¶ã€‚NaturalReasoningã«åŸºã¥ãé«˜å“è³ªãªã€ŒNaturalThoughtsã€ã‚’ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã€ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’åˆ†æã€‚ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã®æ‹¡å¤§ãŒæ€§èƒ½å‘ä¸Šã«å¯„ä¸ã—ã€å¤šæ§˜ãªæ¨è«–æˆ¦ç•¥ã‚’å¿…è¦ã¨ã™ã‚‹ä¾‹ãŒåŠ¹æœçš„ã§ã‚ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã€‚LlamaãŠã‚ˆã³Qwenãƒ¢ãƒ‡ãƒ«ã§ã®è©•ä¾¡ã«ã‚ˆã‚Šã€NaturalThoughtsãŒæ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¸Šå›ã‚Šã€STEMæ¨è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jaseweston/status/1940656092054204498?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1768" target="_blank" rel="noopener noreferrer">NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions, Weizhe Yuan+, arXiv'25</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-06-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2110" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Chain-of-Experts: Unlocking the Communication Power of  Mixture-of-Experts Models, Zihan Wang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Chain-of-Expertsï¼ˆCoEï¼‰ã¯ã€é€æ¬¡çš„ãªå°‚é–€å®¶é–“ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å°å…¥ã—ãŸæ–°ã—ã„Mixture-of-Expertsï¼ˆMoEï¼‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åå¾©çš„ã«å‡¦ç†ã™ã‚‹ã€‚å„åå¾©ã‚¹ãƒ†ãƒƒãƒ—ã§å°‚ç”¨ã®ãƒ«ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ã€å‹•çš„ãªå°‚é–€å®¶é¸æŠã‚’å¯èƒ½ã«ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®è¡¨ç¾èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚CoEã¯æ•°å­¦çš„æ¨è«–ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€å¾“æ¥ã®MoEã¨æ¯”è¼ƒã—ã¦æ¤œè¨¼æå¤±ã‚’ä½ä¸‹ã•ã›ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã™ã‚‹ã€‚åå¾©çš„æ®‹å·®æ§‹é€ ã¨å°‚é–€å®¶ã®å°‚é–€åŒ–ãŒã€ã‚ˆã‚Šè¡¨ç¾åŠ›è±Šã‹ãªçµæœã‚’ã‚‚ãŸã‚‰ã™ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1938728784351658087?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiLingual.html" target="_blank" rel="noopener noreferrer">#MultiLingual</a>
<a class="button" href="articles/COLM.html" target="_blank" rel="noopener noreferrer">#COLM</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-06-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2109" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data  Processing to Every Language, Guilherme Penedo+, COLM'25</a>
<span class="snippet"><span>GPT Summary</span>- å¤šè¨€èªLLMsã®æ€§èƒ½å‘ä¸Šã®ãŸã‚ã«ã€FineWebã«åŸºã¥ãæ–°ã—ã„äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ææ¡ˆã€‚9ã¤ã®è¨€èªã«å¯¾ã—ã¦è¨­è¨ˆé¸æŠè‚¢ã‚’æ¤œè¨¼ã—ã€éè‹±èªã‚³ãƒ¼ãƒ‘ã‚¹ãŒå¾“æ¥ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚ˆã‚Šã‚‚é«˜æ€§èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å†ãƒãƒ©ãƒ³ã‚¹æ‰‹æ³•ã‚‚å°å…¥ã—ã€1000ä»¥ä¸Šã®è¨€èªã«ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã—ãŸ20ãƒ†ãƒ©ãƒã‚¤ãƒˆã®å¤šè¨€èªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆFineWeb2ã‚’å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gui_penedo/status/1938631842720022572?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>v1<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1942" target="_blank" rel="noopener noreferrer">The FineWeb Datasets: Decanting the Web for the Finest Text Data at   Scale, Guilherme Penedo+, NeurIPS'24</a>
</p>
<p>abstã‚’è¦‹ã‚‹é™ã‚ŠFinewebã‚’å¤šè¨€èªã«æ‹¡å¼µã—ãŸæ¨¡æ§˜</p>
<p>openreview:


<a href="https://openreview.net/forum?id=jnRBe6zatP#discussion" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=jnRBe6zatP#discussion</a>


</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/OpenSource.html" target="_blank" rel="noopener noreferrer">#OpenSource</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<a class="button" href="articles/Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2105" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision  Encoders for Multimodal Learning, Xianhang Li+, ICCV'25</a>
<span class="snippet"><span>GPT Summary</span>- OpenVisionã¯ã€å®Œå…¨ã«ã‚ªãƒ¼ãƒ—ãƒ³ã§ã‚³ã‚¹ãƒˆåŠ¹æœã®é«˜ã„ãƒ“ã‚¸ãƒ§ãƒ³ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã®ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’ææ¡ˆã—ã€CLIPã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚æ—¢å­˜ã®ç ”ç©¶ã‚’åŸºã«æ§‹ç¯‰ã•ã‚Œã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã®é€²å±•ã«å®Ÿç”¨çš„ãªåˆ©ç‚¹ã‚’ç¤ºã—ã¾ã™ã€‚5.9Mã‹ã‚‰632.1Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’æä¾›ã—ã€å®¹é‡ã¨åŠ¹ç‡ã®æŸ”è»Ÿãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å®Ÿç¾ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/cihangxie/status/1920575141849030882?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>v2ã¸ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/cihangxie/status/1963297223753494832?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>äº‹å‰å­¦ç¿’æ™‚ã«text, image encoderã®contrastive lossã§å­¦ç¿’ã—ã¦ã„ãŸãŒã€text encoderã‚’ç„¡ãã—image encoderã«å…¥åŠ›ã•ã‚ŒãŸimageã‹ã‚‰captionã‚’ç”Ÿæˆã™ã‚‹caption lossã®ã¿ã«ã™ã‚‹ã“ã¨ã§æ€§èƒ½ã‚’è½ã¨ã™ã“ã¨ãªãåŠ¹ç‡ã‚’æ”¹å–„<br><br>ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒšãƒ¼ãƒ‘ãƒ¼ãŒå‡ºãŸæ¨¡æ§˜<br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2820" target="_blank" rel="noopener noreferrer">[Paper Note] OpenVision 2: A Family of Generative Pretrained Visual Encoders for
  Multimodal Learning, Yanqing Liu+, arXiv'25</a>
<p>HF:


<a href="https://huggingface.co/collections/UCSC-VLAA/openvision-681a4c27ee1f66411b4ae919" target="_blank" rel="noopener noreferrer">https://huggingface.co/collections/UCSC-VLAA/openvision-681a4c27ee1f66411b4ae919</a>


<br>pj page: 


<a href="https://ucsc-vlaa.github.io/OpenVision/" target="_blank" rel="noopener noreferrer">https://ucsc-vlaa.github.io/OpenVision/</a>


</p>
<p>CLIP, SigLIPã¨ã¯ç•°ãªã‚Šå®Œå…¨ã«ã‚ªãƒ¼ãƒ—ãƒ³ãªVision Encoder<br><img src="https://github.com/user-attachments/assets/b7c8eb07-45df-4ab3-9cd2-6b31af46e761" alt="image" loading="lazy"></p>
<p>v2ã®è§£èª¬:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1963442911108084161?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<span class="issue_date">Issue Date: 2025-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2099" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers, Weiming Ren+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- VAMBAãƒ¢ãƒ‡ãƒ«ã¯ã€Mamba-2ãƒ–ãƒ­ãƒƒã‚¯ã‚’ç”¨ã„ã¦ãƒ“ãƒ‡ã‚ªãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç·šå½¢ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›ãªã—ã§1024ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†å¯èƒ½ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’50%å‰Šæ¸›ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã‚’å€å¢—ã€‚1æ™‚é–“ã®ãƒ“ãƒ‡ã‚ªç†è§£ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯LVBenchã§4.3%ã®ç²¾åº¦å‘ä¸Šã‚’é”æˆã—ã€æ§˜ã€…ãªãƒ“ãƒ‡ã‚ªç†è§£ã‚¿ã‚¹ã‚¯ã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenhuchen/status/1938064510369280136?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<span class="issue_date">Issue Date: 2025-06-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2088" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Drop-Upcycling: Training Sparse Mixture of Experts with Partial   Re-initialization, Taishi Nakamura+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- Drop-Upcyclingæ‰‹æ³•ã‚’ææ¡ˆã—ã€MoEãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚’å‘ä¸Šã€‚äº‹å‰ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸå¯†ãªãƒ¢ãƒ‡ãƒ«ã®çŸ¥è­˜ã‚’æ´»ç”¨ã—ã¤ã¤ã€ä¸€éƒ¨ã®é‡ã¿ã‚’å†åˆæœŸåŒ–ã™ã‚‹ã“ã¨ã§å°‚é–€å®¶ã®å°‚é–€åŒ–ã‚’ä¿ƒé€²ã€‚å¤§è¦æ¨¡å®Ÿé¨“ã«ã‚ˆã‚Šã€5.9Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®MoEãƒ¢ãƒ‡ãƒ«ãŒ13Bå¯†ãªãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã®æ€§èƒ½ã‚’é”æˆã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ã‚¹ãƒˆã‚’ç´„1/4ã«å‰Šæ¸›ã€‚ã™ã¹ã¦ã®å®Ÿé¨“ãƒªã‚½ãƒ¼ã‚¹ã‚’å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>OpenReview:


<a href="https://openreview.net/forum?id=gx1wHnf5Vp" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=gx1wHnf5Vp</a>


</p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1546" target="_blank" rel="noopener noreferrer">Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints, Aran Komatsuzaki+, ICLR'23</a>
</p>
<p>ææ¡ˆæ‰‹æ³•ã®å…¨ä½“åƒã¨Diversity re-initializationã®æ¦‚è¦ã€‚å…ƒã®Upcyclingã§ã¯å…¨ã¦identicalãªé‡ã¿ã§replicateã•ã‚Œã¦ã„ãŸãŸã‚ã€ã“ã‚ŒãŒå€‹ã€…ã®expertãŒlong termã§ã®å­¦ç¿’ã§ç‰¹åŒ–ã™ã‚‹ã“ã¨ã®å¦¨ã’ã«ãªã‚Šã€æœ€çµ‚çš„ã«æœ€å¤§é™ã®capabilityã‚’ç™ºæ®ã§ããšã€åæŸãŒé…ã„è¦å› ã¨ãªã£ã¦ã„ãŸã€‚ã“ã‚Œã‚’ã€Upcyclingã—ãŸé‡ã¿ã®ã†ã¡ã€ä¸€éƒ¨ã®indexã®ã¿ã‚’å†åˆæœŸåŒ–ã™ã‚‹ã“ã¨ã§ã€replicateå…ƒã®çŸ¥è­˜ã‚’ä¿æŒã—ã¤ã¤ã€expertsã®å¤šæ§˜æ€§ã‚’é«˜ã‚ã‚‹ã“ã¨ã§è§£æ±ºã™ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/46ec75a2-30b1-4f48-9f21-cf5f6e30df95" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/ef3c66b2-32a5-46ab-bb31-828fb4570b53" alt="image" loading="lazy"><br><br>ææ¡ˆæ‰‹æ³•ã¯ä»»æ„ã®activation functioné©ç”¨å¯èƒ½ã€‚ä»Šå›ã¯FFN Layerã®activation functionã¨ã—ã¦ä¸€èˆ¬çš„ãªSwiGLUã‚’æ¡ç”¨ã—ãŸå ´åˆã§èª¬æ˜ã—ã¦ã„ã‚‹ã€‚<br><br>Drop-Upcyclingã®æ‰‹æ³•ã¨ã—ã¦ã¯ã€é€šå¸¸ã®Upcyclingã¨åŒæ§˜ã€FFN Layerã®é‡ã¿ã‚’nå€‹ã®expertsã®æ•°ã ã‘replicateã™ã‚‹ã€‚ãã®å¾Œã€re-initializationã‚’å®Ÿæ–½ã™ã‚‹æ¯”ç‡rã«åŸºã¥ã„ã¦ã€[1, intermediate size d_f]ã®ç¯„å›²ã‹ã‚‰r*d_få€‹ã®indexã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã€‚æœ€çµ‚çš„ã«SwiGLUã€ãŠã‚ˆã³FFNã«ãŠã‘ã‚‹3ã¤ã®Weight W_{gate, up, down}ã«ãŠã„ã¦ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸindexã¨å¯¾å¿œã™ã‚‹row/columnã¨å¯¾å¿œã™ã‚‹é‡ã¿ã‚’re-initializeã™ã‚‹ã€‚<br><br>re-initializeã™ã‚‹éš›ã«ã¯ã€å„W_{gate, up, down}ä¸­ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚ŒãŸindexã¨å¯¾å¿œã™ã‚‹ãƒ™ã‚¯ãƒˆãƒ«ã®å¹³å‡ã¨åˆ†æ•£ã‚’ãã‚Œãã‚Œç‹¬ç«‹ã—ã¦æ±‚ã‚ã€ãã‚Œã‚‰ã®å¹³å‡ã¨åˆ†æ•£ã‚’æŒã¤æ­£è¦åˆ†å¸ƒã‹ã‚‰ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã€‚<br><br>å­¦ç¿’ã®åˆæœŸã‹ã‚‰é«˜ã„æ€§èƒ½ã‚’ç™ºæ®ã—ã€long termã§ã®æ€§èƒ½ã‚‚å‘ä¸Šã—ã¦ã„ã‚‹ã€‚ã¾ãŸã€learning curveã®å½¢çŠ¶ã‚‚scratchã‹ã‚‰å­¦ç¿’ã—ãŸå ´åˆã¨åŒæ§˜ã®å½¢çŠ¶ã¨ãªã£ã¦ãŠã‚Šã€çŸ¥è­˜ã®è»¢ç§»ã¨expertsã®specializationãŒã†ã¾ãé€²ã‚“ã ã“ã¨ãŒç¤ºå”†ã•ã‚Œã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/945e5ae5-05cd-4117-80e8-078b47f0e53c" alt="image" loading="lazy"></p>
<p>è§£èª¬:


<a href="https://llm-jp.nii.ac.jp/news/post-566/" target="_blank" rel="noopener noreferrer">https://llm-jp.nii.ac.jp/news/post-566/</a>


</p></span><br><br>
<a class="button" href="articles/RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/InformationRetrieval.html" target="_blank" rel="noopener noreferrer">#InformationRetrieval</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/RepresentationLearning.html" target="_blank" rel="noopener noreferrer">#RepresentationLearning</a>
<span class="issue_date">Issue Date: 2025-06-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2087" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] NEAR$^2$: A Nested Embedding Approach to Efficient Product Retrieval and  Ranking, Shenbin Qian+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Eã‚³ãƒãƒ¼ã‚¹æƒ…å ±æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ„å›³ã‚’æ­£ç¢ºã«ç†è§£ã—ã¤ã¤ã€å¤§è¦æ¨¡ãªå•†å“ã‚«ã‚¿ãƒ­ã‚°ã‚’åŠ¹ç‡çš„ã«å‡¦ç†ã™ã‚‹ã“ã¨ãŒé›£ã—ã„ã€‚æœ¬è«–æ–‡ã§ã¯ã€NEAR$^2$ã¨ã„ã†ãƒã‚¹ãƒˆã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã€æ¨è«–æ™‚ã®åŸ‹ã‚è¾¼ã¿ã‚µã‚¤ã‚ºã‚’æœ€å¤§12å€åŠ¹ç‡åŒ–ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ã‚¹ãƒˆã‚’å¢—ã‚„ã•ãšã«ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã®ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ã•ã¾ã–ã¾ãªIRèª²é¡Œã«å¯¾ã—ã¦ç•°ãªã‚‹æå¤±é–¢æ•°ã‚’ç”¨ã„ã¦æ¤œè¨¼ã—ãŸçµæœã€æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚å°ã•ãªåŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã§ã®æ€§èƒ½å‘ä¸Šã‚’é”æˆã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_reachsumit/status/1937697219387490566?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2025-06-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2085" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Mercury: Ultra-Fast Language Models Based on Diffusion, Inception Labs+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æ–°ã—ã„æ‹¡æ•£å‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«Mercuryã‚’ç™ºè¡¨ã€‚ç‰¹ã«ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å‘ã‘ã®Mercury Coderã¯ã€Miniã¨Smallã®2ã‚µã‚¤ã‚ºã§æä¾›ã•ã‚Œã€é€Ÿåº¦ã¨å“è³ªã§æœ€å…ˆç«¯ã‚’é”æˆã€‚ç‹¬ç«‹è©•ä¾¡ã§ã¯ã€Mercury Coder MiniãŒ1109ãƒˆãƒ¼ã‚¯ãƒ³/ç§’ã€SmallãŒ737ãƒˆãƒ¼ã‚¯ãƒ³/ç§’ã‚’è¨˜éŒ²ã—ã€ä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã€‚ã•ã‚‰ã«ã€å®Ÿä¸–ç•Œã§ã®æ¤œè¨¼çµæœã‚„å…¬é–‹APIã€ç„¡æ–™ãƒ—ãƒ¬ã‚¤ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚‚æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/arankomatsuzaki/status/1937360864262389786?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆï¼ˆãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆé€Ÿåº¦ï¼‰ãŒã€SoTAã‚‰ã—ã„dLLMãƒ¢ãƒ‡ãƒ«</p>
<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1938026627642101858?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<span class="issue_date">Issue Date: 2025-06-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2053" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Wait, We Don't Need to "Wait" Removing Thinking Tokens Improves  Reasoning Efficiency, Chenlong Wang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- è‡ªå·±åçœã‚’æŠ‘åˆ¶ã™ã‚‹ã€ŒNoWaitã€ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã€æ¨è«–ã®åŠ¹ç‡ã‚’å‘ä¸Šã€‚10ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å¤§27%-51%ã®æ€è€ƒã®é€£é–ã®é•·ã•ã‚’å‰Šæ¸›ã—ã€æœ‰ç”¨æ€§ã‚’ç¶­æŒã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«æ¨è«–ã®ãŸã‚ã®åŠ¹æœçš„ãªã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span><p>Wait, Hmmã¨ã„ã£ãŸlong CoTã‚’èª˜å°ã™ã‚‹ã‚ˆã†ãªtokenã‚’æŠ‘åˆ¶ã™ã‚‹ã“ã¨ã§ã€Accã¯ã»ã¼å¤‰ã‚ã‚‰ãšã«ç”Ÿæˆã•ã‚Œã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å‰Šæ¸›å¯èƒ½ã€ã¨ã„ã£ãŸå›³ã«è¦‹ãˆã‚‹ã€‚Reasoningãƒ¢ãƒ‡ãƒ«ã§ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é€Ÿåº¦ã‚’å‘ä¸Šã—ãŸã„å ´åˆã«åŠ¹æœãŒã‚ã‚Šãã†ã€‚<br><img src="https://github.com/user-attachments/assets/c0abd2b4-f019-435e-b72f-f588fa0eb782" alt="image" loading="lazy"></p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1935130111608492060?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<span class="issue_date">Issue Date: 2025-06-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2048" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path  Lengths in LLMs, Roy Eisenstadt+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- LLMã®æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ã«ãŠã‘ã‚‹æ€è€ƒæ®µéšã®é•·ã•ã‚’èª¿æ•´ã™ã‚‹ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æ¢æ±‚ã€‚é€²æ—ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€å¯è¦–åŒ–ã™ã‚‹ã“ã¨ã§è¨ˆç”»ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’æ˜ã‚‰ã‹ã«ã—ã€ä¸è¦ãªã‚¹ãƒ†ãƒƒãƒ—ã‚’æ¸›ã‚‰ã™ã€Œã‚ªãƒ¼ãƒãƒ¼ã‚¯ãƒ­ãƒƒã‚­ãƒ³ã‚°ã€æ‰‹æ³•ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è€ƒãˆã™ãã‚’è»½æ¸›ã—ã€å›ç­”ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã€æ¨è«–ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’æ¸›å°‘ã•ã›ã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1934357202619310559?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-06-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2035" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Resa: Transparent Reasoning Models via SAEs, Shangshang Wang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Resaã¨ã„ã†1.5Bã®æ¨è«–ãƒ¢ãƒ‡ãƒ«ç¾¤ã‚’ææ¡ˆã—ã€åŠ¹ç‡çš„ãªã‚¹ãƒ‘ãƒ¼ã‚¹ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆSAE-Tuningï¼‰æ‰‹æ³•ã‚’ç”¨ã„ã¦è¨“ç·´ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€97%ä»¥ä¸Šã®æ¨è«–æ€§èƒ½ã‚’ä¿æŒã—ã¤ã¤ã€è¨“ç·´ã‚³ã‚¹ãƒˆã‚’2000å€ä»¥ä¸Šå‰Šæ¸›ã—ã€è¨“ç·´æ™‚é–“ã‚’450å€ä»¥ä¸ŠçŸ­ç¸®ã€‚è»½ã„RLè¨“ç·´ã‚’æ–½ã—ãŸãƒ¢ãƒ‡ãƒ«ã§é«˜ã„æ¨è«–æ€§èƒ½ã‚’å®Ÿç¾ã—ã€æŠ½å‡ºã•ã‚ŒãŸæ¨è«–èƒ½åŠ›ã¯ä¸€èˆ¬åŒ–å¯èƒ½ã‹ã¤ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–å¯èƒ½ã§ã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚å…¨ã¦ã®æˆæœç‰©ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1933101904529363112?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è‘—è€…ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/upupwang/status/1933207676663865482?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è«–æ–‡ä¸­ã§åˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹Source Modelã®ä¸€ã¤:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1935" target="_blank" rel="noopener noreferrer">[Paper Note] Tina: Tiny Reasoning Models via LoRA, Shangshang Wang+, arXiv'25</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<span class="issue_date">Issue Date: 2025-06-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2033" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Text-to-LoRA: Instant Transformer Adaption, Rujikorn Charakorn+, ICML'25</a>
<span class="snippet"><span>GPT Summary</span>- Text-to-LoRAï¼ˆT2Lï¼‰ã¯ã€è‡ªç„¶è¨€èªã«ã‚ˆã‚‹èª¬æ˜ã«åŸºã¥ã„ã¦å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã‚’è¿…é€Ÿã«é©å¿œã•ã›ã‚‹æ‰‹æ³•ã§ã€å¾“æ¥ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®é«˜ã‚³ã‚¹ãƒˆã¨æ™‚é–“ã‚’å…‹æœã—ã¾ã™ã€‚T2Lã¯ã€LoRAã‚’å®‰ä¾¡ãªãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã§æ§‹ç¯‰ã™ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã€ã‚¿ã‚¹ã‚¯ç‰¹æœ‰ã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¨åŒç­‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã¾ã™ã€‚ã¾ãŸã€æ•°ç™¾ã®LoRAã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åœ§ç¸®ã—ã€æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã§ä¸€èˆ¬åŒ–å¯èƒ½ã§ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®å°‚é–€åŒ–ã‚’æ°‘ä¸»åŒ–ã—ã€è¨ˆç®—è¦ä»¶ã‚’æœ€å°é™ã«æŠ‘ãˆãŸè¨€èªãƒ™ãƒ¼ã‚¹ã®é©å¿œã‚’å®Ÿç¾ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/roberttlange/status/1933074366603919638?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãªã€ãªã‚‹ã»ã©ã€ã“ã‚“ãªæ‰‹ãŒâ€¦ï¼</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Alignment.html" target="_blank" rel="noopener noreferrer">#Alignment</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Safety.html" target="_blank" rel="noopener noreferrer">#Safety</a>
<span class="issue_date">Issue Date: 2025-06-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2027" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety  Assurance, Ruizhong Qiu+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æ—¢å­˜ã®LLMã®å®‰å…¨ä¿è¨¼ç ”ç©¶ã¯ä¸»ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ®µéšã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã‚‹ãŒã€è„±ç„æ”»æ’ƒã«å¯¾ã—ã¦è„†å¼±ã§ã‚ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚æœ¬ç ”ç©¶ã§ã¯ã€æ¨è«–ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’ç”¨ã„ãŸæ–°ãŸãªå®‰å…¨æ€§å‘ä¸Šæ‰‹æ³•SAFFRONã‚’ææ¡ˆã—ã€è¨ˆç®—ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å‰Šæ¸›ã™ã‚‹å¤šåˆ†å²å ±é…¬ãƒ¢ãƒ‡ãƒ«ï¼ˆMRMï¼‰ã‚’å°å…¥ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å ±é…¬ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã®æ•°ã‚’æ¸›ã‚‰ã—ã€æ¢ç´¢-åŠ¹ç‡æ€§ã®ã‚¸ãƒ¬ãƒ³ãƒã‚’å…‹æœã™ã‚‹ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šæ‰‹æ³•ã®æœ‰åŠ¹æ€§ã‚’ç¢ºèªã—ã€è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨å®‰å…¨å ±é…¬ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gaotangli/status/1932289294657626189?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-06-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2025" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Log-Linear Attention, Han Guo+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- å¯¾æ•°ç·šå½¢æ³¨æ„ã‚’ææ¡ˆã—ã€ç·šå½¢æ³¨æ„ã®åŠ¹ç‡æ€§ã¨ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹æ³¨æ„ã®è¡¨ç¾åŠ›ã‚’ä¸¡ç«‹ã€‚å›ºå®šã‚µã‚¤ã‚ºã®éš ã‚ŒçŠ¶æ…‹ã‚’å¯¾æ•°çš„ã«æˆé•·ã™ã‚‹éš ã‚ŒçŠ¶æ…‹ã«ç½®ãæ›ãˆã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å¯¾æ•°ç·šå½¢ã«æŠ‘ãˆã‚‹ã€‚Mamba-2ã¨Gated DeltaNetã®å¯¾æ•°ç·šå½¢ãƒãƒªã‚¢ãƒ³ãƒˆãŒç·šå½¢æ™‚é–“ã®ãƒãƒªã‚¢ãƒ³ãƒˆã¨æ¯”è¼ƒã—ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã“ã¨ã‚’ç¢ºèªã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1932194773559107911?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1931432543766847887?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<span class="issue_date">Issue Date: 2025-06-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2013" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Unleashing the Reasoning Potential of Pre-trained LLMs by Critique  Fine-Tuning on One Problem, Yubo Wang+, EMNLP'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€å¼·åŠ›ãªå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®æ¨è«–èƒ½åŠ›ã‚’å¼•ãå‡ºã™ãŸã‚ã«ã€æ‰¹è©•å¾®èª¿æ•´ï¼ˆCFTï¼‰ãŒåŠ¹æœçš„ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚CFTã¯ã€å˜ä¸€ã®å•é¡Œã«å¯¾ã™ã‚‹å¤šæ§˜ãªè§£ã‚’åé›†ã—ã€æ•™å¸«LLMã«ã‚ˆã‚‹æ‰¹è©•ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹ç¯‰ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚QwenãŠã‚ˆã³Llamaãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ãŸçµæœã€æ•°å­¦ã‚„è«–ç†æ¨è«–ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§é¡•è‘—ãªæ€§èƒ½å‘ä¸Šã‚’è¦³å¯Ÿã—ã¾ã—ãŸã€‚ç‰¹ã«ã€ã‚ãšã‹5æ™‚é–“ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã€Qwen-Math-7B-CFTã¯ä»–ã®æ‰‹æ³•ã¨åŒç­‰ä»¥ä¸Šã®æˆæœã‚’ä¸Šã’ã¾ã—ãŸã€‚CFTã¯è¨ˆç®—åŠ¹ç‡ãŒé«˜ãã€ç¾ä»£ã®LLMã®æ¨è«–èƒ½åŠ›ã‚’å¼•ãå‡ºã™ãŸã‚ã®ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenhuchen/status/1930447298527670662?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1832" target="_blank" rel="noopener noreferrer">Critique Fine-Tuning: Learning to Critique is More Effective than   Learning to Imitate, Yubo Wang+, COLM'25</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1938" target="_blank" rel="noopener noreferrer">Reinforcement Learning for Reasoning in Large Language Models with One   Training Example, Yiping Wang+, NeurIPS'25</a>
</p>
<p>å‚è€ƒ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/weiliu99/status/1930826904522875309?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2025-05-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1984" target="_blank" rel="noopener noreferrer" class="title-link">dKV-Cache: The Cache for Diffusion Language Models, Xinyin Ma+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æ‹¡æ•£è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆDLMï¼‰ã®é…ã„æ¨è«–ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«ã€é…å»¶KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ææ¡ˆã€‚ã“ã‚Œã¯ã€ç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®è¡¨ç¾ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã«åŸºã¥ãã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°æˆ¦ç•¥ã§ã€2ã¤ã®ãƒãƒªã‚¢ãƒ³ãƒˆã‚’è¨­è¨ˆã€‚dKV-Cache-Decodeã¯æå¤±ã®å°‘ãªã„åŠ é€Ÿã‚’æä¾›ã—ã€dKV-Cache-Greedyã¯é«˜ã„ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’å®Ÿç¾ã€‚æœ€çµ‚çš„ã«ã€æ¨è«–é€Ÿåº¦ã‚’2ã€œ10å€å‘ä¸Šã•ã›ã€DLMã®æ€§èƒ½ã‚’å¼·åŒ–ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/arankomatsuzaki/status/1925384029718946177?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ææ¡ˆæ‰‹æ³•ã‚’é©ç”¨ã—ãŸå ´åˆã€ARãªãƒ¢ãƒ‡ãƒ«ã¨Diffusion Modelã§ã€å®Ÿéš›ã®ã¨ã“ã‚ã©ã®ç¨‹åº¦ã®decodingé€Ÿåº¦ã®å·®ãŒã‚ã‚‹ã®ã ã‚ã†ã‹ï¼Ÿãã†ã„ã£ãŸåˆ†æã¯ã–ãƒ¼ãƒ¼ã£ã¨è¦‹ãŸæ„Ÿã˜è¦‹å½“ãŸã‚‰ãªã‹ã£ãŸã‚ˆã†ã«æ€ãˆã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Scaling%20Laws.html" target="_blank" rel="noopener noreferrer">#Scaling Laws</a>
<span class="issue_date">Issue Date: 2025-05-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1981" target="_blank" rel="noopener noreferrer" class="title-link">Parallel Scaling Law for Language Models, Mouxiang Chen+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ãŠã„ã¦ã€ä¸¦åˆ—è¨ˆç®—ã‚’å¢—åŠ ã•ã›ã‚‹æ–°ã—ã„æ‰‹æ³•ã€ŒParScaleã€ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®å‰æ–¹ãƒ‘ã‚¹ã‚’ä¸¦åˆ—ã«å®Ÿè¡Œã—ã€å‡ºåŠ›ã‚’å‹•çš„ã«é›†ç´„ã™ã‚‹ã“ã¨ã§ã€æ¨è«–åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ParScaleã¯ã€å°‘ãªã„ãƒ¡ãƒ¢ãƒªå¢—åŠ ã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã§åŒç­‰ã®æ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ã—ã€æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’å†åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ã‚¹ãƒˆã‚‚å‰Šæ¸›å¯èƒ½ã€‚æ–°ã—ã„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã¯ã€ãƒªã‚½ãƒ¼ã‚¹ãŒé™ã‚‰ã‚ŒãŸçŠ¶æ³ã§ã®å¼·åŠ›ãªãƒ¢ãƒ‡ãƒ«å±•é–‹ã‚’ä¿ƒé€²ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1924959706331939099?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/405" target="_blank" rel="noopener noreferrer">[Paper Note] Prefix-Tuning: Optimizing Continuous Prompts for Generation, Xiang Lisa Li+, arXiv'21, 2021.01</a>
<br><br>ã¨è€ƒãˆæ–¹ãŒä¼¼ã¦ã„ã‚‹</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<span class="issue_date">Issue Date: 2025-05-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1980" target="_blank" rel="noopener noreferrer" class="title-link">AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via  Reinforcement Learning, Chenwei Lou+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- AdaCoTï¼ˆAdaptive Chain-of-Thoughtï¼‰ã¯ã€LLMsãŒæ¨è«–ã‚’é©å¿œçš„ã«è¡Œã†æ–°ã—ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã€CoTã®å‘¼ã³å‡ºã—ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’æœ€é©åŒ–ã—ã¾ã™ã€‚å¼·åŒ–å­¦ç¿’ã‚’ç”¨ã„ã¦ã€ã‚¯ã‚¨ãƒªã®è¤‡é›‘ã•ã«åŸºã¥ã„ã¦CoTã®å¿…è¦æ€§ã‚’åˆ¤æ–­ã—ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã—ã¾ã™ã€‚å®Ÿé¨“ã§ã¯ã€AdaCoTãŒCoTãƒˆãƒªã‚¬ãƒ¼ç‡ã‚’3.18%ã«ä½ä¸‹ã•ã›ã€å¿œç­”ãƒˆãƒ¼ã‚¯ãƒ³ã‚’69.06%æ¸›å°‘ã•ã›ã¤ã¤ã€é«˜ã„æ€§èƒ½ã‚’ç¶­æŒã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>RLã®Rewardã«ãŠã„ã¦ã€bassã®ãƒªãƒ¯ãƒ¼ãƒ‰ã ã‘ã§ãªãã€<br>- reasoningã‚’ãªãã—ãŸå ´åˆã®ãƒšãƒŠãƒ«ãƒ†ã‚£é …<br>- reasoningã‚’overuseã—ãŸå ´åˆã®ãƒšãƒŠãƒ«ãƒ†ã‚£é …<br>- formattingã«é–¢ã™ã‚‹ãƒšãƒŠãƒ«ãƒ†ã‚£é …<br>ã‚’è¨­å®šã—ã€reasoningã®æœ‰ç„¡ã‚’é©åˆ‡ã«åˆ¤æ–­ã§ããŸå ´åˆã«rewardãŒæœ€å¤§åŒ–ã•ã‚Œã‚‹ã‚ˆã†ãªå½¢ã«ã—ã¦ã„ã‚‹ã€‚(2.2.2)<br><br>ãŒã€multi-stageã®RLã§ã¯ï¼ˆstageã”ã¨ã«åˆ©ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å¤‰æ›´ã™ã‚‹ãŒï¼‰ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®åˆ†å¸ƒã«ã¯æ­ªã¿ãŒã‚ã‚Šã€ãŸã¨ãˆã°å¸¸ã«CoTãŒæœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚‚å­˜åœ¨ã—ã¦ãŠã‚Šï¼ˆæ•°å­¦ã«é–¢ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãªã©ï¼‰ã€ãã®å ´åˆå¸¸ã«CoTã‚’ã™ã‚‹ã‚ˆã†ãªåˆ†å¸ƒã‚’å­¦ç¿’ã—ã¦ã—ã¾ã„ã€AdaptiveãªCoT decisionãŒå´©å£Šã—ãŸã‚Šã€ä¸å®‰å®šã«ãªã£ã¦ã—ã¾ã†ï¼ˆdecision boundary collapseã¨å‘¼ã¶ï¼‰ã€‚ç‰¹ã«ã“ã‚ŒãŒfinal stageã§èµ·ãã‚‹ã¨æœ€æ‚ªã§ã€ã“ã‚Œã¾ã§Adaptiveã«CoTã•ã‚Œã‚‹ã‚ˆã†å­¦ç¿’ã•ã‚Œã¦ããŸã‚‚ã®ãŒå…¨ã¦å´©å£Šã—ã¦ã—ã¾ã†ã€‚ã“ã‚Œã‚’é˜²ããŸã‚ã«ã€Selective Loss Maskingã¨ã„ã†lossã‚’å°å…¥ã—ã¦ã„ã‚‹ã€‚å…·ä½“çš„ã«ã¯ã€decision token [^1]ã®lossã¸ã®è²¢çŒ®ã‚’ãƒã‚¹ã‚­ãƒ³ã‚°ã™ã‚‹ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã§ã€CoTãŒç”Ÿã˜ã‚‹ratioã«ãƒã‚¤ã‚¢ã‚¹ãŒã‹ã‹ã‚‰ãªã„ã‚ˆã†ã«ã™ã‚‹ã€‚ä»Šå›ã¯ã€Decision tokenã¨ã—ã¦ã€`<think>`ãƒˆãƒ¼ã‚¯ãƒ³ç›´å¾Œã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’decision tokenã¨ã¿ãªã—ã€lossã«å¯¾ã™ã‚‹è²¢çŒ®ã‚’ãƒã‚¹ã‚¯ã—ã¦ã„ã‚‹ï¼ˆSelective Loss Maskingï¼‰ã€‚<br><br>[^1]: CoTã™ã‚‹ã‹ã©ã†ã‹ã¯å¤šãã®å ´åˆã“ã®Decision Tokenã«ã‚ˆã£ã¦æ±ºã¾ã‚‹ã€ã¨ã„ã£ãŸã“ã¨ãŒã©ã£ã‹ã®ç ”ç©¶ã«ç¤ºã•ã‚Œã¦ã„ãŸã¯ãš&lt;/p&gt;<p>ã„ã¤ã‹å¿…è¦ã«ãªã£ãŸã‚‰ã—ã£ã‹ã‚Šèª­ã‚€ãŒã€å…¨ã¦ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã§Selective Loss Maskingã‚’ã—ãŸã‚‰ã€SFTã§warm upã—ãŸæ®µéšã‹ã‚‰ã‚ã¾ã‚ŠCoTã®ratioãŒå¤‰åŒ–ã—ãªã„ã‚ˆã†ãªå­¦ç¿’ã®ã•ã‚Œæ–¹ã«ãªã‚‹æ°—ãŒã™ã‚‹ãŒã€ã©ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã«å¯¾ã—ã¦applyã™ã‚‹ã®ã ã‚ã†ã‹ã€‚</p>&lt;/span&gt;<br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<span class="issue_date">Issue Date: 2025-05-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1977" target="_blank" rel="noopener noreferrer" class="title-link">Insights into DeepSeek-V3: Scaling Challenges and Reflections on  Hardware for AI Architectures, Chenggang Zhao+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- DeepSeek-V3ã¯ã€2,048å°ã®NVIDIA H800 GPUã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢åˆ¶ç´„ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã®å…±åŒè¨­è¨ˆã‚’ç¤ºã™ã€‚ãƒ¡ãƒ¢ãƒªåŠ¹ç‡å‘ä¸Šã®ãŸã‚ã®ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰æ½œåœ¨æ³¨æ„ã‚„ã€è¨ˆç®—ã¨é€šä¿¡ã®æœ€é©åŒ–ã‚’å›³ã‚‹å°‚é–€å®¶ã®æ··åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€FP8æ··åˆç²¾åº¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãªã©ã®é©æ–°ã‚’å¼·èª¿ã€‚ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«åŸºã¥ãå°†æ¥ã®æ–¹å‘æ€§ã«ã¤ã„ã¦è­°è«–ã—ã€AIãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«å¿œãˆã‚‹ãŸã‚ã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã¨ãƒ¢ãƒ‡ãƒ«ã®å…±åŒè¨­è¨ˆã®é‡è¦æ€§ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/deedydas/status/1924512147947848039?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span></think></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/Test-Time%20Scaling.html" target="_blank" rel="noopener noreferrer">#Test-Time Scaling</a>
<a class="button" href="articles/Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<a class="button" href="articles/Verification.html" target="_blank" rel="noopener noreferrer">#Verification</a>
<a class="button" href="articles/SpeculativeDecoding.html" target="_blank" rel="noopener noreferrer">#SpeculativeDecoding</a>
<span class="issue_date">Issue Date: 2025-05-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1961" target="_blank" rel="noopener noreferrer" class="title-link">Faster Cascades via Speculative Decoding, Harikrishna Narasimhan+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ã¨æ¨æ¸¬ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ¨è«–åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã§ã‚ã‚Šã€ç•°ãªã‚‹ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æŒã¤ã€‚ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ã¯é›£ã—ã„å…¥åŠ›ã«å¯¾ã—ã¦å¤§ããªãƒ¢ãƒ‡ãƒ«ã‚’é…å»¶çš„ã«ä½¿ç”¨ã—ã€æ¨æ¸¬ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¯ä¸¦è¡Œæ¤œè¨¼ã§å¤§ããªãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã™ã‚‹ã€‚æ–°ãŸã«ææ¡ˆã™ã‚‹æ¨æ¸¬ã‚«ã‚¹ã‚±ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æŠ€è¡“ã¯ã€ä¸¡è€…ã®åˆ©ç‚¹ã‚’çµ„ã¿åˆã‚ã›ã€æœ€é©ãªé…å»¶ãƒ«ãƒ¼ãƒ«ã‚’ç‰¹å®šã™ã‚‹ã€‚å®Ÿé¨“çµæœã¯ã€ææ¡ˆæ‰‹æ³•ãŒã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ãŠã‚ˆã³æ¨æ¸¬ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚ˆã‚Šã‚‚å„ªã‚ŒãŸã‚³ã‚¹ãƒˆå“è³ªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1922059828429832259?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>OpenReview: 


<a href="https://openreview.net/forum?id=vo9t20wsmd" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=vo9t20wsmd</a>


</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-05-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1944" target="_blank" rel="noopener noreferrer" class="title-link">Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon   Pretraining Dataset, Dan Su+, ACL'25</a>
<span class="snippet"><span>GPT Summary</span>- FineWeb-Eduã¨DCLMã¯ã€ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã«ã‚ˆã‚Šãƒ‡ãƒ¼ã‚¿ã®90%ã‚’å‰Šé™¤ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«é©ã•ãªããªã£ãŸã€‚è‘—è€…ã¯ã€ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«åˆ†é¡å™¨ã‚„åˆæˆãƒ‡ãƒ¼ã‚¿ã®è¨€ã„æ›ãˆã‚’ç”¨ã„ã¦ã€ç²¾åº¦ã¨ãƒ‡ãƒ¼ã‚¿é‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æ”¹å–„ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã€‚1Tãƒˆãƒ¼ã‚¯ãƒ³ã§8Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€DCLMã«å¯¾ã—ã¦MMLUã‚’5.6ãƒã‚¤ãƒ³ãƒˆå‘ä¸Šã•ã›ãŸã€‚æ–°ã—ã„6.3Tãƒˆãƒ¼ã‚¯ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€DCLMã¨åŒç­‰ã®æ€§èƒ½ã‚’æŒã¡ãªãŒã‚‰ã€4å€ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’å«ã¿ã€é•·ãƒˆãƒ¼ã‚¯ãƒ³ãƒ›ãƒ©ã‚¤ã‚ºãƒ³ã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚15Tãƒˆãƒ¼ã‚¯ãƒ³ã®ãŸã‚ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸ8Bãƒ¢ãƒ‡ãƒ«ã¯ã€Llama 3.1ã®8Bãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-05-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1938" target="_blank" rel="noopener noreferrer" class="title-link">Reinforcement Learning for Reasoning in Large Language Models with One   Training Example, Yiping Wang+, NeurIPS'25</a>
<span class="snippet"><span>GPT Summary</span>- 1-shot RLVRã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€LLMã®æ•°å­¦çš„æ¨è«–èƒ½åŠ›ãŒå¤§å¹…ã«å‘ä¸Šã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚Qwen2.5-Math-1.5Bãƒ¢ãƒ‡ãƒ«ã¯ã€MATH500ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒ36.0%ã‹ã‚‰73.6%ã«æ”¹å–„ã•ã‚Œã€ä»–ã®æ•°å­¦çš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã‚‚åŒæ§˜ã®å‘ä¸ŠãŒè¦‹ã‚‰ã‚ŒãŸã€‚1-shot RLVRä¸­ã«ã¯ã€ã‚¯ãƒ­ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ä¸€èˆ¬åŒ–ã‚„æŒç¶šçš„ãªãƒ†ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®æ”¹å–„ãŒè¦³å¯Ÿã•ã‚Œã€ãƒãƒªã‚·ãƒ¼å‹¾é…æå¤±ãŒä¸»ãªè¦å› ã§ã‚ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã®è¿½åŠ ã‚‚é‡è¦ã§ã€çµæœå ±é…¬ãªã—ã§ã‚‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‘ä¸Šã—ãŸã€‚ã“ã‚Œã‚‰ã®æˆæœã¯ã€RLVRã®ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã«é–¢ã™ã‚‹ã•ã‚‰ãªã‚‹ç ”ç©¶ã‚’ä¿ƒé€²ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p><img src="https://github.com/user-attachments/assets/03cd9200-7fed-4c6d-a5a6-2379d2c8950a" alt="image" loading="lazy"></p>
<p>ä¸‹è¨˜ãƒã‚¹ãƒˆã§Qwenã«å¯¾ã—ã¦promptã‚’é©åˆ‡ã«ä¸ãˆã‚‹ã“ã¨ã§ã€è¿½åŠ ã®post trainingç„¡ã—ã§é«˜ã„æ•°å­¦ã«é–¢ã™ã‚‹èƒ½åŠ›ã‚’å¼•ãå‡ºã›ãŸã¨ã„ã†æƒ…å ±ãŒã‚ã‚‹ã€‚ãŠãã‚‰ãäº‹å‰å­¦ç¿’æ™‚ã«æ•°å­¦ã®QAãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã£ã¦ç¶™ç¶šäº‹å‰å­¦ç¿’ã•ã‚Œã¦ãŠã‚Šã€ã“ã®èƒ½åŠ›ã¯ãã®éš›ã«èº«ã«ã¤ã„ã¦ã„ã‚‹ãŸã‚ã€æ•°å­¦ã«å¯¾ã™ã‚‹é«˜ã„èƒ½åŠ›ã¯å®Ÿã¯ç°¡å˜ã«å¼•ãå‡ºã™ã“ã¨ãŒã§ãã‚‹ã®ã‹ã‚‚ã—ã‚Œãªã„ï¼ˆã ã‹ã‚‰1ã‚µãƒ³ãƒ—ãƒ«ã§ã‚‚æ€§èƒ½ãŒå‘ä¸Šã—ãŸã®ã§ã¯ãªã„ã‹ï¼Ÿï¼‰ã¨ã„ã£ãŸè€ƒå¯ŸãŒã‚ã‚‹ã€‚<br><br>å‚è€ƒ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/weiliu99/status/1930826904522875309?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2011" target="_blank" rel="noopener noreferrer">[Paper Note] ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in  Large Language Models, Mingjie Liu+, NeurIPS'25</a>
<br><br>ã¨ã¯ã©ã®ã‚ˆã†ãªé–¢ä¿‚æ€§ãŒã‚ã‚‹ã ã‚ã†ã‹ï¼Ÿ</p>
<p>è‘—è€…ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/ypwang61/status/1968834328508379563?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/GRPO.html" target="_blank" rel="noopener noreferrer">#GRPO</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-05-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1935" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Tina: Tiny Reasoning Models via LoRA, Shangshang Wang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Tinaã¯ã€ã‚³ã‚¹ãƒˆåŠ¹ç‡ã‚ˆãå¼·åŠ›ãªæ¨è«–èƒ½åŠ›ã‚’å®Ÿç¾ã™ã‚‹å°å‹ã®æ¨è«–ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã§ã‚ã‚Šã€1.5Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«å¼·åŒ–å­¦ç¿’ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§é«˜ã„æ¨è«–æ€§èƒ½ã‚’ç¤ºã™ã€‚Tinaã¯ã€å¾“æ¥ã®SOTAãƒ¢ãƒ‡ãƒ«ã¨ç«¶äº‰åŠ›ãŒã‚ã‚Šã€AIME24ã§20%ä»¥ä¸Šã®æ€§èƒ½å‘ä¸Šã‚’é”æˆã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ã‚¹ãƒˆã¯ã‚ãšã‹9ãƒ‰ãƒ«ã§260å€ã®ã‚³ã‚¹ãƒˆå‰Šæ¸›ã‚’å®Ÿç¾ã€‚LoRAã‚’é€šã˜ãŸåŠ¹ç‡çš„ãªRLæ¨è«–ã®åŠ¹æœã‚’æ¤œè¨¼ã—ã€ã™ã¹ã¦ã®ã‚³ãƒ¼ãƒ‰ã¨ãƒ¢ãƒ‡ãƒ«ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rasbt/status/1920107023980462575?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ï¼ˆãŠãã‚‰ãï¼‰Reasoningãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ã€LoRAã¨RLã‚’çµ„ã¿åˆã‚ã›ã¦ã€reasoningèƒ½åŠ›ã‚’å‘ä¸Šã•ã›ãŸåˆã‚ã¦ã®ç ”ç©¶</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/Quantization.html" target="_blank" rel="noopener noreferrer">#Quantization</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<span class="issue_date">Issue Date: 2025-04-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1898" target="_blank" rel="noopener noreferrer" class="title-link">BitNet b1.58 2B4T Technical Report, Shuming Ma+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- BitNet b1.58 2B4Tã¯ã€20å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®1ãƒ“ãƒƒãƒˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã€4å…†ãƒˆãƒ¼ã‚¯ãƒ³ã§è¨“ç·´ã•ã‚Œã¾ã—ãŸã€‚è¨€èªç†è§£ã‚„æ•°å­¦çš„æ¨è«–ãªã©ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§è©•ä¾¡ã•ã‚Œã€åŒã‚µã‚¤ã‚ºã®ãƒ•ãƒ«ãƒ—ãƒ¬ã‚·ã‚¸ãƒ§ãƒ³LLMã¨åŒç­‰ã®æ€§èƒ½ã‚’ç¤ºã—ã¤ã¤ã€è¨ˆç®—åŠ¹ç‡ãŒå‘ä¸Šã—ã¦ã„ã¾ã™ã€‚ãƒ¡ãƒ¢ãƒªã€ã‚¨ãƒãƒ«ã‚®ãƒ¼æ¶ˆè²»ã€ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒå‰Šæ¸›ã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¯Hugging Faceã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1912783876365177235?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>åœ§å€’çš„çœãƒ¡ãƒ¢ãƒªã‹ã¤cpuã§ã®inferenceé€Ÿåº¦ã‚‚æ—©ãã†<br><img src="https://github.com/user-attachments/assets/dacf05e4-9cb3-48b4-9a98-532f7245eb8e" alt="image" loading="lazy"></p>
<p>- ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯Transformerã‚’åˆ©ç”¨<br>- Linear layerã¨ã—ã¦BitLinear Layerã‚’åˆ©ç”¨<br>  - é‡ã¿ã¯{1, 0, -1}ã®3å€¤ã‚’ã¨ã‚‹<br>  - activationã¯8bitã®integerã«é‡å­åŒ–<br>  - Layer Normalizationã¯subln normalization <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1899" target="_blank" rel="noopener noreferrer">Foundation Transformers, Hongyu Wang+, PMLR'23</a>
 ã‚’åˆ©ç”¨</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-04-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1866" target="_blank" rel="noopener noreferrer" class="title-link">Scalable-Softmax Is Superior for Attention, Ken M. Nakanishi, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- SSMaxã‚’ææ¡ˆã—ã€Softmaxã®ä»£æ›¿ã¨ã—ã¦Transformerãƒ¢ãƒ‡ãƒ«ã«çµ±åˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã®é‡è¦æƒ…å ±ã®å–å¾—ãŒå‘ä¸Šã—ã€äº‹å‰å­¦ç¿’ä¸­ã®æå¤±æ¸›å°‘ãŒé€Ÿããªã‚‹ã€‚SSMaxã¯æ³¨æ„ã‚¹ã‚³ã‚¢ã‚’æ”¹å–„ã—ã€é•·ã•ã®ä¸€èˆ¬åŒ–ã‚’ä¿ƒé€²ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1863" target="_blank" rel="noopener noreferrer">Llama 4 Series, Meta, 2025.04</a>
<br><br>ã§æ¡ç”¨ã•ã‚Œã¦ã„ã‚‹æ‰‹æ³•ã§ã€ãƒ–ãƒ­ã‚°ãƒã‚¹ãƒˆä¸­ã§å¼•ç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚Long Contextã«ãªã£ãŸå ´åˆã«softmaxã®åˆ†å¸ƒãŒå‡ä¸€ã«ãªã‚‹ï¼ˆï¼é‡è¦ãªæƒ…å ±ã«attendã™ã‚‹èƒ½åŠ›ãŒå‰ŠãŒã‚Œã‚‹ï¼‰ã“ã¨ã‚’é˜²ããŸã‚ã®æ‰‹æ³•ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚</p>
<p>è§£èª¬ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/nrehiew_/status/1908613993998045534"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-04-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1847" target="_blank" rel="noopener noreferrer" class="title-link">Demystifying LLM-based Software Engineering Agents, Chunqiu Steven Xia+, FSE'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ€è¿‘ã®LLMã®é€²å±•ã«ã‚ˆã‚Šã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã‚¿ã‚¹ã‚¯ã®è‡ªå‹•åŒ–ãŒé€²ã‚“ã§ã„ã‚‹ãŒã€è¤‡é›‘ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®å¿…è¦æ€§ã«ç–‘å•ãŒç”Ÿã˜ã¦ã„ã‚‹ã€‚ã“ã‚Œã«å¯¾ã—ã€Agentlessã¨ã„ã†ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¬ã‚¹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã€ã‚·ãƒ³ãƒ—ãƒ«ãªä¸‰æ®µéšãƒ—ãƒ­ã‚»ã‚¹ã§å•é¡Œã‚’è§£æ±ºã€‚SWE-bench Liteãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€é«˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ä½ã‚³ã‚¹ãƒˆã‚’é”æˆã€‚ç ”ç©¶ã¯è‡ªå¾‹å‹ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢é–‹ç™ºã«ãŠã‘ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ã§è§£é‡ˆå¯èƒ½ãªæŠ€è¡“ã®å¯èƒ½æ€§ã‚’ç¤ºã—ã€ä»Šå¾Œã®ç ”ç©¶ã®æ–¹å‘æ€§ã‚’åˆºæ¿€ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬:


<a href="https://note.com/ainest/n/nac1c795e3825" target="_blank" rel="noopener noreferrer">https://note.com/ainest/n/nac1c795e3825</a>


</p>
<p>LLMã«ã‚ˆã‚‹è¨ˆç”»ã®ç«‹æ¡ˆã€ç’°å¢ƒã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã«ã‚ˆã‚‹æ„æ€æ±ºå®šãªã©ã®è¤‡é›‘ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ã¯ãªãã€Localizationï¼ˆéšå±¤çš„ã«å•é¡Œã®ã‚ã‚‹ç®‡æ‰€ã‚’åŒå®šã™ã‚‹ï¼‰ã¨Repairï¼ˆLLMã§è¤‡æ•°ã®ãƒ‘ãƒƒãƒå€™è£œã‚’ç”Ÿæˆã™ã‚‹ï¼‰ã€PatchValidation(å†ç¾ãƒ†ã‚¹ãƒˆã¨å›å¸°ãƒ†ã‚¹ãƒˆã®ä¸¡æ–¹ã‚’é€šã˜ã¦çµæœãŒè‰¯ã‹ã£ãŸãƒ‘ãƒƒãƒã‚’é¸ã¶ï¼‰ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ—ãƒ­ã‚»ã‚¹ã‚’é€šã˜ã¦Issueã‚’è§£æ±ºã™ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/6d042dfe-9780-4410-9077-b265af5456d1" alt="image" loading="lazy"><br><br>ã“ã‚Œã«ã‚ˆã‚Šã€ä½ã‚³ã‚¹ãƒˆã§é«˜ã„æ€§èƒ½ã‚’é”æˆã—ã¦ã„ã‚‹ã€ã¨ã„ã£ãŸå†…å®¹ãªæ¨¡æ§˜ã€‚<br><img src="https://github.com/user-attachments/assets/3934126f-3a4d-406c-8860-c3ed35a351c4" alt="image" loading="lazy"></p>
<p>Agentlessã¨å‘¼ã°ã‚Œæ‰‹æ³•ã ãŒã€preprintç‰ˆã«ã‚ã£ãŸã‚¿ã‚¤ãƒˆãƒ«ã®æ¥é ­è¾ã ã£ãŸåŒå‘¼ç§°ãŒproceedingç‰ˆã§ã¯ç„¡ããªã£ã¦ã„ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<span class="issue_date">Issue Date: 2025-03-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1819" target="_blank" rel="noopener noreferrer" class="title-link">Stop Overthinking: A Survey on Efficient Reasoning for Large Language  Models, Yang Sui+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€LLMsã«ãŠã‘ã‚‹åŠ¹ç‡çš„ãªæ¨è«–ã®é€²å±•ã‚’ä½“ç³»çš„ã«èª¿æŸ»ã—ã€ä»¥ä¸‹ã®ä¸»è¦ãªæ–¹å‘ã«åˆ†é¡ã—ã¾ã™ï¼š(1) ãƒ¢ãƒ‡ãƒ«ãƒ™ãƒ¼ã‚¹ã®åŠ¹ç‡çš„æ¨è«–ã€(2) æ¨è«–å‡ºåŠ›ãƒ™ãƒ¼ã‚¹ã®åŠ¹ç‡çš„æ¨è«–ã€(3) å…¥åŠ›ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ™ãƒ¼ã‚¹ã®åŠ¹ç‡çš„æ¨è«–ã€‚ç‰¹ã«ã€å†—é•·ãªå‡ºåŠ›ã«ã‚ˆã‚‹è¨ˆç®—ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’è»½æ¸›ã™ã‚‹æ–¹æ³•ã‚’æ¢æ±‚ã—ã€å°è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ¨è«–èƒ½åŠ›ã‚„è©•ä¾¡æ–¹æ³•ã«ã¤ã„ã¦ã‚‚è­°è«–ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>Reasoning Modelã«ãŠã„ã¦ã€Over Thinkingç¾è±¡ï¼ˆä¸è¦ãªreasoning stepã‚’ç”Ÿæˆã—ã¦ã—ã¾ã†ï¼‰ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã®æ‰‹æ³•ã«é–¢ã™ã‚‹Surveyã€‚<br><img src="https://github.com/user-attachments/assets/a411f2cf-2ba1-4e58-8dc7-ac1ae2ff0de6" alt="image" loading="lazy"><br><br>ä¸‹è¨˜Figure2ã‚’è¦‹ã‚‹ã¨ã‚ˆãã¾ã¨ã¾ã£ã¦ã„ã¦ã€ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’èª­ã‚€ã¨ã ã„ãŸã„åˆ†ã‹ã‚‹ã€‚ãªã‚‹ã»ã©ã€‚<br>Length Rewardã«ã¤ã„ã¦ã¯ã€<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1746" target="_blank" rel="noopener noreferrer">Demystifying Long Chain-of-Thought Reasoning in LLMs, Edward Yeo+, arXiv'25</a>
<br><br>ã§è€ƒå¯Ÿã•ã‚Œã¦ã„ã‚‹é€šã‚Šã€Reward HackingãŒèµ·ãã‚‹ã®ã§è¨­è¨ˆã®ä»•æ–¹ã«æ°—ã‚’ã¤ã‘ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚<br><br><img src="https://github.com/user-attachments/assets/fd6880bd-95e1-4ca6-9593-8ffc9390962a" alt="image" loading="lazy"></p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_reachsumit/status/1902977896685396275?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å„ã‚«ãƒ†ã‚´ãƒªã«ãŠã‘ã‚‹literatureã‚‚è¦‹ã‚„ã™ãã¾ã¨ã‚ã‚‰ã‚Œã¦ã„ã‚‹ã€‚å¿…è¦ã«å¿œã˜ã¦å‚ç…§ã—ãŸã„ã€‚<br><img src="https://github.com/user-attachments/assets/b6be0d79-35c5-45a8-878b-2b6be67c2f76" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<span class="issue_date">Issue Date: 2025-03-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1813" target="_blank" rel="noopener noreferrer" class="title-link">The First Few Tokens Are All You Need: An Efficient and Effective  Unsupervised Prefix Fine-Tuning Method for Reasoning Models, Ke Ji+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- éæ•™å¸«ã‚ã‚Šãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆUPFTï¼‰ã‚’ææ¡ˆã—ã€LLMã®æ¨è«–åŠ¹ç‡ã‚’å‘ä¸Šã€‚åˆæœŸã®ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹éƒ¨åˆ†æ–‡å­—åˆ—ã«åŸºã¥ã„ã¦è¨“ç·´ã—ã€ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã‚„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚’ä¸è¦ã«ã€‚UPFTã¯ã€æ•™å¸«ã‚ã‚Šæ‰‹æ³•ã¨åŒç­‰ã®æ€§èƒ½ã‚’ç¶­æŒã—ã¤ã¤ã€è¨“ç·´æ™‚é–“ã‚’75%ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚³ã‚¹ãƒˆã‚’99%å‰Šæ¸›ã€‚æœ€å°é™ã®éæ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§å¤§å¹…ãªæ¨è«–å‘ä¸Šã‚’å®Ÿç¾ã—ã€ãƒªã‚½ãƒ¼ã‚¹åŠ¹ç‡ã®è‰¯ã„ä»£æ›¿æ‰‹æ®µã‚’æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ–œã‚èª­ã¿ã ãŒã€reasoning traceã®å†’é ­éƒ¨åˆ†ã¯é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¦ãŠã‚Šã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãŸå¤šãã®responseã®reasoning traceã«ãŠã„ã¦å…±é€šã—ã¦ã„ã‚‹ã‚‚ã®ã¯é‡è¦ã¨ã„ã†ç›´æ„Ÿã‹ã‚‰ï¼ˆPrefix Self-Consistencyï¼‰ã€reasoning traceã®å†’é ­éƒ¨åˆ†ã‚’é©åˆ‡ã«ç”Ÿæˆã§ãã‚‹ã‚ˆã†ã«ãƒ¢ãƒ‡ãƒ«ã‚’Finetuningã™ã‚‹ã€‚å¾“æ¥ã®Rejection Samplingã‚’ç”¨ã„ãŸæ‰‹æ³•ã§ã¯ã€è¤‡æ•°ã®responseã‚’ç”Ÿæˆã•ã›ã¦ã€æœ€çµ‚çš„ãªanswerãŒæ­£è§£ã®ã‚‚ã®ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ãŸã‚æ­£è§£ãƒ©ãƒ™ãƒ«ãŒå¿…è¦ã¨ãªã‚‹ãŒã€ææ¡ˆæ‰‹æ³•ã§ã¯reasoning traceã®å†’é ­éƒ¨åˆ†ã®å…±é€šã™ã‚‹subsequenceã‚’majority voteã™ã‚‹ã ã‘ãªã®ã§ãƒ©ãƒ™ãƒ«ãŒä¸è¦ã§ã‚ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/ff3938da-dcd0-4b6c-a764-26d2e8caa63a" alt="image" loading="lazy"><br><br>reasoning prefixã‚’å­¦ç¿’ã™ã‚‹éš›ã¯ä¸‹è¨˜ã®ã‚ˆã†ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ç”¨ã„ã‚‹ã€‚ã“ã®ã¨ãã«ã€prefixã®spanã®ã¿ã‚’åˆ©ç”¨ã—ã¦å­¦ç¿’ã™ã‚‹ã“ã¨ã§å¤§å¹…ã«å­¦ç¿’æ™‚é–“ã‚’å‰Šæ¸›ã§ãã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/63aabe47-9c40-41cb-8d5a-5039900f6edc" alt="image" loading="lazy"><br><br>ã¾ãŸã€ãã®ã‚ˆã†ãªå­¦ç¿’ã‚’è¡Œã†ã¨catastrophic forgettingã®ãƒªã‚¹ã‚¯ãŒéå¸¸ã«é«˜ã„ãŒã€ã“ã‚Œã‚’é˜²ããŸã‚ã«ã€ãƒãƒ«ãƒã‚¿ã‚¹ã‚¯ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿæ–½ã™ã‚‹ã€‚å…·ä½“çš„ã«ã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®p%ã«ã¤ã„ã¦ã¯å…¨ä½“ã®reasoning traceã‚’ç”Ÿæˆã—ã¦å­¦ç¿’ã«åˆ©ç”¨ã™ã‚‹ã€‚ã“ã®ã¨ãã«ã€æœ€çµ‚çš„ãªå›ç­”ã®æ­£èª¤ã‚’æ°—ã«ã›ãštraceã‚’ç”Ÿæˆã—ã¦å­¦ç¿’ã«åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ©ãƒ™ãƒ«ãƒ•ãƒªãƒ¼ãªç‰¹æ€§ã‚’ç¶­æŒã§ãã‚‹ï¼ˆã¤ã¾ã‚Šã€ã“ã¡ã‚‰ã®ãƒ‡ãƒ¼ã‚¿ã¯è‰¯ã„reasoning traceã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªãã€ã‚ãã¾ã§catastrophic forgettingã‚’é˜²ããŸã‚ã«ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã‚ˆã†ãªtraceã‚‚ãã¡ã‚“ã¨ç”Ÿæˆã§ãã‚Œã°è‰¯ã„ã€ã¨ã„ã†æ„Ÿè¦šã ã¨æ€ã‚ã‚Œã‚‹ï¼‰ã€‚<br><img src="https://github.com/user-attachments/assets/6e83c686-5bcf-4db8-9b8a-63c39570a4dc" alt="image" loading="lazy"><br><br>Appendixã«Qwenã‚’ç”¨ã„ã¦temperature 0.7ã§16å€‹ã®responseã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€traceã®å†’é ­éƒ¨åˆ†ãŒå…±é€šã—ã¦ã„ã‚‹æ§˜å­ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚</p>
<p>ä¸‹è¨˜è«–æ–‡ã§long-CoTã‚’å­¦ç¿’ã•ã›ã‚‹éš›ã®long-CoTãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã€reasoningãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç”Ÿæˆã—ãŸtraceã¨éreasoning modelã‹ã‚‰ç”Ÿæˆã—ãŸtraceã«ã‚ˆã‚‹long-CoTãƒ‡ãƒ¼ã‚¿ã‚’æ¯”è¼ƒã—ãŸã¨ã“ã‚å‰è€…ã®æ–¹ãŒä¸€è²«ã—ã¦å­¦ç¿’æ€§èƒ½ãŒè‰¯ã‹ã£ãŸã¨ã‚ã‚‹ãŒã€ã“ã®ç ”ç©¶ã§ã‚‚reasoning traceã‚’ã¤ã‚ˆã¤ã‚ˆãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã—ãŸã‚‰æ€§èƒ½ä¸ŠãŒã‚‹ã‚“ã ã‚ã†ã‹ã€‚<br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1746" target="_blank" rel="noopener noreferrer">Demystifying Long Chain-of-Thought Reasoning in LLMs, Edward Yeo+, arXiv'25</a>
</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<a class="button" href="articles/Normalization.html" target="_blank" rel="noopener noreferrer">#Normalization</a>
<span class="issue_date">Issue Date: 2025-03-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1795" target="_blank" rel="noopener noreferrer" class="title-link">Transformers without Normalization, Jiachen Zhu+, CVPR'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€æ­£è¦åŒ–å±¤ãªã—ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãŒDynamic Tanhï¼ˆDyTï¼‰ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€åŒç­‰ã¾ãŸã¯ãã‚Œä»¥ä¸Šã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚DyTã¯ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã®ä»£æ›¿ã¨ã—ã¦æ©Ÿèƒ½ã—ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ãªã—ã§åŠ¹æœã‚’ç™ºæ®ã—ã¾ã™ã€‚å¤šæ§˜ãªè¨­å®šã§ã®å®Ÿé¨“ã«ã‚ˆã‚Šã€æ­£è¦åŒ–å±¤ã®å¿…è¦æ€§ã«å¯¾ã™ã‚‹æ–°ãŸãªæ´å¯Ÿã‚’æä¾›ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>ãªã‚“â€¦ã ã¨â€¦ã€‚LayerNormalizationã‚’ä¸‹è¨˜ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ã‚ˆã†ãªtanhã‚’ç”¨ã„ãŸè¶…çµ¶ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆparameterized thnh [Lecunæ°ãƒã‚¹ãƒˆ](



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/ylecun/status/1900610590315249833?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q)ï¼‰ã«ç½®æ›ã™ã‚‹ã ã‘ã£ã½ã„ï¼Ÿ"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><img src="https://github.com/user-attachments/assets/474d3ee4-4c08-4b00-9a41-126ca5d5207e" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/5aea9f93-85d9-4e0b-b9db-bb407d596493" alt="image" loading="lazy"><br><br>åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’ç¶­æŒã—ãªãŒã‚‰ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®inference, trainingã®æ™‚é–“ã‚’8%ç¨‹åº¦å‰Šæ¸›ã€‚<br><img src="https://github.com/user-attachments/assets/98f8caa3-3ef2-4594-a45a-ae0aa2cf2ef6" alt="image" loading="lazy"></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-03-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1775" target="_blank" rel="noopener noreferrer" class="title-link">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse   Attention, Jingyang Yuan+, ACL'25</a>
<span class="snippet"><span>GPT Summary</span>- é•·æ–‡ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ãŸã‚ã«ã€è¨ˆç®—åŠ¹ç‡ã‚’æ”¹å–„ã™ã‚‹ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã€ŒNSAã€ã‚’ææ¡ˆã€‚NSAã¯å‹•çš„ãªéšå±¤ã‚¹ãƒ‘ãƒ¼ã‚¹æˆ¦ç•¥ã‚’ç”¨ã„ã€ãƒˆãƒ¼ã‚¯ãƒ³åœ§ç¸®ã¨é¸æŠã‚’çµ„ã¿åˆã‚ã›ã¦ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆèªè­˜ã¨ãƒ­ãƒ¼ã‚«ãƒ«ãªç²¾åº¦ã‚’ä¸¡ç«‹ã€‚å®Ÿè£…æœ€é©åŒ–ã«ã‚ˆã‚Šã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’å®Ÿç¾ã—ã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å¯èƒ½ã«ã™ã‚‹ã“ã¨ã§è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã€‚NSAã¯ãƒ•ãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’ç¶­æŒã—ã¤ã¤ã€é•·ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã—ã¦å¤§å¹…ãªã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’é”æˆã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dair_ai/status/1893698286545969311?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ACL'25ã®Best Paperã®ä¸€ã¤:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1950644063952052643?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/TMLR.html" target="_blank" rel="noopener noreferrer">#TMLR</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<span class="issue_date">Issue Date: 2024-11-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1505" target="_blank" rel="noopener noreferrer" class="title-link">Mixture-of-Transformers: A Sparse and Scalable Architecture for   Multi-Modal Foundation Models, Weixin Liang+, TMLR'25</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å‡¦ç†ã‚’åŠ¹ç‡åŒ–ã™ã‚‹ãŸã‚ã«ã€Mixture-of-Transformersï¼ˆMoTï¼‰ã‚’ææ¡ˆã€‚MoTã¯è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã—ã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã”ã¨ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆ†é›¢ã—ã¦ç‰¹åŒ–ã—ãŸå‡¦ç†ã‚’å®Ÿç¾ã€‚Chameleon 7Bè¨­å®šã§ã¯ã€55.8%ã®FLOPsã§å¯†ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«åŒ¹æ•µã™ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€éŸ³å£°ã‚’å«ã‚€å ´åˆã‚‚37.2%ã®FLOPsã§åŒæ§˜ã®çµæœã‚’é”æˆã€‚ã•ã‚‰ã«ã€Transfusionè¨­å®šã§ã¯ã€7Bã®MoTãƒ¢ãƒ‡ãƒ«ãŒå¯†ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ç”»åƒæ€§èƒ½ã«å¯¾ã—ã¦FLOPsã®3åˆ†ã®1ã§åŒ¹æ•µã—ã€760Mã®ãƒ¢ãƒ‡ãƒ«ã¯ä¸»è¦ãªç”»åƒç”ŸæˆæŒ‡æ¨™ã§ä¸Šå›ã‚‹çµæœã‚’å¾—ãŸã€‚MoTã¯å®Ÿç”¨çš„ãªåˆ©ç‚¹ã‚‚ç¤ºã—ã€ç”»åƒå“è³ªã‚’47.2%ã€ãƒ†ã‚­ã‚¹ãƒˆå“è³ªã‚’75.6%ã®çµŒéæ™‚é–“ã§é”æˆã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Alignment.html" target="_blank" rel="noopener noreferrer">#Alignment</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-09-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3009" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Back to Basics: Revisiting REINFORCE Style Optimization for Learning   from Human Feedback in LLMs, Arash Ahmadian+, ACL'24, 2024.02</a>
<span class="snippet"><span>GPT Summary</span>- RLHFã«ãŠã‘ã‚‹æ•´åˆæ€§ã®é‡è¦æ€§ã‚’è€ƒæ…®ã—ã€PPOã®é«˜ã‚³ã‚¹ãƒˆã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã®å•é¡Œã‚’æŒ‡æ‘˜ã€‚ã‚·ãƒ³ãƒ—ãƒ«ãªREINFORCEã‚¹ã‚¿ã‚¤ãƒ«ã®æœ€é©åŒ–æ‰‹æ³•ãŒPPOã‚„æ–°ææ¡ˆã®æ‰‹æ³•ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ã€LLMã®æ•´åˆæ€§ç‰¹æ€§ã«é©å¿œã™ã‚‹ã“ã¨ã§ä½ã‚³ã‚¹ãƒˆã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³RLæœ€é©åŒ–ãŒå¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ææ¡ˆã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/Scheduler.html" target="_blank" rel="noopener noreferrer">#Scheduler</a>
<span class="issue_date">Issue Date: 2025-08-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2540" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] MiniCPM: Unveiling the Potential of Small Language Models with Scalable  Training Strategies, Shengding Hu+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- æ€¥æˆé•·ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®é–‹ç™ºã«ãŠã‘ã‚‹ã‚³ã‚¹ãƒˆã®æ‡¸å¿µã‹ã‚‰ã€å°è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆSLMsï¼‰ã®å¯èƒ½æ€§ãŒæ³¨ç›®ã•ã‚Œã¦ã„ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€MiniCPMã¨ã„ã†1.2BãŠã‚ˆã³2.4Bã®éåŸ‹ã‚è¾¼ã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒªã‚¢ãƒ³ãƒˆã‚’ç´¹ä»‹ã—ã€ã“ã‚Œã‚‰ãŒ7B-13Bã®LLMsã¨åŒç­‰ã®èƒ½åŠ›ã‚’æŒã¤ã“ã¨ã‚’ç¤ºã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ã¯åºƒç¯„ãªå®Ÿé¨“ã‚’ã€ãƒ‡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ã¯Warmup-Stable-Decayï¼ˆWSDï¼‰å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‚’å°å…¥ã—ã€åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿-ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•ã‚’ç ”ç©¶ã—ãŸã€‚MiniCPMãƒ•ã‚¡ãƒŸãƒªãƒ¼ã«ã¯MiniCPM-DPOã€MiniCPM-MoEã€MiniCPM-128KãŒå«ã¾ã‚Œã€å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã¦ã„ã‚‹ã€‚MiniCPMãƒ¢ãƒ‡ãƒ«ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>Warmup-Stable-Decay (WSD)</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Coding.html" target="_blank" rel="noopener noreferrer">#Coding</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<span class="issue_date">Issue Date: 2025-08-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2444" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Better &amp; Faster Large Language Models via Multi-token Prediction, Fabian Gloeckle+, ICML'24</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’è¤‡æ•°ã®å°†æ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åŒæ™‚ã«äºˆæ¸¬ã™ã‚‹ã‚ˆã†ã«è¨“ç·´ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ã€ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã®å‘ä¸Šã‚’å›³ã‚‹ã€‚å…·ä½“çš„ã«ã¯ã€nå€‹ã®ç‹¬ç«‹ã—ãŸå‡ºåŠ›ãƒ˜ãƒƒãƒ‰ã‚’ç”¨ã„ã¦æ¬¡ã®nãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã—ã€è¨“ç·´æ™‚é–“ã«ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’ã‹ã‘ãšã«ä¸‹æµã®èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ç‰¹ã«ã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€ææ¡ˆãƒ¢ãƒ‡ãƒ«ã¯å¼·åŠ›ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€æ¨è«–æ™‚ã«æœ€å¤§3å€ã®é€Ÿåº¦å‘ä¸Šã‚‚å®Ÿç¾ã€‚</span>
<span class="snippet"><span>Comment</span><p>next tokenã ã‘ã§ãªãã€next 4-tokenã‚’äºˆæ¸¬ã—ã¦å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€MBPP/HumanEvalã«ãŠã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒ1.3Bã‚’è¶…ãˆãŸæ™‚ç‚¹ã§ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼ˆ=åŒã˜ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã¨ãªã‚‹ã‚ˆã†ã«èª¿æ•´ã•ã‚ŒãŸnext-token predictionï¼‰ã‚’outperformã—ã¯ã˜ã‚ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹ã«ã¤ã‚Œã¦æ€§èƒ½ã®å·®ãŒé¡•è‘—ã«è¡¨ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ‰ãƒ¡ã‚¤ãƒ³ã«ãŠã„ã¦äº‹å‰å­¦ç¿’ã€ãŠã‚ˆã³finetuningã®åŒæ–¹ã§åŠ¹æœãŒã‚ã‚‹ã€‚ãŸã ã—ã€3.7ç¯€ã§ç¤ºã•ã‚Œã¦ã„ã‚‹é€šã‚Šã€ã“ã‚Œã¯ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã®ã¿ã“ã®ã‚ˆã†ãªé¡•è‘—ãªæ”¹å–„ãŒã¿ã‚‰ã‚Œã¦ãŠã‚Šã€è‡ªç„¶è¨€èªãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã¯ã“ã“ã¾ã§é¡•è‘—ãªæ”¹å–„ã¯ã—ã¦ã„ãªã„ã‚ˆã†ã«è¦‹ãˆã‚‹ï¼ˆ5.1ç¯€ã§è€ƒå¯Ÿã•ã‚Œã¦ã„ãã†; æ˜¨ä»Šã®LLMã§ã¯äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãªã©ã®ãƒ‡ãƒ¼ã‚¿ãŒå…¥ã‚‹ã®ãŒæ™®é€šãªã®ã§åˆ©ç”¨ã™ã‚‹æ©æµã¯ã‚ã‚Šãã†; Abstractive Summarizationã§ã¯æ€§èƒ½ãŒæ”¹å–„ã—ã¦ã„ã‚‹(Figure6); GSM8Kã§ã¯200Bã¾ã§ã¯next 2 tokenã‚’äºˆæ¸¬ã™ã‚‹ã¨æ€§èƒ½ãŒæ”¹å–„ã—ã¦ã„ã‚‹ãŒ500B tokenå­¦ç¿’ã™ã‚‹ã¨next token predictionã®æ–¹ãŒæ€§èƒ½ãŒè‰¯ããªã‚‹ï¼‰ã€‚å…¨ä½“çš„ã«perplexityã®æ”¹å–„ï¼ˆ=æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ãŠã„ã¦æ­£è§£ãƒˆãƒ¼ã‚¯ãƒ³ã®ç”Ÿæˆç¢ºç‡ã‚’æ”¹å–„ã™ã‚‹ï¼‰ã¨ã„ã†ã‚ˆã‚Šã¯ã€ãƒ¢ãƒ‡ãƒ«ã®"æœ€çµ‚çš„ãªç”Ÿæˆçµæœâ€ã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã—ãŸè©•ä¾¡ã¨ãªã£ã¦ã„ã‚‹ã€‚<br><br>ãƒ¢ãƒ‡ãƒ«ã¯å…±æœ‰ã®ãƒˆãƒ©ãƒ³ã‚¯f_s (ãŠãã‚‰ãheadé–“ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å…±æœ‰ã—ã¦ã„ã‚‹ä¸€é€£ã®transformerãƒ–ãƒ­ãƒƒã‚¯) ã‚’æŒã£ã¦ãŠã‚Šinput x_t:1ã«å¯¾å¿œã™ã‚‹latent representation z_t:1ã‚’ç”Ÿæˆã™ã‚‹ã€‚latent representationã‚’output headã«inputã™ã‚‹ã“ã¨ã§ã€ãã‚Œãã‚Œã®headãŒåˆè¨ˆã§nå€‹ã®next tokenã‚’äºˆæ¸¬ã™ã‚‹ã€‚<br>&lt;img width="608" height="1021" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/433d69cb-5593-483b-b591-6445c482ed2e"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/433d69cb-5593-483b-b591-6445c482ed2e"&lt;/a&gt;


/&gt;<br><br>next n-tokenã‚’äºˆæ¸¬ã™ã‚‹éš›ã«ã¯ã€GPUãƒ¡ãƒ¢ãƒªã‚’å¤§å¹…ã«é£Ÿã£ã¦ã—ã¾ã† ï¼ˆlogitsã®shapeãŒ(n, V)ã¨ãªã‚Šãã‚Œã‚‰ã®å‹¾é…ã‚‚ä¿æŒã—ãªã‘ã‚Œã°ãªã‚‰ãªã„) ã“ã¨ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¨ãªã‚‹ãŒã€f_sã¾ã§forward passã‚’å®Ÿè¡Œã—ãŸã‚‰ã€å„headã«å¯¾ã—ã¦forward/backward passã‚’é †ç•ªã«å®Ÿè¡Œã—ã¦ã€logitsã®å€¤ã¯ç ´æ£„ã—å‹¾é…ã®æƒ…å ±ã ã‘f_sã«è“„ç©ã™ã‚‹ã“ã¨ã§ã€é•·æœŸçš„ã«ä¿æŒã™ã‚‹æƒ…å ±ã‚’å„headã®ã‹ã‚‰é€†ä¼æ¬ã•ã‚ŒãŸå‹¾é…æƒ…å ±ã®ã¿ã«ã™ã‚‹ã“ã¨ã§ã“ã‚Œã‚’è§£æ±ºã—ã¦ã„ã‚‹ã€‚<br>&lt;img width="597" height="478" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/3f5ff3fc-5934-4f12-9327-23b689526464"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/3f5ff3fc-5934-4f12-9327-23b689526464"&lt;/a&gt;


/&gt;<br><br>å®Ÿéš›ã«inferenceã‚’ã™ã‚‹ã¨ãã¯next tokenã‚’äºˆæ¸¬ã™ã‚‹ãƒ˜ãƒƒãƒ‰ã®å‡ºåŠ›ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã‚’å‰æã¨ã—ã¦ã„ã‚‹ãŒã€å…¨ã¦ã®ãƒ˜ãƒƒãƒ‰ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€tæ™‚ç‚¹ã§t+nãƒˆãƒ¼ã‚¯ãƒ³ã®äºˆæ¸¬ã‚’å¯èƒ½ãªãŸã‚ã€self-speculative decodingã‚’å®Ÿæ–½ã—inference timeã‚’çŸ­ç¸®ã™ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚<br><br>3.4ã§ç¤ºã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«ã€nã®å€¤ã¯å¤§ãã‘ã‚Œã°å¤§ãã„ã»ã©è‰¯ã„ã¨ã„ã†ã‚ã‘ã§ã¯ãªãã€4ç¨‹åº¦ï¼ˆbyte levelãªãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯8 bytesï¼‰ãŒæœ€é©ãªã‚ˆã†ã§ã‚ã‚‹ã€‚ãŒã€Table1ã‚’è¦‹ã‚‹ã¨ã€ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã£ã¦ã¯n=6ãŒè‰¯ã‹ã£ãŸã‚Šï¼ˆi.e., æœ€é©ãªnã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä¾å­˜ï¼‰è¤‡æ•°ã‚¨ãƒãƒƒã‚¯å­¦ç¿’ã™ã‚‹ã¨multi token predictionã®åŠ¹æœãŒè–„ããªã£ã¦ã„ãã†ï¼ˆi.e., åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³ã®äºˆæ¸¬ã‚’è¤‡æ•°å›å­¦ç¿’ã™ã‚‹ã®ã§å®Ÿè³ªmulti token predictionã¨ä¼¼ãŸã‚ˆã†ãªã“ã¨ã‚’ã‚„ã£ã¦ã„ã‚‹ã€‚è¨€ã„æ›ãˆã‚‹ã¨ã€multi token predictionã¯è¤‡æ•°epochã®å­¦ç¿’ã‚’å…ˆå–ã‚Šã—ã¦ã„ã‚‹ã¨ã¿ãªã›ã‚‹ï¼Ÿï¼‰ãªã®ã¯æ³¨æ„ãŒå¿…è¦ãã†ã€‚</p>
<p>å…¨ä½“çš„ã«è¤‡æ•°epochã‚’å­¦ç¿’ã™ã‚‹ã¨æ©æµãŒãªããªã£ã¦ã„ãï¼ˆã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰ or next token predictionã‚ˆã‚Šã‚‚æ€§èƒ½ãŒæ‚ªåŒ–ã™ã‚‹ï¼ˆè‡ªç„¶è¨€èªï¼‰ã®ã§ã€LLMã®äº‹å‰å­¦ç¿’ã«ãŠã„ã¦ã€è¤‡æ•°epochã‚’å­¦ç¿’ã™ã‚‹ã‚ˆã†ãªå½“ãŸã‚Šå‰ã¿ãŸã„ãªä¸–ç•Œç·šãŒè¨ªã‚ŒãŸã‚‰ã€ã“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¡ç”¨ã™ã‚‹ã¨æ€§èƒ½ã¯ã‚€ã—ã‚æ‚ªåŒ–ã—ãã†ãªæ°—ã¯ã™ã‚‹ã€‚</p>
<p>MBPP/HumanEval:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2439" target="_blank" rel="noopener noreferrer">[Paper Note] Program Synthesis with Large Language Models, Jacob Austin+, arXiv'21</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2438" target="_blank" rel="noopener noreferrer">[Paper Note] Evaluating Large Language Models Trained on Code, Mark Chen+, arXiv'21</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Scaling%20Laws.html" target="_blank" rel="noopener noreferrer">#Scaling Laws</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-05-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1992" target="_blank" rel="noopener noreferrer" class="title-link">Densing Law of LLMs, Chaojun Xiao+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®æ€§èƒ½å‘ä¸Šã«ä¼´ã†ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã®åŠ¹ç‡ã®èª²é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€ã€Œã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£å¯†åº¦ã€ã¨ã„ã†æ–°ã—ã„æŒ‡æ¨™ã‚’ææ¡ˆã€‚ã“ã‚Œã¯ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆLLMã®æœ‰åŠ¹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã¨å®Ÿéš›ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã®æ¯”ç‡ã‚’ç”¨ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®åŠ¹æœã¨åŠ¹ç‡ã‚’è©•ä¾¡ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æä¾›ã™ã‚‹ã€‚åˆ†æã«ã‚ˆã‚Šã€LLMsã®ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£å¯†åº¦ã¯ç´„3ã‹æœˆã”ã¨ã«å€å¢—ã™ã‚‹å‚¾å‘ãŒã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã€ä»Šå¾Œã®LLMé–‹ç™ºã«ãŠã‘ã‚‹é‡è¦æ€§ãŒå¼·èª¿ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1926785750277693859?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p><img src="https://github.com/user-attachments/assets/8cdcfe78-6682-481b-a6b0-a175b84d735c" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-05-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1942" target="_blank" rel="noopener noreferrer" class="title-link">The FineWeb Datasets: Decanting the Web for the Finest Text Data at   Scale, Guilherme Penedo+, NeurIPS'24</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€15å…†ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ãªã‚‹FineWebãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç´¹ä»‹ã—ã€LLMã®æ€§èƒ½å‘ä¸Šã«å¯„ä¸ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚FineWebã¯é«˜å“è³ªãªäº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ–¹æ³•ã‚’æ–‡æ›¸åŒ–ã—ã€é‡è¤‡æ’é™¤ã‚„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æˆ¦ç•¥ã‚’è©³ç´°ã«èª¿æŸ»ã—ã¦ã„ã¾ã™ã€‚ã¾ãŸã€FineWebã‹ã‚‰æ´¾ç”Ÿã—ãŸ1.3å…†ãƒˆãƒ¼ã‚¯ãƒ³ã®FineWeb-Eduã‚’ç”¨ã„ãŸLLMã¯ã€MMLUã‚„ARCãªã©ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã€ãƒ¢ãƒ‡ãƒ«ã¯å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬:


<a href="https://zenn.dev/deepkawamura/articles/da9aeca6d6d9f9" target="_blank" rel="noopener noreferrer">https://zenn.dev/deepkawamura/articles/da9aeca6d6d9f9</a>


</p>
<p>openreview:


<a href="https://openreview.net/forum?id=n6SCkn2QaG#discussion" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=n6SCkn2QaG#discussion</a>


</p></span><br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<span class="issue_date">Issue Date: 2025-03-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1785" target="_blank" rel="noopener noreferrer" class="title-link">Full Parameter Fine-tuning for Large Language Models with Limited Resources, Lv+, ACL'24, 2024.08</a>
<span class="snippet"><span>GPT Summary</span>- æ–°ã—ã„ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã€ŒLOMOã€ã‚’ææ¡ˆã—ã€å‹¾é…è¨ˆç®—ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ã‚’1ã‚¹ãƒ†ãƒƒãƒ—ã§èåˆã™ã‚‹ã“ã¨ã§ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€24GBã®ãƒ¡ãƒ¢ãƒªã‚’æŒã¤8å°ã®RTX 3090ã§65Bãƒ¢ãƒ‡ãƒ«ã®å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¯èƒ½ã«ã€‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯æ¨™æº–çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨æ¯”è¼ƒã—ã¦10.8%å‰Šæ¸›ã€‚</span>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2024-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1627" target="_blank" rel="noopener noreferrer" class="title-link">A Survey on LLM Inference-Time Self-Improvement, Xiangjue Dong+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- LLMæ¨è«–ã«ãŠã‘ã‚‹è‡ªå·±æ”¹å–„æŠ€è¡“ã‚’ä¸‰ã¤ã®è¦–ç‚¹ã‹ã‚‰æ¤œè¨ã€‚ç‹¬ç«‹ã—ãŸè‡ªå·±æ”¹å–„ã¯ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ç„¦ç‚¹ã€æ–‡è„ˆã«å¿œã˜ãŸè‡ªå·±æ”¹å–„ã¯è¿½åŠ ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã€ãƒ¢ãƒ‡ãƒ«æ”¯æ´ã®è‡ªå·±æ”¹å–„ã¯ãƒ¢ãƒ‡ãƒ«é–“ã®å”åŠ›ã‚’é€šã˜ã¦è¡Œã†ã€‚é–¢é€£ç ”ç©¶ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨èª²é¡Œã€ä»Šå¾Œã®ç ”ç©¶ã¸ã®æ´å¯Ÿã‚’æä¾›ã€‚</span>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2024-11-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1540" target="_blank" rel="noopener noreferrer" class="title-link">Observational Scaling Laws and the Predictability of Language Model  Performance, Yangjun Ruan+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€ç´„100ã®å…¬é–‹ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã‚’æ§‹ç¯‰ã™ã‚‹æ–°ã—ã„è¦³å¯Ÿã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã€‚ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼é–“ã®èƒ½åŠ›å¤‰å‹•ã‚’è€ƒæ…®ã—ã€æ€§èƒ½ãŒä½æ¬¡å…ƒã®èƒ½åŠ›ç©ºé–“ã®é–¢æ•°ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¤‡é›‘ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç¾è±¡ã®äºˆæ¸¬å¯èƒ½æ€§ã‚’ç¤ºã—ã€GPT-4ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ€§èƒ½ã‚’éã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‹ã‚‰äºˆæ¸¬ã§ãã‚‹ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã—ã€Chain-of-Thoughtã‚„Self-Consistencyã®å½±éŸ¿ã‚’äºˆæ¸¬ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>ç¸¦è»¸ãŒdownstreamã‚¿ã‚¹ã‚¯ã®ä¸»æˆåˆ†ï¼ˆã®ã†ã¡æœ€ã‚‚å¤§ãã„80%ã‚’èª¬æ˜ã™ã‚‹æˆåˆ†ï¼‰ã®å¤‰åŒ–ï¼ˆâ‰’LLMã®æ€§èƒ½ï¼‰ã§ã€æ¨ªè»¸ãŒlog scaleã®æŠ•å…¥è¨ˆç®—é‡ã€‚<br>Qwenã‚‚é ‘å¼µã£ã¦ã„ã‚‹ãŒã€æŠ•å…¥ãƒ‡ãƒ¼ã‚¿é‡ã«å¯¾ã™ã‚‹æ€§èƒ½ï¼ˆâ‰’ãƒ‡ãƒ¼ã‚¿ã®å“è³ªï¼‰ã§ã¯ã€å…ˆé§†ã‘çš„ãªç ”ç©¶ã§ã‚ã‚‹PhiãŒã‚„ã¯ã‚Šåœ§å€’çš„?<br><img src="https://github.com/user-attachments/assets/c38286df-37c1-4c72-832f-676832845c0e" alt="image" loading="lazy"></p>
<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/766" target="_blank" rel="noopener noreferrer">Textbooks Are All You Need, Suriya Gunasekar+, N/A, arXiv'23</a>
<br><br>ã‚‚å‚ç…§ã®ã“ã¨</p></span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/Japanese.html" target="_blank" rel="noopener noreferrer">#Japanese</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2024-11-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1524" target="_blank" rel="noopener noreferrer" class="title-link">Balancing Speed and Stability: The Trade-offs of FP8 vs. BF16 Training  in LLMs, Kazuki Fujii+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã¯ã€ãã®è¨€èªç†è§£èƒ½åŠ›ã¨é©ç”¨å¯èƒ½æ€§ã‹ã‚‰æ³¨ç›®ã‚’é›†ã‚ã¦ãŠã‚Šã€ç‰¹ã«Llama 3ã‚·ãƒªãƒ¼ã‚ºã¯4050å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®åŠ¹ç‡åŒ–ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹ä¸­ã€NVIDIAã®H100 GPUã¯FP8ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å°å…¥ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“ã‚’çŸ­ç¸®ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚åˆæœŸç ”ç©¶ã§ã¯FP8ãŒæ€§èƒ½ã‚’æãªã‚ãšã«åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚Œã¦ã„ã‚‹ãŒã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®‰å®šæ€§ã‚„ä¸‹æµã‚¿ã‚¹ã‚¯ã¸ã®å½±éŸ¿ã¯ã¾ã ä¸æ˜ã§ã‚ã‚‹ã€‚æœ¬ç ”ç©¶ã¯ã€LLMsã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã‘ã‚‹BF16ã¨FP8ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æ¢ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/okoge_kaz/status/1857639065421754525?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>FP8ã§ç¶™ç¶šçš„äº‹å‰å­¦ç¿’ã‚’ã™ã‚‹ã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã¯å‘ä¸Šã™ã‚‹ãŒã€lossã®ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’ç”Ÿã˜ãŸã‚Šã€downstreamã‚¿ã‚¹ã‚¯ã®æ€§èƒ½ãŒBF16ã‚ˆã‚Šã‚‚ä½ä¸‹ã—ãŸã‚Šã™ã‚‹ï¼ˆæ—¥æœ¬èªã¨è‹±èªã®ä¸¡æ–¹ï¼‰ã¨ã®å ±å‘Šã®ã‚ˆã†ã§ã‚ã‚‹ã€‚ç¾çŠ¶ã‚¢ãƒ–ã‚¹ãƒˆã¨ä»˜éŒ²ã—ã‹è¨˜è¼‰ãŒãªã„ãŒã€å†…å®¹ã¯ã“ã‚Œã‹ã‚‰æ›´æ–°ã•ã‚Œã‚‹ã®ã ã‚ã†ã‹ã€‚<br><br><img src="https://github.com/user-attachments/assets/8d60d59b-de00-483a-bff0-04a4145715c1" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2024-11-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1523" target="_blank" rel="noopener noreferrer" class="title-link">Understanding LLMs: A Comprehensive Overview from Training to Inference, Yiheng Liu+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- ChatGPTã®æ™®åŠã«ä¼´ã„ã€LLMsã®ã‚³ã‚¹ãƒˆåŠ¹ç‡ã®è‰¯ã„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã¸ã®é–¢å¿ƒãŒé«˜ã¾ã£ã¦ã„ã‚‹ã€‚æœ¬è«–æ–‡ã§ã¯ã€LLMsã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æŠ€è¡“ã¨æ¨è«–ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæŠ€è¡“ã®é€²åŒ–ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã€ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚„ãƒ¢ãƒ‡ãƒ«åœ§ç¸®ãªã©ã®ã•ã¾ã–ã¾ãªå´é¢ã‚’è­°è«–ã™ã‚‹ã€‚ã¾ãŸã€LLMsã®åˆ©ç”¨æ–¹æ³•ã¨å°†æ¥ã®ç™ºå±•ã«ã¤ã„ã¦ã®æ´å¯Ÿã‚‚æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>[Perplexityï¼ˆå‚è€ƒ;Hallucinationã«æ³¨æ„ï¼‰](


<a href="https://www.perplexity.ai/search/yi-xia-nolun-wen-wodu-minei-ro-7vGwDK_AQX.HDO7j9H8iNA)" target="_blank" rel="noopener noreferrer">https://www.perplexity.ai/search/yi-xia-nolun-wen-wodu-minei-ro-7vGwDK_AQX.HDO7j9H8iNA)</a>


</p>
<p>å˜ãªã‚‹LLMã®ç†è«–çš„ãªèª¬æ˜ã«ã¨ã©ã¾ã‚‰ãšã€å®Ÿç”¨çš„ã«å¿…è¦ãªå„ç¨®ä¸¦åˆ—å‡¦ç†æŠ€è¡“ã€Mixed Precisionã€Offloadingãªã©ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚‚ã¾ã¨ã¾ã£ã¦ã„ã‚‹ã®ãŒã¨ã¦ã‚‚è‰¯ã„ã¨æ€ã†ã€‚</p>
<p>LLM Frameworkã®ã¨ã“ã‚ã«ã€ãƒ¡ã‚¸ãƒ£ãƒ¼ãªã‚‚ã®ãŒç¶²ç¾…ã•ã‚Œã¦ã„ãªã„ã‚ˆã†ã«æ„Ÿã˜ã‚‹ã€‚ãŸã¨ãˆã°ã€Unslothã‚„Liger-Kernelãªã©ã¯Transformersã®éƒ¨åˆ†ã§è¨€åŠã•ã‚Œã¦ã¦ã‚‚è‰¯ã„ã®ã§ã¯ã€ã¨æ„Ÿã˜ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/InstructionTuning.html" target="_blank" rel="noopener noreferrer">#InstructionTuning</a>
<span class="issue_date">Issue Date: 2024-11-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1504" target="_blank" rel="noopener noreferrer" class="title-link">DELIFT: Data Efficient Language model Instruction Fine Tuning, Ishika Agarwal+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- DELIFTã¨ã„ã†æ–°ã—ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆã—ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å„ã‚¹ãƒ†ãƒ¼ã‚¸ã§ãƒ‡ãƒ¼ã‚¿é¸æŠã‚’æœ€é©åŒ–ã€‚ãƒšã‚¢ãƒ¯ã‚¤ã‚ºãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’ç”¨ã„ã¦ãƒ‡ãƒ¼ã‚¿ã®æœ‰ç›Šæ€§ã‚’å®šé‡åŒ–ã—ã€æœ€å¤§70%ã®ãƒ‡ãƒ¼ã‚¿å‰Šæ¸›ã‚’å®Ÿç¾ã€‚è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«ç¯€ç´„ã—ã€æ—¢å­˜ã®æ–¹æ³•ã‚’ä¸Šå›ã‚‹åŠ¹ç‡æ€§ã¨åŠ¹æœã‚’ç¤ºã™ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Test-Time%20Scaling.html" target="_blank" rel="noopener noreferrer">#Test-Time Scaling</a>
<span class="issue_date">Issue Date: 2024-11-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1501" target="_blank" rel="noopener noreferrer" class="title-link">Scaling LLM Test-Time Compute Optimally can be More Effective than  Scaling Model Parameters, Charlie Snell+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- LLMã®æ¨è«–æ™‚ã®è¨ˆç®—ã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€æŒ‘æˆ¦çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ”¹å–„ã™ã‚‹æ–¹æ³•ã‚’ç ”ç©¶ã€‚ç‰¹ã«ã€å¯†ãªãƒ—ãƒ­ã‚»ã‚¹ãƒ™ãƒ¼ã‚¹ã®æ¤œè¨¼è€…å ±é…¬ãƒ¢ãƒ‡ãƒ«ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¿œã˜ãŸå¿œç­”ã®é©å¿œçš„æ›´æ–°ã‚’åˆ†æã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é›£æ˜“åº¦ã«ã‚ˆã£ã¦åŠ¹æœãŒå¤‰åŒ–ã—ã€è¨ˆç®—æœ€é©æˆ¦ç•¥ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§åŠ¹ç‡ã‚’4å€ä»¥ä¸Šå‘ä¸Šã€‚ã•ã‚‰ã«ã€ãƒ†ã‚¹ãƒˆæ™‚è¨ˆç®—ã‚’ç”¨ã„ã‚‹ã“ã¨ã§å°ã•ãªãƒ¢ãƒ‡ãƒ«ãŒå¤§ããªãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p><img src="https://github.com/user-attachments/assets/0562a65e-b2f1-4ff4-b806-107313fc255e" alt="image" loading="lazy"></p>
<p>[Perplexityï¼ˆå‚è€ƒ;Hallucinationã«æ³¨æ„ï¼‰](


<a href="https://www.perplexity.ai/search/yi-xia-noyan-jiu-wodu-mi-nei-r-1e1euXgLTH.G0Wlp.V2iqA)" target="_blank" rel="noopener noreferrer">https://www.perplexity.ai/search/yi-xia-noyan-jiu-wodu-mi-nei-r-1e1euXgLTH.G0Wlp.V2iqA)</a>


</p></span><br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<span class="issue_date">Issue Date: 2024-10-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1467" target="_blank" rel="noopener noreferrer" class="title-link">What Matters in Transformers? Not All Attention is Needed, Shwai He+, N_A, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼å†…ã®Blocksã€MLPã€Attentionå±¤é–“ã®å†—é•·æ€§ã‚’èª¿æŸ»ã—ã€Attentionå±¤ã®é«˜ã„é¡ä¼¼æ€§ã«ã‚ˆã‚Šãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã—ãŸã€‚å…·ä½“çš„ã«ã¯ã€Llama-2-70Bã§ã¯Attentionå±¤ã®åŠåˆ†ã‚’å‰Šé™¤ã™ã‚‹ã“ã¨ã§48.4%ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’é”æˆã—ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ã‚ãšã‹2.4%ä½ä¸‹ã—ã¾ã—ãŸã€‚ã¾ãŸã€Attentionå±¤ã¨MLPå±¤ã‚’åŒæ™‚ã«å‰Šé™¤ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ã€31å±¤å‰Šé™¤ã—ã¦ã‚‚Llama-2-13Bã¯90%ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã—ã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ä»Šå¾Œã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆã«è²´é‡ãªæ´å¯Ÿã‚’æä¾›ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>é€šå¸¸LLMã¯transformer decoderã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’stackã™ã‚‹ã“ã¨ã§å½¢æˆã•ã‚Œã‚‹ãŒã€ç©ã¿ä¸Šã’ãŸãƒ–ãƒ­ãƒƒã‚¯ã€ã‚ã‚‹ã„ã¯layerã£ã¦ã»ã‚“ã¨ã«å…¨éƒ¨å¿…è¦ãªã®?ã¨ã„ã†ç–‘å•ã«ç­”ãˆã¦ãã‚Œã‚‹è«–æ–‡ã®ã‚ˆã†ã§ã‚ã‚‹ã€‚<br><br>transformer blockãã®ã‚‚ã®ã€ã‚ã‚‹ã„ã¯MLP layerã‚’å‰Šé™¤ã™ã‚‹ã¨peformanceã¯å¤§å¹…ã«ä½ä¸‹ã™ã‚‹ãŒã€attention layerã‚’å‰Šé™¤ã—ã¦ã‚‚performanceã®ä½ä¸‹ãŒèµ·ããªã‹ã£ãŸæ¨¡æ§˜ã€‚ã“ã‚Œã«ã‚ˆã‚Šé«˜é€ŸåŒ–ãŒå®Ÿç¾å¯èƒ½ã€‚<br><br>å‰Šé™¤ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã‚„layerã¯inputã¨outputã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãŒé«˜ã„ã‚‚ã®ã‚’å‰Šé™¤ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦å®Ÿç¾ã€‚<br><br><img src="https://github.com/user-attachments/assets/da1e6a56-1bc4-4206-9423-acd7512300c8" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/724ddf50-cd63-437d-9df2-73423dd77a6e" alt="image" loading="lazy"><br><br>æ¯”è¼ƒçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå°ã•ã„7B, 13Bãƒ¢ãƒ‡ãƒ«ã§ã®å®Ÿé¨“çµæœ<br><img src="https://github.com/user-attachments/assets/19253c9e-7eae-4084-a8c2-e99680b34649" alt="image" loading="lazy"><br><br>ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã§ã®å®Ÿé¨“çµæœ<br><img src="https://github.com/user-attachments/assets/18eef07e-623c-482c-9a6b-9ea65450ecea" alt="image" loading="lazy"></p>
<p>ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå¤‰ã‚ã‚‰ãªã„ç¯„å›²ã ã¨ã€attention layer dropã«ã‚ˆã‚Šã€7B, 13Bãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯23%ç¨‹åº¦ã€70Bã®å ´åˆã¯35%ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<span class="issue_date">Issue Date: 2024-10-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1459" target="_blank" rel="noopener noreferrer" class="title-link">Addition is All You Need for Energy-efficient Language Models, Hongyin Luo+, N_A, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€æµ®å‹•å°æ•°ç‚¹ä¹—ç®—ã‚’é«˜ç²¾åº¦ã§æ•´æ•°åŠ ç®—å™¨ã«ã‚ˆã£ã¦è¿‘ä¼¼ã™ã‚‹L-Mulã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€8ãƒ“ãƒƒãƒˆæµ®å‹•å°æ•°ç‚¹ä¹—ç®—ã«æ¯”ã¹ã¦è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã¤ã¤ã€ã‚ˆã‚Šé«˜ã„ç²¾åº¦ã‚’å®Ÿç¾ã€‚L-Mulã‚’ãƒ†ãƒ³ã‚½ãƒ«å‡¦ç†ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«é©ç”¨ã™ã‚‹ã“ã¨ã§ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚³ã‚¹ãƒˆã‚’95ï¼…ï¼ˆè¦ç´ ã”ã¨ã®ä¹—ç®—ï¼‰ãŠã‚ˆã³80ï¼…ï¼ˆãƒ‰ãƒƒãƒˆç©ï¼‰å‰Šæ¸›å¯èƒ½ã€‚å®Ÿé¨“çµæœã¯ç†è«–çš„èª¤å·®æ¨å®šã¨ä¸€è‡´ã—ã€L-Mulã¯å¾“æ¥ã®æµ®å‹•å°æ•°ç‚¹ä¹—ç®—ã¨åŒç­‰ã¾ãŸã¯ãã‚Œä»¥ä¸Šã®ç²¾åº¦ã‚’é”æˆã€‚ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«å†…ã®æµ®å‹•å°æ•°ç‚¹ä¹—ç®—ã‚’L-Mulã«ç½®ãæ›ãˆã‚‹ã“ã¨ã§ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¨è«–ã«ãŠã„ã¦é«˜ã„ç²¾åº¦ã‚’ç¶­æŒã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<a class="button" href="articles/RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<span class="issue_date">Issue Date: 2024-09-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1420" target="_blank" rel="noopener noreferrer" class="title-link">Enhancing Performance and Scalability of Large-Scale Recommendation  Systems with Jagged Flash Attention, Rengan Xu+, N_A, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿ãƒ¼ã®çµ±åˆã«ã‚ˆã‚Šã€æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®èƒ½åŠ›ãŒå‘ä¸Šã™ã‚‹ä¸€æ–¹ã§ã€GPUè¨ˆç®—ã‚³ã‚¹ãƒˆãŒèª²é¡Œã¨ãªã£ã¦ã„ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€ã‚«ãƒ†ã‚´ãƒªç‰¹å¾´ã®é•·ã•ã«ã‚ˆã‚‹GPUåˆ©ç”¨ã®è¤‡é›‘ã•ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€ã€ŒJagged Feature Interaction Kernelsã€ã‚’ææ¡ˆã—ã€å‹•çš„ã‚µã‚¤ã‚ºã®ãƒ†ãƒ³ã‚½ãƒ«ã‚’åŠ¹ç‡çš„ã«æ‰±ã†æ‰‹æ³•ã‚’é–‹ç™ºã€‚ã•ã‚‰ã«ã€Jaggedãƒ†ãƒ³ã‚½ãƒ«ã‚’Flash Attentionã¨çµ±åˆã—ã€æœ€å¤§9å€ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã¨22å€ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã‚’å®Ÿç¾ã€‚å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€10%ã®QPSæ”¹å–„ã¨18%ã®ãƒ¡ãƒ¢ãƒªç¯€ç´„ã‚’ç¢ºèªã—ã€è¤‡é›‘ãªæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å¯èƒ½ã«ã—ãŸã€‚</span>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2024-09-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1386" target="_blank" rel="noopener noreferrer" class="title-link">From Decoding to Meta-Generation: Inference-time Algorithms for Large  Language Models, Sean Welleck+, N_A, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- æ¨è«–æ™‚ã®è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹æ‹¡å¤§ã®åˆ©ç‚¹ã«ç„¦ç‚¹ã‚’å½“ã¦ã€ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ç”Ÿæˆã€ãƒ¡ã‚¿ç”Ÿæˆã€åŠ¹ç‡çš„ç”Ÿæˆã®3ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’çµ±ä¸€çš„ã«æ¢æ±‚ã€‚ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ç”Ÿæˆã¯ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç”¨ã„ã€ãƒ¡ã‚¿ç”Ÿæˆã¯ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã‚„å¤–éƒ¨æƒ…å ±ã‚’æ´»ç”¨ã—ã€åŠ¹ç‡çš„ç”Ÿæˆã¯ã‚³ã‚¹ãƒˆå‰Šæ¸›ã¨é€Ÿåº¦å‘ä¸Šã‚’ç›®æŒ‡ã™ã€‚å¾“æ¥ã®è‡ªç„¶è¨€èªå‡¦ç†ã€ç¾ä»£ã®LLMsã€æ©Ÿæ¢°å­¦ç¿’ã®è¦–ç‚¹ã‚’çµ±åˆã—ãŸèª¿æŸ»ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒ„ã‚¤ãƒ¼ãƒˆ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gneubig/status/1833522477605261799?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>CMUã®ãƒãƒ¼ãƒ ã«ã‚ˆã‚‹inference timeã®é«˜é€ŸåŒ–ã«é–¢ã™ã‚‹ã‚µãƒ¼ãƒ™ã‚¤</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2024-07-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1338" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] FlashAttention-3: Fast and Accurate Attention with Asynchrony and   Low-precision, Jay Shah+, NeurIPS'24</a>
<span class="snippet"><span>GPT Summary</span>- FlashAttention-3ã¯ã€Hopper GPUä¸Šã§Attentionã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã«ã€3ã¤ã®æŠ€è¡“ã‚’é–‹ç™ºã—ã€H100 GPUã§1.5-2.0å€ã®é€Ÿåº¦å‘ä¸Šã‚’å®Ÿç¾ã€‚FP16ã§740 TFLOPs/sã€FP8ã§ç´„1.2 PFLOPs/sã«é”ã—ã€FP8ã§ã¯æ•°å€¤èª¤å·®ãŒ2.6å€ä½ã„ã“ã¨ã‚’ç¢ºèªã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=tVConYid20&referrer=%5Bthe%20profile%20of%20Tri%20Dao%5D(%2Fprofile%3Fid%3D~Tri_Dao1)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=tVConYid20&referrer=%5Bthe%20profile%20of%20Tri%20Dao%5D(%2Fprofile%3Fid%3D~Tri_Dao1)</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<span class="issue_date">Issue Date: 2024-04-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1293" target="_blank" rel="noopener noreferrer" class="title-link">Phi-3 Technical Report: A Highly Capable Language Model Locally on Your  Phone, Marah Abdin+, N_A, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- phi-3-miniã¯38å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€3.3å…†ãƒˆãƒ¼ã‚¯ãƒ³ã§è¨“ç·´ã•ã‚Œã¦ã„ã¾ã™ã€‚Mixtral 8x7Bã‚„GPT-3.5ãªã©ã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã«åŒ¹æ•µã™ã‚‹ç·åˆçš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æŒã¡ãªãŒã‚‰ã€ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ã«ãƒ‡ãƒ—ãƒ­ã‚¤å¯èƒ½ãªã‚µã‚¤ã‚ºã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€å³å¯†ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸWebãƒ‡ãƒ¼ã‚¿ã¨åˆæˆãƒ‡ãƒ¼ã‚¿ã§æ§‹æˆã•ã‚Œã¦ãŠã‚Šã€å …ç‰¢æ€§ã€å®‰å…¨æ€§ã€ãŠã‚ˆã³ãƒãƒ£ãƒƒãƒˆå½¢å¼ã«é©åˆã—ã¦ã„ã¾ã™ã€‚ã¾ãŸã€phi-3-smallã¨phi-3-mediumã¨ã„ã†ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã‚‚ç´¹ä»‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1039" target="_blank" rel="noopener noreferrer">Textbooks Are All You Need II: phi-1.5 technical report, Yuanzhi Li+, N/A, arXiv'23</a>
 ã®æ¬¡ã®æ¬¡ï¼ˆPhi2.0ã«ã¤ã„ã¦ã¯ãƒ¡ãƒ¢ã£ã¦ãªã‹ã£ãŸï¼‰ã€‚ã‚¹ãƒãƒ›ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã§ãã‚‹ãƒ¬ãƒ™ãƒ«ã®ã‚µã‚¤ã‚ºã§ã€GPT3.5Turboç¨‹åº¦ã®æ€§èƒ½ã‚’å®Ÿç¾ã—ãŸã‚‰ã—ã„</p>
<p>Llama2ã¨åŒã˜ãƒ–ãƒ­ãƒƒã‚¯ã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹ãŸã‚ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯Llama2ã¨å…±é€šã€‚<br><br></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Pruning.html" target="_blank" rel="noopener noreferrer">#Pruning</a>
<span class="issue_date">Issue Date: 2024-04-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1292" target="_blank" rel="noopener noreferrer" class="title-link">The Unreasonable Ineffectiveness of the Deeper Layers, Andrey Gromov+, N_A, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- ä¸€èˆ¬çš„ãªã‚ªãƒ¼ãƒ—ãƒ³ã‚¦ã‚§ã‚¤ãƒˆã®äº‹å‰å­¦ç¿’ã•ã‚ŒãŸLLMã®ãƒ¬ã‚¤ãƒ¤ãƒ¼å‰ªå®šæˆ¦ç•¥ã‚’ç ”ç©¶ã—ã€ç•°ãªã‚‹è³ªå•å¿œç­”ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ä½ä¸‹ã‚’æœ€å°é™ã«æŠ‘ãˆã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã—ãŸã€‚ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æœ€å¤§åŠåˆ†ã‚’å‰Šé™¤ã™ã‚‹ã“ã¨ã§ã€æœ€é©ãªãƒ–ãƒ­ãƒƒã‚¯ã‚’ç‰¹å®šã—ã€å¾®èª¿æ•´ã—ã¦æå‚·ã‚’ä¿®å¾©ã—ã¾ã™ã€‚PEFTæ‰‹æ³•ã‚’ä½¿ç”¨ã—ã€å®Ÿé¨“ã‚’å˜ä¸€ã®A100 GPUã§å®Ÿè¡Œå¯èƒ½ã«ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’å‰Šæ¸›ã—ã€æ¨è«–ã®ãƒ¡ãƒ¢ãƒªã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’æ”¹å–„ã§ãã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚Œã¾ã™ã€‚ã¾ãŸã€LLMãŒãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å‰Šé™¤ã«å¯¾ã—ã¦å …ç‰¢ã§ã‚ã‚‹ã“ã¨ã¯ã€æµ…ã„ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒçŸ¥è­˜ã‚’æ ¼ç´ã™ã‚‹ä¸Šã§é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¦ã„ã‚‹å¯èƒ½æ€§ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>ä¸‹è¨˜ãƒ„ã‚¤ãƒ¼ãƒˆã«ã‚ˆã‚‹ã¨ã€å­¦ç¿’æ¸ˆã¿LLMã‹ã‚‰ã€ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã§å…¥å‡ºåŠ›é–“ã®é¡ä¼¼åº¦ãŒé«˜ã„å±¤ã‚’é™¤ã„ã¦ã‚‚ã‚¿ã‚¹ã‚¯ã®ç²¾åº¦ãŒè½ã¡ãšã€ç‰¹ã«æ·±ã„å±¤ã‚’2-4å‰²å‰Šé™¤ã—ã¦ã‚‚ç²¾åº¦ãŒè½ã¡ãªã„ã¨ã®ã“ã¨ã€‚<br><br>å‚è€ƒ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1773110076502368642?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>VRAMã«è¼‰ã›ã‚‹ã®ãŒå¤§å¤‰ãªã®ã§ã€ã“ã®ã‚ˆã†ãªæåˆˆã‚ŠæŠ€è¡“ãŒæœ‰åŠ¹ã ã¨åˆ†ã‹ã‚‹ã®ã¯ã‚ã‚ŠãŒãŸã„ã€‚LoRAã‚„é‡å­åŒ–ã‚‚åˆ©ç”¨ã—ã¦ã„ã‚‹ã£ã½ã„ã€‚</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1273" target="_blank" rel="noopener noreferrer" class="title-link">Mixture-of-Depths: Dynamically allocating compute in transformer-based  language models, David Raposo+, N_A, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- Transformerãƒ™ãƒ¼ã‚¹ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã¯ã€å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã«å‡ç­‰ã«FLOPsã‚’åˆ†æ•£ã•ã›ã‚‹ä»£ã‚ã‚Šã«ã€ç‰¹å®šã®ä½ç½®ã«FLOPsã‚’å‹•çš„ã«å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã‚’å­¦ç¿’ã§ãã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®æ·±ã•ã«ã‚ãŸã£ã¦å‰²ã‚Šå½“ã¦ã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã«ã€ç•°ãªã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§è¨ˆç®—ã‚’å‹•çš„ã«å‰²ã‚Šå½“ã¦ã‚‹ã€‚ã“ã®æ‰‹æ³•ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³ã®æ•°ã‚’åˆ¶é™ã™ã‚‹ã“ã¨ã§åˆè¨ˆè¨ˆç®—äºˆç®—ã‚’å¼·åˆ¶ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³ã¯top-kãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ä½¿ç”¨ã—ã¦æ±ºå®šã•ã‚Œã‚‹ã€‚ã“ã®æ–¹æ³•ã«ã‚ˆã‚Šã€FLOPsã‚’å‡ç­‰ã«æ¶ˆè²»ã—ã¤ã¤ã€è¨ˆç®—ã®æ”¯å‡ºãŒäºˆæ¸¬å¯èƒ½ã§ã‚ã‚Šã€å‹•çš„ã‹ã¤ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ•æ„Ÿã§ã‚ã‚‹ã€‚ã“ã®ã‚ˆã†ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã€è¨ˆç®—ã‚’å‹•çš„ã«å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã‚’å­¦ç¿’ã—ã€åŠ¹ç‡çš„ã«è¡Œã†ã“ã¨ãŒã§ãã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å‚è€ƒ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theseamouse/status/1775782800362242157?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1270" target="_blank" rel="noopener noreferrer" class="title-link">Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference, Piotr Nawrot+, N_A, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®ç”ŸæˆåŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€Dynamic Memory Compressionï¼ˆDMCï¼‰ãŒææ¡ˆã•ã‚ŒãŸã€‚DMCã¯ã€ç•°ãªã‚‹ãƒ˜ãƒƒãƒ‰ã¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ç•°ãªã‚‹åœ§ç¸®ç‡ã‚’é©ç”¨ã™ã‚‹æ–¹æ³•ã‚’å­¦ç¿’ã—ã€äº‹å‰å­¦ç¿’æ¸ˆã¿LLMsã«é©ç”¨ã•ã‚Œã‚‹ã€‚DMCã¯ã€å…ƒã®ä¸‹æµãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€å¤§4å€ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥åœ§ç¸®ã§ç¶­æŒã—ã¤ã¤ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚DMCã¯ã€GQAã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã•ã‚‰ãªã‚‹åˆ©ç›Šã‚’ã‚‚ãŸã‚‰ã™å¯èƒ½æ€§ãŒã‚ã‚Šã€é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨å¤§ããªãƒãƒƒãƒã‚’å‡¦ç†ã™ã‚‹éš›ã«æœ‰ç”¨ã§ã‚ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å‚è€ƒ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1776755029581676943?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è«–æ–‡ä¸­ã®Figure1ãŒéå¸¸ã«ã‚ã‹ã‚Šã‚„ã™ã„ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/d416547e-f9ca-4c6c-8ebb-7d164bef5283" alt="image" loading="lazy"><br><br></p>
<p>GQA <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1271" target="_blank" rel="noopener noreferrer">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head
  Checkpoints, Joshua Ainslie+, N/A, arXiv'23</a>
 ã¨æ¯”è¼ƒã—ã¦ã€2~4å€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åœ§ç¸®ã—ã¤ã¤ã€ã‚ˆã‚Šé«˜ã„æ€§èƒ½ã‚’å®Ÿç¾ã€‚70Bãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯ã€GQAã§8å€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åœ§ç¸®ã—ãŸä¸Šã§ã€DMCã§è¿½åŠ ã§2å€åœ§ç¸®ã‚’ã‹ã‘ãŸã¨ã“ã‚ã€åŒç­‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®Ÿç¾ã—ã¦ã„ã‚‹ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/7b131f07-5eab-4830-88cc-5f6fd0508958" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<span class="issue_date">Issue Date: 2024-03-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1245" target="_blank" rel="noopener noreferrer" class="title-link">LoRA+: Efficient Low Rank Adaptation of Large Models, Soufiane Hayou+, N_A, ICML'24</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€Huã‚‰ï¼ˆ2021ï¼‰ã«ã‚ˆã£ã¦å°å…¥ã•ã‚ŒãŸLow Rank Adaptationï¼ˆLoRAï¼‰ãŒã€å¤§åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã®é©åˆ‡ãªå¾®èª¿æ•´ã‚’å¦¨ã’ã‚‹ã“ã¨ã‚’æŒ‡æ‘˜ã—ã¾ã™ã€‚ã“ã®å•é¡Œã¯ã€LoRAã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãƒãƒˆãƒªãƒƒã‚¯ã‚¹Aã¨BãŒåŒã˜å­¦ç¿’ç‡ã§æ›´æ–°ã•ã‚Œã‚‹ã“ã¨ã«èµ·å› ã—ã¾ã™ã€‚æˆ‘ã€…ã¯ã€Aã¨Bã«åŒã˜å­¦ç¿’ç‡ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒåŠ¹ç‡çš„ãªç‰¹å¾´å­¦ç¿’ã‚’å¦¨ã’ã‚‹ã“ã¨ã‚’ç¤ºã—ã€ç•°ãªã‚‹å­¦ç¿’ç‡ã‚’è¨­å®šã™ã‚‹ã“ã¨ã§ã“ã®å•é¡Œã‚’ä¿®æ­£ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚ä¿®æ­£ã•ã‚ŒãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’LoRA$+$ã¨å‘¼ã³ã€å¹…åºƒã„å®Ÿé¨“ã«ã‚ˆã‚Šã€LoRA$+$ã¯æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã€å¾®èª¿æ•´é€Ÿåº¦ã‚’æœ€å¤§2å€é«˜é€ŸåŒ–ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>LoRAã§å°å…¥ã•ã‚Œã‚‹ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—Aã¨Bã‚’ç•°ãªã‚‹å­¦ç¿’ç‡ã§å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€LoRAã¨åŒã˜è¨ˆç®—ã‚³ã‚¹ãƒˆã§ã€2å€ä»¥ä¸Šã®é«˜é€ŸåŒ–ã€ã‹ã¤é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®Ÿç¾ã™ã‚‹æ‰‹æ³•<br><br><img src="https://github.com/user-attachments/assets/cde925fa-bfe8-4385-ae55-d80f7bf034f5" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/c054c5a6-56a2-4aa5-b7f2-0ae87a808f58" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/f32a7aba-e4b1-4d28-920d-00f81e9b85e8" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/python.html" target="_blank" rel="noopener noreferrer">#python</a>
<a class="button" href="articles/LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-08-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2474" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Efficient Memory Management for Large Language Model Serving with  PagedAttention, Woosuk Kwon+, SOSP'23</a>
<span class="snippet"><span>GPT Summary</span>- PagedAttentionã‚’ç”¨ã„ãŸvLLMã‚·ã‚¹ãƒ†ãƒ ã‚’ææ¡ˆã—ã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒªã®ç„¡é§„ã‚’å‰Šæ¸›ã—ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã§ã®æŸ”è»Ÿãªå…±æœ‰ã‚’å®Ÿç¾ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€åŒãƒ¬ãƒ™ãƒ«ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã§LLMã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’2-4å€å‘ä¸Šã€‚ç‰¹ã«é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚„å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§åŠ¹æœãŒé¡•è‘—ã€‚ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ä¸­ã€‚</span>
<span class="snippet"><span>Comment</span><p>ï¼ˆä»Šæ›´ãªãŒã‚‰ï¼‰vLLMã¯ã“ã¡ã‚‰:<br>


<a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">https://github.com/vllm-project/vllm</a>


<br><br>ç¾åœ¨ã®ä¸»è¦ãªLLM Inference/Serving Engineã®ã²ã¨ã¤ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/python.html" target="_blank" rel="noopener noreferrer">#python</a>
<a class="button" href="articles/LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Inference.html" target="_blank" rel="noopener noreferrer">#Inference</a>
<span class="issue_date">Issue Date: 2025-06-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2028" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked  Prefills, Amey Agrawal+, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- SARATHIã¯ã€LLMã®æ¨è«–åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã§ã€ãƒ—ãƒ¬ãƒ•ã‚£ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã€ãƒ‡ã‚³ãƒ¼ãƒ‰ãƒã‚­ã‚·ãƒãƒ«ãƒãƒƒãƒã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã§è¨ˆç®—åˆ©ç”¨ç‡ã‚’æœ€å¤§åŒ–ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’æœ€å¤§10å€å‘ä¸Šã•ã›ã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚‚æ”¹å–„ã€‚ç‰¹ã«ã€A6000 GPUä¸Šã®LLaMA-13Bãƒ¢ãƒ‡ãƒ«ã§é¡•è‘—ãªæ€§èƒ½å‘ä¸Šã‚’ç¤ºã—ã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒãƒ–ãƒ«ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>vLLMã§ã‚‚æ¡ç”¨ã•ã‚Œã¦ã„ã‚‹ `Chunked Prefills` ã¨ `Decode-Maximal Batching` ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚<br>![Image](https://github.com/user-attachments/assets/4db0f73d-bdf4-4c2b-a765-2c9b242904f1)</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="articles/Parallelism.html" target="_blank" rel="noopener noreferrer">#Parallelism</a>
<span class="issue_date">Issue Date: 2025-05-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1970" target="_blank" rel="noopener noreferrer" class="title-link">Sequence Parallelism: Long Sequence Training from System Perspective, Li+, ACL'23</a>
<span class="snippet"><span>Comment</span><p>å…¥åŠ›ç³»åˆ—ã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦ã€ãƒ‡ãƒã‚¤ã‚¹ã”ã¨ã«æ‹…å½“ã™ã‚‹ãƒãƒ£ãƒ³ã‚¯ã‚’æ±ºã‚ã‚‹ã“ã¨ã§åŸç†ä¸Šç„¡é™ã®é•·ã•ã®ç³»åˆ—ã‚’æ‰±ãˆã‚‹ã‚ˆã†ã«ã—ãŸä¸¦åˆ—åŒ–æ‰‹æ³•ã€‚ç³»åˆ—ã‚’ãƒ‡ãƒã‚¤ã‚¹é–“ã§æ¨ªæ–­ã™ã‚‹å ´åˆattention scoreã‚’ã©ã®ã‚ˆã†ã«è¨ˆç®—ã™ã‚‹ã‹ãŒèª²é¡Œã«ãªã‚‹ãŒã€ãã®ãŸã‚ã«Ring Self attentionã¨å‘¼ã°ã‚Œã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚ã¾ãŸã€MLPãƒ–ãƒ­ãƒƒã‚¯ã¨Multi Head Attentonãƒ–ãƒ­ãƒƒã‚¯ã®è¨ˆç®—ã‚‚ã€BatchSize * Sequence Lengthã®å¤§ãã•ãŒã€ãã‚Œãã‚Œ32*Hidden Size, 16*Attention Head size * 
<strong># of Attention Headã‚ˆã‚Šã‚‚å¤§ãããªã£ãŸå ´åˆã«ã€Tensor Parallelismã‚ˆã‚Šã‚‚ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒè‰¯ããªã‚‹ã‚‰ã—ã„ã€‚<br><img src="https://github.com/user-attachments/assets/f3ba9010-da3a-4c3a-8515-d3715466ff59" alt="image" loading="lazy">&lt;/p&gt;<p>Data Parallel, Pipeline Parallel, Tensor Parallelã€å…¨ã¦ã«äº’æ›æ€§ãŒã‚ã‚‹ã¨ã®ã“ã¨ï¼ˆä½µç”¨å¯èƒ½ï¼‰</p>
<p>ãã®ã»ã‹ã®ä¸¦åˆ—åŒ–ã®è§£èª¬ã«ã¤ã„ã¦ã¯<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1184" target="_blank" rel="noopener noreferrer">å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’æ”¯ãˆã‚‹åˆ†æ•£ä¸¦åˆ—å­¦ç¿’ã®ã—ãã¿ Part1</a>
&lt;/strong&gt;
<br>
<br><br>ã‚’å‚ç…§ã®ã“ã¨ã€‚</p>&lt;/span&gt;<br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/PositionalEncoding.html" target="_blank" rel="noopener noreferrer">#PositionalEncoding</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-04-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1865" target="_blank" rel="noopener noreferrer" class="title-link">The Impact of Positional Encoding on Length Generalization in   Transformers, Amirhossein Kazemnejad+, NeurIPS'23</a>
<span class="snippet"><span>GPT Summary</span>- é•·ã•ä¸€èˆ¬åŒ–ã¯Transformerãƒ™ãƒ¼ã‚¹ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹é‡è¦ãªèª²é¡Œã§ã‚ã‚Šã€ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆPEï¼‰ãŒãã®æ€§èƒ½ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã€‚5ã¤ã®ç•°ãªã‚‹PEæ‰‹æ³•ï¼ˆAPEã€T5ã®ç›¸å¯¾PEã€ALiBiã€Rotaryã€NoPEï¼‰ã‚’æ¯”è¼ƒã—ãŸçµæœã€ALiBiã‚„Rotaryãªã©ã®ä¸€èˆ¬çš„ãªæ‰‹æ³•ã¯é•·ã•ä¸€èˆ¬åŒ–ã«é©ã—ã¦ãŠã‚‰ãšã€NoPEãŒä»–ã®æ‰‹æ³•ã‚’ä¸Šå›ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚NoPEã¯è¿½åŠ ã®è¨ˆç®—ã‚’å¿…è¦ã¨ã›ãšã€çµ¶å¯¾PEã¨ç›¸å¯¾PEã®ä¸¡æ–¹ã‚’è¡¨ç¾å¯èƒ½ã§ã‚ã‚‹ã€‚ã•ã‚‰ã«ã€ã‚¹ã‚¯ãƒ©ãƒƒãƒãƒ‘ãƒƒãƒ‰ã®å½¢å¼ãŒãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ã‚‚ç¤ºã•ã‚ŒãŸã€‚ã“ã®ç ”ç©¶ã¯ã€æ˜ç¤ºçš„ãªä½ç½®åŸ‹ã‚è¾¼ã¿ãŒé•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¸ã®ä¸€èˆ¬åŒ–ã«å¿…é ˆã§ãªã„ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1863" target="_blank" rel="noopener noreferrer">Llama 4 Series, Meta, 2025.04</a>
<br><br>ã«ãŠã„ã¦ã€Llama4 ScoutãŒ10Mã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’å®Ÿç¾ã§ãã‚‹ç†ç”±ã®ä¸€ã¤ã¨ã®ã“ã¨ã€‚<br><br>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/drjimfan/status/1908615861650547081?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>Llama4ã®ãƒ–ãƒ­ã‚°ãƒã‚¹ãƒˆã«ã‚‚ãã®æ—¨è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹:<br>&gt;A key innovation in the Llama 4 architecture is the use of interleaved attention layers without positional embeddings. Additionally, we employ inference time temperature scaling of attention to enhance length generalization.<br><br>[The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation](


<a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/?utm_source=twitter&utm_medium=organic_social&utm_content=image&utm_campaign=llama4)" target="_blank" rel="noopener noreferrer">https://ai.meta.com/blog/llama-4-multimodal-intelligence/?utm_source=twitter&utm_medium=organic_social&utm_content=image&utm_campaign=llama4)</a>


</span></strong></p>
<p>æ–œã‚èª­ã¿ã ãŒã€length generalizationã‚’è©•ä¾¡ã™ã‚‹ä¸Šã§downstream taskã«ç„¦ç‚¹ã‚’å½“ã¦ã€3ã¤ã®ä»£è¡¨çš„ãªã‚«ãƒ†ã‚´ãƒªã«ç›¸å½“ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§è©•ä¾¡ã—ãŸã¨ã“ã‚ã€ã“ã®è¦³ç‚¹ã«ãŠã„ã¦ã¯T5ã®relative positinal encodingã¨NoPEï¼ˆä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ³ã‚°ç„¡ã—ï¼‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒè‰¯ãã€<br><br><img src="https://github.com/user-attachments/assets/dddadfff-ab28-4073-96c3-831eb16845a0" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/c6ec8e0e-7abb-4330-be23-2261486a477c" alt="image" loading="lazy"><br><br>NoPEã¯çµ¶å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ç›¸å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç†è«–ä¸Šå®Ÿç¾å¯èƒ½ã§ã‚ã‚Š[^1]<br><img src="https://github.com/user-attachments/assets/bbcf797a-d394-42d4-b017-08d7dba4261c" alt="image" loading="lazy"><br><br>å®Ÿéš›ã«å­¦ç¿’ã•ã‚ŒãŸç•°ãªã‚‹2ã¤ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãã‚Œãã‚Œinputã—ã€åŒã˜æ·±ã•ã®Layerã®å…¨ã¦ã®attention distributionã®çµ„ã¿åˆã‚ã›ã‹ã‚‰Jensen Shannon Divergenceã§è·é›¢ã‚’ç®—å‡ºã—ã€æœ€ã‚‚å°ã•ã„ã‚‚ã®ã‚’2ãƒ¢ãƒ‡ãƒ«é–“ã®å½“è©²layerã®è·é›¢ã¨ã—ã¦å¯è¦–åŒ–ã™ã‚‹ã¨ä¸‹è¨˜ã®ã‚ˆã†ã«ãªã‚Šã€NoPEã¨T5ã®relative positional encodingãŒæœ€ã‚‚é¡ä¼¼ã—ã¦ã„ã‚‹ã“ã¨ã‹ã‚‰ã€NoPEãŒå­¦ç¿’ã‚’é€šã˜ã¦ï¼ˆå®Ÿç”¨ä¸Šã¯ï¼‰ç›¸å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ã‚ˆã†ãªã‚‚ã®ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒåˆ†ã‹ã£ãŸã€‚<br><img src="https://github.com/user-attachments/assets/9619c7e5-0612-45de-8717-1634bee509b7" alt="image" loading="lazy"><br><br>[^1]:æ·±ã•1ã®Layerã®Hidden State H^1ã‹ã‚‰çµ¶å¯¾ä½ç½®ã®å¾©å…ƒãŒå¯èƒ½ã§ã‚ã‚Šï¼ˆã¤ã¾ã‚Šã€å½“è©²ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®HãŒçµ¶å¯¾ä½ç½®ã«é–¢ã™ã‚‹æƒ…å ±ã‚’ä¿æŒã—ã¦ã„ã‚‹ï¼‰ã€ã“ã®å‰æã®ã‚‚ã¨ã€å¾Œç¶šã®LayerãŒã“ã®æƒ…å ±ã‚’ä¸Šæ›¸ãã—ãªã„ã¨ä»®å®šã—ãŸå ´åˆã«ã€ç›¸å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Ÿç¾ã§ãã‚‹ã€‚</p>
<p>ã¾ãŸã€CoT/Scratchpadã¯long sequenceã«å¯¾ã™ã‚‹æ±åŒ–æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒsmall scaleã§ã¯ã‚ã‚‹ãŒå…ˆè¡Œç ”ç©¶ã§ç¤ºã•ã‚Œã¦ãŠã‚Šã€Positional Encodingã‚’å¤‰åŒ–ã•ã›ãŸæ™‚ã«CoT/Scratchpadã®æ€§èƒ½ã«ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ã‚’èª¿æŸ»ã€‚<br><br>å…·ä½“çš„ã«ã¯ã€CoT/Scratchpadã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒã©ã®ã‚ˆã†ãªã‚‚ã®ãŒæœ‰åŠ¹ã‹ã‚‚æ˜ã‚‰ã‹ã§ã¯ãªã„ã®ã§ã€5ç¨®é¡ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ„ã¿åˆã‚ã›ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æ§‹æˆã—ã€mathematical reasoningã‚¿ã‚¹ã‚¯ã§ä»¥ä¸‹ã®ã‚ˆã†ãªè¨­å®šã§è¨“ç·´ã—<br><br>- ã•ã¾ã–ã¾ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ„ã¿åˆã‚ã›ã§ç•°ãªã‚‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä½œæˆã—ã€<br>- å…¨ã¦ã®ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚ã‚Š/ãªã—ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´<br><br>ã“ã‚Œã‚‰ã‚’æ¯”è¼ƒã—ãŸã€‚ã“ã®çµæœã€CoT/Scratchpadã¯ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«é–¢ä¿‚ãªãã€ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã§ã®ã¿æœ‰åŠ¹ï¼ˆæœ‰åŠ¹ã‹ã©ã†ã‹ã¯ã‚¿ã‚¹ã‚¯ä¾å­˜ï¼‰ã§ã‚ã‚‹ã“ã¨ãŒåˆ†ã‹ã£ãŸã€‚ã“ã®ã“ã¨ã‹ã‚‰ã€CoT/Scratcpadï¼ˆã¤ã¾ã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®inputã¨outputã®ä»•æ–¹ï¼‰å˜ä½“ã§ã€long contextã«å¯¾ã™ã‚‹æ±åŒ–æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ããªã„ã®ã§ã€Positional Encodingï¼ˆâ‰’ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼‰ã«ã‚ˆã‚‹long contextã«å¯¾ã™ã‚‹æ±åŒ–æ€§èƒ½ã®å‘ä¸ŠãŒéå¸¸ã«é‡è¦ã§ã‚ã‚‹ã“ã¨ãŒæµ®ãå½«ã‚Šã«ãªã£ãŸã€‚<br><img src="https://github.com/user-attachments/assets/e23c4fbf-84de-4344-a01e-1e7e9e66fa7e" alt="image" loading="lazy"><br><br>ã¾ãŸã€CoT/ScratchpadãŒæœ‰åŠ¹ã ã£ãŸAdditionã«å¯¾ã—ã¦å„Positional Embeddingãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®attentionãŒã©ã®ä½ç½®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŒ‡ã—ã¦ã„ã‚‹ã‹ã‚’ç›¸å¯¾è·é›¢ã§å¯è¦–åŒ–ã—ãŸã¨ã“ã‚ï¼ˆ0ãŒå½“è©²ãƒˆãƒ¼ã‚¯ãƒ³ã€ã¤ã¾ã‚Šç¾åœ¨ã®Scratchpadã«ç€ç›®ã—ã¦ãŠã‚Šã€1ãŒé ã„ãƒˆãƒ¼ã‚¯ãƒ³ã€ã¤ã¾ã‚Šinputã«ç€ç›®ã—ã¦ã„ã‚‹ã“ã¨ã‚’è¡¨ã™ã‚ˆã†ã«æ­£è¦åŒ–ï¼‰ã€NoPEã¨Relative Positional EncodingãŒshort/long rangeã«ãã‚Œãã‚Œãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã™ã‚‹ã‚ˆã†ãªbinomialãªåˆ†å¸ƒãªã®ã«å¯¾ã—ã€ä»–ã®Positional Encodingã§ã¯ã‚ˆã‚Šuniformãªåˆ†å¸ƒã§ã‚ã‚‹ã“ã¨ãŒåˆ†ã‹ã£ãŸã€‚ã“ã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã¯NoPEã¨Relative POã®æ€§èƒ½ãŒé«˜ã‹ã£ãŸãŸã‚ã€binomialãªåˆ†å¸ƒã®æ–¹ãŒã‚ˆã‚Šæœ€é©ã§ã‚ã‚ã†ã“ã¨ãŒç¤ºå”†ã•ã‚ŒãŸã€‚<br><img src="https://github.com/user-attachments/assets/833e6a81-8611-4e79-9d2e-473f7ebee2d0" alt="image" loading="lazy"><br></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Quantization.html" target="_blank" rel="noopener noreferrer">#Quantization</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<span class="issue_date">Issue Date: 2024-09-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1407" target="_blank" rel="noopener noreferrer" class="title-link">LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models, Yixiao Li+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- LoftQã¨ã„ã†æ–°ã—ã„é‡å­åŒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€LLMã«ãŠã‘ã‚‹é‡å­åŒ–ã¨LoRAãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’åŒæ™‚ã«é©ç”¨ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«ã¨ãƒ•ãƒ«ç²¾åº¦ãƒ¢ãƒ‡ãƒ«ã®ä¸ä¸€è‡´ã‚’è»½æ¸›ã—ã€ä¸‹æµã‚¿ã‚¹ã‚¯ã®ä¸€èˆ¬åŒ–ã‚’æ”¹å–„ã€‚è‡ªç„¶è¨€èªç†è§£ã‚„è³ªå•å¿œç­”ãªã©ã®ã‚¿ã‚¹ã‚¯ã§ã€ç‰¹ã«é›£æ˜“åº¦ã®é«˜ã„æ¡ä»¶ä¸‹ã§æ—¢å­˜æ‰‹æ³•ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1271" target="_blank" rel="noopener noreferrer" class="title-link">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head  Checkpoints, Joshua Ainslie+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- Multi-query attentionï¼ˆMQAï¼‰ã¯ã€å˜ä¸€ã®key-value headã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãŠã‚Šã€ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®æ¨è«–ã‚’åŠ‡çš„ã«é«˜é€ŸåŒ–ã—ã¦ã„ã¾ã™ã€‚ãŸã ã—ã€MQAã¯å“è³ªã®ä½ä¸‹ã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚Šã€ã•ã‚‰ã«ã¯ã€ã‚ˆã‚Šé€Ÿã„æ¨è«–ã®ãŸã‚ã ã‘ã«åˆ¥å€‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ãŒæœ›ã¾ã—ããªã„å ´åˆã‚‚ã‚ã‚Šã¾ã™ã€‚æ—¢å­˜ã®ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨ˆé‡ã®5%ã‚’ä½¿ç”¨ã—ã¦MQAã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã«ã‚¢ãƒƒãƒ—ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®ãƒ¬ã‚·ãƒ”ã‚’ææ¡ˆã—ã€ã•ã‚‰ã«ã€è¤‡æ•°ã®key-value headã‚’ä½¿ç”¨ã™ã‚‹ãƒãƒ«ãƒã‚¯ã‚¨ãƒªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ä¸€èˆ¬åŒ–ã§ã‚ã‚‹ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã‚¯ã‚¨ãƒªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆGQAï¼‰ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ã‚¢ãƒƒãƒ—ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸGQAãŒã€MQAã¨åŒç­‰ã®é€Ÿåº¦ã§ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã«åŒ¹æ•µã™ã‚‹å“è³ªã‚’é”æˆã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>é€šå¸¸ã®Multi-Head AttentionãŒQKVãŒ1å¯¾1å¯¾å¿œãªã®ã«å¯¾ã—ã€Multi Query Attention (MQA) <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1272" target="_blank" rel="noopener noreferrer">Fast Transformer Decoding: One Write-Head is All You Need, Noam Shazeer, N/A, arXiv'19</a>
  ã¯å…¨ã¦ã®Qã«å¯¾ã—ã¦KVã‚’å…±æœ‰ã™ã‚‹ã€‚ä¸€æ–¹ã€GQAã¯ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«KVã‚’å…±æœ‰ã™ã‚‹ç‚¹ã§ç•°ãªã‚‹ã€‚MQAã¯å¤§å¹…ã«Infeerence` speedãŒæ”¹å–„ã™ã‚‹ãŒã€ç²¾åº¦ãŒåŠ£åŒ–ã™ã‚‹å•é¡ŒãŒã‚ã£ãŸã€‚ã“ã®ç ”ç©¶ã§ã¯é€šå¸¸ã®Multi-Head Attentionã«å¯¾ã—ã¦ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã®äº‹å‰å­¦ç¿’ã«å¯¾ã—ã¦è¿½åŠ ã®5%ã®è¨ˆç®—é‡ã§GQAãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/70ec2179-428c-47b8-af53-cb3cc0e4f022" alt="image" loading="lazy"><br><br></p>
<p>Main Result. Multi-Head Attentionã«å¯¾ã—ã¦ã€inference timeãŒå¤§å¹…ã«æ”¹å–„ã—ã¦ã„ã‚‹ãŒã€Multi-Query Attentionã‚ˆã‚Šã‚‚é«˜ã„æ€§èƒ½ã‚’ç¶­æŒã—ã¦ã„ã‚‹ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/3687aeb4-90b8-403d-853b-740121dd5f98" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<span class="issue_date">Issue Date: 2024-01-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1211" target="_blank" rel="noopener noreferrer" class="title-link">VeRA: Vector-based Random Matrix Adaptation, Dawid J. Kopiczko+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€å¤§è¦æ¨¡ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã®fine-tuningã«ãŠã„ã¦ã€è¨“ç·´å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å‰Šæ¸›ã™ã‚‹ãŸã‚ã®æ–°ã—ã„æ‰‹æ³•ã§ã‚ã‚‹ãƒ™ã‚¯ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ©ãƒ³ãƒ€ãƒ è¡Œåˆ—é©å¿œï¼ˆVeRAï¼‰ã‚’ææ¡ˆã™ã‚‹ã€‚VeRAã¯ã€å…±æœ‰ã•ã‚Œã‚‹ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã¨å°ã•ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€åŒã˜æ€§èƒ½ã‚’ç¶­æŒã—ãªãŒã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å‰Šæ¸›ã™ã‚‹ã€‚GLUEã‚„E2Eã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã€ç”»åƒåˆ†é¡ã‚¿ã‚¹ã‚¯ã§ã®åŠ¹æœã‚’ç¤ºã—ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚‚å¿œç”¨ã§ãã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-11-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1163" target="_blank" rel="noopener noreferrer" class="title-link">Exponentially Faster Language Modelling, Peter Belcak+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- UltraFastBERTã¯ã€æ¨è«–æ™‚ã«ã‚ãšã‹0.3%ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã—ã‹ä½¿ç”¨ã›ãšã€åŒç­‰ã®æ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ãŒã§ãã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚UltraFastBERTã¯ã€é«˜é€Ÿãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆFFFï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€åŠ¹ç‡çš„ãªå®Ÿè£…ã‚’æä¾›ã—ã¾ã™ã€‚æœ€é©åŒ–ã•ã‚ŒãŸãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®å®Ÿè£…ã«æ¯”ã¹ã¦78å€ã®é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã—ã€ãƒãƒƒãƒå‡¦ç†ã•ã‚ŒãŸæ¨è«–ã«å¯¾ã—ã¦ã¯40å€ã®é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ¼ãƒ‰ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã€ãŠã‚ˆã³ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚‚å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<a class="button" href="articles/Prompting.html" target="_blank" rel="noopener noreferrer">#Prompting</a>
<span class="issue_date">Issue Date: 2023-11-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1135" target="_blank" rel="noopener noreferrer" class="title-link">Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads  to Answers Faster, Hongxuan Zhang+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- ã“ã®ç ”ç©¶ã§ã¯ã€FastCoTã¨ã„ã†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã¾ã™ã€‚FastCoTã¯ã€LLMã‚’ä½¿ç”¨ã—ã¦ä¸¦åˆ—ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨è‡ªå·±å›å¸°ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’åŒæ™‚ã«è¡Œã„ã€è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’æœ€å¤§é™ã«æ´»ç”¨ã—ã¾ã™ã€‚ã¾ãŸã€FastCoTã¯æ¨è«–æ™‚é–“ã‚’ç´„20%ç¯€ç´„ã—ã€æ€§èƒ½ã®ä½ä¸‹ãŒã»ã¨ã‚“ã©ãªã„ã“ã¨ã‚’å®Ÿé¨“ã§ç¤ºã—ã¾ã—ãŸã€‚ã•ã‚‰ã«ã€ç•°ãªã‚‹ã‚µã‚¤ã‚ºã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«å¯¾ã—ã¦ã‚‚é ‘å¥æ€§ã‚’ç¤ºã™ã“ã¨ãŒã§ãã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>è«–æ–‡ä¸­ã®å›³ã‚’è¦‹ãŸãŒã€å…¨ãã‚ã‹ã‚‰ãªã‹ã£ãŸãƒ»ãƒ»ãƒ»ã€‚ã¡ã‚ƒã‚“ã¨èª­ã¾ãªã„ã¨ã‚ã‹ã‚‰ãªãã†ã§ã‚ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/QuestionAnswering.html" target="_blank" rel="noopener noreferrer">#QuestionAnswering</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<span class="issue_date">Issue Date: 2023-09-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1045" target="_blank" rel="noopener noreferrer" class="title-link">LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models, Yukang Chen+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’åˆ¶é™ã—ãªãŒã‚‰å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºã‚’æ‹¡å¼µã™ã‚‹åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã§ã‚ã‚‹LongLoRAã‚’ææ¡ˆã—ã¾ã™ã€‚å¾“æ¥ã®æ–¹æ³•ã§ã¯ã€LLMsã®é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚µã‚¤ã‚ºã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯é«˜ã„è¨ˆç®—ã‚³ã‚¹ãƒˆã¨GPUãƒªã‚½ãƒ¼ã‚¹ãŒå¿…è¦ã§ã—ãŸãŒã€ææ¡ˆæ‰‹æ³•ã§ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ‹¡å¼µã‚’é«˜é€ŸåŒ–ã—ã€éè‡ªæ˜ãªè¨ˆç®—ã‚³ã‚¹ãƒˆã®å‰Šæ¸›ã‚’å®Ÿç¾ã—ã¾ã™ã€‚ã¾ãŸã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã‚‚å†è©•ä¾¡ã—ã€LongLoRAã¯ã•ã¾ã–ã¾ãªã‚¿ã‚¹ã‚¯ã§å¼·åŠ›ãªå®Ÿé¨“çµæœã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ã•ã‚‰ã«ã€æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚ã‚‹LongQAã‚‚åé›†ã•ã‚Œã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p># æ¦‚è¦<br><br>contexté•·ãŒå¤§ãã„å ´åˆã§ã‚‚åŠ¹ç‡çš„ã«LoRAã™ã‚‹æ‰‹æ³•ã€‚é€šå¸¸ã®LoRAã§ã¯context lengthãŒå¤§ãããªã‚‹ã«ã¤ã‚Œã¦perplexityãŒå¤§ãããªã£ã¦ã—ã¾ã†ã€‚ä¸€æ–¹ã€é€šå¸¸ã®Finetuningã§ã¯perplexityã¯é«˜ã„æ€§èƒ½ã‚’ç¶­æŒã™ã‚‹ãŒã€è¨ˆç®—ã‚³ã‚¹ãƒˆã¨VRAMã®æ¶ˆè²»é‡ãŒè†¨å¤§ã«ãªã£ã¦ã—ã¾ã†ã€‚LongLoRAã§ã¯ã€perplexityã‚’é€šå¸¸ã®Finetuningã¨åŒç­‰ã«æŠ‘ãˆã¤ã¤ã€VRAMæ¶ˆè²»é‡ã‚‚LoRAã¨åŒç­‰ã€ã‹ã¤ã‚ˆã‚Šå°ã•ãªè¨ˆç®—é‡ã§Finetuningã‚’å®Ÿç¾ã—ã¦ã„ã‚‹ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/fc3d17c7-b1ac-4741-9895-bce70cf0b356" alt="image" loading="lazy"><br><br><br><br># æ‰‹æ³•æ¦‚è¦<br><br>attentionã‚’context lengthå…¨ä½“ã§è¨ˆç®—ã™ã‚‹ã¨inputé•·ã®äºŒä¹—ã®è¨ˆç®—é‡ãŒã‹ã‹ã‚‹ãŸã‚ã€contextã‚’ã„ãã¤ã‹ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†å‰²ã—ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«attentionã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã§è¨ˆç®—é‡å‰Šæ¸›ã€‚ã•ã‚‰ã«ã€ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®attentionã®é–“ã®ä¾å­˜é–¢ä¿‚ã‚’æ‰ãˆã‚‹ãŸã‚ã«ã€ã‚°ãƒ«ãƒ¼ãƒ—ã‚’shiftã•ã›ã¦è¨ˆç®—ã—ãŸã‚‚ã®ã¨æœ€çµ‚çš„ã«çµ„ã¿åˆã‚ã›ã¦ã„ã‚‹ã€‚ã¾ãŸã€embedding, normalization layerã‚‚trainableã«ã—ã¦ã„ã‚‹ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/2b443a4c-73da-4610-8ee2-cccdeab21efa" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2023-09-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1039" target="_blank" rel="noopener noreferrer" class="title-link">Textbooks Are All You Need II: phi-1.5 technical report, Yuanzhi Li+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- ç§ãŸã¡ã¯ã€å°ã•ãªTransformerãƒ™ãƒ¼ã‚¹ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹TinyStoriesã¨ã€å¤§è¦æ¨¡ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹phi-1ã®èƒ½åŠ›ã«ã¤ã„ã¦èª¿æŸ»ã—ã¾ã—ãŸã€‚ã¾ãŸã€phi-1ã‚’ä½¿ç”¨ã—ã¦æ•™ç§‘æ›¸ã®å“è³ªã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã€å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ”¹å–„ã™ã‚‹æ–¹æ³•ã‚’ææ¡ˆã—ã¾ã—ãŸã€‚ã•ã‚‰ã«ã€phi-1.5ã¨ã„ã†æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã€è‡ªç„¶è¨€èªã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦æ€§èƒ½ãŒå‘ä¸Šã—ã€è¤‡é›‘ãªæ¨è«–ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã—ãŸã€‚phi-1.5ã¯ã€è‰¯ã„ç‰¹æ€§ã¨æ‚ªã„ç‰¹æ€§ã‚’æŒã£ã¦ãŠã‚Šã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/766" target="_blank" rel="noopener noreferrer">Textbooks Are All You Need, Suriya Gunasekar+, N/A, arXiv'23</a>
 ã«ç¶šãè«–æ–‡</p></span><br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-08-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/921" target="_blank" rel="noopener noreferrer" class="title-link">Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding, Xuefei Ning+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- ã“ã®ç ”ç©¶ã§ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®ç”Ÿæˆé…å»¶ã‚’æ¸›ã‚‰ã™ãŸã‚ã«ã€æ€è€ƒã®éª¨çµ„ã¿ï¼ˆSoTï¼‰ã¨ã„ã†æ‰‹æ³•ã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚SoTã¯ã€å›ç­”ã®éª¨çµ„ã¿ã‚’ã¾ãšç”Ÿæˆã—ã€ãã®å¾Œã«å†…å®¹ã‚’ä¸¦åˆ—ã§å‡¦ç†ã™ã‚‹ã“ã¨ã§é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚ã¾ãŸã€å›ç­”å“è³ªã®å‘ä¸Šã‚‚æœŸå¾…ã•ã‚Œã¾ã™ã€‚SoTã¯ãƒ‡ãƒ¼ã‚¿ä¸­å¿ƒã®æœ€é©åŒ–ã®åˆã‚ã®è©¦ã¿ã§ã‚ã‚Šã€LLMsã®äººé–“ã‚‰ã—ã„æ€è€ƒã‚’å¯èƒ½ã«ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>æœ€åˆã«å›ç­”ã®æ çµ„ã¿ã ã‘ç”Ÿæˆã—ã¦ã€ãã‚Œãã‚Œã®å†…å®¹ã‚’ä¸¦åˆ—ã§å‡ºåŠ›ã•ã›ã‚‹ã“ã¨ã§ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’é«˜é€ŸåŒ–ã—ã¾ã—ã‚‡ã†ã€ã¨ã„ã†è©±ã€‚<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/fb25d8ba-dff7-4f6f-be25-0973488f6e8a" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-07-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/905" target="_blank" rel="noopener noreferrer" class="title-link">FrugalGPT: How to Use Large Language Models While Reducing Cost and  Improving Performance, Lingjiao Chen+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®ä½¿ç”¨ã«ã¯é«˜ã„ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚‹ãŸã‚ã€LLMsã®æ¨è«–ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã™ã‚‹ãŸã‚ã®3ã¤ã®æˆ¦ç•¥ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é©å¿œã€LLMã®è¿‘ä¼¼ã€LLMã®ã‚«ã‚¹ã‚±ãƒ¼ãƒ‰ï¼‰ã‚’ææ¡ˆã™ã‚‹ã€‚FrugalGPTã¨ã„ã†å…·ä½“çš„ãªæ‰‹æ³•ã‚’ç´¹ä»‹ã—ã€æœ€å¤§98ï¼…ã®ã‚³ã‚¹ãƒˆå‰Šæ¸›ã¨4ï¼…ã®ç²¾åº¦å‘ä¸Šã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€LLMsã®æŒç¶šå¯èƒ½ãªä½¿ç”¨ãŒå¯èƒ½ã¨ãªã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>é™ã‚‰ã‚ŒãŸäºˆç®—ã®ä¸­ã§ã€ã„ã‹ã«è¤‡æ•°ã®LLM APIã‚’ä½¿ã„ã€å®‰ã„ã‚³ã‚¹ãƒˆã§é«˜ã„æ€§èƒ½ã‚’é”æˆã™ã‚‹ã‹ã‚’è¿½æ±‚ã—ãŸç ”ç©¶ã€‚<br><br>LLM Cascadeãªã©ã¯ã“ã®æ çµ„ã¿ã§ãªãã¦ã‚‚è‰²ã€…ã¨ä½¿ã„é“ãŒã‚ã‚Šãã†ã€‚Question Concatenationã¯å®Ÿè³ªBatch Promptingã€‚</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Prompting.html" target="_blank" rel="noopener noreferrer">#Prompting</a>
<span class="issue_date">Issue Date: 2023-07-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/900" target="_blank" rel="noopener noreferrer" class="title-link">Batch Prompting: Efficient Inference with Large Language Model APIs, Zhoujun Cheng+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡ãªè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã‚’åŠ¹æœçš„ã«ä½¿ç”¨ã™ã‚‹ãŸã‚ã«ã€ãƒãƒƒãƒãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã¨ã„ã†æ‰‹æ³•ã‚’ææ¡ˆã—ã¾ã™ã€‚ã“ã®æ‰‹æ³•ã¯ã€LLMãŒ1ã¤ã®ã‚µãƒ³ãƒ—ãƒ«ã§ã¯ãªããƒãƒƒãƒã§æ¨è«–ã‚’è¡Œã†ã“ã¨ã‚’å¯èƒ½ã«ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆã¨æ™‚é–“ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã—ãªãŒã‚‰ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã—ã¾ã™ã€‚ã•ã¾ã–ã¾ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®å®Ÿé¨“ã«ã‚ˆã‚Šã€ãƒãƒƒãƒãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ãŒLLMã®æ¨è«–ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã€è‰¯å¥½ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚ã¾ãŸã€ãƒãƒƒãƒãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã¯ç•°ãªã‚‹æ¨è«–æ–¹æ³•ã«ã‚‚é©ç”¨ã§ãã¾ã™ã€‚è©³ç´°ã¯GitHubã®ãƒªãƒã‚¸ãƒˆãƒªã§ç¢ºèªã§ãã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/16aaed9b-da2b-4c38-86df-e223bdbec826" alt="image" loading="lazy"><br><br>10ç¨®é¡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è©¦ã—ãŸçµæœã€ãƒãƒƒãƒã«ã—ã¦ã‚‚æ€§èƒ½ã¯ä¸ŠãŒã£ãŸã‚Šä¸‹ãŒã£ãŸã‚Šã—ã¦ã„ã‚‹ã€‚è‘—è€…ã‚‰ã¯é¡ä¼¼ã—ãŸæ€§èƒ½ãŒå‡ºã¦ã„ã‚‹ã®ã§ã€ã‚³ã‚¹ãƒˆå‰Šæ¸›ã«ãªã‚‹ã¨çµè«–ã¥ã‘ã¦ã„ã‚‹ã€‚</p>
<p>Batch sizeãŒå¤§ãããªã‚‹ã«é€£ã‚Œã¦æ€§èƒ½ãŒä½ä¸‹ã—ã€ã‹ã¤ã‚¿ã‚¹ã‚¯ã®é›£æ˜“åº¦ãŒé«˜ã„ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ä½ä¸‹ãŒè‘—ã—ã„ã“ã¨ãŒå ±å‘Šã•ã‚Œã¦ã„ã‚‹ã€‚ã¾ãŸã€contextãŒé•·ã‘ã‚Œã°é•·ã„ã»ã©ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤§ããã—ãŸéš›ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ä½ä¸‹ãŒè‘—ã—ã„ã€‚</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/Quantization.html" target="_blank" rel="noopener noreferrer">#Quantization</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2023-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/881" target="_blank" rel="noopener noreferrer" class="title-link">QLoRA: Efficient Finetuning of Quantized LLMs, Tim Dettmers+, N_A, NeurIPS'23</a>
<span class="snippet"><span>GPT Summary</span>- ç§ãŸã¡ã¯ã€QLoRAã¨ã„ã†åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã‚’ææ¡ˆã—ã¾ã™ã€‚ã“ã®æ‰‹æ³•ã¯ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã—ã€48GBã®å˜ä¸€ã®GPUä¸Šã§65Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã¾ãŸã€16ãƒ“ãƒƒãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã—ã¾ã™ã€‚QLoRAã¯ã€å‡çµã•ã‚ŒãŸ4ãƒ“ãƒƒãƒˆé‡å­åŒ–ã•ã‚ŒãŸäº‹å‰å­¦ç¿’æ¸ˆã¿è¨€èªãƒ¢ãƒ‡ãƒ«ã®å‹¾é…ã‚’Low Rank Adaptersï¼ˆLoRAï¼‰ã«é€†ä¼æ’­ã•ã›ã¾ã™ã€‚ç§ãŸã¡ã®æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã§ã‚ã‚‹Guanacoã¯ã€Vicunaãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ä»¥å‰ã«å…¬é–‹ã•ã‚ŒãŸã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚Šã€ChatGPTã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒ™ãƒ«ã®99.3%ã«é”ã—ã¾ã™ã€‚ã¾ãŸã€å˜ä¸€ã®GPUä¸Šã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯24æ™‚é–“ã—ã‹ã‹ã‹ã‚Šã¾ã›ã‚“ã€‚QLoRAã¯ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’çŠ ç‰²ã«ã™ã‚‹ã“ã¨ãªããƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„ã™ã‚‹ãŸã‚ã®ã„ãã¤ã‹ã®é©æ–°ã‚’å°å…¥ã—ã¦ã„ã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€4ãƒ“ãƒƒãƒˆNormalFloatï¼ˆNF4ï¼‰ã¨ã„ã†æƒ…å ±ç†è«–çš„ã«æœ€é©ãªæ–°ã—ã„ãƒ‡ãƒ¼ã‚¿å‹ã€ãƒ€ãƒ–ãƒ«é‡å­åŒ–ã«ã‚ˆã‚‹å¹³å‡ãƒ¡ãƒ¢ãƒªãƒ•ãƒƒãƒˆãƒ—ãƒªãƒ³ãƒˆã®å‰Šæ¸›ã€ãŠã‚ˆã³ãƒšãƒ¼ã‚¸ãƒ‰ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªã‚¹ãƒ‘ã‚¤ã‚¯ã®ç®¡ç†ã§ã™ã€‚ç§ãŸã¡ã¯QLoRAã‚’ä½¿ç”¨ã—ã¦1,000ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€8ã¤ã®å‘½ä»¤ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¤ãƒ—ï¼ˆLLaMAã€T5ï¼‰ã€ãŠã‚ˆã³å¾“æ¥ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯å®Ÿè¡Œä¸å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆ33BãŠã‚ˆã³65Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ï¼‰ã«ã‚ãŸã‚‹å‘½ä»¤ã®è¿½è·¡ã¨ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®è©³ç´°ãªåˆ†æã‚’æä¾›ã—ã¾ã™ã€‚ç§ãŸã¡ã®çµæœã¯ã€QLoRAã‚’ä½¿ç”¨ã—ã¦å°è¦æ¨¡ãªé«˜å“è³ªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒã€ä»¥å‰ã®SoTAã‚ˆã‚Šã‚‚å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚‚æœ€å…ˆç«¯ã®çµæœã‚’ã‚‚ãŸã‚‰ã™ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ã¾ãŸã€äººé–“ã®è©•ä¾¡ã¨GPT-4ã®è©•ä¾¡ã«åŸºã¥ã„ãŸãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®è©³ç´°ãªåˆ†æã‚’æä¾›ã—ã€GPT-4ã®è©•ä¾¡ãŒå®‰ä¾¡ã§åˆç†çš„ãªäººé–“ã®è©•ä¾¡ã®ä»£æ›¿æ‰‹æ®µã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚ã•ã‚‰ã«ã€ç¾åœ¨ã®ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒ™ãƒ«ã‚’æ­£ç¢ºã«è©•ä¾¡ã™ã‚‹ãŸã‚ã«ã¯ä¿¡é ¼æ€§ãŒãªã„ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚GuanacoãŒChatGPTã¨æ¯”è¼ƒã—ã¦ã©ã“ã§å¤±æ•—ã™ã‚‹ã‹ã‚’ç¤ºã™åˆ†æã‚‚è¡Œã£ã¦ã„ã¾ã™ã€‚ç§ãŸã¡ã¯ã€4ãƒ“ãƒƒãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãŸã‚ã®CUDAã‚«ãƒ¼ãƒãƒ«ã‚’å«ã‚€ã€ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã¨ã‚³ãƒ¼ãƒ‰ã‚’å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å®Ÿè£…: 


<a href="https://github.com/artidoro/qlora" target="_blank" rel="noopener noreferrer">https://github.com/artidoro/qlora</a>


<br>PEFTã«ã‚‚ã‚ã‚‹</p>
<p>å‚è€ƒ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1662946722690236417?s=46&t=TDHYK31QiXKxggPzhZbcAQ"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>OpenReview:


<a href="https://openreview.net/forum?id=OUIFPHEgJU&referrer=%5Bthe%20profile%20of%20Ari%20Holtzman%5D(%2Fprofile%3Fid%3D~Ari_Holtzman1)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=OUIFPHEgJU&referrer=%5Bthe%20profile%20of%20Ari%20Holtzman%5D(%2Fprofile%3Fid%3D~Ari_Holtzman1)</a>


</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/DynamicNetworks.html" target="_blank" rel="noopener noreferrer">#DynamicNetworks</a>
<a class="button" href="articles/Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<span class="issue_date">Issue Date: 2023-07-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/856" target="_blank" rel="noopener noreferrer" class="title-link">PAD-Net: An Efficient Framework for Dynamic Networks, ACL'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ä¸€èˆ¬çš„ãªå•é¡Œç‚¹ã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€éƒ¨åˆ†çš„ã«ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆPAD-Netï¼‰ã‚’ææ¡ˆã—ã¾ã™ã€‚PAD-Netã¯ã€å†—é•·ãªãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’é™çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«å¤‰æ›ã™ã‚‹ã“ã¨ã§ã€å±•é–‹ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã—ã€åŠ¹ç‡çš„ãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’å®Ÿç¾ã—ã¾ã™ã€‚å®Ÿé¨“çµæœã§ã¯ã€PAD-NetãŒç”»åƒåˆ†é¡ã¨è¨€èªç†è§£ã®ã‚¿ã‚¹ã‚¯ã§é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ã€å¾“æ¥ã®ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã—ãŸã€‚</span>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Ensemble.html" target="_blank" rel="noopener noreferrer">#Ensemble</a>
<a class="button" href="articles/TransferLearning.html" target="_blank" rel="noopener noreferrer">#TransferLearning</a>
<span class="issue_date">Issue Date: 2023-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/822" target="_blank" rel="noopener noreferrer" class="title-link">Parameter-efficient Weight Ensembling Facilitates Task-level Knowledge Transfer, ACL'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ€è¿‘ã®ç ”ç©¶ã§ã¯ã€å¤§è¦æ¨¡ãªäº‹å‰å­¦ç¿’æ¸ˆã¿è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã«åŠ¹æœçš„ã«é©å¿œã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚æœ¬ç ”ç©¶ã§ã¯ã€è»½é‡ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¦ã‚¿ã‚¹ã‚¯é–“ã§çŸ¥è­˜ã‚’è»¢é€ã™ã‚‹æ–¹æ³•ã‚’æ¢æ±‚ã—ã€ãã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼ã—ã¾ã—ãŸã€‚å®Ÿé¨“çµæœã¯ã€ææ¡ˆæ‰‹æ³•ãŒãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«æ¯”ã¹ã¦5ï¼…ã€œ8ï¼…ã®æ”¹å–„ã‚’ç¤ºã—ã€ã‚¿ã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«ã®çŸ¥è­˜è»¢é€ã‚’å¤§å¹…ã«ä¿ƒé€²ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚</span>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Zero/Few/ManyShotPrompting.html" target="_blank" rel="noopener noreferrer">#Zero/Few/ManyShotPrompting</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2023-07-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/817" target="_blank" rel="noopener noreferrer" class="title-link">FiD-ICL: A Fusion-in-Decoder Approach for Efficient In-Context Learning, ACL'23</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡ãªäº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸfew-shot in-context learningï¼ˆICLï¼‰ã«ãŠã„ã¦ã€fusion-in-decoderï¼ˆFiDï¼‰ãƒ¢ãƒ‡ãƒ«ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§åŠ¹ç‡ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã“ã¨ã‚’æ¤œè¨¼ã™ã‚‹ã€‚FiD-ICLã¯ä»–ã®ãƒ•ãƒ¥ãƒ¼ã‚¸ãƒ§ãƒ³æ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã€æ¨è«–æ™‚é–“ã‚‚10å€é€Ÿããªã‚‹ã€‚ã¾ãŸã€FiD-ICLã¯å¤§è¦æ¨¡ãªãƒ¡ã‚¿ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚‚å¯èƒ½ã«ã™ã‚‹ã€‚</span>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<span class="issue_date">Issue Date: 2023-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/769" target="_blank" rel="noopener noreferrer" class="title-link">Full Parameter Fine-tuning for Large Language Models with Limited  Resources, Kai Lv+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- LLMsã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯è†¨å¤§ãªGPUãƒªã‚½ãƒ¼ã‚¹ãŒå¿…è¦ã§ã‚ã‚Šã€æ—¢å­˜ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯é™ã‚‰ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ã§ã®å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®èª¿æ•´ã«å¯¾å‡¦ã—ã¦ã„ãªã„ã€‚æœ¬ç ”ç©¶ã§ã¯ã€LOMOã¨ã„ã†æ–°ã—ã„æœ€é©åŒ–æ‰‹æ³•ã‚’ææ¡ˆã—ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã™ã‚‹ã“ã¨ã§ã€8ã¤ã®RTX 3090ã‚’æ­è¼‰ã—ãŸå˜ä¸€ã®ãƒã‚·ãƒ³ã§65Bãƒ¢ãƒ‡ãƒ«ã®å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¯èƒ½ã«ãªã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>8xRTX3090 24GBã®ãƒã‚·ãƒ³ã§65Bãƒ¢ãƒ‡ãƒ«ã®å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã‚‹æ‰‹æ³•ã€‚LoRAã®ã‚ˆã†ãªï¼ˆæ–°ãŸã«è¿½åŠ ã—ã‚ŒãŸï¼‰ä¸€éƒ¨ã®é‡ã¿ã‚’ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã™ã‚‹ã‚ˆã†ãªæ çµ„ã¿ã§ã¯ãªã„ã€‚å‹¾é…è¨ˆç®—ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã‚’one stepã§å®Ÿæ–½ã™ã‚‹ã“ã¨ã§å®Ÿç¾ã—ã¦ã„ã‚‹ã¨ã®ã“ã¨ã€‚</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2023-06-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/766" target="_blank" rel="noopener noreferrer" class="title-link">Textbooks Are All You Need, Suriya Gunasekar+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€å°è¦æ¨¡ãªphi-1ã¨ã„ã†æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ç”¨å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç´¹ä»‹ã—ã€8ã¤ã®A100ã§4æ—¥é–“ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸçµæœã€HumanEvalã§pass@1ã®æ­£è§£ç‡50.6ï¼…ã€MBPPã§55.5ï¼…ã‚’é”æˆã—ãŸã“ã¨ã‚’å ±å‘Šã—ã¦ã„ã¾ã™ã€‚ã¾ãŸã€phi-1ã¯ã€phi-1-baseã‚„phi-1-smallã¨æ¯”è¼ƒã—ã¦ã€é©šãã¹ãæ–°ã—ã„æ€§è³ªã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚phi-1-smallã¯ã€HumanEvalã§45ï¼…ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å‚è€ƒ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1671643297616654342?s=46&t=JYDYid2m0v7vYaL7jhZYjQ"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ•™ç§‘æ›¸ã®ã‚ˆã†ãªå“è³ªã®è‰¯ã„ãƒ†ã‚­ã‚¹ãƒˆã§äº‹å‰å­¦ç¿’ã™ã‚‹ã¨æ€§èƒ½ãŒå‘ä¸Šã—ï¼ˆã‚°ãƒ©ãƒ•çœŸã‚“ä¸­ï¼‰ã€ã•ã‚‰ã«è‰¯è³ªãªã‚¨ã‚¯ã‚µã‚µã‚¤ã‚ºã§Finetuningã™ã‚‹ã¨ã‚ˆã‚Šæ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ï¼ˆã‚°ãƒ©ãƒ•å³ï¼‰<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/9f0b945a-f965-42ae-b5d8-ac464359af35" alt="image" loading="lazy"></p>
<p>æ—¥æœ¬èªè§£èª¬: 


<a href="https://dalab.jp/archives/journal/introduction-textbooks-are-all-you-need/" target="_blank" rel="noopener noreferrer">https://dalab.jp/archives/journal/introduction-textbooks-are-all-you-need/</a>


</p>
<p>ã–ã£ãã‚Šè¨€ã†ã¨ã€æ•™ç§‘æ›¸ã§äº‹å‰å­¦ç¿’ã—ã€ã‚¨ã‚¯ã‚µã‚µã‚¤ã‚ºã§Finetuningã™ã‚‹ã¨æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ï¼ˆ= ã‚ˆã‚Šå¤§ãã„ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã®æ€§èƒ½ãŒå¾—ã‚‰ã‚Œã‚‹ï¼‰ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/Inference.html" target="_blank" rel="noopener noreferrer">#Inference</a>
<span class="issue_date">Issue Date: 2023-04-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/601" target="_blank" rel="noopener noreferrer" class="title-link">Efficiently Scaling Transformer Inference, Reiner Pope+, N_A, MLSys'23</a>
<span class="snippet"><span>GPT Summary</span>- - å¤§è¦æ¨¡Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã®ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€æœ€é©ãªå¤šæ¬¡å…ƒåˆ†å‰²æŠ€è¡“ã‚’é¸æŠã™ã‚‹ãŸã‚ã®å˜ç´”ãªè§£æãƒ¢ãƒ‡ãƒ«ã‚’é–‹ç™º- ä½ãƒ¬ãƒ™ãƒ«ã®æœ€é©åŒ–ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€500B+ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«FLOPSåˆ©ç”¨ç‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã«ãŠã„ã¦ã€FasterTransformerãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆã‚’ä¸Šå›ã‚‹æ–°ã—ã„Paretoãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã‚’å®Ÿç¾- é©åˆ‡ãªåˆ†å‰²ã«ã‚ˆã‚Šã€ãƒãƒ«ãƒã‚¯ã‚¨ãƒªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ä½ã„ãƒ¡ãƒ¢ãƒªè¦ä»¶ã«ã‚ˆã‚Šã€32å€ã®å¤§ããªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯èƒ½- int8ã‚¦ã‚§ã‚¤ãƒˆé‡å­åŒ–ã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆä¸­ã®ä½ãƒãƒƒãƒã‚µã‚¤ã‚ºãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Š29msã§ã‚ã‚Šã€å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®å¤§ãƒãƒƒãƒã‚µã‚¤ã‚ºå‡¦ç†ã«ãŠã„ã¦76ï¼…ã®MFUã‚’å®Ÿç¾ã—ã€PaLM 540Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦2048ãƒˆãƒ¼ã‚¯ãƒ³ã®é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>ç‰¹ã«Multiquery Attentionã¨ã„ã†æŠ€è¡“ãŒTransformerã®inferenceã®ã‚³ã‚¹ãƒˆå‰Šæ¸›ã«æœ‰åŠ¹ã‚‰ã—ã„</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/TACL.html" target="_blank" rel="noopener noreferrer">#TACL</a>
<span class="issue_date">Issue Date: 2023-04-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/525" target="_blank" rel="noopener noreferrer" class="title-link">Efficient Methods for Natural Language Processing: A Survey, Treviso+, TACL'23</a>
<span class="snippet"><span>GPT Summary</span>- NLPã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã«ã¯ã‚¹ã‚±ãƒ¼ãƒ«ã®æ‹¡å¤§ãŒé‡è¦ã ãŒã€ãƒªã‚½ãƒ¼ã‚¹æ¶ˆè²»ã‚‚å¢—åŠ ã™ã‚‹ã€‚é™ã‚‰ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ã§åŠ¹ç‡çš„ã«NLPã‚’å®Ÿæ–½ã™ã‚‹æ–¹æ³•ã‚’çµ±åˆã—ã€æŒ‡é‡ã‚’æä¾›ã€‚åŠ¹ç‡çš„ãªæ‰‹æ³•ã®é–‹ç™ºã«å‘ã‘ãŸç ”ç©¶æ–¹å‘ã‚’ç¤ºå”†ã€‚</span>
<span class="snippet"><span>Comment</span><p>ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã§ã‚´ãƒªæŠ¼ã™ã‚ˆã†ãªæ–¹æ³•ã§ã¯ãªãã€"Efficient"ã«è¡Œã†ãŸã‚ã®æ‰‹æ³•ã‚’ã¾ã¨ã‚ã¦ã„ã‚‹<br><br><img src="https://user-images.githubusercontent.com/12249301/234287218-2d42766f-5c5c-4cf9-859e-c2b0a5dfd4c3.jpeg" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/Stability.html" target="_blank" rel="noopener noreferrer">#Stability</a>
<span class="issue_date">Issue Date: 2025-09-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2644" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] StableMoE: Stable Routing Strategy for Mixture of Experts, Damai Dai+, arXiv'22</a>
<span class="snippet"><span>GPT Summary</span>- StableMoEã¯ã€ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã®å¤‰å‹•å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«2ã¤ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’æŒã¤Mixture-of-Expertsæ‰‹æ³•ã‚’ææ¡ˆã€‚æœ€åˆã®ã‚¹ãƒ†ãƒ¼ã‚¸ã§ä¸€è²«ã—ãŸãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æˆ¦ç•¥ã‚’å­¦ç¿’ã—ã€è»½é‡ãƒ«ãƒ¼ã‚¿ãƒ¼ã«è’¸ç•™ã€‚ç¬¬äºŒã®ã‚¹ãƒ†ãƒ¼ã‚¸ã§ãã®ãƒ«ãƒ¼ã‚¿ãƒ¼ã‚’ç”¨ã„ã¦ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã¸ã®å‰²ã‚Šå½“ã¦ã‚’å›ºå®šã€‚è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã¨å¤šè¨€èªæ©Ÿæ¢°ç¿»è¨³ã§ã®å®Ÿé¨“ã«ã‚ˆã‚Šã€StableMoEã¯åæŸé€Ÿåº¦ã¨æ€§èƒ½ã§æ—¢å­˜æ‰‹æ³•ã‚’ä¸Šå›ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/vikhyatk/status/1962225296314429543?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-02-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1754" target="_blank" rel="noopener noreferrer" class="title-link">Switch Transformers: Scaling to Trillion Parameter Models with Simple  and Efficient Sparsity, William Fedus+, JMLR'22</a>
<span class="snippet"><span>GPT Summary</span>- Switch Transformerã‚’ææ¡ˆã—ã€Mixture of Experts (MoE)ã®è¤‡é›‘ã•ã‚„é€šä¿¡ã‚³ã‚¹ãƒˆã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ä¸å®‰å®šæ€§ã‚’æ”¹å–„ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ä½ç²¾åº¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã®å¤§è¦æ¨¡ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¯èƒ½ã«ãªã‚Šã€æœ€å¤§7å€ã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦å‘ä¸Šã‚’å®Ÿç¾ã€‚ã•ã‚‰ã«ã€1å…†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€T5-XXLãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦4å€ã®é€Ÿåº¦å‘ä¸Šã‚’é”æˆã€‚</span>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<span class="issue_date">Issue Date: 2023-08-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1000" target="_blank" rel="noopener noreferrer" class="title-link">Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than  In-Context Learning, Haokun Liu+, N_A, arXiv'22</a>
<span class="snippet"><span>GPT Summary</span>- Few-shot in-context learningï¼ˆICLï¼‰ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŠ¹ç‡ã®è‰¯ã„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆPEFTï¼‰ã‚’æ¯”è¼ƒã—ã€PEFTãŒé«˜ã„ç²¾åº¦ã¨ä½ã„è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ã¾ãŸã€æ–°ã—ã„PEFTãƒ¡ã‚½ãƒƒãƒ‰ã§ã‚ã‚‹ï¼ˆIAï¼‰^3ã‚’ç´¹ä»‹ã—ã€ã‚ãšã‹ãªæ–°ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã—ã‹å°å…¥ã—ãªã„ã¾ã¾ã€å¼·åŠ›ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã™ã‚‹ã€‚ã•ã‚‰ã«ã€T-Fewã¨ã„ã†ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¬ã‚·ãƒ”ã‚’ææ¡ˆã—ã€ã‚¿ã‚¹ã‚¯å›ºæœ‰ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„ä¿®æ­£ãªã—ã«æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã«é©ç”¨ã§ãã‚‹ã€‚RAFTãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§T-Fewã‚’ä½¿ç”¨ã—ã€è¶…äººçš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã€æœ€å…ˆç«¯ã‚’6ï¼…çµ¶å¯¾çš„ã«ä¸Šå›ã‚‹ã€‚</span>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2023-05-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/688" target="_blank" rel="noopener noreferrer" class="title-link">FlashAttention: Fast and Memory-Efficient Exact Attention with  IO-Awareness, Tri Dao+, N_A, arXiv'22</a>
<span class="snippet"><span>GPT Summary</span>- ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã—ã¦é…ãã€ãƒ¡ãƒ¢ãƒªã‚’å¤šãæ¶ˆè²»ã™ã‚‹ãŸã‚ã€æ³¨æ„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’æ”¹å–„ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚FlashAttentionã¯ã€ã‚¿ã‚¤ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦ã€GPUã®é«˜å¸¯åŸŸå¹…ãƒ¡ãƒ¢ãƒªï¼ˆHBMï¼‰ã¨GPUã®ã‚ªãƒ³ãƒãƒƒãƒ—SRAMé–“ã®ãƒ¡ãƒ¢ãƒªèª­ã¿å–ã‚Š/æ›¸ãè¾¼ã¿ã®æ•°ã‚’æ¸›ã‚‰ã—ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’é«˜é€Ÿã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã‚‹ã€‚FlashAttentionã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã§ã‚ˆã‚Šé•·ã„æ–‡è„ˆã‚’å¯èƒ½ã«ã—ã€ã‚ˆã‚Šé«˜å“è³ªãªãƒ¢ãƒ‡ãƒ«ã‚„ã€å®Œå…¨ã«æ–°ã—ã„æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>ã‚ˆã‚Šé«˜é€ŸãªGPUä¸Šã®SRAMä¸Šã§è¨ˆç®—ã§ãã‚‹ã‚ˆã†ã«QKVã‚’ãƒ–ãƒ­ãƒƒã‚¯å˜ä½ã«åˆ†å‰²ã—ã¦è¨ˆç®—ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šé«˜ã„è¨ˆç®—åŠ¹ç‡ã‚’å®Ÿç¾ã™ã‚‹FlashAttentionã‚’ææ¡ˆ[^1]<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/e3cb11b7-f413-4831-bea6-97886b683ff7" alt="image" loading="lazy"><br><br>[^1]: ï¼ˆ2025.05.24è¿½è¨˜)ä¸‹è¨˜æ—¥æœ¬èªãƒ–ãƒ­ã‚°ã‚’å‚è€ƒã«ä¸€éƒ¨æ–‡è¨€ã‚’è¨‚æ­£ã—ã¾ã—ãŸã€‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚</p>
<p>æ—¥æœ¬èªè§£èª¬:


<a href="https://zenn.dev/sinchir0/articles/21bb6e96c7b05b" target="_blank" rel="noopener noreferrer">https://zenn.dev/sinchir0/articles/21bb6e96c7b05b</a>


<br>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/sinchir0/status/1926199436406849786?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ—¥æœ¬èªè§£èª¬:


<a href="https://zenn.dev/uchiiii/articles/306d0bb7ef67a7" target="_blank" rel="noopener noreferrer">https://zenn.dev/uchiiii/articles/306d0bb7ef67a7</a>


<br>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/aquarobot0202/status/1957109068797018545?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/CollaborativeFiltering.html" target="_blank" rel="noopener noreferrer">#CollaborativeFiltering</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/EducationalDataMining.html" target="_blank" rel="noopener noreferrer">#EducationalDataMining</a>
<a class="button" href="articles/KnowledgeTracing.html" target="_blank" rel="noopener noreferrer">#KnowledgeTracing</a>
<a class="button" href="articles/Contents-based.html" target="_blank" rel="noopener noreferrer">#Contents-based</a>
<a class="button" href="articles/NAACL.html" target="_blank" rel="noopener noreferrer">#NAACL</a>
<span class="issue_date">Issue Date: 2022-08-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/463" target="_blank" rel="noopener noreferrer" class="title-link">GRAM: Fast Fine-tuning of Pre-trained Language Models for Content-based   Collaborative Filtering, Yoonseok Yang+, NAACL'22</a>
<span class="snippet"><span>GPT Summary</span>- ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹ã®å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆCCFï¼‰ã«ãŠã„ã¦ã€PLMã‚’ç”¨ã„ãŸã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ãƒªã‚½ãƒ¼ã‚¹ã‚’æ¶ˆè²»ã™ã‚‹ãŸã‚ã€GRAMï¼ˆå‹¾é…è“„ç©æ‰‹æ³•ï¼‰ã‚’ææ¡ˆã€‚Single-step GRAMã¯ã‚¢ã‚¤ãƒ†ãƒ ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å‹¾é…ã‚’é›†ç´„ã—ã€Multi-step GRAMã¯å‹¾é…æ›´æ–°ã®é…å»¶ã‚’å¢—åŠ ã•ã›ã¦ãƒ¡ãƒ¢ãƒªã‚’å‰Šæ¸›ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€Knowledge Tracingã¨News Recommendationã®ã‚¿ã‚¹ã‚¯ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚’æœ€å¤§146å€æ”¹å–„ã€‚</span>
<span class="snippet"><span>Comment</span><p>RiiiDãŒNAACL'22ã«è«–æ–‡é€šã—ã¦ãŸ</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/ZeroshotHyperparameterTransfer.html" target="_blank" rel="noopener noreferrer">#ZeroshotHyperparameterTransfer</a>
<span class="issue_date">Issue Date: 2025-08-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2582" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Tensor Programs V: Tuning Large Neural Networks via Zero-Shot  Hyperparameter Transfer, Greg Yang+, NeurIPS'21</a>
<span class="snippet"><span>GPT Summary</span>- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯é«˜ã‚³ã‚¹ãƒˆã§ã‚ã‚Šã€ç‰¹ã«å¤§è¦æ¨¡ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã„ã¦è² æ‹…ãŒå¤§ãã„ã€‚æ–°ãŸã«ææ¡ˆã™ã‚‹muTransferã¯ã€æœ€å¤§æ›´æ–°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ï¼ˆmuPï¼‰ã‚’åˆ©ç”¨ã—ã€å°ã•ãªãƒ¢ãƒ‡ãƒ«ã§ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸHPã‚’ãƒ•ãƒ«ã‚µã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã«ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã§è»¢é€ã™ã‚‹æ‰‹æ³•ã§ã‚ã‚‹ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€1300ä¸‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰BERT-largeã‚’è¶…ãˆã‚‹æ€§èƒ½ã‚’é”æˆã—ã€4000ä¸‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ã¯GPT-3ã‚’ä¸Šå›ã‚‹çµæœã‚’å¾—ãŸã€‚ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ã‚¹ãƒˆã¯ãã‚Œãã‚Œäº‹å‰å­¦ç¿’ã‚³ã‚¹ãƒˆã®åŒç­‰ã¾ãŸã¯7%ã«æŠ‘ãˆã‚‰ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview: 


<a href="https://openreview.net/forum?id=Bx6qKuBM2AD" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=Bx6qKuBM2AD</a>


</p>
<p>å°è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿæ–½ã—ã€åŒæ§˜ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã€**å„layerã®widthãŒå¤§ãã„ã‚‚ã®**ã«å¯¾ã—ã¦ã‚‚ã€å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§æœ€é©ã§ã‚ã£ãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’zero-shotã§è»¢ç§»ã™ã‚‹ã“ã¨ã§ near optimalãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å­¦ç¿’ã§ãã‚‹mu Transferã‚’ææ¡ˆã€‚<br><br>ãƒ¢ãƒ‡ãƒ«ã®æ·±ã•ï¼ˆä»¥å¤–ã«ã‚‚ä¸‹è¡¨ä¸­ã®*å°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã«å¯¾ã—ã¦ã‚‚é™å®šçš„ã«è»¢ç§»å¯èƒ½ãªæ¨¡æ§˜ã€‚Post-Layer Normã®Transformerã‚„ã§ã¯ã‚ã¾ã‚Šã†ã¾ãã„ã‹ãªã„ã“ã¨ãŒ11ç¯€ã«è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ï¼ˆå®Ÿé¨“ã¯pre-Layer Norm Transformer, ResNetã«å¯¾ã—ã¦è¡Œã‚ã‚Œã¦ã„ã‚‹æ¨¡æ§˜ï¼‰ã€‚<br>ã¾ãŸã€6.1ç¯€ã§ã¯ã€ï¼ˆå®Ÿé¨“çš„ã«ï¼‰åˆ©ç”¨ã™ã‚‹å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚±ãƒ¼ãƒ«ã¨ã—ã¦å¹…256, æ·±ã•4, ãƒãƒƒãƒã‚µã‚¤ã‚º32, sequenceé•·128, è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°5000ã‚’æœ€ä½æº€ãŸã—ã¦ãŠã‚Šã€ã‹ã¤ã‚¹ã‚±ãƒ¼ãƒ«ã•ã›ã‚‹å¹…ãŒå¦¥å½“ãªç¯„å›²å†…ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ã€ã¨ã„ã£ãŸè©±ãŒè¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ã€‚<br><br>å‰æçŸ¥è­˜ï¼ˆmuPï¼‰ã‚„æ¡ä»¶ãŒå¤šãã†ãªæ°—ãŒã™ã‚‹ã®ã§ã€ã—ã£ã‹ã‚Šç¢ºèªã—ãŸæ–¹ãŒã‚ˆã•ãã†ã€‚<br>ãŸã¨ãˆã°ã€muPã§åˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã‚„ã€è»¢é€å¯èƒ½ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«é™ã‚ŠãŒã‚ã‚‹ï¼ˆe.g. å­¦ç¿’ç‡ï¼‰ã€ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹finetuningãªã©ã¯è»¢é€ã§ããªã„ãªã©ã€‚<br><br><br>&lt;img width="872" height="336" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/e5aeb152-5c9e-4ba2-9152-4bfef0d7c27c"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/e5aeb152-5c9e-4ba2-9152-4bfef0d7c27c"&lt;/a&gt;


/&gt;</p>
<p>muP:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2583" target="_blank" rel="noopener noreferrer">[Paper Note] Feature Learning in Infinite-Width Neural Networks, Greg Yang+, PMLR'21</a>
</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<span class="issue_date">Issue Date: 2023-08-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1009" target="_blank" rel="noopener noreferrer" class="title-link">ViLT: Vision-and-Language Transformer Without Convolution or Region   Supervision, Wonjae Kim+, N_A, ICML'21</a>
<span class="snippet"><span>GPT Summary</span>- VLPï¼ˆVision-and-Language Pre-trainingï¼‰ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ãƒ“ã‚¸ãƒ§ãƒ³ã¨è¨€èªã®ã‚¿ã‚¹ã‚¯ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã¦ã„ã‚‹ãŒã€ç¾åœ¨ã®æ–¹æ³•ã¯åŠ¹ç‡æ€§ã¨è¡¨ç¾åŠ›ã®é¢ã§å•é¡ŒãŒã‚ã‚‹ã€‚ãã“ã§ã€æœ¬ç ”ç©¶ã§ã¯ç•³ã¿è¾¼ã¿ãƒ•ãƒªãƒ¼ã®ãƒ“ã‚¸ãƒ§ãƒ³ã¨è¨€èªã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒï¼ˆViLTï¼‰ãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã™ã‚‹ã€‚ViLTã¯é«˜é€Ÿã§ã‚ã‚ŠãªãŒã‚‰ç«¶äº‰åŠ›ã®ã‚ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã€ã‚³ãƒ¼ãƒ‰ã¨äº‹å‰å­¦ç¿’æ¸ˆã¿ã®é‡ã¿ã¯GitHubã§åˆ©ç”¨å¯èƒ½ã§ã‚ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬:


<a href="https://tech.fusic.co.jp/posts/2021-12-29-vilt/" target="_blank" rel="noopener noreferrer">https://tech.fusic.co.jp/posts/2021-12-29-vilt/</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2025-08-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2388" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Longformer: The Long-Document Transformer, Iz Beltagy+, arXiv'20</a>
<span class="snippet"><span>GPT Summary</span>- Longformerã¯ã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç·šå½¢ã«å‡¦ç†ã§ãã‚‹æ³¨æ„æ©Ÿæ§‹ã‚’æŒã¤Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã§ã€æ•°åƒãƒˆãƒ¼ã‚¯ãƒ³ã®æ–‡æ›¸ã‚’æ‰±ãˆã‚‹ã€‚å±€æ‰€çš„ãªã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ³¨æ„ã¨ã‚¿ã‚¹ã‚¯ã«åŸºã¥ãã‚°ãƒ­ãƒ¼ãƒãƒ«æ³¨æ„ã‚’çµ„ã¿åˆã‚ã›ã€æ–‡å­—ãƒ¬ãƒ™ãƒ«ã®è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã€‚äº‹å‰å­¦ç¿’ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã€é•·æ–‡ã‚¿ã‚¹ã‚¯ã§RoBERTaã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚ã¾ãŸã€Longformer-Encoder-Decoderï¼ˆLEDï¼‰ã‚’å°å…¥ã—ã€é•·æ–‡ç”Ÿæˆã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹åŠ¹æœã‚’ç¢ºèªã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>ï¼ˆå›ºå®šã•ã‚ŒãŸå°ã•ã‚ã®windowsã‚µã‚¤ã‚ºã®ä¸­ã§ã®ã¿attentionã‚’è¨ˆç®—ã™ã‚‹ï¼‰sliding window attentionã‚’ææ¡ˆã€‚Figure2ã‚’è¦‹ã‚‹ã¨ã€é€šå¸¸ã®Attentionã¨æ¯”è¼ƒã—ã¦ã€ç¾åœ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®å‘¨è¾ºã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ã—ã‹æ³¨ç›®ã—ãªã„ç‰¹æ€§ãŒå›³ç¤ºã•ã‚Œã¦ãŠã‚Šã€ã‚¤ãƒ¡ãƒ¼ã‚¸ãŒæ´ã¿ã‚„ã™ã„ã€‚<br><br>&lt;img width="795" height="231" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/d1eccdaf-5b5b-4444-ad31-44c54c345d79"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/d1eccdaf-5b5b-4444-ad31-44c54c345d79"&lt;/a&gt;


/&gt;</p>
<p>OpenLLMã®æ–‡è„ˆã ã¨ã€Mistralã«æ¡ç”¨ã•ã‚Œã¦è©±é¡Œã«ãªã£ãŸã‹ã‚‚ï¼Ÿ<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1309" target="_blank" rel="noopener noreferrer">Mistral 7B, Albert Q. Jiang+, N/A, arXiv'23</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2356" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Transformers are RNNs: Fast Autoregressive Transformers with Linear  Attention, Angelos Katharopoulos+, ICML'20</a>
<span class="snippet"><span>GPT Summary</span>- è‡ªå·±æ³¨æ„ã‚’ã‚«ãƒ¼ãƒãƒ«ç‰¹å¾´ãƒãƒƒãƒ—ã®ç·šå½¢ãƒ‰ãƒƒãƒˆç©ã¨ã—ã¦è¡¨ç¾ã™ã‚‹ã“ã¨ã§ã€Transformersã®è¤‡é›‘æ€§ã‚’$\mathcal{O}\left(N^2\right)$ã‹ã‚‰$\mathcal{O}\left(N\right)$ã«å‰Šæ¸›ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è‡ªå·±å›å¸°å‹Transformersã®é€Ÿåº¦ãŒæœ€å¤§4000å€å‘ä¸Šã—ã€å¾“æ¥ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã€‚</span>
<span class="snippet"><span>Comment</span><p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1210" target="_blank" rel="noopener noreferrer">Transformers are Multi-State RNNs, Matanel Oren+, N/A, EMNLP'24</a>
 </p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2355" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Reformer: The Efficient Transformer, Nikita Kitaev+, ICLR'20</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€å±€æ‰€æ„Ÿåº¦ãƒãƒƒã‚·ãƒ¥ã‚’ç”¨ã„ãŸæ³¨æ„æ©Ÿæ§‹ã¨å¯é€†æ®‹å·®å±¤ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¨ˆç®—é‡ã‚’O($L^2$)ã‹ã‚‰O($L\log L$)ã«å‰Šæ¸›ã—ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨é€Ÿåº¦ã‚’å‘ä¸Šã•ã›ãŸReformerãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿç¾ã€‚ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¨åŒç­‰ã®æ€§èƒ½ã‚’ç¶­æŒã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview: 


<a href="https://openreview.net/forum?id=rkgNKkHtvB" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=rkgNKkHtvB</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2354" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Linformer: Self-Attention with Linear Complexity, Sinong Wang+, arXiv'20</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã¯è‡ªç„¶è¨€èªå‡¦ç†ã§æˆåŠŸã‚’åã‚ã¦ã„ã‚‹ãŒã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã—ã¦ã¯é«˜ã‚³ã‚¹ãƒˆã€‚è‡ªå·±æ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã§è¿‘ä¼¼ã—ã€è¤‡é›‘ã•ã‚’$O(n^2)$ã‹ã‚‰$O(n)$ã«å‰Šæ¸›ã™ã‚‹æ–°ã—ã„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¡ãƒ¢ãƒªã¨æ™‚é–“åŠ¹ç‡ãŒå‘ä¸Šã—ãŸç·šå½¢ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã€ŒLinformerã€ãŒæ¨™æº–ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã®æ€§èƒ½ã‚’ç¤ºã™ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/PositionalEncoding.html" target="_blank" rel="noopener noreferrer">#PositionalEncoding</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2359" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context, Zihang Dai+, ACL'19</a>
<span class="snippet"><span>GPT Summary</span>- Transformer-XLã¯ã€å›ºå®šé•·ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¶…ãˆãŸé•·æœŸçš„ãªä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã™ã‚‹æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã®å†å¸°ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¨æ–°ã—ã„ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¡ç”¨ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€RNNã‚ˆã‚Š80%ã€å¾“æ¥ã®Transformersã‚ˆã‚Š450%é•·ã„ä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã—ã€è©•ä¾¡æ™‚ã«ã¯æœ€å¤§1,800å€ã®é€Ÿåº¦å‘ä¸Šã‚’å®Ÿç¾ã€‚enwiki8ã‚„WikiText-103ãªã©ã§æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã€æ•°åƒãƒˆãƒ¼ã‚¯ãƒ³ã®ä¸€è²«ã—ãŸãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚‚å¯èƒ½ã€‚ã‚³ãƒ¼ãƒ‰ã¨ãƒ¢ãƒ‡ãƒ«ã¯Tensorflowã¨PyTorchã§åˆ©ç”¨å¯èƒ½ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/329" target="_blank" rel="noopener noreferrer">äº‹å‰å­¦ç¿’è¨€èªãƒ¢ãƒ‡ãƒ«ã®å‹•å‘ / Survey of Pretrained Language Models, Kyosuke Nishida, 2019</a>
</p>
<p>3.2ç¯€ã®å®šå¼åŒ–ã‚’è¦‹ã‚‹ã¨ã€ä¸€ã¤å‰ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒ»layerã”ã¨ã®hidden stateã‚’ã€ç¾åœ¨ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å¯¾å¿œã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã¨layerã®hidden stateã«concatã—ï¼ˆéå»ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«å½±éŸ¿ã‚’ä¸ãˆãªã„ã‚ˆã†ã«å‹¾é…ã‚’ä¼æ¬ã•ã›ãªã„Stop-Gradientã‚’é©ç”¨ã™ã‚‹ï¼‰ã€QKVã®ã†ã¡ã€KVã®è¨ˆç®—ã«æ´»ç”¨ã—ã¦ã„ã‚‹ã€‚ã¾ãŸã€çµ¶å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’åˆ©ç”¨ã™ã‚‹ã¨ãƒ¢ãƒ‡ãƒ«ãŒã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–“ã®æ™‚ç³»åˆ—çš„ãªé–¢ä¿‚ã‚’èªè­˜ã§ããªããªã‚‹ãŸã‚ã€ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«ã¯ç›¸å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’åˆ©ç”¨ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç¾åœ¨ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®KVãŒä¸€ã¤å‰ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«ã‚ˆã£ã¦æ¡ä»¶ã¥ã‘ã‚‰ã‚Œã€contextã¨ã—ã¦è€ƒæ…®ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã¨ãªã‚Šã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–“ã‚’è·¨ã„ã ä¾å­˜é–¢ä¿‚ã®è€ƒæ…®ãŒå®Ÿç¾ã•ã‚Œã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-05-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1957" target="_blank" rel="noopener noreferrer" class="title-link">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, Mingxing Tan+, ICML'19</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€ConvNetsã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’æ·±ã•ã€å¹…ã€è§£åƒåº¦ã®ãƒãƒ©ãƒ³ã‚¹ã‚’è€ƒæ…®ã—ã¦ä½“ç³»çš„ã«ç ”ç©¶ã—ã€æ–°ã—ã„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ‰‹æ³•ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€MobileNetsã‚„ResNetã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¨¼ã—ã€EfficientNetsã¨ã„ã†æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’è¨­è¨ˆã€‚ç‰¹ã«EfficientNet-B7ã¯ã€ImageNetã§84.3%ã®ãƒˆãƒƒãƒ—1ç²¾åº¦ã‚’é”æˆã—ã€å¾“æ¥ã®ConvNetsã‚ˆã‚Šã‚‚å°å‹ã‹ã¤é«˜é€Ÿã§ã‚ã‚‹ã€‚CIFAR-100ã‚„Flowersãªã©ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚‚æœ€å…ˆç«¯ã®ç²¾åº¦ã‚’è¨˜éŒ²ã€‚ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒè«–æ–‡ã‚’ãƒ¡ãƒ¢ã£ã¦ãªã‹ã£ãŸã®ã§è¿½åŠ ã€‚<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/346" target="_blank" rel="noopener noreferrer">EfficientNetè§£èª¬, omiita (ã‚ªãƒŸãƒ¼ã‚¿), 2019</a>
<br><br>ã‚‚å‚ç…§ã®ã“ã¨ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1272" target="_blank" rel="noopener noreferrer" class="title-link">Fast Transformer Decoding: One Write-Head is All You Need, Noam Shazeer, N_A, arXiv'19</a>
<span class="snippet"><span>GPT Summary</span>- ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯é«˜é€Ÿã‹ã¤ç°¡å˜ã ãŒã€å¢—åˆ†æ¨è«–ã¯å¤§ããª"keys"ã¨"values"ãƒ†ãƒ³ã‚½ãƒ«ã‚’ç¹°ã‚Šè¿”ã—èª­ã¿è¾¼ã‚€ãŸã‚ã«é…ããªã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚ãã“ã§ã€ã‚­ãƒ¼ã¨å€¤ã‚’å…±æœ‰ã™ã‚‹ãƒãƒ«ãƒã‚¯ã‚¨ãƒªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆã—ã€ãƒ¡ãƒ¢ãƒªå¸¯åŸŸå¹…è¦ä»¶ã‚’ä½æ¸›ã™ã‚‹ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€é«˜é€Ÿãªãƒ‡ã‚³ãƒ¼ãƒ‰ãŒå¯èƒ½ã§ã€ã‚ãšã‹ãªå“è³ªã®ä½ä¸‹ã—ã‹ãªã„ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>Multi Query Attentionè«–æ–‡ã€‚KVã®setã«å¯¾ã—ã¦ã€å˜ä¸€ã®Queryã®ã¿ã§Multi-Head Attentionã‚’ä»£æ›¿ã™ã‚‹ã€‚åŠ‡çš„ã«Decoderã®InferenceãŒæ—©ããªã‚Šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒæ¸›ã‚‹ãŒã€è«–æ–‡ä¸­ã§ã¯è¨€åŠã•ã‚Œã¦ã„ãªã„ï¼Ÿã‚ˆã†ã ãŒã€æ€§èƒ½ã¨å­¦ç¿’ã®å®‰å®šæ€§ãŒèª²é¡Œã¨ãªã‚‹ã‚ˆã†ã§ã‚ã‚‹ã€‚<br><br><br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/e2d77b43-70c3-4922-a822-bf95d6b4704f" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2353" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Efficient Attention: Attention with Linear Complexities, Zhuoran Shen+, arXiv'18</a>
<span class="snippet"><span>GPT Summary</span>- æ–°ã—ã„åŠ¹ç‡çš„ãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ææ¡ˆã—ã€ãƒ‰ãƒƒãƒˆç©ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¨åŒç­‰ã®æ€§èƒ½ã‚’ç¶­æŒã—ã¤ã¤ã€ãƒ¡ãƒ¢ãƒªã¨è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æŸ”è»Ÿãªçµ±åˆãŒå¯èƒ½ã¨ãªã‚Šã€ç²¾åº¦å‘ä¸Šã‚’å®Ÿç¾ã€‚å®Ÿé¨“çµæœã§ã¯ã€MS-COCO 2017ã§ã®ç‰©ä½“æ¤œå‡ºã‚„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®æ€§èƒ½å‘ä¸ŠãŒç¢ºèªã•ã‚Œã€Scene Flowãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯æœ€å…ˆç«¯ã®ç²¾åº¦ã‚’é”æˆã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>Figure1ã‚’è¦‹ã‚‹ã¨ã‚³ãƒ³ã‚»ãƒ—ãƒˆãŒä¸€ç›®ã§ã‚ã‹ã‚Šã€éå¸¸ã«ã‚ã‹ã‚Šã‚„ã™ã„<br>&lt;img width="1068" height="580" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/18e6a7da-fc07-495f-bda6-bcef4acab321"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/18e6a7da-fc07-495f-bda6-bcef4acab321"&lt;/a&gt;


/&gt;</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/82" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Learning to skim text, Yu+, ACL'17</a>
<span class="snippet"><span>Comment</span><p>è§£èª¬ã‚¹ãƒ©ã‚¤ãƒ‰ï¼š


<a href="http://www.lr.pi.titech.ac.jp/~haseshun/acl2017suzukake/slides/07.pdf" target="_blank" rel="noopener noreferrer">http://www.lr.pi.titech.ac.jp/~haseshun/acl2017suzukake/slides/07.pdf</a>


</p>
<p>RNNã«ãŠã„ã¦é‡è¦ãªéƒ¨åˆ†ä»¥å¤–ã¯èª­ã¿é£›ã°ã™ã“ã¨ã§åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ç ”ç©¶ã€‚ã„ãã¤èª­ã¿é£›ã°ã™ã‹ã‚‚æ½œåœ¨å¤‰æ•°ã¨ã—ã¦ä¸€ç·’ã«å­¦ç¿’ã™ã‚‹ã€‚æ½œåœ¨å¤‰æ•°ï¼ˆé›¢æ•£å¤‰æ•°ï¼‰ãªã®ã§ã€æ™®é€šã«å°¤åº¦æœ€å¤§åŒ–ã™ã‚‹ã‚„ã‚Šæ–¹ã§ã¯å­¦ç¿’ã§ããšã€ãŠã¾ã‘ã«é›¢æ•£å¤‰æ•°ãªã®ã§ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ä½¿ãˆãªã„ã®ã§ã€å¼·åŒ–å­¦ç¿’ã§å­¦ç¿’ã™ã‚‹ã€‚<br><br><br><br>Vanilla LSTMã¨æ¯”è¼ƒã—ã€è‰²ã€…ãªã‚¿ã‚¹ã‚¯ã§å®Ÿé¨“ã—ãŸçµæœã€æ€§èƒ½ã‚‚ï¼ˆå°‘ã—ï¼‰ä¸ŠãŒã‚‹ã—ã€ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚‚ã™ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Hybrid.html" target="_blank" rel="noopener noreferrer">#Hybrid</a>
<span class="issue_date">Issue Date: 2025-10-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3526" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Notes] KIMI LINEAR: AN EXPRESSIVE, EFFICIENT ATTENTION ARCHITECTURE, Kimi Team, 2025.10</a>
<span class="snippet"><span>Comment</span><p>HF:


<a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct" target="_blank" rel="noopener noreferrer">https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/kimi_moonshot/status/1983937694360322136?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/nrehiew_/status/1983891931823505518?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1983992979153985676?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rasbt/status/1984617030356451642?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/ChatGPT.html" target="_blank" rel="noopener noreferrer">#ChatGPT</a>
<a class="button" href="articles/Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<a class="button" href="articles/mid-training.html" target="_blank" rel="noopener noreferrer">#mid-training</a>
<a class="button" href="articles/GRPO.html" target="_blank" rel="noopener noreferrer">#GRPO</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Inference.html" target="_blank" rel="noopener noreferrer">#Inference</a>
<a class="button" href="articles/MinimalCode.html" target="_blank" rel="noopener noreferrer">#MinimalCode</a>
<a class="button" href="articles/KV%20Cache.html" target="_blank" rel="noopener noreferrer">#KV Cache</a>
<span class="issue_date">Issue Date: 2025-10-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3377" target="_blank" rel="noopener noreferrer" class="title-link">nanochat, karpathy, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/karpathy/status/1977755427569111362?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ–°ãŸãªã‚¹ãƒ”ãƒ¼ãƒ‰ãƒ©ãƒ³ãŒ...!!</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-10-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3338" target="_blank" rel="noopener noreferrer" class="title-link">modded-nanogpt medium world record: Re-using intermediate activations in the output latents, shimu's blog, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/stochasticchasm/status/1979985191356731663?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Multi.html" target="_blank" rel="noopener noreferrer">#Multi</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/ProprietaryLLM.html" target="_blank" rel="noopener noreferrer">#ProprietaryLLM</a>
<a class="button" href="articles/Parallelism.html" target="_blank" rel="noopener noreferrer">#Parallelism</a>
<a class="button" href="articles/ContextEngineering.html" target="_blank" rel="noopener noreferrer">#ContextEngineering</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-10-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3302" target="_blank" rel="noopener noreferrer" class="title-link">Introducing SWE-grep and SWE-grep-mini: RL for Multi-Turn, Fast Context Retrieval, Cognition, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/walden_yan/status/1978884662601617859?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æœ€å¤§ã§4 turnã®é–“8ã¤ã®ãƒ„ãƒ¼ãƒ«ã‚³ãƒ¼ãƒ«ï¼ˆguessingã¨ã—ã¦ã¯å¾“æ¥ãƒ¢ãƒ‡ãƒ«ã¯1--2, Sonnet-4.5ã¯1--4)ã‚’ä¸¦åˆ—ã™ã‚‹ï¼ˆ3 turnã¯æ¢ç´¢ã€æœ€å¾Œã®1 turnã‚’answerã®ãŸã‚ã«ä½¿ã†) parallel tool calls ã‚’åŠ¹æœçš„ã«å®Ÿæ–½ã§ãã‚‹ã‚ˆã†ã«ã€on policy RLã§ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã®RLã‚’å®Ÿæ–½ã™ã‚‹ã“ã¨ã§ã€é«˜é€Ÿã§æ­£ç¢ºãªcontext retrievalã‚’å®Ÿç¾ã—ãŸã€ã¨ã„ã†æ„Ÿã˜ã‚‰ã—ã„ã€‚<br><br>å¾“æ¥ã®embedding-basedãªdense retrieverã¯é€Ÿã„ãŒæ­£ç¢ºæ€§ã«æ¬ ã‘ã€Agenticãªsearchã¯æ­£ç¢ºã ãŒé…ã„ã¨ã„ã†åŒæ–¹ã®æ¬ ç‚¹ã‚’è£œã†å½¢ã€‚</p>
<p>parallel tool callã¨ã„ã†ã®ã¯å…·ä½“çš„ã«ã©ã†ã„ã†trajectoryã«ãªã‚‹ã®ã‹â€¦ï¼Ÿ</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-10-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3130" target="_blank" rel="noopener noreferrer" class="title-link">PipelineRL, Piche+, ServiceNow, 2025.04</a>
<span class="snippet"><span>Comment</span><p>code:


<a href="https://github.com/ServiceNow/PipelineRL" target="_blank" rel="noopener noreferrer">https://github.com/ServiceNow/PipelineRL</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/vllm_project/status/1974732295627301254?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Inflight Weight Updates</p>
<p>ï¼ˆã“ã®è¾ºã®ç´°ã‹ã„å®Ÿè£…ã®è©±ã¯ã‚ã¾ã‚Šè©³ã—ããªã„ã®ã§èª¤ã‚ŠãŒã‚ã‚‹å¯èƒ½æ€§ãŒçµæ§‹ã‚ã‚Šã¾ã™ï¼‰<br>é€šå¸¸ã®on-policy RLã§ã¯å…¨ã¦ã®GPUä¸Šã§ã®sequenceã®ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆãŒçµ‚ã‚ã‚‹ã¾ã§å¾…ã¡ã€å…¨ã¦ã®ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆå®Œäº†å¾Œã«ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’æ›´æ–°ã™ã‚‹ãŸã‚ã€é•·ã„sequenceã®ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’ã™ã‚‹GPUã®å‡¦ç†ãŒçµ‚ã‚ã‚‹ã¾ã§ã€çŸ­ã„sequenceã®ç”Ÿæˆã§æ¸ˆã‚“ã GPUã¯å¾…æ©Ÿã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚ä¸€æ–¹ã€PipelineRLã¯sequenceã®ãƒ‡ã‚³ãƒ¼ãƒ‰ã®é€”ä¸­ã§ã‚‚é‡ã¿ã‚’æ›´æ–°ã—ã€ç”Ÿæˆé€”ä¸­ã®sequenceã¯å¤ã„KV Cacheã‚’ä¿æŒã—ãŸã¾ã¾æ–°ã—ã„é‡ã¿ã§sequenceã®ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚’ç¶™ç¶šã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚ŠGPU Utilizationã‚’æœ€å¤§åŒ–ã§ãã‚‹ï¼ˆãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆå®Œäº†ã®ãŸã‚ã®å¾…æ©Ÿæ™‚é–“ãŒç„¡ããªã‚‹ï¼‰ã€‚ã¾ãŸã€ä¸€è¦‹å¤ã„KV Cacheã‚’å‰æã«æ–°ãŸãªé‡ã¿ã§ç¶™ç¶šã—ã¦éƒ¨åˆ†sequenceã‚’ç¶™ç¶šã™ã‚‹ã¨ãƒãƒªã‚·ãƒ¼ã®gapã«ã‚ˆã‚Šæ€§èƒ½ãŒæ‚ªåŒ–ã™ã‚‹ã‚ˆã†ã«æ€ãˆã‚‹ãŒã€æ€§èƒ½ãŒæ‚ªåŒ–ã—ãªã„ã“ã¨ãŒå®Ÿé¨“çš„ã«ç¤ºã•ã‚Œã¦ã„ã‚‹æ¨¡æ§˜ã€‚<br><img src="https://github.com/user-attachments/assets/6794927a-3fa9-4a68-ba48-d112d495e0ab" alt="image" loading="lazy"><br><br>Conventional RLã®ç–‘ä¼¼ã‚³ãƒ¼ãƒ‰éƒ¨åˆ†ã‚’è¦‹ã‚‹ã¨ã¨ã¦ã‚‚ã‚ã‹ã‚Šã‚„ã™ãã¦å‚è€ƒã«ãªã‚‹ã€‚Conventional RLï¼ˆPPOã¨ã‹ï¼‰ã§ã¯ã€å®Ÿè£…ä¸Šã¯è¤‡æ•°ã®ãƒãƒƒãƒã«åˆ†ã‘ã¦é‡ã¿ã®æ›´æ–°ãŒè¡Œã‚ã‚Œã‚‹ï¼ˆã‚‰ã—ã„ï¼‰ã€‚ã“ã®ã¨ãã€GPUã®åˆ©ç”¨ã‚’æœ€å¤§åŒ–ã—ã‚ˆã†ã¨ã™ã‚‹ã¨ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤§ããã›ã–ã‚‹ã‚’å¾—ãªã„ã€‚ã“ã®ãŸã‚ã€é€æ¬¡æ›´æ–°ã‚’ã—ãŸã¨ãã®policyã®gapãŒã©ã‚“ã©ã‚“è“„ç©ã—ã¦ã„ãå¤§ãããªã‚‹ï¼ˆ=ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã§ç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ãŒã€å®Ÿéš›ã«é‡ã¿æ›´æ–°ã™ã‚‹ã¨ãã«ã¯lagãŒè“„ç©ã•ã‚Œã¦ã„ãã©ã‚“ã©ã‚“off-policyãƒ‡ãƒ¼ã‚¿ã«å¤‰åŒ–ã—ã¦ã„ã£ã¦ã—ã¾ã†ï¼‰ã¨ã„ã†å¼Šå®³ãŒã‚ã‚‹æ¨¡æ§˜ã€‚ã‹ã¨ã„ã£ã¦lagã‚’æœ€å°ã«ã™ã‚‹ãŸã‚ã«å°ã•ã„ãƒãƒƒãƒã‚µã‚¤ã‚ºã«ã™ã‚‹ã¨gpuã®åŠ¹ç‡ã‚’åœ§å€’çš„ã«çŠ ç‰²ã«ã™ã‚‹ã®ã§ã§ããªã„ã€‚Inflight Weight Updatesã§ã¯ã“ã®ã‚ˆã†ãªãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è§£æ±ºã§ãã‚‹æ¨¡æ§˜ã€‚<br><br>ã¾ãŸã€trainerã¨inferenceéƒ¨åˆ†ã¯å®Œå…¨ã«ç‹¬ç«‹ã•ã›ã‚‰ã‚Œã€ã‹ã¤plug-and-playã§é‡ã¿ã‚’æ›´æ–°ã™ã‚‹ã€ã¨ã„ã£ãŸä½¿ã„æ–¹ã‚‚æƒ³å®šã§ãã‚‹æ¨¡æ§˜ã€‚</p>
<p>ã‚ã¨ã“ã‚Œã¯ä½™è«‡ã ãŒã€å¼•ç”¨ãƒã‚¹ãƒˆã®ä¸»ã¯ä¸‹è¨˜ç ”ç©¶ã§attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æœ€åˆã«ææ¡ˆã—ãŸBahdanauæ°ã§ã‚ã‚‹ã€‚<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1954" target="_blank" rel="noopener noreferrer">Neural Machine Translation by Jointly Learning to Align and Translate, Dzmitry Bahdanau+, ICLR'15</a>
</p>
<p>ç¶šå ±:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dbahdanau/status/1974889569607876747?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è«–æ–‡:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3615" target="_blank" rel="noopener noreferrer">[Paper Note] PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence
  Generation, Alexandre PichÃ©+, arXiv'25, 2025.09</a>
</p>
<p>ç¶šå ±:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/finbarrtimbers/status/1986481494823739581?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-09-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3010" target="_blank" rel="noopener noreferrer" class="title-link">We reverse-engineered Flash Attention 4, Modal Blog, 2025.09</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iwashi86/status/1972085451055157725?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Flash Attention4ã¯æ•°å­¦çš„ãªãƒˆãƒªãƒƒã‚¯ã‚ˆã‚Šã‚‚éåŒæœŸå‡¦ç†ã®è¤‡é›‘ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€Blackwellã«æœ€é©åŒ–ã€ã¨ã®ã“ã¨</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2025-09-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2720" target="_blank" rel="noopener noreferrer" class="title-link">Fast-dLLM v2: Efficient Block-Diffusion Large Language Model, Wu+, 2025.09</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/songhan_mit/status/1964375581761388828?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<a class="button" href="articles/Reference%20Collection.html" target="_blank" rel="noopener noreferrer">#Reference Collection</a>
<span class="issue_date">Issue Date: 2025-08-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2620" target="_blank" rel="noopener noreferrer" class="title-link">LongCat-Flash-Chat, meituan-longcat, 2025.08</a>
<span class="snippet"><span>Comment</span><p>ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒ¬ãƒãƒ¼ãƒˆ:


<a href="https://github.com/meituan-longcat/LongCat-Flash-Chat/blob/main/tech_report.pdf" target="_blank" rel="noopener noreferrer">https://github.com/meituan-longcat/LongCat-Flash-Chat/blob/main/tech_report.pdf</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1961955926136832381?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Agentå‘¨ã‚Šã®ãƒ™ãƒ³ãƒã§é«˜æ€§èƒ½ãªnon thinkingãƒ¢ãƒ‡ãƒ«ã€‚æ¯ç§’100+ãƒˆãƒ¼ã‚¯ãƒ³ã®ç”Ÿæˆé€Ÿåº¦ã§ã€MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã€‚Dynamic Activation...?</p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2621" target="_blank" rel="noopener noreferrer">[Paper Note] Shortcut-connected Expert Parallelism for Accelerating   Mixture-of-Experts, Weilin Cai+, ICLR'25</a>
</p>
<p>Dynamic Activation (activation paramãŒå…¥åŠ›ã«å¿œã˜ã¦å¤‰åŒ–(å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’MoEã«ãŠã„ã¦å‡ä¸€ã«æ‰±ã‚ãªã„ï¼‰ã™ã‚‹ã“ã¨ã§åŠ¹ç‡åŒ–ï¼‰ã¯ã€ä¸‹è¨˜ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§å®Ÿç¾ã—ã¦ã„ã‚‹æ¨¡æ§˜<br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2622" target="_blank" rel="noopener noreferrer">[Paper Note] MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation   Experts, Peng Jin+, ICLR'25</a>
</p>
<p>ã—ã‹ã—ä¸­å›½ã¯æœ¬å½“ã«æ¬¡ã€…ã«è‰²ã€…ãªä¼æ¥­ã‹ã‚‰åŸºç›¤ãƒ¢ãƒ‡ãƒ«ãŒå‡ºã¦ãã‚‹ãªãâ€¦ã™ã”ã„</p>
<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2623" target="_blank" rel="noopener noreferrer">[Paper Note] Scaling Exponents Across Parameterizations and Optimizers, Katie Everett+, ICML'24</a>
 </p>
<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/nrehiew_/status/1962186876099739767?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1962980770550628841?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2606" target="_blank" rel="noopener noreferrer" class="title-link">fastvlm-webgpu, Apple, 2025.08</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/fartashfg/status/1961441954157244448?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>pj page:


<a href="https://fastvlm.net" target="_blank" rel="noopener noreferrer">https://fastvlm.net</a>


</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/python.html" target="_blank" rel="noopener noreferrer">#python</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="articles/Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<a class="button" href="articles/SpeculativeDecoding.html" target="_blank" rel="noopener noreferrer">#SpeculativeDecoding</a>
<span class="issue_date">Issue Date: 2025-08-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2499" target="_blank" rel="noopener noreferrer" class="title-link">vLLMã®Speculative Decodingã«ã‚ˆã‚‹æ¨è«–é«˜é€ŸåŒ–ã‚’è©¦ã™, Aratako, 2025.05</a>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/python.html" target="_blank" rel="noopener noreferrer">#python</a>
<a class="button" href="articles/Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/MinimalCode.html" target="_blank" rel="noopener noreferrer">#MinimalCode</a>
<span class="issue_date">Issue Date: 2025-08-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2472" target="_blank" rel="noopener noreferrer" class="title-link">simple-paged-attention, torotoki, 2025.06</a>
<span class="snippet"><span>Comment</span><p>CUDA + C++ã«ã‚ˆã‚‹ãƒŸãƒ‹ãƒãƒ«ãªpaged-attentionã®å®Ÿè£…ã€‚ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ç†è§£+å®Ÿè£…ç†è§£ã®å‚è€ƒã«éå¸¸ã«è‰¯ã•ãã†ã€‚</p>
<p>PagedAttentionã¯ ç¾åœ¨ã®ä¸»è¦ãªLLM Inference/Serving Engineã®ã²ã¨ã¤ã§ã‚ã‚‹vLLM ã§ï¼ˆææ¡ˆ|å®Ÿè£…ï¼‰ã•ã‚ŒãŸæŠ€è¡“ã§ã‚ã‚Šã€å…ƒè«–æ–‡ã¯ä¸‹è¨˜:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2474" target="_blank" rel="noopener noreferrer">[Paper Note] Efficient Memory Management for Large Language Model Serving with  PagedAttention, Woosuk Kwon+, SOSP'23</a>
</p>
<p>ã“ã®è¾ºã‚‚ã‚ã‚ã›ã¦èª­ã‚€ã¨ãŠã‚‚ã—ã‚ã„ã‹ã‚‚ã—ã‚Œãªã„:<br>


<a href="https://nttdocomo-developers.jp/entry/2024/12/19/090000_6" target="_blank" rel="noopener noreferrer">https://nttdocomo-developers.jp/entry/2024/12/19/090000_6</a>


</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="articles/python.html" target="_blank" rel="noopener noreferrer">#python</a>
<a class="button" href="articles/LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<span class="issue_date">Issue Date: 2025-08-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2349" target="_blank" rel="noopener noreferrer" class="title-link">LMCache, LMCache, 2025.07</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/akshay_pachaar/status/1951626977213059406?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>KV Cacheã‚’è‰²ã€…ãªã¨ã“ã‚ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦ãŠã„ã¦ã€prefixã ã‘ã§ãªãå…¨ã¦ã®reusedå¯èƒ½ãªã‚‚ã®ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ã“ã¨ã§ã€TTFTã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’å¤§å¹…ã«å‘ä¸Šã™ã‚‹ã‚‰ã—ã„ã€‚ç‰¹ã«long contextãªã‚¿ã‚¹ã‚¯ã§åŠ›ã‚’ç™ºæ®ã—ã€vLLMã¨çµ„ã¿åˆã‚ã›ã‚‹ã¨ä¸‹è¨˜ã®ã‚ˆã†ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šçµæœ<br><img src="https://github.com/user-attachments/assets/d40301c3-f7a9-4f2a-a66e-65c3357ce2b9" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Coding.html" target="_blank" rel="noopener noreferrer">#Coding</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<span class="issue_date">Issue Date: 2025-08-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2333" target="_blank" rel="noopener noreferrer" class="title-link">Qwen3-Coder-30B-A3B-Instruct, QwenTeam, 2025.08</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/alibaba_qwen/status/1950925444057792808?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p><img src="https://github.com/user-attachments/assets/d2c30b64-10df-40b2-bcac-f029bdc9f1f1" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2025-08-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2331" target="_blank" rel="noopener noreferrer" class="title-link">Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference, ByteDance Seed,</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1951092714164101590?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p><img src="https://github.com/user-attachments/assets/b7ba3b05-760d-4820-b685-0058706286ff" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="articles/Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<a class="button" href="articles/SpeculativeDecoding.html" target="_blank" rel="noopener noreferrer">#SpeculativeDecoding</a>
<span class="issue_date">Issue Date: 2025-07-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2291" target="_blank" rel="noopener noreferrer" class="title-link">Speculative Decodingï¼šFaster Inference Without Paying for More GPU, ELYZA, 2025.07</a>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<a class="button" href="articles/Optimizer.html" target="_blank" rel="noopener noreferrer">#Optimizer</a>
<a class="button" href="articles/Decoder.html" target="_blank" rel="noopener noreferrer">#Decoder</a>
<span class="issue_date">Issue Date: 2025-07-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2223" target="_blank" rel="noopener noreferrer" class="title-link">Modded-NanoGPT, KellerJordan, 2024.05</a>
<span class="snippet"><span>Comment</span><p>NanoGPT speedrun</p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2118" target="_blank" rel="noopener noreferrer">[Paper Note] The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT  Improvements, Bingchen Zhao+, arXiv'25</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2208" target="_blank" rel="noopener noreferrer">ãã¿ã¯NanoGPT speedrunã‚’çŸ¥ã£ã¦ã„ã‚‹ã‹ï¼Ÿ, PredNext, 2025.07</a>
</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/AWS.html" target="_blank" rel="noopener noreferrer">#AWS</a>
<a class="button" href="articles/MLOps.html" target="_blank" rel="noopener noreferrer">#MLOps</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/A/B%20Testing.html" target="_blank" rel="noopener noreferrer">#A/B Testing</a>
<a class="button" href="articles/TwoTowerModel.html" target="_blank" rel="noopener noreferrer">#TwoTowerModel</a>
<span class="issue_date">Issue Date: 2025-06-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2113" target="_blank" rel="noopener noreferrer" class="title-link">æ—¥çµŒé›»å­ç‰ˆã®ã‚¢ãƒ—ãƒªãƒˆãƒƒãƒ—ã€ŒãŠã™ã™ã‚ã€ã‚’Two Towerãƒ¢ãƒ‡ãƒ«ã§ãƒªãƒ—ãƒ¬ãƒ¼ã‚¹ã—ã¾ã—ãŸ, NIKKEI, 2025.05</a>
<span class="snippet"><span>Comment</span><p>ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è–¦ã‚’ã™ã‚‹ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«ãŠã„ã¦ã€ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹+å”èª¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°(Jubatus)ã‹ã‚‰Two Towerãƒ¢ãƒ‡ãƒ«ã«åˆ‡ã‚Šæ›¿ãˆãŸéš›ã«ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒ300mså¢—ãˆã¦ã—ã¾ã£ãŸãŸã‚ã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®šã—ä¸€éƒ¨ã‚’ãƒ‘ãƒƒãƒå‡¦ç†ã«ã—ã¤ã¤ã‚‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ã‚’æ®‹ã™ã“ã¨ã§è§£æ±ºã—ãŸã¨ã„ã†è©±ã€‚AWSã®æ§‹æˆã€A/Bãƒ†ã‚¹ãƒˆã‚„è² è·ãƒ†ã‚¹ãƒˆã®è©±ã‚‚ã‚ã‚Šã€å®Ÿç”¨çš„ã§éå¸¸ã«èˆˆå‘³æ·±ã‹ã£ãŸã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="articles/Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-06-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2095" target="_blank" rel="noopener noreferrer" class="title-link">Nemo-RL, Nvidia, 2025.05</a>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-03-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1830" target="_blank" rel="noopener noreferrer" class="title-link">Nemotron-H: A Family of Accurate, Efficient Hybrid Mamba-Transformer Models, Nvidia, 2025.03</a>
<span class="snippet"><span>Comment</span><p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1820" target="_blank" rel="noopener noreferrer">Hunyuan T1, Tencent, 2025.03</a>
</p>
<p>Transformerã®Self-attention Layerã‚’Mamba2 Layerã«ç½®æ›ã™ã‚‹ã“ã¨ã§ã€æ§˜ã€…ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§åŒç­‰ã®æ€§èƒ½ã€ã‚ã‚‹ã„ã¯ä¸Šå›ã‚‹æ€§èƒ½ã§3å€ç¨‹åº¦ã®Inference timeã®é«˜é€ŸåŒ–ã‚’ã—ã¦ã„ã‚‹ï¼ˆ65536 input, 1024 outputï¼‰ã€‚<br><br>56Bç¨‹åº¦ã®mediumã‚µã‚¤ã‚ºã®ãƒ¢ãƒ‡ãƒ«ã¨ã€8Bç¨‹åº¦ã®è»½é‡ãªãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦è¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹ã€‚ç‰¹ã«ã€8Bãƒ¢ãƒ‡ãƒ«ã§Mambaã¨Transformerã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã¨ã€é€šå¸¸ã®Transformerãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã—ã¦ã„ã‚‹ã€‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«15 Trillion Tokenã‚’åˆ©ç”¨ã—ã¦ãŠã‚Šã€ã“ã®ãƒ‡ãƒ¼ã‚¿é‡ã§ã®Apple to Appleã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£é–“ã®æ¯”è¼ƒã¯ã€ç¾çŠ¶ã§ã¯æœ€ã‚‚å¤§è¦æ¨¡ãªã‚‚ã®ã¨ã®ã“ã¨ã€‚æ€§èƒ½ã¯å¤šãã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã«ã—ã¦ã‚‚åŒç­‰ã€Commonsense Understandingã§ã¯ä¸Šå›ã£ã¦ã„ã‚‹ã€‚<br><br>ã¾ãŸã€å­¦ç¿’ã—ãŸNemotron-Hã‚’ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æŒã¤VLMã«ã¤ã„ã¦ã‚‚ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒè¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/pretrained-LM.html" target="_blank" rel="noopener noreferrer">#pretrained-LM</a>
<span class="issue_date">Issue Date: 2024-12-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1606" target="_blank" rel="noopener noreferrer" class="title-link">ModernBERT, AnswerDotAI, 2024.12</a>
<span class="snippet"><span>GPT Summary</span>- ModernBERTã¯ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å°‚ç”¨ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã§ã€å¾“æ¥ã®BERTã«æ¯”ã¹ã¦å¤§å¹…ãªãƒ‘ãƒ¬ãƒ¼ãƒˆæ”¹å–„ã‚’å®Ÿç¾ã€‚2å…†ãƒˆãƒ¼ã‚¯ãƒ³ã§è¨“ç·´ã•ã‚Œã€8192ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’æŒã¡ã€åˆ†é¡ã‚¿ã‚¹ã‚¯ã‚„ãƒªãƒˆãƒªãƒ¼ãƒãƒ«ã§æœ€å…ˆç«¯ã®çµæœã‚’ç¤ºã™ã€‚é€Ÿåº¦ã¨ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚‚å„ªã‚Œã¦ãŠã‚Šã€ä¸€èˆ¬çš„ãªGPUã§ã®æ¨è«–ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>æœ€è¿‘ã®é€²åŒ–ã—ã¾ãã£ãŸTransformeré–¢é€£ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’Encodnr-Onlyãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹BERTã«å–ã‚Šè¾¼ã‚“ã ã‚‰æ€§èƒ½ä¸ŠãŒã‚‹ã—ã€BERTã®æ–¹ãŒã‚³ã‚¹ãƒ‘ãŒè‰¯ã„ã‚¿ã‚¹ã‚¯ã¯ãŸãã•ã‚“ã‚ã‚‹ã‚ˆã€ç³»ã®è©±ã€ã‹ã¤ãã®å®Ÿè£…ã ã¨æ€ã‚ã‚Œã‚‹ã€‚<br>ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒšãƒ¼ãƒ‘ãƒ¼ä¸­ã«è¨˜è¼‰ã¯ãªã„ãŒã€è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã¨åŒã˜ã‚¿ã‚¹ã‚¯ã§ã®Decoder-Onlyãƒ¢ãƒ‡ãƒ«ï¼ˆSFTæœ‰ã‚Šç„¡ã—ä¸¡æ–¹ï¼‰ã¨ã®æ€§èƒ½ã‚’æ¯”è¼ƒã—ãŸã‚‰ã©ã®ç¨‹åº¦ã®æ€§èƒ½ãªã®ã ã‚ã†ã‹ï¼Ÿ</p>
<p>ãã‚‚ãã‚‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒæ‰‹å…ƒã«ã‚ã£ã¦ã€BERTã‚’Finetuningã™ã‚‹ã ã‘ã§ååˆ†ãªæ€§èƒ½ãŒå‡ºã‚‹ã®ãªã‚‰ï¼ˆBERTã¯GPUä½¿ã†ã®ã§ãã‚‚ãã‚‚xgboostã¨ã‹ã§ã‚‚è‰¯ã„ãŒï¼‰ã€ã‚ã–ã‚ã–LLMä½¿ã†å¿…è¦ãªã„ã¨æ€ã‚ã‚Œã‚‹ã€‚BERTã®Finetuningã¯ãã“ã¾ã§æ™‚é–“ã¯ã‹ã‹ã‚‰ãªã„ã—ã€inferenceã‚‚é€Ÿã„ã€‚<br><br>å‚è€ƒ:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1024" target="_blank" rel="noopener noreferrer">Prompt2Model: Generating Deployable Models from Natural Language   Instructions, Vijay Viswanathan+, N/A, EMNLP'23</a>
</p>
<p>æ—¥æœ¬èªè§£èª¬:


<a href="https://zenn.dev/dev_commune/articles/3f5ab431abdea1?utm_source=substack&utm_medium=email" target="_blank" rel="noopener noreferrer">https://zenn.dev/dev_commune/articles/3f5ab431abdea1?utm_source=substack&utm_medium=email</a>


</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2024-12-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1599" target="_blank" rel="noopener noreferrer" class="title-link">Fast LLM Inference From Scratch, Andrew Chan, 2024.12</a>
<span class="snippet"><span>Comment</span><p>ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã›ãšã«C++ã¨CUDAã‚’åˆ©ç”¨ã—ã¦LLMã®æ¨è«–ã‚’å®Ÿæ–½ã™ã‚‹æ–¹æ³•ã®è§£èª¬è¨˜äº‹</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Slide.html" target="_blank" rel="noopener noreferrer">#Slide</a>
<span class="issue_date">Issue Date: 2024-11-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1518" target="_blank" rel="noopener noreferrer" class="title-link">TensorRT-LLMã«ã‚ˆã‚‹æ¨è«–é«˜é€ŸåŒ–, Hiroshi Matsuda, NVIDIA AI Summit 2024.11 </a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hmtd223/status/1856887876665184649?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>éå¸¸ã«èˆˆå‘³æ·±ã„ã®ã§å¾Œã§èª­ã‚€</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<a class="button" href="articles/Reference%20Collection.html" target="_blank" rel="noopener noreferrer">#Reference Collection</a>
<span class="issue_date">Issue Date: 2024-11-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1487" target="_blank" rel="noopener noreferrer" class="title-link">ZeRO: DeepSpeedã®ç´¹ä»‹, ãƒ¬ãƒˆãƒªãƒ, 2021.07 </a>
<span class="snippet"><span>Comment</span><p>ZeROã®èª¬æ˜ãŒã‚ã‹ã‚Šã‚„ã™ã„</p>
<p>ã“ã¡ã‚‰ã®è¨˜äº‹ã‚‚ã‚ã‹ã‚Šã‚„ã™ã„<br><br>


<a href="https://zenn.dev/turing_motors/articles/d00c46a79dc976" target="_blank" rel="noopener noreferrer">https://zenn.dev/turing_motors/articles/d00c46a79dc976</a>


</p>
<p>DeepSpeedã®ã‚³ãƒ³ãƒ•ã‚£ã‚°ã®ä¸€è¦§<br><br>


<a href="https://www.deepspeed.ai/docs/config-json/" target="_blank" rel="noopener noreferrer">https://www.deepspeed.ai/docs/config-json/</a>


</p>
<p>transformersã«ãŠã‘ã‚‹deepspeedã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:<br>


<a href="https://huggingface.co/transformers/v4.9.2/main_classes/deepspeed.html" target="_blank" rel="noopener noreferrer">https://huggingface.co/transformers/v4.9.2/main_classes/deepspeed.html</a>


</p>
<p>å‚è€ƒ: deepspeedã®ä½¿ã„æ–¹ã¾ã¨ã‚<br>


<a href="https://note.com/fukudawataru/n/n5152e6f587c8" target="_blank" rel="noopener noreferrer">https://note.com/fukudawataru/n/n5152e6f587c8</a>


</p>
<p>ZeRO Stage3ã‚’ä½¿ã†å ´åˆã€ãƒšãƒ¼ã‚¸å¾Œæ–¹ã«ã—ã‚Œã£ã¨ã¨ã‚“ã§ã‚‚ãªãé‡è¦ãªã“ã¨ãŒæ›¸ã„ã¦ã‚ã‚‹ã®ã§æ°—ã‚’ã¤ã‘ã¾ã—ã‚‡ã†ã€‚ã€‚ã€‚ã€‚<br><br>


<a href="https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/deepspeed#constructing-massive-models" target="_blank" rel="noopener noreferrer">https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/deepspeed#constructing-massive-models</a>


<br><br><br><br><img src="https://github.com/user-attachments/assets/677b6656-1302-4b1b-8be6-ca954c7edda6" alt="image" loading="lazy"><br><br></p>
<p>ZeROã¯parameterã¨optimizerã®memory footprintã®æœ€é©åŒ–ã‚’é ‘å¼µã£ã¦ã„ã¦ã€activation memory footprintï¼ˆãƒãƒƒãƒã‚’forward passã«æµã™æ™‚ã«æ¶ˆè²»ã•ã‚Œã‚‹ãƒ¡ãƒ¢ãƒªï¼‰ã®å‰Šæ¸›ã¯ã€tiling, activation/gradient checkpointingã¨ã‹ã§é ‘å¼µã£ã¦ã­ã¨ã„ã†<br><br><br><br>ã¨ã„ã†è©±ãŒæœ¬å®¶issueã®4047ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã€‚</p>
<p>çµè«–: ã¤ã¾ã¥ã„ãŸã‚‰DeepSpeedã®Issueã‚’ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§æ¤œç´¢ã‹ã‘ã‚‹ã®ãŒä¸€ç•ªåŠ¹æœçš„</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="articles/Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<a class="button" href="articles/MinimalCode.html" target="_blank" rel="noopener noreferrer">#MinimalCode</a>
<span class="issue_date">Issue Date: 2024-11-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1479" target="_blank" rel="noopener noreferrer" class="title-link">Lingua, Meta</a>
<span class="snippet"><span>Comment</span><p>ç ”ç©¶ç›®çš„ã®ãŸã‚ã®ã€minimalã€ã‹ã¤é«˜é€ŸãªLLM training/inferenceã®ã‚³ãƒ¼ãƒ‰ãŒæ ¼ç´ã•ã‚ŒãŸãƒªãƒã‚¸ãƒˆãƒªã€‚ç‹¬è‡ªã®ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ‡ãƒ¼ã‚¿ã€ãƒ­ã‚¹ãªã©ãŒç°¡å˜ã«å®Ÿè£…ã§ãã‚‹æ¨¡æ§˜ã€‚<br><br><img src="https://github.com/user-attachments/assets/47f70515-3de0-455f-9fc4-0e2e17442eed" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Quantization.html" target="_blank" rel="noopener noreferrer">#Quantization</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2024-10-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1471" target="_blank" rel="noopener noreferrer" class="title-link">Introducing quantized Llama models with increased speed and a reduced memory footprint, Meta, 2024.10</a>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/InstructionTuning.html" target="_blank" rel="noopener noreferrer">#InstructionTuning</a>
<span class="issue_date">Issue Date: 2024-10-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1450" target="_blank" rel="noopener noreferrer" class="title-link">Unsloth</a>
<span class="snippet"><span>Comment</span><p>single-GPUã§ã€LLMã®LoRA/QLoRAã‚’é«˜é€Ÿ/çœãƒ¡ãƒ¢ãƒªã«å®Ÿè¡Œã§ãã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2024-09-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1419" target="_blank" rel="noopener noreferrer" class="title-link">LLMã®åŠ¹ç‡åŒ–ãƒ»é«˜é€ŸåŒ–ã‚’æ”¯ãˆã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ , Tatsuya Urabe, 2024.09</a>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Chip.html" target="_blank" rel="noopener noreferrer">#Chip</a>
<span class="issue_date">Issue Date: 2024-09-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1399" target="_blank" rel="noopener noreferrer" class="title-link">Sohu, etched, 2024.06</a>
<span class="snippet"><span>Comment</span><p>&gt;By burning the transformer architecture into our chip, we canâ€™t run most traditional AI models: the DLRMs powering Instagram ads, protein-folding models like AlphaFold 2, or older image models like Stable Diffusion 2. We canâ€™t run CNNs, RNNs, or LSTMs either.<br><br>transformerä»¥å¤–ã®å¤§æŠµã®ãƒ¢ãƒ‡ãƒ«ã§ã¯å‹•ä½œã—ãªã„ãŒã€ä»£ã‚ã‚Šã«H-100ã‚ˆã‚Šã‚‚20å€æ—©ã„inferenceã‚’å®Ÿç¾ã§ãã‚‹ãƒãƒƒãƒ—ã‚‰ã—ã„ã€‚<br><img src="https://github.com/user-attachments/assets/1fb2c6b4-3837-4bec-9a64-f8b4878e5941" alt="image" loading="lazy"><br><br>&gt;With over 500,000 tokens per second in Llama 70B throughput, Sohu lets you build products impossible on GPUs.<br><br>ã„ã‚„ã„ã‚„ã„ã‚„Llama-70Bã§0.5M Token/secã¯æ—©ã™ãã‚‹ï¼ï¼ï¼</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<span class="issue_date">Issue Date: 2024-08-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1356" target="_blank" rel="noopener noreferrer" class="title-link">Liger-Kernel, 2024.08</a>
<span class="snippet"><span>Comment</span><p>LLMã‚’å­¦ç¿’ã™ã‚‹æ™‚ã«ã€ãƒ¯ãƒ³ãƒ©ã‚¤ãƒ³è¿½åŠ ã™ã‚‹ã ã‘ã§ã€ãƒãƒ«ãƒGPUãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’20%æ”¹å–„ã—ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’60%å‰Šæ¸›ã™ã‚‹ã‚‰ã—ã„<br><br>å…ƒãƒ„ã‚¤ãƒ¼ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hsu_byron/status/1827072737673982056?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã“ã‚Œã ã‘ã§ã„ã„<br><img src="https://github.com/user-attachments/assets/abce24ed-f979-43db-ac51-e850f2ae877a" alt="image" loading="lazy"></p>
<p>Unsloth <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1450" target="_blank" rel="noopener noreferrer">Unsloth</a>
 ã¯LoRA/QLoRAãŒå¯èƒ½ãªä¸€æ–¹ã§ã¾ã Multi-GPUã¯ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ã€‚ä¸€æ–¹ã€Liger-Kernelã¯LoRAã‚ˆã‚Šã‚‚full-parameter tuningã¨Multi-GPUã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã—ã¦ãŠã‚Šã€ç›®çš„ã«å¿œã˜ã¦ä½¿ã„åˆ†ã‘ãŒå¿…è¦ã€‚<br><br><br><br>


<a href="https://github.com/linkedin/Liger-Kernel/issues/57" target="_blank" rel="noopener noreferrer">https://github.com/linkedin/Liger-Kernel/issues/57</a>


</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="articles/python.html" target="_blank" rel="noopener noreferrer">#python</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<span class="issue_date">Issue Date: 2024-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1343" target="_blank" rel="noopener noreferrer" class="title-link">DeepSpeed, vLLM, CTranslate2 ã§ rinna 3.6b ã®ç”Ÿæˆé€Ÿåº¦ã‚’æ¯”è¼ƒã™ã‚‹, 2024.06</a>
<span class="snippet"><span>Comment</span><p>[vllm](


<a href="https://github.com/vllm-project/vllm)%E3%82%92%E4%BD%BF%E3%81%86%E3%81%AE%E3%81%8C%E4%B8%80%E7%95%AA%E3%81%8A%E6%89%8B%E8%BB%BD%E3%81%A7%E3%80%81inference%E9%80%9F%E5%BA%A6%E3%81%8C%E9%80%9F%E3%81%9D%E3%81%86%E3%80%82PagedAttention%E3%81%A8%E5%91%BC%E3%81%B0%E3%82%8C%E3%82%8B%E3%82%AD%E3%83%A3%E3%83%83%E3%82%B7%E3%83%A5%E3%82%92%E5%88%A9%E7%94%A8%E3%81%97%E3%81%A6%E9%AB%98%E9%80%9F%E5%8C%96%E3%81%97%E3%81%A6%E3%81%84%E3%82%8B%E3%81%A3%E3%81%BD%E3%81%84%E3%80%82" target="_blank" rel="noopener noreferrer">https://github.com/vllm-project/vllm)ã‚’ä½¿ã†ã®ãŒä¸€ç•ªãŠæ‰‹è»½ã§ã€inferenceé€Ÿåº¦ãŒé€Ÿãã†ã€‚PagedAttentionã¨å‘¼ã°ã‚Œã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨ã—ã¦é«˜é€ŸåŒ–ã—ã¦ã„ã‚‹ã£ã½ã„ã€‚</a>


<br><br>ï¼ˆå›³ã¯ãƒ–ãƒ­ã‚°ä¸­ã‚ˆã‚Šå¼•ç”¨ï¼‰<br><br><br><br><img src="https://github.com/user-attachments/assets/36c83605-f925-4e19-b06c-2059c837e359" alt="image" loading="lazy"><br><br></p>
<p>ã“ã¡ã‚‰ã‚‚å‚ç…§ã®ã“ã¨<br><br>vLLMã®ä»•çµ„ã¿ã‚’ã–ã£ãã‚Šã¨ç†è§£ã™ã‚‹ï¼š


<a href="https://dalab.jp/archives/journal/vllm/#PagedAttention" target="_blank" rel="noopener noreferrer">https://dalab.jp/archives/journal/vllm/#PagedAttention</a>


</p>
<p>vLLMã§Reasoning Modelã‚’Servingã™ã‚‹ã¨ãã¯ã€`--enable-reasoning`ç­‰ã®è¿½åŠ ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚‹ç‚¹ã«æ³¨æ„<br>


<a href="https://docs.vllm.ai/en/stable/features/reasoning_outputs.html" target="_blank" rel="noopener noreferrer">https://docs.vllm.ai/en/stable/features/reasoning_outputs.html</a>


</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="articles/Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<span class="issue_date">Issue Date: 2024-04-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1297" target="_blank" rel="noopener noreferrer" class="title-link">AirLLM, 2024.04</a>
<span class="snippet"><span>Comment</span><p>4GBã®Single GPUã§ã€70Bãƒ¢ãƒ‡ãƒ«ã®inferenceã‚’å®Ÿç¾ã§ãã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚ãƒˆãƒ¼ã‚¯ãƒ³ã®ç”Ÿæˆé€Ÿåº¦ã¯æ¤œè¨¼ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚transformer decoderã®å„layerã®æ¼”ç®—ã¯ç‹¬ç«‹ã—ã¦ã„ã‚‹ãŸã‚ã€GPUã«å…¨ã¦ã®layerã‚’è¼‰ã›ãšã€å¿…è¦ãªåˆ†ã ã‘è¼‰ã›ã¦inferenceã™ã‚‹ã¨ã„ã£ãŸæ“ä½œã‚’ç¹°ã‚Šè¿”ã™æ¨¡æ§˜ã€‚<br><br>å…ƒãƒ„ã‚¤ãƒ¼ãƒˆ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rohanpaul_ai/status/1784349737899982943?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-12-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1188" target="_blank" rel="noopener noreferrer" class="title-link">optimize-llm, HuggingFace</a>
<span class="snippet"><span>Comment</span><p>LLMã‚’optimizeã™ã‚‹å®Ÿç”¨çš„ãªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«</p>
<p>ã“ã¡ã‚‰ã‚‚æœ‰ç”¨ãªã®ã§å‚ç…§ã®ã“ã¨<br><br><br><br>ã€GPU inferenceã€‘<br><br>


<a href="https://huggingface.co/docs/transformers/main/perf_infer_gpu_one" target="_blank" rel="noopener noreferrer">https://huggingface.co/docs/transformers/main/perf_infer_gpu_one</a>


<br><br></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2023-12-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1187" target="_blank" rel="noopener noreferrer" class="title-link">ã€ç¶šã€‘Flash Attentionã‚’ä½¿ã£ã¦LLMã®æ¨è«–ã‚’é«˜é€Ÿãƒ»è»½é‡åŒ–ã§ãã‚‹ã‹ï¼Ÿ</a>
<span class="snippet"><span>Comment</span><p>use_cacheãŒTrue/Falseã®å ´åˆã®FlashAttention2ã®inference timeã¨VRAMä½¿ç”¨é‡ã®å‚¾å‘ã‚’sequence_lengthã”ã¨ã«è€ƒå¯Ÿã—ã¦ã„ã‚‹ã€‚<br><br>use_cacheã¯Key Value cacheã®ã‚ªãƒ³ã‚ªãƒ•ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‰ã‚Œã‚‹ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã‚ã‚‹ã€‚autoregressiveãªãƒ¢ãƒ‡ãƒ«ã®inferenceæ™‚ã«ã¯ã€ä½•åº¦ã‚‚åŒã˜input tokenã«å¯¾ã™ã‚‹KVã®è¨ˆç®—ãŒç”Ÿã˜ã‚‹ãŸã‚ï¼ˆMç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã—ãŸå¾Œã€M+1ç•ªç›®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ç”Ÿæˆã‚’ã™ã‚‹å ´åˆã€M-1ç•ªç›®ã¾ã§ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®KVã‚’å†è¨ˆç®—ã›ã­ã°ãªã‚‰ãªã„ï¼‰ã€cacheã‚’ã™ã‚‹ã“ã¨ã§å¤§å¹…ã«è¨ˆç®—é€Ÿåº¦ãŒæ”¹å–„ã•ã‚Œã‚‹ã€‚<br><br>use_cacheã‚’Trueã«ã§ãã‚‹ãªã‚‰FlashAttention2ã®æ©æµã¯å°ã•ã„ï¼ˆinference timeãŒå°‘ã—æ—©ããªã‚‹ã®ã¿ï¼‰ãŸã‚ã€æ½¤æ²¢ãªVRAMãŒã‚ã‚‹ãªã‚‰å¾—ã‚‰ã‚Œã‚‹æ©æµã¯å°ã•ã„ã€‚<br>é€†ã«VRAMç¯€ç´„ã—ã¦use_cacheã‚’Falseã«ã›ã–ã‚‹ã‚’å¾—ãªã„ã®ã§ã‚ã‚Œã°ã€FlashAttention2ã«ã‚ˆã‚ŠVRAMä½¿ç”¨é‡ã‚’sequence_legthã®ç·šå½¢ã«æŠ‘ãˆã‚‹ã“ã¨ãŒã§ãã€ã‹ã¤inference timeã‚‚çŸ­ããªã‚‹ã€‚<br><br>â†‘ä¸Šè¨˜ã¯ã‚ãã¾ã§inferenceã‚’ã™ã‚‹å ´åˆã®ã¿ã®è©±ã§ã‚ã‚Šï¼ˆtrainæ™‚ã¯autoregressive modelã§ã¯causal maskã‚’ç”¨ã„ã€teacher forcingã§ä¸¦åˆ—ã«ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ãã‚‚ãã‚‚KV-cacheã™ã‚‹æ„å‘³ãŒãªã„ï¼‰ã€trainingã‚’ã™ã‚‹å ´åˆFlashAttention2ã§å¤§å¹…ã«VRAMä½¿ç”¨é‡ã‚’æ¸›ã‚‰ã›ã‚‹ã®ã§ã€ãã“ã¯åˆ†ã‘ã¦è€ƒãˆã‚‹ã“ã¨ã€‚<br>


<a href="https://qiita.com/jovyan/items/ff3d0a49163c7afa33ce" target="_blank" rel="noopener noreferrer">https://qiita.com/jovyan/items/ff3d0a49163c7afa33ce</a>


</p>
<p>Flash Attentionã‚’ä½¿ã£ã¦LLMã®æ¨è«–ã‚’é«˜é€Ÿãƒ»è»½é‡åŒ–ã§ãã‚‹ã‹ï¼Ÿ<br>


<a href="https://qiita.com/jovyan/items/11deb9d4601e4705a60d" target="_blank" rel="noopener noreferrer">https://qiita.com/jovyan/items/11deb9d4601e4705a60d</a>


<br><br>ã“ã¡ã‚‰ã®è¨˜äº‹ã‚‚éå¸¸ã«å‹‰å¼·ã«ãªã‚‹</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Tools.html" target="_blank" rel="noopener noreferrer">#Tools</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<span class="issue_date">Issue Date: 2023-11-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1150" target="_blank" rel="noopener noreferrer" class="title-link">GPT4All, 2023</a>
<span class="snippet"><span>Comment</span><p>ãƒ­ãƒ¼ã‚«ãƒ«ãƒã‚·ãƒ³ã§ChatGPT likeãªUIã§ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’å‹•ä½œã•ã›ã‚‰ã‚Œã‚‹Opensourceã€‚<br>Mistral7Bã‚„GGUFãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ãƒ¢ãƒ‡ãƒ«ã®ã‚ˆã¤ãªï¼ˆãŠãã‚‰ãé‡å­åŒ–ã•ã‚ŒãŸã‚‚ã®ã‚‚å«ã‚€ï¼‰ãƒ­ãƒ¼ã‚«ãƒ«ãƒã‚·ãƒ³ã§å‹•ä½œã•ã›ã‚‰ã‚Œã‚‹è¦æ¨¡æ„Ÿã®ãƒ¢ãƒ‡ãƒ«ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã€‚<br>


<a href="https://gpt4all.io/index.html" target="_blank" rel="noopener noreferrer">https://gpt4all.io/index.html</a>


</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2023-11-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1111" target="_blank" rel="noopener noreferrer" class="title-link">tsuzumi, NTTâ€™23</a>
<span class="snippet"><span>Comment</span><p>NTTè£½ã®LLMã€‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯7Bã¨è»½é‡ã ãŒé«˜æ€§èƒ½ã€‚<br>MTBenchã®ã‚ˆã†ãªGPT4ã«å‹æ•—ã‚’åˆ¤å®šã•ã›ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã€åœ°ç†ã€æ­´å²ã€æ”¿æ²»ã€ç¤¾ä¼šã«é–¢ã™ã‚‹è³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯ï¼ˆå›³6ï¼‰ã§gpt3.5turboã¨åŒç­‰ã€å›½ç”£LLMã®ä¸­ã§ãƒˆãƒƒãƒ—ã®æ€§èƒ½ã€‚GPT3.5turboã«ã¯ã€ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚„æ•°å­¦ãªã©ã®èƒ½åŠ›ã§ã¯åŠ£ã‚‹ã¨ã®ã“ã¨ã€‚<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/d064e0dc-b598-4853-9466-f56f39986acc" alt="image" loading="lazy"><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/c8251b2e-f865-4069-a3b7-9bfb848554bb" alt="image" loading="lazy"><br>&gt; ï¼Š6 Rakudaãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯<br>æ—¥æœ¬èªã®è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®ä¸€ã¤ã§ã€æ—¥æœ¬ã®åœ°ç†ãƒ»æ”¿æ²»ãƒ»æ­´å²ãƒ»ç¤¾ä¼šã«é–¢ã™ã‚‹è³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯ã«ã‚ˆã£ã¦è©•ä¾¡ã‚’è¡Œã†ã€‚<br>URLï¼š


<a href="https://yuzuai.jp/benchmark" target="_blank" rel="noopener noreferrer">https://yuzuai.jp/benchmark</a>


<br><br>&gt;ï¼Š7 Japanese Vicuna QAãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯<br>Rakudaã‚ˆã‚Šã‚‚ã•ã‚‰ã«å¹…åºƒã„ã‚«ãƒ†ã‚´ãƒªã§è¨€èªãƒ¢ãƒ‡ãƒ«ã®QAã‚„æŒ‡ç¤ºé‚è¡Œã®èƒ½åŠ›ã‚’å•ã†è©•ä¾¡æ–¹æ³•ã€‚ä¸€èˆ¬çŸ¥è­˜ã€ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ãªã©å¤šæ•°ã®è³ªå•ã‹ã‚‰æ§‹æˆã•ã‚Œã‚‹ã€‚<br>URLï¼š


<a href="https://github.com/hitoshizuku7/LLM_Judge_ku/blob/main/README.md" target="_blank" rel="noopener noreferrer">https://github.com/hitoshizuku7/LLM_Judge_ku/blob/main/README.md</a>


</p>
<p>tsuzumiã¯ã‚¢ãƒ€ãƒ—ã‚¿ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹ã“ã¨ãªãã€ã•ã¾ã–ã¾ãªçŸ¥è­˜ã‚’æŒãŸã›ãŸã‚Šã€æŒ¯ã‚‹èˆã„ã‚’å¤‰ãˆãŸã‚Šã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã¨ã®ã“ã¨ï¼ˆLoRAã‚¢ãƒ€ãƒ—ã‚¿ã®ã‚ˆã†ãªã‚‚ã®ã ã¨æ€ã‚ã‚Œã‚‹ï¼‰ã€‚<br>ã¾ã¦ã€å°†æ¥çš„ã«è¦–è¦šã‚„è´è¦šãªã©ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å¯¾å¿œã‚‚å®Ÿæ–½ã€‚</p>
<p>æ€æƒ³ãŒLoRA Hub <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/917" target="_blank" rel="noopener noreferrer">LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA   Composition, Chengsong Huang+, N/A, COLM'24</a>
 ã«è¿‘ãã€ã‚¢ãƒ€ãƒ—ã‚¿ã‚’ç€è„±ã™ã‚Œã°æŸ”è»Ÿã«ç”Ÿæˆã‚’å¤‰ãˆã‚‰ã‚Œã‚‹ã®ã¯æœ‰ç”¨ã ã¨æ€ã†ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/Catastrophic%20Forgetting.html" target="_blank" rel="noopener noreferrer">#Catastrophic Forgetting</a>
<span class="issue_date">Issue Date: 2023-10-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1109" target="_blank" rel="noopener noreferrer" class="title-link">å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®Fine-tuningã«ã‚ˆã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ç²å¾—ã®æ¤œè¨, PFN Blog, 2023.10</a>
<span class="snippet"><span>Comment</span><p>ä»¥ä¸‹è¨˜äº‹ä¸­ã§èˆˆå‘³æ·±ã‹ã£ãŸéƒ¨åˆ†ã‚’å¼•ç”¨<br>&gt; ã¾ã¨ã‚ã‚‹ã¨ã€LoRAã¯ã€[3]ã§è¨€ã‚ã‚Œã¦ã„ã‚‹ã€äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã¯å¤§é‡ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšä½ã„å›ºæœ‰æ¬¡å…ƒã‚’æŒã¡ã€Fine-tuningã«æœ‰åŠ¹ãªä½æ¬¡å…ƒã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã‚‚å­˜åœ¨ã™ã‚‹ã€ã¨ã„ã†ä¸»å¼µã«ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ã•ã‚Œã€Î”Wã«ãŠã‘ã‚‹é‡ã¿ã®æ›´æ–°ã®å›ºæœ‰æ¬¡å…ƒã‚‚ä½ã„ã¨ã„ã†ä»®èª¬ã®ã‚‚ã¨ã§ã€ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã§å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã«ãªã‚Šã¾ã™ã€‚<br><br>LoRAãŒæ‹ ã‚Šæ‰€ã¨ã™ã‚‹ä»®èª¬ãŒèª¬æ˜ã•ã‚Œã¦ãŠã‚Šã€å‹‰å¼·ã«ãªã£ãŸã€‚<br><br>&gt; ã“ã†ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’åœ§ç¸®ã™ã‚‹ä»–ã®æŠ€è¡“ã«ã¯æåˆˆã‚Šã‚„çŸ¥è­˜è’¸ç•™ãŒã‚ã‚Šã¾ã™ãŒã€é‡å­åŒ–ã¯ã€ã»ã¨ã‚“ã©ã®å ´åˆã«æåˆˆã‚Šã‚ˆã‚Šå„ªã‚Œã¦ã„ã‚‹ã¨ã•ã‚Œ[5]ã€è’¸ç•™ã‚ˆã‚Šã‚‚æ‰‹è»½ã«é«˜ç²¾åº¦ãªãƒ¢ãƒ‡ãƒ«ãŒå¾—ã‚‰ã‚Œã‚‹å¯èƒ½æ€§ãŒé«˜ãã€LLMã«ãŠã„ã¦ã‚‚æœ‰åŠ›ãªæŠ€è¡“ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚<br><br>ã“ã‚Œã‚‚çŸ¥ã‚‰ãªã‹ã£ãŸã—ã€æ–‡çŒ®ä»˜ãã§è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒå¤§å¤‰ã‚ã‚ŠãŒãŸã„ã€‚<br><br>&gt; QLoRAä»¥å¤–ã®LoRAã®æ´¾ç”Ÿæ‰‹æ³•ã¨ã—ã¦ã¯ã€ãƒ©ãƒ³ã‚¯ã‚’é©å¿œçš„ã«å®šã‚ã‚‹AdaLoRA[7] ã‚„DyLoRA[8]ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’æ‹¡å¤§ã§ãã‚‹LongLoRA[9]ã€è¡Œåˆ—Aã®é‡ã¿ã‚’freezeã™ã‚‹ã“ã¨ã§ã•ã‚‰ã«è»½é‡åŒ–ã‚’è¡Œã†LoRA-FAã€è¡Œåˆ—ç©ã‚’ã‚¢ãƒ€ãƒãƒ¼ãƒ«ç©ã‚„ã‚¯ãƒ­ãƒãƒƒã‚«ãƒ¼ç©ã§è¨ˆç®—ã™ã‚‹LoHAã‚„LoKRãªã©ãŒã‚ã‚Šã¾ã™ï¼ˆä¸€éƒ¨ã¯LLMã§ã¯ãªãStable Diffusionã®å­¦ç¿’ã§ç”¨ã„ã‚‰ã‚Œã‚‹æ‰‹æ³•ã®é€šç§°ã§ã™ï¼‰ã€‚<br><br>ã“ã®è¾ºã¯å®Ÿéš›ã«LoRAã‚’ä½¿ã†ã“ã¨ã«ãªã£ãŸã‚‰å‹‰å¼·ã—ãŸã„ã€‚<br><br>&gt; è¨€èªãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã¯é€šå¸¸ã€Causal LMã®å ´åˆã¯ã€Next Token Predictionã«ãŠã‘ã‚‹Perplexityã®æœ€å°åŒ–ã«ã‚ˆã‚‹æ•™å¸«ãªã—å­¦ç¿’ã«ã‚ˆã£ã¦æœ€é©åŒ–ã•ã‚Œã¾ã™ã€‚<br><br>HuggingFaceã®å®Ÿè£…ã®è©±ã ã¨æ€ã†ãŒã€ãã†ã ã‚ã†ãªã¨æ€ã£ã¦ã¯ã„ãŸãŒã‚½ãƒ¼ã‚¹ã‚’ç¢ºèªã§ãã¦ã„ãªã‹ã£ãŸã®ã§å‹‰å¼·ã«ãªã£ãŸã€‚<br><br>&gt; 7Bã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ä»¥ä¸‹ã®ã‚°ãƒ©ãƒ•ã®ã‚ˆã†ã«ã€ãƒ‡ãƒ¼ã‚¿ã®ä»¶æ•°ã‚’å¢—ã‚„ã™ã¨å­¦ç¿’ãŒã†ã¾ãã„ã‹ãªã„ã¨ã„ã†çµæœãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚ã¾ãŸã€LoRAã®ãƒ©ãƒ³ã‚¯ã¯ä½ã„æ–¹ãŒå­¦ç¿’ãŒå®‰å®šã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚æ­£ç­”ç‡ãŒè‘—ã—ãä½ã„ã‚‚ã®ã¯ã€å­¦ç¿’æ™‚ã®ãƒ­ã‚¹ï¼ˆäº¤å·®ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ï¼‰ãŒéå¸¸ã«å¤§ãããªã£ã¦ãŠã‚Šã€é¸æŠè‚¢ã‚’é–“é•ãˆã‚‹ã¨ã„ã†ã‚ˆã‚Šã¯è¨€èªãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦ã®æ©Ÿèƒ½ãŒå¤±ã‚ã‚Œã¦ã„ã¾ã—ãŸã€‚<br><br>&gt; ä»–ã«ã¯ã€Instructionãƒ‡ãƒ¼ã‚¿ï¼ˆ1ã¤ã®ã‚¯ã‚¤ã‚ºã®Q&amp;Aï¼‰ãŒ2500ä»¶ã‚’è¶…ãˆã‚‹ã¨ãƒ­ã‚¹ãŒæ‚ªåŒ–ã™ã‚‹ã“ã¨ã‚„ã€2000ä»¶ã§ã‚‚2epochç¹°ã‚Šè¿”ã™ã¨catastrophic forgettingãŒè¦‹ã‚‰ã‚Œã€è¨€èªãƒ¢ãƒ‡ãƒ«ãã®ã‚‚ã®ã®æ€§èƒ½ãŒå¤±ã‚ã‚Œæ„å‘³ã®ãªã„å‡ºåŠ›ã‚’ã—ã¦ã„ã¾ã—ãŸã€‚[17] ã§ã‚‚è¨€åŠã•ã‚Œã¦ã„ã¾ã™ãŒã€æ—¥æœ¬èªã®å­¦ç¿’ã§ã¯ã€æ•°Bã®ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹LoRAã«ã‚ˆã‚‹Instruction Tuningã¯ã‚ã¾ã‚ŠåŠ¹æœãŒå¾—ã‚‰ã‚Œãªã„å¯èƒ½æ€§ãŒé«˜ã„ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚<br><br>&gt; ä¸€æ–¹ã€13Bã®ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€8ã€16ã€32ã€64ã„ãšã‚Œã®ãƒ©ãƒ³ã‚¯ã§ã‚‚å¤§ããªå·®ã¯è¦‹ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚<br>&gt; ã“ã‚Œã‚‰ã‹ã‚‰ã€Addtional Trainingã§å­¦ç¿’ã•ã›ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒInstruction Tuningã«å¯¾ã—ã¦è†¨å¤§ã§ã‚ã‚‹å ´åˆã«ã¯å…ˆã«å­¦ç¿’ã—ãŸæ–¹ãŒã‚ˆãã€å°‘æ•°ã®å ´åˆã¯å¾Œã«å­¦ç¿’ã•ã›ã¦ã‚‚Instruction Tuningã®åŠ¹æœã«ã¯æ‚ªå½±éŸ¿ãŒãªã„ã¨ã„ã†ã“ã¨ãŒç¤ºå”†ã•ã‚Œã¾ã—ãŸã€‚<br><br>&gt; ã¾ãŸå­¦ç¿’ã¯ã€åˆæœŸå­¦ç¿’ç‡ã‚’å°ã•ãã—ãŸæ–¹ãŒå®‰å®šã™ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ã¨æ€ã‚ã‚Œã¾ã™ã€‚LoRAã®è«–æ–‡[2] ã§ã¯GPTã®Fine-tuneã¯2e-4ã§è¡Œã‚ã‚Œã¦ãŠã‚Šã€hugging faceã®å®Ÿè£…ã§ã‚‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯2e-4ã¨ãªã£ã¦ã„ã¾ã™ãŒã€ä»–ã®è«–æ–‡ã‚„ãƒ–ãƒ­ã‚°ã§ã¯3e-5ã§ã®ä¾‹ãªã©ã‚‚ã‚ã‚Šã¾ã™ã€‚ã—ã‹ã—ã€å˜ã«ä¸‹ã’ã‚Œã°å®‰å®šã™ã‚‹ã¨ã„ã†ã“ã¨ã§ã‚‚ãªãã€ï¼‘å›ã®è©¦è¡Œã«ãŠã‘ã‚‹è¨ˆç®—ã‚³ã‚¹ãƒˆã¨ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã«ãªã‚‹å¯èƒ½æ€§ã¯ã‚ã‚Šã¾ã™ã€‚<br><br>Additional Trainingã¨ã¯Finetuningã®ã“ã¨ã§ä¾¿å®œä¸Šã®æœ¬ãƒ–ãƒ­ã‚°ã§ã®å‘¼ç§°ã€‚å®Ÿéš›ã®æ–‡æ›¸ä¸­ã§ã¯å›³ãŒè¤‡æ•°å€‹æŒŸã¾ã‚Œã¦ã„ã‚‹ã€‚<br>ã“ã†ã—ãŸå®Ÿéš›ã«æ‰‹ã‚’å‹•ã‹ã—ãŸä¸Šã§ãªã„ã¨å¾—ã‚‰ã‚Œãªã„çŸ¥è¦‹ã‚’å…¬é–‹ã—ã¦ãã‚Œã‚‹ã®ã¯éå¸¸ã«ã‚ã‚ŠãŒãŸã„ã“ã¨ã ã—ã€æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã§LoRAã‚’ã™ã‚‹éš›ã«éå¸¸ã«å‚è€ƒã«ãªã‚Šãã†ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2023-10-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1107" target="_blank" rel="noopener noreferrer" class="title-link">StableDiffusion, LLMã®GPUãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã®ã‚ã‚Œã“ã‚Œ</a>
<span class="snippet"><span>Comment</span><p>Gradient Accumulation, Gradient Checkpointingã®èª¬æ˜ãŒä¸å¯§ã§ã‚ã‹ã‚Šã‚„ã™ã‹ã£ãŸã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2023-07-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/899" target="_blank" rel="noopener noreferrer" class="title-link">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning, 2023</a>
<span class="snippet"><span>GPT Summary</span>- FlashAttention-2ã¯ã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«ãŠã‘ã‚‹Transformerã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ææ¡ˆã•ã‚ŒãŸæ‰‹æ³•ã§ã™ã€‚FlashAttention-2ã¯ã€éå¯¾ç§°ãªGPUãƒ¡ãƒ¢ãƒªéšå±¤ã‚’åˆ©ç”¨ã—ã¦ãƒ¡ãƒ¢ãƒªã®ç¯€ç´„ã¨ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã—ã€æœ€é©åŒ–ã•ã‚ŒãŸè¡Œåˆ—ä¹—ç®—ã«æ¯”ã¹ã¦ç´„2å€ã®é«˜é€ŸåŒ–ã‚’é”æˆã—ã¾ã™ã€‚ã¾ãŸã€FlashAttention-2ã¯GPTã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦ã‚‚é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã—ã€æœ€å¤§225 TFLOPs/sã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã«é”ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>Flash Attention1ã‚ˆã‚Šã‚‚2å€é«˜é€ŸãªFlash Attention 2</p>
<p>Flash Attention1ã¯ã“ã¡ã‚‰ã‚’å‚ç…§<br>


<a href="https://arxiv.org/pdf/2205.14135.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2205.14135.pdf</a>


<br><br>QK Matrixã®è¨ˆç®—ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†ã‘ã¦SRAMã«é€ã£ã¦å‡¦ç†ã™ã‚‹ã“ã¨ã§ã€3å€é«˜é€ŸåŒ–ã—ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’10-20å€ã‚’é”æˆã€‚<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/935f61f3-97ce-4e76-826b-040f92ca567c" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/Quantization.html" target="_blank" rel="noopener noreferrer">#Quantization</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<span class="issue_date">Issue Date: 2023-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/886" target="_blank" rel="noopener noreferrer" class="title-link">LLaMA2ã‚’3è¡Œã§è¨“ç·´</a>
<span class="snippet"><span>Comment</span><p>LLaMA2ã‚’3è¡Œã§ã€1ã¤ã®A100GPUã€QLoRAã§ã€è‡ªå‰ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¨“ç·´ã™ã‚‹æ–¹æ³•</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/python.html" target="_blank" rel="noopener noreferrer">#python</a>
<span class="issue_date">Issue Date: 2023-05-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/675" target="_blank" rel="noopener noreferrer" class="title-link">Assisted Generation: a new direction toward low-latency text generation, 2023</a>
<span class="snippet"><span>Comment</span><p>1 lineåŠ ãˆã‚‹ã¨transformerã®generationãŒæœ€å¤§3å€ç¨‹åº¦é«˜é€ŸåŒ–ã•ã‚Œã‚‹ã‚ˆã†ã«ãªã£ãŸã‚‰ã—ã„</p>
<p><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/fecc1c5e-b9e5-4844-af96-ba48c3d60fae" alt="image" loading="lazy"><br><br>assistant modelã‚’ãƒ­ãƒ¼ãƒ‰ã—generateã«å¼•æ•°ã¨ã—ã¦æ¸¡ã™ã ã‘<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/7dabf3bf-cd32-469c-abba-f1269318576d" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<span class="issue_date">Issue Date: 2023-04-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/531" target="_blank" rel="noopener noreferrer" class="title-link">Training a recommendation model with dynamic embeddings</a>
<span class="snippet"><span>Comment</span><p>dynamic embeddingã‚’ä½¿ã£ãŸæ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰æ–¹æ³•ã®è§£èª¬</p>
<p>ï¼ˆç†è§£ãŒé–“é•ã£ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ãŒï¼‰æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¯å…¸å‹çš„ã«ã¯ãƒ¦ãƒ¼ã‚¶ã¨ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ™ã‚¯ãƒˆãƒ«è¡¨ç¾ã—ã€é–¢é€£åº¦ã‚’æ¸¬ã‚‹ã“ã¨ã§æ¨è–¦ã‚’ã—ã¦ã„ã‚‹ã€‚ã“ã®æ çµ„ã¿ã‚’ã‚ã£ã¡ã‚ƒã‚¹ã‚±ãƒ¼ãƒ«ã•ã›ã‚‹ã¨ã¨ã‚“ã§ã‚‚ãªã„æ•°ã®Embeddingã‚’ä¿æŒã™ã‚‹ã“ã¨ã«ãªã‚Šã€ãƒ¡ãƒ¢ãƒªä¸Šã«Embeddingãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ä¿æŒã—ã¦ç½®ã‘ãªããªã‚‹ã€‚ç‰¹ã«ã“ã‚Œã¯online machine learningï¼ˆãŸã¨ãˆã°ãƒ¦ãƒ¼ã‚¶ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒã‚¢ã‚¤ãƒ†ãƒ ã®sequenceã§è¡¨ç¾ã•ã‚ŒãŸã¨ãã€ãã®sequenceã‚’è¡¨ã™Embeddingã‚’è¨ˆç®—ã—ä¿æŒã—ã¦ãŠãã€ã‚¢ã‚¤ãƒ†ãƒ ã¨ã®é–¢é€£åº¦ã‚’æ¸¬ã‚‹ã“ã¨ã§æ¨è–¦ã™ã‚‹ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ±ºã‚ã‚‹ã€ã¿ãŸã„ãªã“ã¨ãŒå¿…è¦ï¼‰ã§ã¯é¡•è‘—ã§ã‚ã‚‹ï¼ˆã“ã®è¾ºã®ç†è§£ãŒæµ…ã„ï¼‰ã€‚ã—ã‹ã—ã€ã»ã¨ã‚“ã©ã®Embeddingã¯rarely seenãªã®ã§ã€å³å¯†ãªEmbeddingã‚’ä¿æŒã—ã¦ãŠãã“ã¨ã«å®Ÿç”¨ä¸Šã®æ„å‘³ã¯ãªãã€ãã‚Œã‚‰ã‚’å˜ä¸€ã®ãƒ™ã‚¯ãƒˆãƒ«ã§ã§ãã‚‹ã¨ãƒ¡ãƒ¢ãƒªç¯€ç´„ã«ãªã£ã¦å¬‰ã—ã„ï¼ˆã“ã†ã„ã£ãŸå‡¦ç†ã‚’ã—ã¦ã‚‚topNã®æ¨è–¦çµæœã¯å¤‰ã‚ã‚‰ãªã„ã¨æ€ã‚ã‚Œã‚‹ã®ã§ï¼‰ã€‚<br>ã“ã‚ŒãŒdynamic embeddingã®ãƒ¢ãƒãƒ™ã§ã‚ã‚Šã€ã©ã†ã‚„ã£ã¦ãã‚Œã‚’TFã§å®Ÿè£…ã™ã‚‹ã‹è§£èª¬ã—ã¦ã„ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/Slide.html" target="_blank" rel="noopener noreferrer">#Slide</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2023-04-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/528" target="_blank" rel="noopener noreferrer" class="title-link">LoRAè«–æ–‡è§£èª¬, Hayato Tsukagoshi, 2023.04</a>
<span class="snippet"><span>Comment</span><p>ãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ä¸€éƒ¨ã®ç·šå½¢å±¤ã®éš£ã«ã€ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—A,Bã‚’å°å…¥ã—ã€A,Bã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã¿ã‚’finetuningã®å¯¾è±¡ã¨ã™ã‚‹ã“ã¨ã§ã€ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’æ¿€æ¸›ã•ã›ãŸä¸Šã§åŒç­‰ã®äºˆæ¸¬æ€§èƒ½ã‚’é”æˆã—ã€æ¨è«–é€Ÿåº¦ã‚‚å¤‰ã‚ã‚‰ãªã„ã‚ˆã†ã«ã™ã‚‹finetuningæ‰‹æ³•ã®è§£èª¬</p>
<p>LoRAã‚’ä½¿ã†ã¨ã€ã§ã‹ã™ãã‚‹ãƒ¢ãƒ‡ãƒ«ã ã¨ã€ãã‚‚ãã‚‚GPUã«è¼‰ã‚‰ãªã„å•é¡Œã‚„ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‹ã™ããƒ¯ãƒ­ã‚¿å•é¡ŒãŒå›é¿ã§ãã‚‹ã€‚<br><br>å‰è€…ã¯äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®BPã®ãŸã‚ã®å‹¾é…ã‚’ä¿å­˜ã—ã¦ãŠãå¿…è¦ãŒãªããªã‚‹ãŸã‚å­¦ç¿’æ™‚ã«ãƒ¡ãƒ¢ãƒªç¯€ç´„ã«ãªã‚‹ã€‚å¾Œè€…ã¯A,Bã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã ã‘ä¿å­˜ã™ã‚Œã°ã„ã„ã®ã§ã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®ç¯€ç´„ã«ãªã‚‹ã€‚<br><br>ã‹ã¤ã€å­¦ç¿’é€Ÿåº¦ãŒ25%ç¨‹åº¦æ—©ããªã‚‹ã€‚</p>
<p>æ—¢å­˜ç ”ç©¶ã§ã‚ã‚‹Adapterï¼ˆtransformerã®ä¸­ã«å­¦ç¿’å¯èƒ½ãªMLPã‚’å·®ã—è¾¼ã‚€æ‰‹æ³•ï¼‰ã¯æ¨è«–ã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã€prefix tuningã¯å­¦ç¿’ãŒéå¸¸ã«é›£ã—ãã€é«˜ã„æ€§èƒ½ã‚’é”æˆã™ã‚‹ãŸã‚ã«prefixã¨ã—ã¦128 tokenå…¥ã‚ŒãŸã‚Šã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ã€‚</p>
<p>huggingfaceãŒã™ã§ã«LoRAã‚’å®Ÿè£…ã—ã¦ã„ã‚‹<br>


<a href="https://github.com/huggingface/peft" target="_blank" rel="noopener noreferrer">https://github.com/huggingface/peft</a>


</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<span class="issue_date">Issue Date: 2021-06-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/384" target="_blank" rel="noopener noreferrer" class="title-link">FastSeq: Make Sequence Generation Faster, Yan+, ACLâ€™21</a>
<span class="snippet"><span>Comment</span><p>BART, DistilBART, T5, GPT2ç­‰ã®ã•ã¾ã–ã¾ãªTransformer-basedãªæ‰‹æ³•ã§ã€4-9å€Inference speedã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="articles/python.html" target="_blank" rel="noopener noreferrer">#python</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2021-06-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/373" target="_blank" rel="noopener noreferrer" class="title-link">intel MKL</a>
<span class="snippet"><span>Comment</span><p>intel CPUã§pythonã®æ•°å€¤è¨ˆç®—ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª(numpyã¨ã‹ã¯ã‚„ããªã‚‹ã‚‰ã—ã„; Anacondaã ã¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§å…¥ã£ã¦ã‚‹ã¨ã‹ãªã‚“ã¨ã‹)</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/ImageClassification.html" target="_blank" rel="noopener noreferrer">#ImageClassification</a>
<span class="issue_date">Issue Date: 2021-05-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/346" target="_blank" rel="noopener noreferrer" class="title-link">EfficientNetè§£èª¬, omiita ï¼ˆã‚ªãƒŸãƒ¼ã‚¿ï¼‰, 2019</a>
<span class="snippet"><span>Comment</span><p>æ—¢å­˜ç”»åƒèªè­˜ãƒ¢ãƒ‡ãƒ«ã®æ§‹é€ ã¯å¤‰åŒ–ã•ã›ãšã€åºƒã•ã€æ·±ã•ã€è§£åƒåº¦ã‚’è¤‡åˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€å¾“æ¥ã‚ˆã‚Šã‚‚å°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã€ã‹ã¤å­¦ç¿’é€Ÿåº¦ã§SoTAã‚’é”æˆã€‚åºƒã•ã€æ·±ã•ã€è§£åƒåº¦ã¯ãã‚Œãã‚Œæ€§èƒ½ã«äº’ã„ã«å½±éŸ¿ã—ã‚ã£ã¦ãŠã‚Šã€å¾“æ¥ã®ã‚ˆã†ã«åˆ¥ã€…ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã®ã§ã¯ãªãã€3ã¤ã®ãƒãƒ©ãƒ³ã‚¹ã‚’ã¨ã‚ŠãªãŒã‚‰ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã€‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹éš›ã¯ã€çµæœçš„ã«ã¯ãã‚Œãã‚Œã‚’ã‚ã‚‹å€¤ã§å®šæ•°å€ã™ã‚Œã°è‰¯ãã€ãã®ã‚ã‚‹å€¤ã¯æœ€å¤§ãƒ¡ãƒ¢ãƒªã‚„æœ€å¤§FLOPSæ•°ä»¥ä¸‹ï¼ˆãŠã‚ˆã³FLOPSãŒ2ã®Î¦ä¹—ã§å¢—åŠ ã™ã‚‹ã‚ˆã†ãªï¼‰ã¨ã„ã£ãŸåˆ¶ç´„ä¸‹ã§AccuracyãŒæœ€å¤§åŒ–ã•ã‚Œã‚‹å€¤ã‚’ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒã§è¦‹ã¤ã‘ã‚‹ï¼ˆã‚‰ã—ã„ã€‚ã–ã£ãã‚Šã¨ã—ãŸç†è§£ï¼‰ã€‚<br>è»¢ç§»å­¦ç¿’ã—ã¦ã‚‚å¤šãã®ã‚¿ã‚¹ã‚¯ã§SoTAé”æˆã—ãŸã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/81" target="_blank" rel="noopener noreferrer" class="title-link">Efficient Methods and Hardware for Deep Learning, Han, Stanford University, 2017.05</a>
<button onclick="hideContent(0)" style="display: none;">hide</button>
&lt;/div&gt;
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<script>
  document.addEventListener('DOMContentLoaded', function() {
    const tweets = document.querySelectorAll('.tweet-embed[data-embed]');

    if ('IntersectionObserver' in window) {
      const observer = new IntersectionObserver((entries, obs) => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            const el = entry.target;
            const html = el.getAttribute('data-embed');
            if (html) {
              const placeholder = el.querySelector('.tweet-placeholder');
              if (placeholder) placeholder.remove();

              el.innerHTML = html.trim();

              if (window.twttr?.widgets?.load) {
                window.twttr.widgets.load(el);
              }
            }
            obs.unobserve(el); // å‡¦ç†æ¸ˆã¿ã¯ç›£è¦–è§£é™¤
          }
        });
      }, {
        rootMargin: '500px 0px', // ç”»é¢æ‰‹å‰200pxã§èª­ã¿è¾¼ã¿é–‹å§‹
        threshold: 0
      });

      tweets.forEach(tweet => observer.observe(tweet));

    } else {
      // IntersectionObserveræœªå¯¾å¿œãƒ–ãƒ©ã‚¦ã‚¶ç”¨ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
      function lazyLoadFallback() {
        tweets.forEach(el => {
          if (el.getAttribute('data-embed') && el.getBoundingClientRect().top < window.innerHeight) {
            const html = el.getAttribute('data-embed');
            const loadingImg = el.querySelector('.tweet-loading');
            if (loadingImg) loadingImg.remove();
            el.innerHTML = html.trim();
            el.removeAttribute('data-embed');
            if (window.twttr?.widgets?.load) {
              window.twttr.widgets.load(el);
            }
          }
        });
      }
      window.addEventListener('scroll', lazyLoadFallback);
      lazyLoadFallback();
    }
  });
</script>
</div>


    </div>

</article>
<div class="post-nav">
<a class="previous" href="/paper_notes/articles/EducationalDataMining.html" title="EducationalDataMiningã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">EducationalDataMiningã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§</a><a class="next" href="/paper_notes/articles/Embeddings.html" title="Embeddingsã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">Embeddingsã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§</a>
</div>
<div class="post-related">
      <div>Related Articles</div>
      <ul>
        <li class="">
          <a class="post-link" href="/paper_notes/articles/VCS.html" title="VCSã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            VCSã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/UnitTest.html" title="UnitTestã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            UnitTestã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/Speech.html" title="Speechã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            Speechã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/SentimentAnalysis.html" title="SentimentAnalysisã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            SentimentAnalysisã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
</ul>
    </div>
<div class="post-comments"></div></section>
</div>


  </section>
  <section class="sidebar" style="margin-left: 15px;">
    <!-- Get sidebar items --><style type="text/css" media="screen">
.post-menu ul {
  list-style: none;
  padding: 0;
  margin: 0;
}
</style>

<div class="post-menu">
  <div class="post-menu-title">TOC</div>
  <div class="post-menu-content"></div>
</div>

<script>
  function generateContent() {
    var menu = document.querySelector(".post-menu");
    var menuContent =  menu.querySelector(".post-menu-content");
    var headings = document.querySelector(".post-content").querySelectorAll("h2, h3, h4, h5, h6");

    // Hide menu when no headings
    if (headings.length === 0) {
      return menu.style.display = "none";
    }

    // Generate post menu
    var menuHTML = '';
    for (var i = 0; i < headings.length; i++) {
      var h = headings[i];
      menuHTML += (
        '<li class="h-' + h.tagName.toLowerCase() + '">'
        + '<a href="#h-' + h.getAttribute('id') + '">' + h.textContent + '</a></li>');
    }

    menuContent.innerHTML = '<ul>' + menuHTML + '</ul>';

    // The header element
    var header = document.querySelector('header.site-header');

    function doMenuCollapse(index, over_items) {
      var items = menuContent.firstChild.children;

      if (over_items == undefined) {
        over_items = 20;
      }

      if (items.length < over_items) {
        return;
      }

      var activeItem = items[index];
      var beginItem = activeItem
      var endItem = activeItem
      var beginIndex = index;
      var endIndex = index + 1;
      while (beginIndex >= 0
        && !items[beginIndex].classList.contains('h-h2')) {
        beginIndex -= 1;
      }
      while (endIndex < items.length
        && !items[endIndex].classList.contains('h-h2')) {
        endIndex += 1;
      }
      for (var i = 0; i < beginIndex; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
      for (var i = beginIndex + 1; i < endIndex; i++) {
        item = items[i]
        // if (!item.classList.contains('h-h2')) {
          item.style.display = '';
        // }
      }
      for (var i = endIndex; i < items.length; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
    }

    // Init menu collapsed
    doMenuCollapse(-1);

    // Active the menu item
    window.addEventListener('scroll', function (event) {
      var lastActive = menuContent.querySelector('.active');
      var changed = true;
      var activeIndex = -1;
      for (var i = headings.length - 1; i >= 0; i--) {
        var h = headings[i];
        var headingRect = h.getBoundingClientRect();
        var headerRect = header.getBoundingClientRect();
        var headerTop = Math.floor(headerRect.top);
        var headerHeight = Math.floor(headerRect.height);
        var headerHeight = headerTop + headerHeight + 20;
        if (headingRect.top <= headerHeight) {
          var id = 'h-' + h.getAttribute('id');
          var a = menuContent.querySelector('a[href="#' + id  + '"]');
          var curActive = a.parentNode;
          if (curActive) {
            curActive.classList.add('active');
            activeIndex = i;
          }
          if (lastActive == curActive) {
            changed = false;
          }
          break;
        }
      }
      if (changed) {
        if (lastActive) {
          lastActive.classList.remove('active');
        }
        doMenuCollapse(activeIndex);
      }
      event.preventDefault();
    });
  }
  generateContent();
</script>
</section>
</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/paper_notes/"></data>

  <div class="wrapper">
    <div class="site-footer-inner">
<div>Copyright Â© 2023-current AkihikoWATANABE. The header images and any thumbnail images for the posts were generated by ChatGPT's DALL-E3.</div>
      <div>Powered by <a title="Jekyll is a simple, blog-aware, static site
      generator." href="https://jekyllrb.com/">Jekyll</a> &amp; <a title="Yat, yet
      another theme." href="https://github.com/jeffreytse/jekyll-theme-yat">Yat Theme</a>.</div>
      <div class="footer-col rss-subscribe">Subscribe <a href="/paper_notes/feed.xml">via RSS</a>
</div>
    </div>
  </div>
</footer>
</body>
  </html>
