<!DOCTYPE html>
<html lang="ja">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-translate-customization" content="108d9124921d80c3-80e20d618ff053c8-g4f02ec6f3dba68b7-c">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Transformerã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§ | ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ</title>
<meta name="generator" content="Jekyll v3.10.0">
<meta property="og:title" content="Transformerã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
<meta name="author" content="AkihikoWATANABE">
<meta property="og:locale" content="ja">
<meta name="description" content="Transformer #EfficiencyImprovement #Pocket #NLP #LanguageModel #Architecture #read-later #Selected Papers/Blogs #One-Line Notes">
<meta property="og:description" content="Transformer #EfficiencyImprovement #Pocket #NLP #LanguageModel #Architecture #read-later #Selected Papers/Blogs #One-Line Notes">
<link rel="canonical" href="http://akihikowatanabe.github.io/paper_notes/articles/Transformer.html">
<meta property="og:url" content="http://akihikowatanabe.github.io/paper_notes/articles/Transformer.html">
<meta property="og:site_name" content="ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-11-18T00:48:11+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Transformerã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"AkihikoWATANABE"},"dateModified":"2025-11-18T00:48:11+00:00","datePublished":"2025-11-18T00:48:11+00:00","description":"Transformer #EfficiencyImprovement #Pocket #NLP #LanguageModel #Architecture #read-later #Selected Papers/Blogs #One-Line Notes","headline":"Transformerã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§","mainEntityOfPage":{"@type":"WebPage","@id":"http://akihikowatanabe.github.io/paper_notes/articles/Transformer.html"},"url":"http://akihikowatanabe.github.io/paper_notes/articles/Transformer.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="icon" href="">
  <link rel="canonical" href="http://akihikowatanabe.github.io">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/main.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/custom_button.css">
  <script src="/paper_notes/assets/js/main.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script><link type="application/atom+xml" rel="alternate" href="http://akihikowatanabe.github.io/paper_notes/feed.xml" title="ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ">
<script>
  function showMore(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "block";
          
      // ã“ã®ãƒœã‚¿ãƒ³ã®å‚ç…§ã‚’å–å¾—ã—ã¦éè¡¨ç¤ºã«ã—ã¾ã™
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "none";
      }
          
      // hideãƒœã‚¿ãƒ³ã®å‚ç…§ã‚’å–å¾—ã—ã¦è¡¨ç¤ºã—ã¾ã™
      const hideButton = contentDiv.querySelector('button[onclick^="hideContent"]');
      if (hideButton) {
        hideButton.style.display = "block";
      }
    }
  }

  function hideContent(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "none";
  
      // moreãƒœã‚¿ãƒ³ã®å‚ç…§ã‚’å–å¾—ã—ã¦è¡¨ç¤ºã—ã¾ã™
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "block";
      }
  
      // ã“ã®ãƒœã‚¿ãƒ³ã‚’éš ã—ã¾ã™
      const hideButtons = document.querySelectorAll('button[onclick^="hideContent"]');
      const hideButton = hideButtons[index];
      if (hideButton) {
        hideButton.style.display = "none";
      }
    }
  }
</script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script charset="UTF-8" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/go.min.js" async></script>



















<script>
// Init highlight js
document.addEventListener('DOMContentLoaded', function(event) {
  var els = document.querySelectorAll('pre code')

  function addLangData(block) {
    var outer = block.parentElement.parentElement.parentElement;
    var lang = block.getAttribute('data-lang');
    for (var i = 0; i < outer.classList.length; i++) {
      var cls = outer.classList[i];
      if (cls.startsWith('language-')) {
        lang = cls;
        break;
      }
    }
    if (!lang) {
      cls = block.getAttribute('class');
      lang = cls ? cls.replace('hljs ', '') : '';
    }
    if (lang.startsWith('language-')) {
      lang = lang.substr(9);
    }
    block.setAttribute('class', 'hljs ' + lang);
    block.parentNode.setAttribute('data-lang', lang);
  }

  function addBadge(block) {
    var enabled = ('true' || 'true').toLowerCase();
    if (enabled == 'true') {
      var pre = block.parentElement;
      pre.classList.add('badge');
    }
  }

  function handle(block) {
    addLangData(block);
    addBadge(block)
    hljs.highlightBlock(block);
  }

  for (var i = 0; i < els.length; i++) {
    var el = els[i];
    handle(el);
  }
});
</script>

<style>
  /* code language badge */
  pre.badge::before {
    content: attr(data-lang);
    color: #fff;
    background-color: #ff4e00;
    padding: 0 .5em;
    border-radius: 0 2px;
    text-transform: uppercase;
    text-align: center;
    min-width: 32px;
    display: inline-block;
    position: absolute;
    right: 0;
  }

  /* fix wrong badge display for firefox browser */
  code > table pre::before {
    display: none;
  }
</style>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe-lightbox.umd.min.js" async></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe.umd.min.js" async></script>
<link href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/photoswipe.min.css" rel="stylesheet">
<style>
  .pswp .pswp__container .pswp__img {
    background-color: white;
  }
</style>

<script>
  function initPhotoSwipe() {
    let customOptions = {};

    try {
      const data = `{"gallery"=>"section.main", "children"=>"a.photo-swipe", "bgOpacity"=>0.8, "padding"=>{"top"=>20, "bottom"=>40, "left"=>100, "right"=>100}}`.replaceAll("=>", ":");
      customOptions = JSON.parse(data);
    } catch (e) {
      console.info("Invalid custom photo previewer options! " + e.message);
    }

    // Define object and gallery options
    const options = Object.assign(
      {
        gallery: "section.main",
        children: "a.photo-swipe",
        photo_scale: 2,
        // dynamic import is not supported in UMD version
        pswpModule: PhotoSwipe,
      },
      customOptions
    );

    const galleryEl = document.querySelector(options.gallery);
    if (!galleryEl) {
      return;
    }

    const imgEls = [];
    const els = galleryEl.querySelectorAll("img:not(.emoji)");
    els.forEach((el) => {
      if (el.src.trim() == "") {
        return;
      }
      if (!imgEls.includes(el)) {
        imgEls.push(el);
      }
    });

    if (imgEls.length === 0) {
      return;
    }

    imgEls.forEach((imgEl) => {
      imgEl.outerHTML = `
      <a class="photo-swipe"
        href="${imgEl.src}"
        data-pswp-width="${
          Math.max(imgEl.naturalWidth, imgEl.width) * options.photo_scale
        }"
        data-pswp-height="${
          Math.max(imgEl.naturalHeight, imgEl.height) * options.photo_scale
        }"
        data-pswp-caption="${imgEl.getAttribute("caption") || imgEl.alt}"
        target="_blank">
        ${imgEl.outerHTML}
      </a>`;
    });

    // Initialize PhotoSwipe 5
    var lightbox = new PhotoSwipeLightbox(options);

    lightbox.init();
  }

  window.addEventListener("load", initPhotoSwipe);
</script>
<meta name="google-site-verification" content="u_DTTPcCZ806iq51zgirHyWq3556HUKGq8AQfH91iFI">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-P70KSB88WH"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-P70KSB88WH');
  </script>

<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>





































































































































<header class="site-header site-header-transparent" role="banner">

  <div class="wrapper">
    <div class="site-header-inner">
<span class="site-brand"><a class="site-brand-inner" rel="author" href="/paper_notes/">
  <img class="site-favicon" title="ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ" src="" onerror="this.style.display='none'">
  ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ
</a>
</span><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger">
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewbox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
              </svg>
            </span>
          </label>

          <div class="trigger">









<span class="page-link">



<div id="google_translate_element" style="display: none;">
</div>

<span class="ct-language">
  <ul class="list-unstyled ct-language-dropdown">
    
      <li>
        <a href="#" class="lang-select" data-lang="en">
          
          <img src="https://cdn.countryflags.com/thumbs/united-states-of-america/flag-400.png" title="English">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="fr">
          
          <img src="https://cdn.countryflags.com/thumbs/france/flag-400.png" title="French">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="zh-CN">
          
          <img src="https://cdn.countryflags.com/thumbs/china/flag-400.png" title="Chinese(Simple)">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ja">
          
          <img src="https://cdn.countryflags.com/thumbs/japan/flag-400.png" title="Japanese">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ko">
          
          <img src="https://cdn.countryflags.com/thumbs/south-korea/flag-400.png" title="Korean">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ru">
          
          <img src="https://cdn.countryflags.com/thumbs/russia/flag-400.png" title="Russian">
          
        </a>
      </li>
    
  </ul>
</span>

<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({
    pageLanguage: 'ja',
    autoDisplay: false,
    layout: google.translate.TranslateElement.InlineLayout.VERTICAL
  }, 'google_translate_element');

  // Links to cross-origin destinations are unsafe
  var gll = document.getElementsByClassName('goog-logo-link')[0];
  if (gll) {
    gll.setAttribute('rel', 'noopener');
  }

  function restoreLang() {
    var iframe = document.getElementsByClassName('goog-te-banner-frame')[0];
    if (!iframe) return;

    var innerDoc = iframe.contentDocument || iframe.contentWindow.document;
    var restore_el = innerDoc.getElementsByTagName("button");

    for (var i = 0; i < restore_el.length; i++) {
      if (restore_el[i].id.indexOf("restore") >= 0) {
        restore_el[i].click();
        var close_el = innerDoc.getElementsByClassName("goog-close-link");
        close_el[0].click();
        return;
      }
    }
  }

  function triggerHtmlEvent(element, eventName) {
    var event;
    if (document.createEvent) {
      event = document.createEvent('HTMLEvents');
      event.initEvent(eventName, true, true);
      element.dispatchEvent(event);
    } else {
      event = document.createEventObject();
      event.eventType = eventName;
      element.fireEvent('on' + event.eventType, event);
    }
  }

  var googleCombo = document.querySelector("select.goog-te-combo");
  var langSelect = document.querySelector('.ct-language');
  langSelect.addEventListener('click', function(event) {
    if (!event.target) {
      return;
    }

    var selected = document.querySelector('.ct-language .ct-language-selected');
    if (selected) {
      selected.classList.remove('ct-language-selected');
    }

    var target = event.target;
    while (target && target !== langSelect ) {
      if (target.matches('.lang-select')) {
        break;
      }
      target = target.parentElement;
    }

    if (target && target.matches('.lang-select')) {
      var lang = target.getAttribute('data-lang');
      if (googleCombo.value == lang) {
        restoreLang();
      } else {
        target.parentElement.classList.add('ct-language-selected');
        googleCombo.value = lang;
        triggerHtmlEvent(googleCombo, 'change');
      }
    }

    event.preventDefault();
  });
}
</script>

<script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit" async></script>
</span>
</div>
        </nav>
</div>
  </div>
</header>

<script>
  function initHeader() {
    var lastScrollY = getScrollPos().y;
    var documentElement = document.documentElement;

    function storeScrollData() {
      var y = getScrollPos().y;documentElement.setAttribute("data-header-transparent", "");var scrollStatus = "";

      if (y <= 0) {
        scrollStatus = "top";
      } else if ((window.innerHeight + y) >= document.body.offsetHeight) {
        scrollStatus = "bottom";
      } else {
        var isScrollDown = (y - lastScrollY > 0) ? true : false;
        scrollStatus = isScrollDown ? "down" : "up";
      }

      lastScrollY = y;
      documentElement.setAttribute("data-scroll-status", scrollStatus);
    }

    window.addEventListener('scroll', function(e) {
      storeScrollData();
    });

    storeScrollData();
  }
  document.addEventListener('DOMContentLoaded', initHeader);
</script>


























































































































































<style>
    html .page-banner .page-banner-img > *:first-child {
      opacity: 0.4;
    }

    html[data-theme="dark"] .page-banner .page-banner-img > *:first-child {
      opacity: 0.2872;
    }
  </style>
<section class="page-banner">
    <div class="page-banner-img">
<div style="background-image: url(/paper_notes/assets/images/banner.png)"></div>
        <img class="img-placeholder" src="/paper_notes/assets/images/banner.png">
</div>
    <div class="wrapper">
      <div class="page-banner-inner">
<header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ</h1>
  <h2 class="post-subtitle">å‹‰å¼·ã—ãŸè«–æ–‡ã‚„æŠ€è¡“ç­‰ã®æƒ…å ±ã‚’Githubã®Issueã«ãƒ¡ãƒ¢ã£ã¦ã„ã‚‹ã²ã¨ã®ãƒ–ãƒ­ã‚°ã€‚
ãã‚Œãªã‚Šã«ãƒ¡ãƒ¢ã®é‡ãŒè“„ç©ã•ã‚Œã¦ããŸã®ã§ã€ä¸€åº¦æ•´ç†ã—ãŸã„ãªã¨æ€ã„ãƒ–ãƒ­ã‚°ã¯ã˜ã‚ã¦ã¿ã¾ã—ãŸï¼
è‡ªç„¶è¨€èªå‡¦ç†(NLP), æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ (RecommenderSystem), Educational Data Mining (EDM), Learning Analytics (LA)ãªã©ã®åˆ†é‡ã®ãƒ¡ãƒ¢ãŒå¤šã„ã¨æ€ã„ã¾ã™ã€‚
æœ€è¿‘ã¯ç‰¹ã«LLMã®å‹‰å¼·ãŒå¤šã‚ã§ã™ :)</h2>

  <div class="post-meta">
    <time class="dt-published" datetime="2025-11-18T00:48:11+00:00" itemprop="datePublished"><i class="fa fa-calendar"></i> Nov 18, 2025
    </time><span class="post-author left-vsplit"><i class="fa fa-pencil"></i> AkihikoWATANABE</span>
    
































    <span class="post-reading-time left-vsplit"><i class="fa fa-clock-o"></i> About 5 hours 28 mins</span>
  </div></header>
</div>
    </div>
  </section><script>
  function hashLocate(hashValue) {
    hashValue = hashValue.replace(/^.*#h-/, '');
    hashValue = decodeURIComponent(hashValue);
    var element = document.getElementById(hashValue);

    if (!element) {
      return;
    }

    var header = document.querySelector('header.site-header');
    var headerRect = header.getBoundingClientRect();
    var headerTop = Math.floor(headerRect.top);
    var headerHeight = Math.floor(headerRect.height);
    var scrollPos = getScrollPos();
    var offsetY = element.offsetTop - (headerTop + headerHeight + 20);

    if (offsetY == scrollPos.y) {
      return;
    }

    if (headerTop == 0  && offsetY > scrollPos.y) {
      offsetY += headerHeight + 2;
    } else if (headerTop < 0  && offsetY < scrollPos.y) {
      offsetY -= headerHeight - 2;
    }

    smoothScrollTo(offsetY);
  }

  // The first event occurred
  window.addEventListener('load', function(event) {
    if (window.location.hash) {
      hashLocate(window.location.hash);
    }
  });

  // The first event occurred
  window.addEventListener('click', function(event) {
    if (event.target.tagName.toLowerCase() == 'a') {
      hashLocate(event.target.getAttribute('href'));
    }
  });
</script>
<div class="theme-toggle">
  <input type="checkbox" id="theme-switch">
  <label for="theme-switch">
    <div class="toggle"></div>
    <div class="names">
      <p class="light">Light</p>
      <p class="dark">Dark</p>
    </div>
  </label>
</div>




<script>
  (function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var nightModeOption = ('off' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
      storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
      var data = storage.getItem('theme');
      try {
        data = JSON.parse(data ? data : '');
      } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
      }
      return data;
    }

    function handleThemeToggle(nightShift) {
      themeData.nightShift = nightShift;
      saveThemeData(themeData);
      html.dataset.theme = nightShift ? 'dark' : 'light';
      setTimeout(function() {
        sw.checked = nightShift ? true : false;
      }, 50);
    }

    function autoThemeToggle() {
      // Next time point of theme toggle
      var now = new Date();
      var toggleAt = new Date();
      var hours = now.getHours();
      var nightShift = hours >= 19 || hours <=7;

      if (nightShift) {
        if (hours > 7) {
          toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
      } else {
        toggleAt.setHours(19);
      }

      toggleAt.setMinutes(0);
      toggleAt.setSeconds(0);
      toggleAt.setMilliseconds(0)

      var delay = toggleAt.getTime() - now.getTime();

      // auto toggle theme mode
      setTimeout(function() {
        handleThemeToggle(!nightShift);
      }, delay);

      return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
      };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
      handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
      var data = autoThemeToggle();

      // Toggle theme by local setting
      if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
      } else {
        handleThemeToggle(themeData.nightShift);
      }
    } else if (nightModeOption == 'manual') {
      handleThemeToggle(themeData.nightShift);
    } else {
      var nightShift = themeData.nightShift;
      if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
      }
      handleThemeToggle(nightShift);
    }
  })();
</script>
<div id="click-to-top" class="click-to-top">
  <i class="fa fa-arrow-up"></i>
</div>
<script>
  (function () {
    const clickToTop = document.getElementById('click-to-top');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 100) {
        clickToTop.classList.add('show')
      }else {
        clickToTop.classList.remove('show')
      }
    });
    clickToTop.addEventListener('click', () => {
      window.smoothScrollTo(0);
    });
  })();
</script>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="framework">
  <section class="main">

     <div class="post">
  <section>









<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="post-content e-content" itemprop="articleBody">

      <h2 id="Transformer"> Transformer</h2>
<div class="visible-content">
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>


<br>


<span class="issue_date">Issue Date: 2025-11-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3699" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Virtual Width Networks, Seed+, arXiv'25, 2025.11</a>
<span class="snippet"><span>GPT Summary</span>- Virtual Width Networks (VWN)ã¯ã€éš ã‚Œå±¤ã®ã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã™ã“ã¨ãªãã€ã‚ˆã‚Šåºƒã„è¡¨ç¾ã‚’å¯èƒ½ã«ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚‹ã€‚VWNã¯ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã®è¨ˆç®—ã‚’ã»ã¼ä¸€å®šã«ä¿ã¡ãªãŒã‚‰åŸ‹ã‚è¾¼ã¿ç©ºé–“ã‚’æ‹¡å¼µã—ã€8å€ã®æ‹¡å¼µã§ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã®æœ€é©åŒ–ã‚’åŠ é€Ÿã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé€²ã‚€ã«ã¤ã‚Œã¦ã“ã®åˆ©ç‚¹ã¯å¢—å¹…ã•ã‚Œã€ä»®æƒ³å¹…ã¨æå¤±å‰Šæ¸›ã®é–“ã«ã¯å¯¾æ•°ç·šå½¢ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°é–¢ä¿‚ãŒã‚ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1990270269873864796?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/papers_anon/status/1990269413195743671?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é‡è¦è«–æ–‡ã«è¦‹ãˆã‚‹ã€‚transformerã®ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã®æ¬¡å…ƒã¯å¤‰ãˆãªã„ã§ãƒ™ã‚¯ãƒˆãƒ«ã®widthã‚’åºƒã’ã‚‹ã“ã¨ã¨åŒç­‰ã®åŠ¹åŠ›ã‚’å¾—ã‚‹ãŸã‚ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ææ¡ˆã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚<br><br>ã–ã£ãã‚Šè¨€ã†ã¨embeddingã‚’Nå€ï¼ˆover-width)ã—ã€ææ¡ˆæ‰‹æ³•ã§ã‚ã‚‹GHCã‚’ç”¨ã„ã¦ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã«æµã›ã‚‹ã‚µã‚¤ã‚ºã«ãƒ™ã‚¯ãƒˆãƒ«ã‚’åœ§ç¸®ã—transformerãƒ–ãƒ­ãƒƒã‚¯ã§å‡¦ç†ã—over-widthã—ãŸæ¬¡å…ƒã«æˆ»ã™å‡¦ç†ã‚’ã™ã‚‹æ©Ÿæ§‹ã¨ã€over-widthã—ãŸembeddingã‚’æ¬¡å…ƒæ•°ã¯å¤‰ãˆãšã«å¤‰æ›ã™ã‚‹linearã‚’å™›ã¾ã›ãŸçµæœã‚’è¶³ã—åˆã‚ã›ã‚‹ã‚ˆã†ãªæ©Ÿæ§‹ã‚’ç”¨æ„ã—ã¦æœ€å¤§ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã§ã‚ã‚‹transformerãƒ–ãƒ­ãƒƒã‚¯ã®è¨ˆç®—é‡ã¯å¤‰ãˆãšã«è¡¨ç¾åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ã€ã¨ã„ã£ãŸæ„Ÿã˜ã®æ‰‹æ³•ãªæ¨¡æ§˜<br><br><img src="https://github.com/user-attachments/assets/8c2fc967-54ff-43fa-a469-ece0cdaa0131" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/8d9a123f-747f-48f7-b04b-167e98ed2350" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NeuralArchitectureSearch.html" target="_blank" rel="noopener noreferrer">#NeuralArchitectureSearch</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<a class="button" href="articles/ObjectDetection.html" target="_blank" rel="noopener noreferrer">#ObjectDetection</a>
<a class="button" href="articles/Realtime.html" target="_blank" rel="noopener noreferrer">#Realtime</a>


<br>


<span class="issue_date">Issue Date: 2025-11-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3679" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] RF-DETR: Neural Architecture Search for Real-Time Detection Transformers, Isaac Robinson+, arXiv'25, 2025.11</a>
<span class="snippet"><span>GPT Summary</span>- RF-DETRã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ãƒœã‚­ãƒ£ãƒ–ãƒ©ãƒªæ¤œå‡ºå™¨ã®ä¸€èˆ¬åŒ–å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«å°å…¥ã•ã‚ŒãŸè»½é‡ã®å°‚é–€æ¤œå‡ºãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã§ã‚ã‚Šã€é‡ã¿å…±æœ‰ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚µãƒ¼ãƒï¼ˆNASï¼‰ã‚’ç”¨ã„ã¦ç²¾åº¦ã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è©•ä¾¡ã—ã¾ã™ã€‚RF-DETRã¯ã€COCOãŠã‚ˆã³Roboflow100-VLã§å¾“æ¥ã®æ‰‹æ³•ã‚’å¤§å¹…ã«ä¸Šå›ã‚Šã€ç‰¹ã«RF-DETRï¼ˆ2x-largeï¼‰ã¯COCOã§60 APã‚’è¶…ãˆãŸåˆã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¤œå‡ºå™¨ã§ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/skalskip92/status/1989004912609411133?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/2D%20(Image).html" target="_blank" rel="noopener noreferrer">#2D (Image)</a>
<a class="button" href="articles/WorldModels.html" target="_blank" rel="noopener noreferrer">#WorldModels</a>


<br>


<span class="issue_date">Issue Date: 2025-11-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3640" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] ChronoEdit: Towards Temporal Reasoning for Image Editing and World  Simulation, Jay Zhangjie Wu+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- ChronoEditãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€ç”»åƒç·¨é›†ã‚’å‹•ç”»ç”Ÿæˆã¨ã—ã¦å†å®šç¾©ã€‚å…¥åŠ›ç”»åƒã¨ç·¨é›†ç”»åƒã‚’å‹•ç”»ã®æœ€åˆã¨æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ ã¨ã—ã€æ™‚é–“çš„ä¸€è²«æ€§ã‚’å­¦ç¿’ã—ãŸå‹•ç”»ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã€‚æ¨è«–æ™‚ã«æ™‚é–“çš„æ¨è«–ã‚¹ãƒ†ãƒ¼ã‚¸ã‚’å°å…¥ã—ã€ç‰©ç†çš„ã«å®Ÿç¾å¯èƒ½ãªå¤‰æ›ã‚’åˆ¶ç´„ã™ã‚‹ç·¨é›†è»Œé“ã‚’ç”Ÿæˆã€‚æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯PBench-Editã§ã€ChronoEditãŒè¦–è¦šçš„å¿ å®Ÿæ€§ã¨ç‰©ç†çš„å¦¥å½“æ€§ã§æœ€å…ˆç«¯ã®æ‰‹æ³•ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>HF:


<a href="https://huggingface.co/nvidia/ChronoEdit-14B-Diffusers" target="_blank" rel="noopener noreferrer">https://huggingface.co/nvidia/ChronoEdit-14B-Diffusers</a>


<br><br>LoRAã«ã‚ˆã‚‹Upscaler:


<a href="https://huggingface.co/nvidia/ChronoEdit-14B-Diffusers-Upscaler-Lora" target="_blank" rel="noopener noreferrer">https://huggingface.co/nvidia/ChronoEdit-14B-Diffusers-Upscaler-Lora</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_akhaliq/status/1988107523992547475?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã‚¹ã‚±ãƒƒãƒ+promptã§ã®ç·¨é›†<br>HF:


<a href="https://huggingface.co/nvidia/ChronoEdit-14B-Diffusers-Paint-Brush-Lora" target="_blank" rel="noopener noreferrer">https://huggingface.co/nvidia/ChronoEdit-14B-Diffusers-Paint-Brush-Lora</a>


<br><br>å…ƒãƒã‚¹ãƒˆ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_akhaliq/status/1990251474258100341?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
</div>
<p><button onclick="showMore(0)">more</button></p>
<div class="hidden-content">
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ImageSegmentation.html" target="_blank" rel="noopener noreferrer">#ImageSegmentation</a>
<a class="button" href="articles/Prompting.html" target="_blank" rel="noopener noreferrer">#Prompting</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<a class="button" href="articles/2D%20(Image).html" target="_blank" rel="noopener noreferrer">#2D (Image)</a>
<a class="button" href="articles/4D%20(Video).html" target="_blank" rel="noopener noreferrer">#4D (Video)</a>
<span class="issue_date">Issue Date: 2025-11-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3632" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] SAM 2: Segment Anything in Images and Videos, Nikhila Ravi+, ICLR'25, 2024.08</a>
<span class="snippet"><span>GPT Summary</span>- Segment Anything Model 2ï¼ˆSAM 2ï¼‰ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯èƒ½ãªè¦–è¦šã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’é€šã˜ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æ”¹å–„ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚¸ãƒ³ã‚’æ§‹ç¯‰ã—ã€æœ€å¤§ã®å‹•ç”»ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’åé›†ã€‚ã‚·ãƒ³ãƒ—ãƒ«ãªãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç”¨ã„ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‹•ç”»å‡¦ç†ã«å¯¾å¿œã€‚SAM 2ã¯ã€å‹•ç”»ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã§å¾“æ¥ã®æ‰‹æ³•ã‚ˆã‚Š3å€å°‘ãªã„ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã§é«˜ç²¾åº¦ã‚’é”æˆã—ã€ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã§ã‚‚å¾“æ¥ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šç²¾åº¦ãŒé«˜ãã€6å€é€Ÿã„ã€‚ãƒ‡ãƒ¼ã‚¿ã€ãƒ¢ãƒ‡ãƒ«ã€ã‚³ãƒ¼ãƒ‰ã€ãƒ‡ãƒ¢ã‚’å…¬é–‹ã—ã€é–¢é€£ã‚¿ã‚¹ã‚¯ã®é‡è¦ãªãƒã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ã‚’ç›®æŒ‡ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=Ha6RTeWMd0" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=Ha6RTeWMd0</a>


</p>
<p>SAMã¯ã“ã¡ã‚‰:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1885" target="_blank" rel="noopener noreferrer">Segment Anything, Alexander Kirillov+, arXiv'23</a>
</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/AAAI.html" target="_blank" rel="noopener noreferrer">#AAAI</a>
<a class="button" href="articles/LearningPhenomena.html" target="_blank" rel="noopener noreferrer">#LearningPhenomena</a>
<span class="issue_date">Issue Date: 2025-11-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3630" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] The Strong Lottery Ticket Hypothesis for Multi-Head Attention Mechanisms, Hikari Otsuka+, arXiv'25, 2025.11</a>
<span class="snippet"><span>GPT Summary</span>- å¼·ã„ãƒ­ãƒƒã‚¿ãƒªãƒ¼ãƒã‚±ãƒƒãƒˆä»®èª¬ï¼ˆSLTHï¼‰ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å†…ã«é«˜æ€§èƒ½ãªã‚µãƒ–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ãŒã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ãŠã‘ã‚‹ç†è§£ã¯ä¸è¶³ã—ã¦ã„ã¾ã™ã€‚æœ¬ç ”ç©¶ã§ã¯ã€ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆMHAï¼‰å†…ã®å¼·ã„ãƒ­ãƒƒã‚¿ãƒªãƒ¼ãƒã‚±ãƒƒãƒˆã®å­˜åœ¨ã‚’ç†è«–çš„ã«åˆ†æã—ã€ç‰¹å®šã®æ¡ä»¶ä¸‹ã§ä»»æ„ã®MHAã‚’é«˜ã„ç¢ºç‡ã§è¿‘ä¼¼ã™ã‚‹ã‚µãƒ–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’è¨¼æ˜ã—ã¾ã™ã€‚ã¾ãŸã€ã“ã®ç†è«–ã‚’ç”¨ã„ã¦æ­£è¦åŒ–å±¤ã®ãªã„ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã«SLTHã‚’æ‹¡å¼µã—ã€è¿‘ä¼¼èª¤å·®ãŒéš ã‚Œæ¬¡å…ƒã®å¢—åŠ ã«ä¼´ã„æŒ‡æ•°é–¢æ•°çš„ã«æ¸›å°‘ã™ã‚‹ã“ã¨ã‚’å®Ÿè¨¼çš„ã«ç¤ºã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/oh_thinkingtime/status/1987107370888303028?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/LatentReasoning.html" target="_blank" rel="noopener noreferrer">#LatentReasoning</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<a class="button" href="articles/RecurrentModels.html" target="_blank" rel="noopener noreferrer">#RecurrentModels</a>
<a class="button" href="articles/RecursiveModels.html" target="_blank" rel="noopener noreferrer">#RecursiveModels</a>
<span class="issue_date">Issue Date: 2025-10-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3514" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Scaling Latent Reasoning via Looped Language Models, Rui-Jie Zhu+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- Ouroã¯ã€æ¨è«–ã‚’äº‹å‰è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚ºã«çµ„ã¿è¾¼ã‚€ã“ã¨ã‚’ç›®æŒ‡ã—ãŸãƒ«ãƒ¼ãƒ—è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLoopLMï¼‰ã§ã‚ã‚Šã€åå¾©è¨ˆç®—ã‚„ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ã‚’é€šã˜ã¦æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚1.4BãŠã‚ˆã³2.6Bãƒ¢ãƒ‡ãƒ«ã¯ã€æœ€å¤§12Bã®æœ€å…ˆç«¯LLMã«åŒ¹æ•µã™ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€çŸ¥è­˜æ“ä½œèƒ½åŠ›ã®å‘ä¸ŠãŒãã®è¦å› ã§ã‚ã‚‹ã“ã¨ã‚’å®Ÿé¨“ã§ç¢ºèªã€‚LoopLMã¯æ˜ç¤ºçš„ãªCoTã‚ˆã‚Šã‚‚æ•´åˆã—ãŸæ¨è«–ã‚’ç”Ÿæˆã—ã€æ¨è«–ã®æ–°ãŸãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®å¯èƒ½æ€§ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚ãƒ¢ãƒ‡ãƒ«ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§æä¾›ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://ouro-llm.github.io" target="_blank" rel="noopener noreferrer">https://ouro-llm.github.io</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/ziniuli/status/1983765674699915767?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/scaling01/status/1984286236438094307?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>åŸºæœ¬æ§‹é€ ã¯decoder-only transformerã§<br>- Multi-Head Attention<br>- RoPE<br>- SwiGLUæ´»æ€§åŒ–<br>- Sandwich Normalization<br>ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹LoopedTransformerã§ã€exit gateã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã§æ—©æœŸã«loopã‚’æ‰“ã¡åˆ‡ã‚Šã€å‡ºåŠ›ã‚’ã™ã‚‹ã“ã¨ã§ã‚³ã‚¹ãƒˆã‚’ç¯€ç´„ã§ãã‚‹ã‚ˆã†ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ãªã£ã¦ã„ã‚‹ã€‚<br><br>ã‚ˆã‚Šå°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã§ã€ã‚ˆã‚Šå¤§ããªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚é«˜ã„æ€§èƒ½ã‚’ç¤ºã™ï¼ˆTable7,8ï¼‰ã€‚ã¾ãŸã€Tã‚’å¢—ã‚„ã™ã¨ãƒ¢ãƒ‡ãƒ«ã®å®‰å…¨æ€§ã‚‚å¢—ã™ï¼ˆï¼æœ‰å®³ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è­˜åˆ¥åŠ›ãŒå¢—ã™ï¼‰ã€‚ãã®ä»£ã‚ã‚Šã€å†å¸°æ•°Tã‚’å¤§ããã™ã‚‹ã¨FLOPsãŒTå€ã«ãªã‚‹ã®ã§ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¯è‰¯ã„ãŒè¨ˆç®—åŠ¹ç‡ã¯æ‚ªã„ã€‚<br><br>linear probingã§å†å¸°ã®æ¬¡ã‚¹ãƒ†ãƒƒãƒ—äºˆæ¸¬ã‚’ã—ãŸã¨ã“ã‚æµ…ã„æ®µéšã§ã¯äºˆæ¸¬ãŒä¸ä¸€è‡´ã«ãªã‚‹ãŸã‚ã€æ€è€ƒãŒé€²åŒ–ã—ã¦ã„ã£ã¦ã„ã‚‹ã®ã§ã¯ãªã„ã‹ã€ã¨ã„ã†è€ƒå¯ŸãŒã‚ã‚‹ã€‚<br><br>ã¾ãŸã€å†å¸°æ•°Tã‚’4ã§å­¦ç¿’ã—ãŸå ´åˆã«ã€inferenceæ™‚ã«Tã‚’5--8ã«ã—ã¦ã‚‚ã‚¹ã‚±ãƒ¼ãƒ«ã—ãªã„(Table10)ã€‚<br><br>ã¾ãŸAppendix D.1ã«ãŠã„ã¦ã€é€šå¸¸ã®transformerã®LoopLMã‚’æ¯”è¼ƒã—ã€5ç¨®é¡ã®å¤§ãã•ã®ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã§æ¯”è¼ƒã€‚é€šå¸¸ã®transformerã§ã¯ãƒ«ãƒ¼ãƒ—ã•ã›ã‚‹ä»£ã‚ã‚Šã«å®Ÿéš›ã«å±¤ã®æ•°ã‚’å¢—ã‚„ã™ã“ã¨ã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’æƒãˆã¦å®Ÿé¨“ã—ãŸã¨ã“ã‚ã€é€šå¸¸ã®transformerã®æ–¹ãŒå¸¸ã«æ€§èƒ½ãŒè‰¯ãã€loopLMã¯å†å¸°æ•°ã‚’å¢—ã‚„ã—ã¦ã‚‚ã‚¹ã‚±ãƒ¼ãƒ«ã›ãšã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹ã«ã¤ã‚Œã¦å·®ãŒãªããªã£ã¦ã„ãã€ã¨ã„ã†ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®é¢ã§ã¯æ®‹å¿µãªçµæœã«çµ‚ã‚ã£ã¦ã„ã‚‹ã‚ˆã†ã ã€‚<br><br>ã¨ã„ã£ãŸè©±ãŒè§£èª¬ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹ã€‚å…ƒè«–æ–‡ã¯å®Œå…¨ã«skim readingã—ã¦è§£èª¬ãƒã‚¹ãƒˆã‚’ä¸»ã«èª­ã‚“ã ã®ã§èª¤ã‚ŠãŒå«ã¾ã‚Œã‚‹ã‹ã‚‚ã—ã‚Œãªã„ç‚¹ã«ã¯æ³¨æ„ã€‚</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/NormalizingFlow.html" target="_blank" rel="noopener noreferrer">#NormalizingFlow</a>
<a class="button" href="articles/Compression.html" target="_blank" rel="noopener noreferrer">#Compression</a>
<span class="issue_date">Issue Date: 2025-10-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3488" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] FARMER: Flow AutoRegressive Transformer over Pixels, Guangting Zheng+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- FARMERã¨ã„ã†æ–°ã—ã„ç”Ÿæˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€æ­£è¦åŒ–ãƒ•ãƒ­ãƒ¼ã¨è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’çµ±åˆã—ã¦é«˜å“è³ªãªç”»åƒåˆæˆã¨å°¤åº¦æ¨å®šã‚’å®Ÿç¾ã€‚æ½œåœ¨ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¸ã®å¤‰æ›ã‚„è‡ªå·±æ•™å¸«ã‚ã‚Šæ¬¡å…ƒå‰Šæ¸›ã«ã‚ˆã‚Šã€ARãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®åŠ¹ç‡ã‚’å‘ä¸Šã€‚æ¨è«–é€Ÿåº¦ã‚’åŠ é€Ÿã™ã‚‹è’¸ç•™ã‚¹ã‚­ãƒ¼ãƒ ã¨ç”»åƒç”Ÿæˆå“è³ªã‚’å‘ä¸Šã•ã›ã‚‹åˆ†é¡å™¨ãƒ•ãƒªãƒ¼ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚’å°å…¥ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€FARMERã¯æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦ç«¶äº‰åŠ›ã®ã‚ã‚‹æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1983015650139222334?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1983034143580795131?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã“ã‚Œã¯...ğŸ‘€ğŸ‘€ğŸ‘€</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/Normalization.html" target="_blank" rel="noopener noreferrer">#Normalization</a>
<span class="issue_date">Issue Date: 2025-10-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3485" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] SeeDNorm: Self-Rescaled Dynamic Normalization, Wenrui Cai+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- SeeDNormã¯ã€å…¥åŠ›ã«åŸºã¥ã„ã¦å‹•çš„ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°ã‚’èª¿æ•´ã™ã‚‹æ–°ã—ã„æ­£è¦åŒ–å±¤ã§ã‚ã‚Šã€RMSNormã®é™ç•Œã‚’å…‹æœã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å…¥åŠ›ã®ãƒãƒ«ãƒ æƒ…å ±ã‚’ä¿æŒã—ã€ãƒ‡ãƒ¼ã‚¿ä¾å­˜ã®è‡ªå·±å†ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å®Ÿç¾ã€‚å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚„ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã§ã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼ã—ã€å¾“æ¥ã®æ­£è¦åŒ–æ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ã¾ã—ãŸã€‚</span>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<a class="button" href="articles/3D%20Reconstruction.html" target="_blank" rel="noopener noreferrer">#3D Reconstruction</a>
<a class="button" href="articles/3D%20(Scene).html" target="_blank" rel="noopener noreferrer">#3D (Scene)</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<a class="button" href="articles/SpatialUnderstanding.html" target="_blank" rel="noopener noreferrer">#SpatialUnderstanding</a>
<span class="issue_date">Issue Date: 2025-10-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3484" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] IGGT: Instance-Grounded Geometry Transformer for Semantic 3D  Reconstruction, Hao Li+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- äººé–“ã®3Dã‚·ãƒ¼ãƒ³ç†è§£ã‚’æ¨¡å€£ã™ã‚‹ãŸã‚ã€ç©ºé–“å†æ§‹ç¯‰ã¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç†è§£ã‚’çµ±åˆã—ãŸInstanceGrounded Geometry Transformerï¼ˆIGGTï¼‰ã‚’ææ¡ˆã€‚IGGTã¯2Dè¦–è¦šå…¥åŠ›ã‚’ç”¨ã„ã¦å¹¾ä½•å­¦çš„æ§‹é€ ã¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’çµ±ä¸€çš„ã«è¡¨ç¾ã—ã€3Dã‚·ãƒ¼ãƒ³ã®ä¸€è²«æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚æ–°ãŸã«æ§‹ç¯‰ã—ãŸInsScene-15Kãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ã€3Dä¸€è²«æ€§ã®ã‚ã‚‹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ¬ãƒ™ãƒ«ã®ãƒã‚¹ã‚¯æ³¨é‡ˆã‚’æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://lifuguan.github.io/IGGT_official/" target="_blank" rel="noopener noreferrer">https://lifuguan.github.io/IGGT_official/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/10027lifuguan/status/1983104396503527512?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1984790549615034788?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-10-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3441" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Positional Encoding Field, Yunpeng Bai+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- Diffusion Transformersï¼ˆDiTsï¼‰ã¯ã€è¦–è¦šç”Ÿæˆã«ãŠã„ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚ã‚Šã€ãƒ‘ãƒƒãƒãƒˆãƒ¼ã‚¯ãƒ³ã¨ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆPEï¼‰ã‚’ç”¨ã„ã¦ã„ã¾ã™ã€‚æœ¬ç ”ç©¶ã§ã¯ã€DiTsãŒã©ã®ã‚ˆã†ã«è¦–è¦šã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æ•´ç†ã™ã‚‹ã‹ã‚’å†è€ƒã—ã€PEã®æ‘‚å‹•ã«å¯¾ã—ã¦ã‚‚ä¸€è²«ã—ãŸå‡ºåŠ›ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚ã“ã‚Œã«åŸºã¥ãã€ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’3Dãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã«æ‹¡å¼µã—ãŸPE-Fieldã‚’ææ¡ˆã—ã€ãƒœãƒªãƒ¥ãƒ¡ãƒˆãƒªãƒƒã‚¯æ¨è«–ã¨éšå±¤çš„ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’çµ„ã¿è¾¼ã¿ã¾ã—ãŸã€‚å¼·åŒ–ã•ã‚ŒãŸDiTã¯ã€æ–°ã—ã„è¦–ç‚¹åˆæˆã¨ç©ºé–“ç”»åƒç·¨é›†ã«ãŠã„ã¦æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’é”æˆã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://yunpeng1998.github.io/PE-Field-HomePage/" target="_blank" rel="noopener noreferrer">https://yunpeng1998.github.io/PE-Field-HomePage/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/byp215bai/status/1981809736535208219?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<span class="issue_date">Issue Date: 2025-10-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3414" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] When Do Transformers Learn Heuristics for Graph Connectivity?, Qilin Ye+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- Transformersã¯ä¸€èˆ¬åŒ–èƒ½åŠ›ã«æ¬ ã‘ã€è„†å¼±ãªãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã«ä¾å­˜ã™ã‚‹ã“ã¨ãŒå¤šã„ã€‚åˆ†é›¢å‹Transformerã‚’ç”¨ã„ã¦ã€$L$å±¤ã®ãƒ¢ãƒ‡ãƒ«ãŒç›´å¾„$3^L$ã¾ã§ã®ã‚°ãƒ©ãƒ•ã‚’è§£æ±ºã§ãã‚‹ã“ã¨ã‚’è¨¼æ˜ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’åˆ†æã—ã€èƒ½åŠ›å†…ã®ã‚°ãƒ©ãƒ•ã§ã¯æ­£ã—ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å­¦ç¿’ã—ã€èƒ½åŠ›ã‚’è¶…ãˆãŸã‚°ãƒ©ãƒ•ã§ã¯å˜ç´”ãªãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’èƒ½åŠ›å†…ã«åˆ¶é™ã™ã‚‹ã“ã¨ã§ã€æ­£ç¢ºãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å­¦ç¿’ãŒä¿ƒé€²ã•ã‚Œã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/deqingfu/status/1981170866886148333?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/Hybrid.html" target="_blank" rel="noopener noreferrer">#Hybrid</a>
<span class="issue_date">Issue Date: 2025-10-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3413" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Every Attention Matters: An Efficient Hybrid Architecture for  Long-Context Reasoning, Ling Team+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- Ring-linearãƒ¢ãƒ‡ãƒ«ã‚·ãƒªãƒ¼ã‚ºã€ç‰¹ã«Ring-mini-linear-2.0ï¼ˆ16Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã¨Ring-flash-linear-2.0ï¼ˆ104Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã‚’ç´¹ä»‹ã€‚ä¸¡ãƒ¢ãƒ‡ãƒ«ã¯ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ¡ç”¨ã—ã€é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®æ¨è«–ã§I/Oã¨è¨ˆç®—ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å‰Šæ¸›ã€‚æ¨è«–ã‚³ã‚¹ãƒˆã¯32å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å¯†ãªãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦1/10ã€å…ƒã®Ringã‚·ãƒªãƒ¼ã‚ºã¨æ¯”ã¹ã¦50%ä»¥ä¸Šå‰Šæ¸›ã€‚æœ€é©ãªãƒ¢ãƒ‡ãƒ«æ§‹é€ ã‚’ç‰¹å®šã—ã€é«˜æ€§èƒ½FP8ã‚ªãƒšãƒ¬ãƒ¼ã‚¿ãƒ¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€Œlingheã€ã«ã‚ˆã‚Šãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ãŒ50%å‘ä¸Šã€‚è¤‡æ•°ã®è¤‡é›‘æ¨è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§SOTAãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã€‚</span>
<span class="snippet"><span>Comment</span><p>HF:


<a href="https://huggingface.co/inclusionAI/Ring-flash-linear-2.0-128k" target="_blank" rel="noopener noreferrer">https://huggingface.co/inclusionAI/Ring-flash-linear-2.0-128k</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1981442875192987882?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/grad62304977/status/1981571978382500203?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/memory.html" target="_blank" rel="noopener noreferrer">#memory</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-10-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3393" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Memory Layers at Scale, Vincent-Pierre Berges+, ICLR'25, 2024.12</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ¡ãƒ¢ãƒªå±¤ã¯ã€è¨ˆç®—è² è·ã‚’å¢—ã‚„ã•ãšã«ãƒ¢ãƒ‡ãƒ«ã«è¿½åŠ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åŠ ãˆã‚‹ãŸã‚ã®å­¦ç¿’å¯èƒ½ãªæ¤œç´¢ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ä½¿ç”¨ã—ã€ã‚¹ãƒ‘ãƒ¼ã‚¹ã«æ´»æ€§åŒ–ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªå±¤ãŒå¯†ãªãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã‚’è£œå®Œã—ã¾ã™ã€‚æœ¬ç ”ç©¶ã§ã¯ã€æ”¹è‰¯ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªå±¤ã‚’ç”¨ã„ãŸè¨€èªãƒ¢ãƒ‡ãƒ«ãŒã€è¨ˆç®—äºˆç®—ãŒ2å€ã®å¯†ãªãƒ¢ãƒ‡ãƒ«ã‚„åŒç­‰ã®è¨ˆç®—ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆæ··åˆãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ã€ç‰¹ã«äº‹å®Ÿã«åŸºã¥ãã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½å‘ä¸ŠãŒé¡•è‘—ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã—ã¾ã—ãŸã€‚å®Œå…¨ã«ä¸¦åˆ—åŒ–å¯èƒ½ãªãƒ¡ãƒ¢ãƒªå±¤ã®å®Ÿè£…ã¨ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã‚’ç¤ºã—ã€1å…†ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§ã®äº‹å‰å­¦ç¿’ã‚’è¡Œã£ãŸçµæœã€æœ€å¤§8Bã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=ATqGm1WyDj" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=ATqGm1WyDj</a>


</p>
<p>transformerã«ãŠã‘ã‚‹FFNã‚’ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ãƒ¤ãƒ¼ã«ç½®ãæ›ãˆã‚‹ã“ã¨ã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å¢—ã‚„ã—ãªãŒã‚‰è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’æŠ‘ãˆã‚‹ã‚ˆã†ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã‚ˆã†ã§ã‚ã‚‹ã€‚ãƒ¡ãƒ¢ãƒªãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯ã€ã‚¯ã‚¨ãƒªqã‚’å¾—ãŸæ™‚ã«top kã®kvã‚’lookupã—ï¼ˆï¼ã“ã“ã§è¨ˆç®—å¯¾è±¡ã¨ãªã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚¹ãƒ‘ãƒ¼ã‚¹ã«ãªã‚‹ï¼‰ã€kqã‹ã‚‰æ±‚ã‚ãŸattention scoreã§vã‚’åŠ é‡å¹³å‡ã™ã‚‹ã“ã¨ã§å‡ºåŠ›ã‚’å¾—ã‚‹ã€‚Memory+ã¨ã„ã†ã•ã‚‰ãªã‚‹æ”¹è‰¯ã‚’åŠ ãˆãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã¯ã€å…¥åŠ›ã«å¯¾ã—ã¦siluã«ã‚ˆã‚‹gatingã¨linearãªå¤‰æ›ã‚’è¿½åŠ ã§å®Ÿæ–½ã™ã‚‹ã“ã¨ã§å‡ºåŠ›ã‚’å¾—ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/e935fc48-b606-47fd-aa74-047b87200779" alt="image" loading="lazy"><br><br>denseãªãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦æ€§èƒ½ãŒé«˜ãã€ãƒ¡ãƒ¢ãƒªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™ã¨æ€§èƒ½ãŒã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/b4f74591-1ecc-4871-bdf4-4cc13274e7c4" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Controllable.html" target="_blank" rel="noopener noreferrer">#Controllable</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/VariationalAutoEncoder.html" target="_blank" rel="noopener noreferrer">#VariationalAutoEncoder</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-10-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3373" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] OminiControl: Minimal and Universal Control for Diffusion Transformer, Zhenxiong Tan+, ICCV'25 Highlight, 2024.11</a>
<span class="snippet"><span>GPT Summary</span>- OminiControlã¯ã€Diffusion Transformerï¼ˆDiTï¼‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ãŠã‘ã‚‹ç”»åƒæ¡ä»¶ä»˜ã‘ã®æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’æœ€å°é™ã«æŠ‘ãˆã¤ã¤ã€æŸ”è»Ÿãªãƒˆãƒ¼ã‚¯ãƒ³ç›¸äº’ä½œç”¨ã¨å‹•çš„ãªä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Ÿç¾ã€‚åºƒç¯„ãªå®Ÿé¨“ã«ã‚ˆã‚Šã€è¤‡æ•°ã®æ¡ä»¶ä»˜ã‘ã‚¿ã‚¹ã‚¯ã§å°‚é–€çš„æ‰‹æ³•ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€åˆæˆã•ã‚ŒãŸç”»åƒãƒšã‚¢ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ŒSubjects200Kã€ã‚’å°å…¥ã€‚åŠ¹ç‡çš„ã§å¤šæ§˜ãªç”»åƒç”Ÿæˆã‚·ã‚¹ãƒ†ãƒ ã®å¯èƒ½æ€§ã‚’ç¤ºå”†ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/yxy2168/status/1980244155667476923?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>DiTã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ï¼ˆMMAä»¥å¤–ã¯ï¼‰å¤‰æ›´ã›ãšã«ã€Condition Image C_Iã‚’VAEã§ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãŸnoisy inputã‚’DiTã®inputã«concatã—é †ä¼æ’­ã•ã›ã‚‹ã“ã¨ã§ã€DiTã‚’unified conditioningãƒ¢ãƒ‡ãƒ«ï¼ˆï¼C_Iã®ç‰¹å¾´é‡ã‚’ä»–ã®inputã¨åŒã˜latent spaceã§å­¦ç¿’ã•ã›çµ±åˆçš„ã«æ‰±ã†ï¼‰ã¨ã—ã¦å­¦ç¿’ã™ã‚‹[^1]ã€‚<br><br>[^1]: æ—¢å­˜ç ”ç©¶ã¯åˆ¥ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‹ã‚‰ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ãŸfeatureãŒåŠ ç®—ã•ã‚Œã¦ã„ã¦ï¼ˆå¼3ï¼‰ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€éƒ¨åˆ†ã«åˆ¥é€”ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¿…è¦ã ã£ãŸã ã‘ã§ãªãã€åŠ ç®—ã¯ç©ºé–“çš„ãªå¯¾å¿œé–¢ä¿‚ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ã†ã¾ãå¯¾å‡¦ã§ããšï¼ˆfeatureã®æ¬¡å…ƒãŒç©ºé–“çš„ãªæƒ…å ±ã«å¯¾å¿œã—ã¦ã„ã‚‹ãŸã‚ï¼‰ã€conditional tokenã¨imageã®äº¤äº’ä½œç”¨ã‚’å¦¨ã’ã¦ã„ãŸã€‚<br><br>ã¾ãŸã€positional encodingã®indexã‚’conditional tokenã¨noisy image tokensã¨å…±æœ‰ã™ã‚‹ã¨ã€ç©ºé–“çš„ãªå¯¾å¿œé–¢ä¿‚ãŒå­˜åœ¨ã™ã‚‹ã‚¿ã‚¹ã‚¯ï¼ˆedge guided generationç­‰ï¼‰ã¯ã†ã¾ãã„ã£ãŸãŒã€è¢«å†™ä½“ã‚’æŒ‡å®šã™ã‚‹ç”Ÿæˆï¼ˆsubject driven generation)ã®ã‚ˆã†ãªå¯¾å¿œé–¢ä¿‚ãŒå­˜åœ¨ã—ãªã„ã‚¿ã‚¹ã‚¯ï¼ˆnon-aligned task)ã®å ´åˆã¯ã†ã¾ãã„ã‹ãªã‹ã£ãŸã€‚ã—ã‹ã—ã€non-aligned taskã®å ´åˆã¯ã€indexã«ã‚ªãƒ•ã‚»ãƒƒãƒˆã‚’åŠ ãˆã‚·ãƒ•ãƒˆã•ã›ã‚‹ï¼ˆå¼4ï¼‰ã“ã¨ã§ã€conditional text/image tokené–“ã§ç©ºé–“çš„ã«overlapã—ãªã„ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã§æ€§èƒ½ãŒå¤§å¹…ã«æ”¹å–„ã—ãŸã€‚<br><br>æ—¢å­˜ç ”ç©¶ã§ã¯ã€C_Iã®å¼·ã•ã‚’ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã™ã‚‹ãŸã‚ã«ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã—ã¦å®šæ•°ã‚’å°å…¥ã—ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸfeatureã‚’åŠ ç®—ã™ã‚‹éš›ã®å¼·ã•ã‚’èª¿æ•´ã—ã¦ã„ãŸãŒï¼ˆ3.2.3ç¯€ï¼‰ã€æœ¬æ‰‹æ³•ã§ã¯concatã‚’ã™ã‚‹ãŸã‚ã“ã®ã‚ˆã†ãªæ–¹æ³•ã¯ä½¿ãˆãªã„ã€‚ãã®ãŸã‚ã€Multi-Modal Attention(MMA)ã«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã£ã¦å¼·ã•ã‚’èª¿æ•´å¯èƒ½ãªbias matrixã‚’å°å…¥ã—ã€C_Iã¨Xã®attentionã®äº¤äº’ä½œç”¨ã®å¼·ã•ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§å¯¾å¿œã—ãŸï¼ˆå¼5,6ï¼‰ã€‚</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/VariationalAutoEncoder.html" target="_blank" rel="noopener noreferrer">#VariationalAutoEncoder</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/Decoder.html" target="_blank" rel="noopener noreferrer">#Decoder</a>
<span class="issue_date">Issue Date: 2025-10-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3372" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] The Free Transformer, FranÃ§ois Fleuret, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- ç„¡ç›£ç£ã§å­¦ç¿’ã•ã‚ŒãŸæ½œåœ¨å¤‰æ•°ã«æ¡ä»¶ä»˜ã‘ã‚‹ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼Transformerã®æ‹¡å¼µã‚’ææ¡ˆã—ã€ä¸‹æµã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ãŒå¤§å¹…ã«å‘ä¸Šã™ã‚‹ã“ã¨ã‚’å®Ÿé¨“ã§ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/mavenlin/status/1980681021483057484?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/teortaxestex/status/1982713029671505958?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Self-SupervisedLearning.html" target="_blank" rel="noopener noreferrer">#Self-SupervisedLearning</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<a class="button" href="articles/Scalability.html" target="_blank" rel="noopener noreferrer">#Scalability</a>
<span class="issue_date">Issue Date: 2025-10-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3334" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Scaling Language-Free Visual Representation Learning, David Fan+, ICCV'25, 2025.04</a>
<span class="snippet"><span>GPT Summary</span>- è¦–è¦šçš„è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼ˆSSLï¼‰ã¯ã€CLIPã«æ¯”ã¹ã¦è¦–è¦šçš„è³ªå•å¿œç­”ï¼ˆVQAï¼‰ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒåŠ£ã‚‹ãŒã€åŒã˜ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è¨“ç·´ã™ã‚‹ã“ã¨ã§ã€è¦–è¦šçš„SSLãƒ¢ãƒ‡ãƒ«ãŒCLIPãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚ã‚¹ã‚±ãƒ¼ãƒ«ãŒè‰¯ã„ã“ã¨ã‚’ç¤ºã—ãŸã€‚è¦–è¦šçš„SSLã¯ã€VQAã‚„å¾“æ¥ã®è¦–è¦šãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§CLIPãƒ¬ãƒ™ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¦–è¦šä¸­å¿ƒã®è¡¨ç¾å­¦ç¿’ã«æ–°ãŸãªæ©Ÿä¼šãŒé–‹ã‹ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://davidfan.io/webssl/" target="_blank" rel="noopener noreferrer">https://davidfan.io/webssl/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/davidjfan/status/1979994285379641487?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/FlowMatching.html" target="_blank" rel="noopener noreferrer">#FlowMatching</a>
<a class="button" href="articles/TTS.html" target="_blank" rel="noopener noreferrer">#TTS</a>
<a class="button" href="articles/LowResource.html" target="_blank" rel="noopener noreferrer">#LowResource</a>
<a class="button" href="articles/ConvolutionalModels.html" target="_blank" rel="noopener noreferrer">#ConvolutionalModels</a>
<span class="issue_date">Issue Date: 2025-10-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3308" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] DiaMoE-TTS: A Unified IPA-Based Dialect TTS Framework with  Mixture-of-Experts and Parameter-Efficient Zero-Shot Adaptation, Ziqi Chen+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- DiaMoE-TTSã¯ã€æ–¹è¨€ã®éŸ³å£°åˆæˆã®ãŸã‚ã®IPAãƒ™ãƒ¼ã‚¹ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€éŸ³å£°è¡¨ç¾ã®æ¨™æº–åŒ–ã¨æ›–æ˜§ã•ã®è§£æ±ºã‚’å›³ã‚‹ã€‚F5-TTSã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’åŸºã«ã€æ–¹è¨€ã«å¯¾å¿œã—ãŸMixture-of-Expertsã‚’å°å…¥ã—ã€åŠ¹ç‡çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é©å¿œã‚’å®Ÿç¾ã€‚ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã§ã‚ªãƒ¼ãƒ—ãƒ³ãƒ‡ãƒ¼ã‚¿é§†å‹•ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚Šã€æ•°æ™‚é–“ã®ãƒ‡ãƒ¼ã‚¿ã§æœªè¦‹ã®æ–¹è¨€ã‚„å°‚é–€çš„ãªãƒ‰ãƒ¡ã‚¤ãƒ³ã«å¯¾ã—ã¦è‡ªç„¶ã§è¡¨ç¾åŠ›è±Šã‹ãªéŸ³å£°ç”Ÿæˆã‚’é”æˆã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1979102214099734746?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<span class="issue_date">Issue Date: 2025-10-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3304" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Frequency-Dynamic Attention Modulation for Dense Prediction, Linwei Chen+, ICCV'25, 2025.07</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€Vision Transformersï¼ˆViTsï¼‰ã®å‘¨æ³¢æ•°å¿œç­”ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«ã€Frequency-Dynamic Attention Modulationï¼ˆFDAMï¼‰ã‚’ææ¡ˆã€‚FDAMã¯ã€æ³¨æ„è¡Œåˆ—ã®ãƒ­ãƒ¼ãƒ‘ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ã‚’åè»¢ã•ã›ã‚‹Attention Inversionï¼ˆAttInvï¼‰ã¨ã€ç•°ãªã‚‹å‘¨æ³¢æ•°æˆåˆ†ã«é‡ã¿ä»˜ã‘ã‚’è¡Œã†Frequency Dynamic Scalingï¼ˆFreqScaleï¼‰ã‹ã‚‰æˆã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¡¨ç¾ã®å´©å£Šã‚’å›é¿ã—ã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚„ç‰©ä½“æ¤œå‡ºãªã©ã®ã‚¿ã‚¹ã‚¯ã§ä¸€è²«ã—ãŸæ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ã€‚ãƒªãƒ¢ãƒ¼ãƒˆã‚»ãƒ³ã‚·ãƒ³ã‚°æ¤œå‡ºã§ã‚‚æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1979109880830267606?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Optimizer.html" target="_blank" rel="noopener noreferrer">#Optimizer</a>
<span class="issue_date">Issue Date: 2025-10-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3276" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Cautious Weight Decay, Lizhang Chen+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- Cautious Weight Decayï¼ˆCWDï¼‰ã¯ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã«ä¾å­˜ã—ãªã„ä¿®æ­£ã§ã€æ›´æ–°ã¨ç¬¦å·ãŒä¸€è‡´ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã®ã¿ã‚¦ã‚§ã‚¤ãƒˆæ¸›è¡°ã‚’é©ç”¨ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å…ƒã®æå¤±ã‚’ä¿æŒã—ã¤ã¤ã€å±€æ‰€çš„ãªãƒ‘ãƒ¬ãƒ¼ãƒˆæœ€é©ç‚¹ã‚’æ¢ç´¢å¯èƒ½ã«ã—ã¾ã™ã€‚CWDã¯ã€æ—¢å­˜ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã«ç°¡å˜ã«é©ç”¨ã§ãã€æ–°ãŸãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¿…è¦ã¨ã›ãšã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ã‚„ImageNetåˆ†é¡ã§æå¤±ã¨ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1978320303681056889?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-10-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3260" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] How Reinforcement Learning After Next-Token Prediction Facilitates  Learning, Nikolaos Tsilivis+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã‚’å¼·åŒ–å­¦ç¿’ã§æœ€é©åŒ–ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã€‚ç‰¹ã«ã€çŸ­ã„ãŠã‚ˆã³é•·ã„ã€Œæ€è€ƒã®é€£é–ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰ã®å­¦ç¿’ã‚’é€šã˜ã¦ã€å¼·åŒ–å­¦ç¿’ãŒæ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã‚’æ”¹å–„ã™ã‚‹ã“ã¨ã‚’ç†è«–çš„ã«ç¤ºã™ã€‚é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒç¨€ãªå ´åˆã€å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚Šè‡ªå·±å›å¸°å‹ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãŒä¸€èˆ¬åŒ–ã§ãã‚‹ã“ã¨ã‚’ç¢ºèªã€‚ã•ã‚‰ã«ã€é•·ã„å¿œç­”ãŒè¨ˆç®—ã‚’å¢—åŠ ã•ã›ã‚‹ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’èª¬æ˜ã—ã€è‡ªå·±å›å¸°å‹ç·šå½¢ãƒ¢ãƒ‡ãƒ«ãŒåŠ¹ç‡çš„ã«$d$ãƒ“ãƒƒãƒˆã®å¶å¥‡ã‚’äºˆæ¸¬ã§ãã‚‹æ¡ä»¶ã‚’ç†è«–çš„ã«è¨¼æ˜ã€‚Llamaã‚·ãƒªãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚‹å®Ÿè¨¼ã‚‚è¡Œã†ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1978015079418245263?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-10-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3258" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Diffusion Transformers with Representation Autoencoders, Boyang Zheng+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€å¾“æ¥ã®VAEã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’äº‹å‰å­¦ç¿’ã•ã‚ŒãŸè¡¨ç¾ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã«ç½®ãæ›ãˆãŸRepresentation Autoencodersï¼ˆRAEï¼‰ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é«˜å“è³ªãªå†æ§‹æˆã¨è±Šã‹ãªæ½œåœ¨ç©ºé–“ã‚’å®Ÿç¾ã—ã€æ‹¡æ•£ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®æ€§èƒ½å‘ä¸Šã‚’å›³ã‚‹ã€‚RAEã¯ã€è£œåŠ©çš„ãªè¡¨ç¾æ•´åˆæå¤±ãªã—ã§æ—©ã„åæŸã‚’é”æˆã—ã€ImageNetã§å„ªã‚ŒãŸç”»åƒç”Ÿæˆçµæœã‚’ç¤ºã—ãŸã€‚RAEã¯ã€æ‹¡æ•£ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®æ–°ã—ã„ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã—ã¦ã®åˆ©ç‚¹ã‚’æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://rae-dit.github.io" target="_blank" rel="noopener noreferrer">https://rae-dit.github.io</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1978001535717216751?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>U-Netã‚’Backboneã¨ã—ãŸVAEã®ä»£ã‚ã‚Šã«ViTã«åŸºã¥ãï¼ˆdown, up- scalingç„¡ã—ã®ï¼‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šå°‘ãªã„è¨ˆç®—é‡ã§é«˜ã„æ€§èƒ½ã‚’é”æˆã—ã¾ã—ãŸã€ã¨ã„ã£ãŸè©±ã«è¦‹ãˆã‚‹ã€‚</p>
<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1977967098736549990?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1978018195953848384?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/AttentionSinks.html" target="_blank" rel="noopener noreferrer">#AttentionSinks</a>
<a class="button" href="articles/CompressionValleys.html" target="_blank" rel="noopener noreferrer">#CompressionValleys</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3194" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Attention Sinks and Compression Valleys in LLMs are Two Sides of the  Same Coin, Enrique Queipo-de-Llano+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æ³¨æ„ã®æ²ˆé™ã¨åœ§ç¸®ã®è°·ã®é–¢é€£æ€§ã‚’ç¤ºã—ã€å¤§è¦æ¨¡ãªæ´»æ€§åŒ–ãŒè¡¨ç¾ã®åœ§ç¸®ã¨ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®æ¸›å°‘ã‚’å¼•ãèµ·ã“ã™ã“ã¨ã‚’ç†è«–çš„ã«è¨¼æ˜ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ãŒä¸­é–“å±¤ã§æ¥µç«¯ãªæ´»æ€§åŒ–ã‚’ç”Ÿã‚€ã¨ã€åœ§ç¸®ã®è°·ã¨æ³¨æ„ã®æ²ˆé™ãŒåŒæ™‚ã«ç¾ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã€‚Transformerãƒ™ãƒ¼ã‚¹ã®LLMãŒãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¸‰ã¤ã®ãƒ•ã‚§ãƒ¼ã‚ºã§å‡¦ç†ã™ã‚‹ã€ŒMix-Compress-Refineã€ç†è«–ã‚’ææ¡ˆã—ã€ã‚¿ã‚¹ã‚¯ä¾å­˜ã®è¡¨ç¾ã®é•ã„ã‚’èª¬æ˜ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1976235853853909048?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Decoder.html" target="_blank" rel="noopener noreferrer">#Decoder</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3192" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Heptapod: Language Modeling on Visual Signals, Yongxin Zhu+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- Heptapodã¯ã€å› æœæ³¨æ„ã‚’ç”¨ã„ãŸç”»åƒè‡ªå‹•å›å¸°ãƒ¢ãƒ‡ãƒ«ã§ã€CFGã¸ã®ä¾å­˜ã‚’æ’é™¤ã—ã€æ„å‘³ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’é¿ã‘ã‚‹ã€‚ä¸»ãªé©æ–°ã¯ã€2Dåˆ†å¸ƒäºˆæ¸¬ã‚’è¡Œã†å› æœTransformerã§ã€ç”»åƒã®2Dç©ºé–“å…¨ä½“ã«ã‚ãŸã‚‹åˆ†å¸ƒã‚’å­¦ç¿’ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç”Ÿæˆçš„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é€šã˜ã¦ç”»åƒã®æ„å‘³ã‚’æ‰ãˆã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚‹ã€‚ImageNetç”Ÿæˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§FIDå€¤2.70ã‚’é”æˆã—ã€å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä¸Šå›ã‚‹æˆæœã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1976233895092945107?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/InformationRetrieval.html" target="_blank" rel="noopener noreferrer">#InformationRetrieval</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/Test-Time%20Scaling.html" target="_blank" rel="noopener noreferrer">#Test-Time Scaling</a>
<a class="button" href="articles/COLM.html" target="_blank" rel="noopener noreferrer">#COLM</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<span class="issue_date">Issue Date: 2025-10-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3175" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] ReasonIR: Training Retrievers for Reasoning Tasks, Rulin Shao+, COLM'25, 2025.04</a>
<span class="snippet"><span>GPT Summary</span>- ReasonIR-8Bã¯ã€ä¸€èˆ¬çš„ãªæ¨è«–ã‚¿ã‚¹ã‚¯å‘ã‘ã«ç‰¹åˆ¥ã«è¨“ç·´ã•ã‚ŒãŸåˆã®ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã§ã‚ã‚Šã€åˆæˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ç”¨ã„ã¦æŒ‘æˆ¦çš„ãªã‚¯ã‚¨ãƒªã¨ãƒãƒ¼ãƒ‰ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚’ä½œæˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€BRIGHTãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æ–°ãŸãªæœ€å…ˆç«¯æˆæœã‚’é”æˆã—ã€RAGã‚¿ã‚¹ã‚¯ã§ã‚‚ä»–ã®ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¬ã‚·ãƒ”ã¯ä¸€èˆ¬çš„ã§ã€å°†æ¥ã®LLMã¸ã®æ‹¡å¼µãŒå®¹æ˜“ã§ã‚ã‚‹ã€‚ã‚³ãƒ¼ãƒ‰ã€ãƒ‡ãƒ¼ã‚¿ã€ãƒ¢ãƒ‡ãƒ«ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rulinshao/status/1975773504307142790?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Llama3.1-8Bã‚’bidirectional encoderã«å¤‰æ›ã—ã¦post-trainingã—ã¦ã„ã‚‹ã€‚</p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3178" target="_blank" rel="noopener noreferrer">[Paper Note] Generative Representational Instruction Tuning, Niklas Muennighoff+, ICLR'25, 2024.02</a>
</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/memory.html" target="_blank" rel="noopener noreferrer">#memory</a>
<span class="issue_date">Issue Date: 2025-10-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3144" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Pretraining with hierarchical memories: separating long-tail and common  knowledge, Hadi Pouransari+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- ç¾ä»£ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ä¾å­˜ã—ã¦ã„ã‚‹ãŒã€ã™ã¹ã¦ã®ä¸–ç•ŒçŸ¥è­˜ã‚’åœ§ç¸®ã™ã‚‹ã®ã¯éç¾å®Ÿçš„ã§ã‚ã‚‹ã€‚ã“ã‚Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€ãƒ¡ãƒ¢ãƒªæ‹¡å¼µã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ææ¡ˆã—ã€å°å‹è¨€èªãƒ¢ãƒ‡ãƒ«ãŒéšå±¤çš„ãªãƒ¡ãƒ¢ãƒªãƒãƒ³ã‚¯ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ä»•çµ„ã¿ã‚’å°å…¥ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€160Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«ã«18Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¡ãƒ¢ãƒªã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€é€šå¸¸ã®ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã®æ€§èƒ½ã‚’é”æˆã€‚ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã«ãŠã‘ã‚‹ãƒ¡ãƒ¢ãƒªã®æœ€é©ãªã‚¿ã‚¤ãƒ—ã¨ã‚µã‚¤ã‚ºã‚’ç ”ç©¶ã—ã€ææ¡ˆã—ãŸãƒ¡ãƒ¢ãƒªãŒå …ç‰¢ã«æ©Ÿèƒ½ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hpouransari/status/1975203449680937171?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/Sparse.html" target="_blank" rel="noopener noreferrer">#Sparse</a>
<span class="issue_date">Issue Date: 2025-09-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3003" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via   Semantic-Aware Permutation, Shuo Yang+, NeurIPS'25 Spotlight, 2025.05</a>
<span class="snippet"><span>GPT Summary</span>- Diffusion Transformersï¼ˆDiTsï¼‰ã®å‹•ç”»ç”Ÿæˆã«ãŠã‘ã‚‹ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã€é‡è¦ãƒˆãƒ¼ã‚¯ãƒ³ã®ç‰¹å®šç²¾åº¦ã‚’æœ€å¤§åŒ–ã—è¨ˆç®—ã®ç„¡é§„ã‚’æœ€å°åŒ–ã™ã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸è¦ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯SVG2ã‚’ææ¡ˆã€‚SVG2ã¯æ„å‘³ã«åŸºã¥ããƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã¨å†é…ç½®ã‚’è¡Œã„ã€è¨ˆç®—åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€HunyuanVideoãŠã‚ˆã³Wan 2.1ã§ãã‚Œãã‚Œæœ€å¤§2.30å€ãŠã‚ˆã³1.89å€ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’é”æˆã—ã€PSNRã‚’ç¶­æŒã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/chenfeng_x/status/1971343654662046184?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>pj page:


<a href="https://svg-project.github.io/v2/" target="_blank" rel="noopener noreferrer">https://svg-project.github.io/v2/</a>


</p>
<p>Q, Kãã‚Œãã‚Œã«ã¤ã„ã¦ç‹¬ç«‹ã—ã¦kmeansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’å®Ÿæ–½ã—ã€æ„å‘³çš„ã«é¡ä¼¼ã—ãŸQ, Kã‚’ã‚¯ãƒ©ã‚¹ã‚¿åŒ–ã—ã€mapä¸Šã§æ•£ã‚‰ã°ã£ã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®é…ç½®ã‚’æ•´é “ã—ã¦è¨ˆç®—æ©Ÿä¸Šã§åŠ¹ç‡çš„ã«æ‰±ãˆã‚‹ã‚ˆã†ã«ã—ã€å„ã‚¯ãƒ©ã‚¹ã‚¿ã®centroidã‚’attention scoreã®è¨ˆç®—ã«ç”¨ã„ã¦ã‚¯ãƒ©ã‚¹ã‚¿å†…ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚¹ã‚³ã‚¢ã‚’è¿‘ä¼¼ã™ã‚‹ã“ã¨ã§è¨ˆç®—ã‚’åŠ¹ç‡åŒ–ã—ã¾ã™ã€ã¨ã„ã£ãŸè©±ãªæ¨¡æ§˜ã€‚ã¾ãŸã€ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªã‚¯ãƒ©ã‚¹ã‚¿ã¨ãã†ã§ã¯ç„¡ã„ã‚‚ã®ãŒã‚ã‚‹ã®ã§ã€på€‹ã®ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãªã‚¯ãƒ©ã‚¹ã‚¿ã‚’é¸æŠã—ã•ã‚‰ã«åŠ¹ç‡åŒ–ã‚’ã™ã‚‹æ¨¡æ§˜ã€‚<br><img src="https://github.com/user-attachments/assets/862cf5c8-5583-4f94-8b67-59177c444176" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<a class="button" href="articles/ContextEngineering.html" target="_blank" rel="noopener noreferrer">#ContextEngineering</a>
<span class="issue_date">Issue Date: 2025-09-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2995" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Massive Values in Self-Attention Modules are the Key to Contextual   Knowledge Understanding, Mingyu Jin+, ICML'25, 2025.02</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã¯æ–‡è„ˆçš„çŸ¥è­˜ã®ç†è§£ã«æˆåŠŸã—ã¦ãŠã‚Šã€ç‰¹ã«æ³¨æ„ã‚¯ã‚¨ãƒªï¼ˆQï¼‰ã¨ã‚­ãƒ¼ï¼ˆKï¼‰ã«ãŠã„ã¦é›†ä¸­ã—ãŸå¤§è¦æ¨¡ãªå€¤ãŒä¸€è²«ã—ã¦ç¾ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ã“ã‚Œã‚‰ã®å€¤ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ä¿å­˜ã•ã‚ŒãŸçŸ¥è­˜ã§ã¯ãªãã€ç¾åœ¨ã®æ–‡è„ˆã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹çŸ¥è­˜ã®è§£é‡ˆã«é‡è¦ã§ã‚ã‚‹ã€‚é‡å­åŒ–æˆ¦ç•¥ã®èª¿æŸ»ã«ã‚ˆã‚Šã€ã“ã‚Œã‚‰ã®å€¤ã‚’ç„¡è¦–ã™ã‚‹ã¨æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã€é›†ä¸­ã—ãŸå¤§è¦æ¨¡ãªå€¤ã®å‡ºç¾ãŒãƒ­ã‚¿ãƒªãƒ¼ãƒã‚¸ã‚·ãƒ§ãƒŠãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆRoPEï¼‰ã«ã‚ˆã£ã¦å¼•ãèµ·ã“ã•ã‚Œã‚‹ã“ã¨ã‚’ç™ºè¦‹ã—ãŸã€‚ã“ã‚Œã‚‰ã®çµæœã¯ã€LLMã®è¨­è¨ˆã¨æœ€é©åŒ–ã«é–¢ã™ã‚‹æ–°ãŸãªæ´å¯Ÿã‚’æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=1SMcxxQiSL&noteId=7BAXSETAwU" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=1SMcxxQiSL&noteId=7BAXSETAwU</a>


</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/VariationalAutoEncoder.html" target="_blank" rel="noopener noreferrer">#VariationalAutoEncoder</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-09-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2870" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Self Forcing: Bridging the Train-Test Gap in Autoregressive Video   Diffusion, Xun Huang+, NeurIPS'25</a>
<span class="snippet"><span>GPT Summary</span>- Self Forcingã¯ã€è‡ªå‹•å›å¸°å‹ãƒ“ãƒ‡ã‚ªæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®æ–°ã—ã„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã§ã€ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¸ãƒ£ãƒ¼ãƒã‚¤ã‚¢ã‚¹ã®å•é¡Œã«å¯¾å‡¦ã—ã¾ã™ã€‚å¾“æ¥ã®æ‰‹æ³•ãŒçœŸã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«åŸºã¥ãã®ã«å¯¾ã—ã€Self Forcingã¯è‡ªå·±ç”Ÿæˆã—ãŸå‡ºåŠ›ã«åŸºã¥ã„ã¦ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ç”Ÿæˆã—ã€å…¨ä½“ã®å“è³ªã‚’è©•ä¾¡ã™ã‚‹ãƒ›ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ãªæå¤±ã‚’ç”¨ã„ã¾ã™ã€‚è¨ˆç®—ã‚³ã‚¹ãƒˆã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒãƒ©ãƒ³ã‚¹ã‚’å–ã‚‹ãŸã‚ã«ã€å°‘æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¨ç¢ºç‡çš„å‹¾é…åˆ‡æ–­ã‚’æ¡ç”¨ã—ã€ãƒ­ãƒ¼ãƒ«ã‚¤ãƒ³ã‚°KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’å°å…¥ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ“ãƒ‡ã‚ªç”ŸæˆãŒå¯èƒ½ã§ã€éå› æœçš„æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ç”Ÿæˆå“è³ªã«åŒ¹æ•µã¾ãŸã¯ãã‚Œã‚’ä¸Šå›ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://self-forcing.github.io" target="_blank" rel="noopener noreferrer">https://self-forcing.github.io</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/xunhuang1995/status/1968797718593098087?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è‡ªå·±å›å¸°çš„ãªå‹•ç”»ç”Ÿæˆï¼ˆã‚’ã™ã‚‹ï¼‰ãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€å­¦ç¿’æ™‚ã¯ground-truchã®contextãŒåˆ©ç”¨ã—ã¦å­¦ç¿’ã•ã‚Œã‚‹ãŒã€æ¨è«–æ™‚ã¯è‡ªèº«ãŒç”Ÿæˆçµæœãã®ã‚‚ã®ã‚’contextã¨ã—ã¦åˆ©ç”¨ã™ã‚‹ãŸã‚ã€å­¦ç¿’-æ¨è«–æ™‚ã«gapãŒç”Ÿã˜ã€ï¼ˆå¾ã€…ã«èª¤å·®ãŒè“„ç©ã™ã‚‹ã“ã¨ã§ï¼‰å“è³ªãŒåŠ£åŒ–ã™ã‚‹ã¨ã„ã†å•é¡Œï¼ˆexposure biasï¼‰ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€å­¦ç¿’æ™‚ã‹ã‚‰è‡ªèº«ãŒç”Ÿæˆã—ãŸå‡ºåŠ›ã‚’contextã¨ã—ã¦ä¸ãˆã¦ç”Ÿæˆã‚’è¡Œã„ï¼ˆãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆï¼‰ã€å‹•ç”»å…¨ä½“ã«å¯¾ã—ã¦åˆ†å¸ƒã®æ•´åˆæ€§ã‚’æ¸¬ã‚‹lossã‚’å°å…¥ï¼ˆ=ãƒ•ãƒ¬ãƒ¼ãƒ å˜ä½ã®èª¤å·®ã‚’æœ€å°åŒ–ã«ã™ã‚‹ã®ã§ã¯ãªãã€å‹•ç”»å…¨ä½“ã«å¯¾ã—ã¦ï¼ˆåˆ†å¸ƒã®ï¼‰èª¤å·®ã‚’æœ€é©åŒ–ã™ã‚‹ï¼‰ã™ã‚‹ã“ã¨ã§ã€exposure biasã‚’è»½æ¸›ã™ã‚‹ã€ã¨ã„ã†è©±ãªæ¨¡æ§˜ã€‚</p>
<p>çµæœçš„ã«ã€å˜ä¸€ã®RTX4090ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ“ãƒ‡ã‚ªç”ŸæˆãŒé«˜å“è³ªã«ç”Ÿæˆå¯èƒ½ã¨ãªã£ãŸï¼ˆã‹ã‚‚ã—ã‚Œãªã„ï¼‰:<br>


<a href="https://note.com/ngc_shj/n/n505b2f7cdfe4" target="_blank" rel="noopener noreferrer">https://note.com/ngc_shj/n/n505b2f7cdfe4</a>


</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<a class="button" href="articles/4D%20(Video).html" target="_blank" rel="noopener noreferrer">#4D (Video)</a>
<span class="issue_date">Issue Date: 2025-09-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2823" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] 4DNeX: Feed-Forward 4D Generative Modeling Made Easy, Zhaoxi Chen+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- 4DNeXã¯ã€å˜ä¸€ã®ç”»åƒã‹ã‚‰å‹•çš„3Dã‚·ãƒ¼ãƒ³ã‚’ç”Ÿæˆã™ã‚‹åˆã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚Šã€äº‹å‰å­¦ç¿’ã•ã‚ŒãŸãƒ“ãƒ‡ã‚ªæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§åŠ¹ç‡çš„ãª4Dç”Ÿæˆã‚’å®Ÿç¾ã€‚å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ4DNeX-10Mã‚’æ§‹ç¯‰ã—ã€RGBã¨XYZã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’çµ±ä¸€çš„ã«ãƒ¢ãƒ‡ãƒ«åŒ–ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€4DNeXã¯æ—¢å­˜æ‰‹æ³•ã‚’ä¸Šå›ã‚‹åŠ¹ç‡æ€§ã¨ä¸€èˆ¬åŒ–èƒ½åŠ›ã‚’ç¤ºã—ã€å‹•çš„ã‚·ãƒ¼ãƒ³ã®ç”Ÿæˆçš„4Dãƒ¯ãƒ¼ãƒ«ãƒ‰ãƒ¢ãƒ‡ãƒ«ã®åŸºç›¤ã‚’æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://4dnex.github.io" target="_blank" rel="noopener noreferrer">https://4dnex.github.io</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/liuziwei7/status/1967604322591789418?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<span class="issue_date">Issue Date: 2025-09-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2817" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Forgetting Transformer: Softmax Attention with a Forget Gate, Zhixuan Lin+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- å¿˜å´ã‚²ãƒ¼ãƒˆã‚’å–ã‚Šå…¥ã‚ŒãŸãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã€ŒFoXã€ã‚’ææ¡ˆã€‚FoXã¯é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚„ä¸‹æµã‚¿ã‚¹ã‚¯ã§ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€ä½ç½®åŸ‹ã‚è¾¼ã¿ã‚’å¿…è¦ã¨ã—ãªã„ã€‚å†å¸°çš„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ã‚‚å„ªã‚ŒãŸèƒ½åŠ›ã‚’ä¿æŒã—ã€æ€§èƒ½å‘ä¸Šã®ãŸã‚ã®ã€ŒProã€ãƒ–ãƒ­ãƒƒã‚¯è¨­è¨ˆã‚’å°å…¥ã€‚ã‚³ãƒ¼ãƒ‰ã¯GitHubã§å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=q2Lnyegkr8" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=q2Lnyegkr8</a>


</p>
<p>code:


<a href="https://github.com/zhixuan-lin/forgetting-transformer" target="_blank" rel="noopener noreferrer">https://github.com/zhixuan-lin/forgetting-transformer</a>


</p>
<p>éå¸¸ã«ãŠã‚‚ã—ã‚ãã†</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Pruning.html" target="_blank" rel="noopener noreferrer">#Pruning</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-09-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2816" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Adaptive Computation Pruning for the Forgetting Transformer, Zhixuan Lin+, COLM'25</a>
<span class="snippet"><span>GPT Summary</span>- Forgeting Transformerï¼ˆFoXï¼‰ã¯ã€å¿˜å´ã‚²ãƒ¼ãƒˆã‚’ç”¨ã„ãŸã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ç‰¹å¾´ã¨ã—ã€å¾“æ¥ã®Transformerã¨æ¯”è¼ƒã—ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã€‚FoXã®ç‰¹æ€§ã‚’æ´»ã‹ã—ã€é©å¿œè¨ˆç®—ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆACPï¼‰ã‚’ææ¡ˆã—ã€è¨ˆç®—ã‚’å‹•çš„ã«ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€FLOPsã¨ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã‚’ç´„70%å‰Šæ¸›ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œæ™‚é–“ã‚’50%ã‹ã‚‰70%çŸ­ç¸®ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’10%ã‹ã‚‰40%å‘ä¸Šã•ã›ãŸã€‚æ€§èƒ½ã®åŠ£åŒ–ã¯ãªãã€é•·ã„æ–‡è„ˆé•·ã§ã¯ã•ã‚‰ãªã‚‹è¨ˆç®—ã‚³ã‚¹ãƒˆã®ç¯€ç´„ãŒå¯èƒ½ã§ã‚ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>code:


<a href="https://github.com/zhixuan-lin/forgetting-transformer" target="_blank" rel="noopener noreferrer">https://github.com/zhixuan-lin/forgetting-transformer</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/zhxlin/status/1967596994362220761?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>openreview:


<a href="https://openreview.net/forum?id=xNj14CY5S1#discussion" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=xNj14CY5S1#discussion</a>


</p>
<p>å…ˆè¡Œç ”ç©¶:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2817" target="_blank" rel="noopener noreferrer">[Paper Note] Forgetting Transformer: Softmax Attention with a Forget Gate, Zhixuan Lin+, ICLR'25</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/TTS.html" target="_blank" rel="noopener noreferrer">#TTS</a>
<span class="issue_date">Issue Date: 2025-09-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2772" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling, Neil Zeghidour+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Delayed Streams Modeling (DSM)ã¯ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãŠã‚ˆã³ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ»ãƒ„ãƒ¼ãƒ»ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å­¦ç¿’ã®ãŸã‚ã®æ–°ã—ã„æ‰‹æ³•ã§ã€å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å®Œå…¨ã«æ¶ˆè²»ã™ã‚‹ã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ–¹å¼ã¨ã¯ç•°ãªã‚Šã€å‡ºåŠ›ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’å­¦ç¿’ã™ã‚‹ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ–¹å¼ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚DSMã¯ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼å°‚ç”¨ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ã€æ™‚é–“çš„ã«æ•´åˆ—ã•ã‚ŒãŸã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã€é…å»¶ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ä»»æ„ã®å‡ºåŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°æ¨è«–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚ç‰¹ã«ã€ãƒ†ã‚­ã‚¹ãƒˆã¨éŸ³å£°ã®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã«ãŠã„ã¦ã€è‡ªå‹•éŸ³å£°èªè­˜ï¼ˆASRï¼‰ã‚„ãƒ†ã‚­ã‚¹ãƒˆãƒ»ãƒˆã‚¥ãƒ»ã‚¹ãƒ”ãƒ¼ãƒï¼ˆTTSï¼‰ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ç«¶äº‰ã§ãã‚‹ã“ã¨ãŒå®Ÿé¨“ã§ç¢ºèªã•ã‚Œã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/arankomatsuzaki/status/1965984604558700751?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultiLingual.html" target="_blank" rel="noopener noreferrer">#MultiLingual</a>
<a class="button" href="articles/Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<span class="issue_date">Issue Date: 2025-09-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2744" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] mmBERT: A Modern Multilingual Encoder with Annealed Language Learning, Marc Marone+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- mmBERTã¯ã€1800ä»¥ä¸Šã®è¨€èªã§3å…†ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦äº‹å‰å­¦ç¿’ã•ã‚ŒãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å°‚ç”¨ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ä½ãƒªã‚½ãƒ¼ã‚¹è¨€èªã‚’çŸ­ã„æ¸›è¡°ãƒ•ã‚§ãƒ¼ã‚ºã«å«ã‚ã‚‹ã“ã¨ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ãŸã€‚æ–°ã—ã„è¦ç´ ã‚’å°å…¥ã—ã€OpenAIã®o3ã‚„Googleã®Gemini 2.5 Proã¨åŒç­‰ã®åˆ†é¡æ€§èƒ½ã‚’é”æˆã€‚mmBERTã¯åˆ†é¡ãŠã‚ˆã³æ¤œç´¢ã‚¿ã‚¹ã‚¯ã§ä»¥å‰ã®ãƒ¢ãƒ‡ãƒ«ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>blog:


<a href="https://huggingface.co/blog/mmbert" target="_blank" rel="noopener noreferrer">https://huggingface.co/blog/mmbert</a>


<br>HF:


<a href="https://huggingface.co/jhu-clsp/mmBERT-checkpoints" target="_blank" rel="noopener noreferrer">https://huggingface.co/jhu-clsp/mmBERT-checkpoints</a>


</p>
<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1761" target="_blank" rel="noopener noreferrer">modernbert-ja-130m, SB Intuitions, 2025.02</a>
<br><br>ã¨æ¯”è¼ƒã—ã¦æ—¥æœ¬èªã®æ€§èƒ½ã¯ã©ã†ã‹ãªã‚</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/ruyimarone/status/1965409895584522330?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1965563064067011053?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/TMLR.html" target="_blank" rel="noopener noreferrer">#TMLR</a>
<a class="button" href="articles/Scheduler.html" target="_blank" rel="noopener noreferrer">#Scheduler</a>
<span class="issue_date">Issue Date: 2025-09-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2665" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Training Dynamics of the Cooldown Stage in Warmup-Stable-Decay Learning   Rate Scheduler, Aleksandr Dremov+, TMLR'25</a>
<span class="snippet"><span>GPT Summary</span>- WSDå­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ãƒ•ã‚§ãƒ¼ã‚ºã‚’åˆ†æã—ã€ç•°ãªã‚‹å½¢çŠ¶ãŒãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ã‚¢ã‚¹-ãƒãƒªã‚¢ãƒ³ã‚¹ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’æ˜ã‚‰ã‹ã«ã€‚æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹ãŒæœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ã‚‚ãŸã‚‰ã™ã“ã¨ã‚’ç¤ºã—ã€ç‰¹ã«$\beta_2$ã®å€¤ãŒé«˜ã„ã¨æ”¹å–„ãŒè¦‹ã‚‰ã‚Œã‚‹ã€‚æå¤±ã®ãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—ã‚’è¦–è¦šåŒ–ã—ã€ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ãƒ•ã‚§ãƒ¼ã‚ºã®æœ€é©åŒ–ã®é‡è¦æ€§ã‚’å¼·èª¿ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/haeggee/status/1962852239036293223?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/Generalization.html" target="_blank" rel="noopener noreferrer">#Generalization</a>
<a class="button" href="articles/RecurrentModels.html" target="_blank" rel="noopener noreferrer">#RecurrentModels</a>
<span class="issue_date">Issue Date: 2025-08-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2612" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Looped Transformers for Length Generalization, Ying Fan+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ«ãƒ¼ãƒ—ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€æœªè¦‹ã®é•·ã•ã®å…¥åŠ›ã«å¯¾ã™ã‚‹ç®—è¡“çš„ãŠã‚ˆã³ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ çš„ã‚¿ã‚¹ã‚¯ã®é•·ã•ä¸€èˆ¬åŒ–ãŒæ”¹å–„ã•ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚RASP-Læ“ä½œã‚’å«ã‚€æ—¢çŸ¥ã®åå¾©è§£æ³•ã«ç„¦ç‚¹ã‚’å½“ã¦ã€ææ¡ˆã™ã‚‹å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§è¨“ç·´ã—ãŸçµæœã€ã•ã¾ã–ã¾ãªã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦é«˜ã„ä¸€èˆ¬åŒ–èƒ½åŠ›ã‚’æŒã¤è§£æ³•ã‚’å­¦ç¿’ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview: 


<a href="https://openreview.net/forum?id=2edigk8yoU" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=2edigk8yoU</a>


</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/WorldModels.html" target="_blank" rel="noopener noreferrer">#WorldModels</a>
<a class="button" href="articles/Game.html" target="_blank" rel="noopener noreferrer">#Game</a>
<span class="issue_date">Issue Date: 2025-08-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2581" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive  World Model, Xianglong He+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Matrix-Game 2.0ã‚’ææ¡ˆã—ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªä¸–ç•Œãƒ¢ãƒ‡ãƒ«ãŒãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§é•·ã„ãƒ“ãƒ‡ã‚ªã‚’ç”Ÿæˆã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚ä¸»ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã¯ã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªæ¡ä»¶ã‚’å¯èƒ½ã«ã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ³¨å…¥ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç”Ÿæˆã®ãŸã‚ã®æ•°ã‚¹ãƒ†ãƒƒãƒ—ã®è’¸ç•™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€25 FPSã§é«˜å“è³ªãª1åˆ†é–“ã®ãƒ“ãƒ‡ã‚ªã‚’ç”Ÿæˆå¯èƒ½ã€‚ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¨ã‚³ãƒ¼ãƒ‰ã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1960840603224433155?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>pj page:


<a href="https://matrix-game-v2.github.io" target="_blank" rel="noopener noreferrer">https://matrix-game-v2.github.io</a>


</p>
<p>å…¬å¼:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/skywork_ai/status/1961271333003956461?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/NormalizingFlow.html" target="_blank" rel="noopener noreferrer">#NormalizingFlow</a>
<span class="issue_date">Issue Date: 2025-08-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2456" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] JetFormer: An Autoregressive Generative Model of Raw Images and Text, Michael Tschannen+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- JetFormerã¯ã€ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®å…±åŒç”Ÿæˆã‚’åŠ¹ç‡åŒ–ã™ã‚‹è‡ªå·±å›å¸°å‹ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼å°‚ç”¨ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã§ã‚ã‚Šã€åˆ¥ã€…ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«ä¾å­˜ã›ãšã€ä¸¡ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’ç†è§£ãƒ»ç”Ÿæˆå¯èƒ½ã€‚æ­£è¦åŒ–ãƒ•ãƒ­ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®ç”Ÿæˆå“è³ªã§æ—¢å­˜ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ç«¶åˆã—ã¤ã¤ã€å …ç‰¢ãªç”»åƒç†è§£èƒ½åŠ›ã‚’ç¤ºã™ã€‚JetFormerã¯é«˜å¿ å®Ÿåº¦ã®ç”»åƒç”Ÿæˆã¨å¼·åŠ›ãªå¯¾æ•°å°¤åº¦å¢ƒç•Œã‚’å®Ÿç¾ã™ã‚‹åˆã®ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=sgAp2qG86e" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=sgAp2qG86e</a>


</p>
<p>ç”»åƒã‚’normalizing flowã§ã‚½ãƒ•ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã«å¤‰æ›ã—ã€transformerã§ã‚½ãƒ•ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã•ã›ã‚‹ã‚ˆã†ã«å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒã‚’åŒã˜ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§å­¦ç¿’ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸã€ã¿ãŸã„ãªè©±ã£ã½ã„ï¼ŸãŠã‚‚ã—ã‚ãã†<br><img src="https://github.com/user-attachments/assets/d8615d39-40bc-4470-8a20-4de574ab78ff" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2025-08-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2428" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Less Is More: Training-Free Sparse Attention with Global Locality for  Efficient Reasoning, Lijie Yang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ã€ŒLessIsMoreã€ã¨ã„ã†æ–°ã—ã„ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ææ¡ˆã€‚ã“ã‚Œã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸è¦ã§ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ´»ç”¨ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³é¸æŠã‚’åŠ¹ç‡åŒ–ã€‚ç²¾åº¦ã‚’ç¶­æŒã—ã¤ã¤ã€ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é€Ÿåº¦ã‚’1.1å€å‘ä¸Šã•ã›ã€ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’2å€å‰Šæ¸›ã€‚æ—¢å­˜æ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦1.13å€ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’å®Ÿç¾ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/lijieyyang/status/1955139186530328633?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒªãƒ¼ã§1.1å€ã®ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é€Ÿåº¦ã§æ€§èƒ½ã‚‚Full Attentionã¨åŒç­‰ä»¥ä¸Šã®Sparse Attentionã‚‰ã—ã„</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/GRPO.html" target="_blank" rel="noopener noreferrer">#GRPO</a>
<a class="button" href="articles/On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<span class="issue_date">Issue Date: 2025-08-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2410" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] AR-GRPO: Training Autoregressive Image Generation Models via  Reinforcement Learning, Shihao Yuan+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- AR-GRPOã¯ã€è‡ªå·±å›å¸°ç”»åƒç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å¼·åŒ–å­¦ç¿’ã‚’çµ±åˆã—ãŸæ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã€ç”Ÿæˆç”»åƒã®å“è³ªã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«GRPOã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’é©ç”¨ã€‚ã‚¯ãƒ©ã‚¹æ¡ä»¶ãŠã‚ˆã³ãƒ†ã‚­ã‚¹ãƒˆæ¡ä»¶ã®ç”»åƒç”Ÿæˆã‚¿ã‚¹ã‚¯ã§å®Ÿé¨“ã‚’è¡Œã„ã€æ¨™æº–ã®ARãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦å“è³ªã¨äººé–“ã®å¥½ã¿ã‚’å¤§å¹…ã«æ”¹å–„ã—ãŸã€‚çµæœã¯ã€ARç”»åƒç”Ÿæˆã«ãŠã‘ã‚‹å¼·åŒ–å­¦ç¿’ã®æœ‰åŠ¹æ€§ã‚’ç¤ºã—ã€é«˜å“è³ªãªç”»åƒåˆæˆã®æ–°ãŸãªå¯èƒ½æ€§ã‚’é–‹ãã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1955234358136373421?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2456" target="_blank" rel="noopener noreferrer">[Paper Note] JetFormer: An Autoregressive Generative Model of Raw Images and Text, Michael Tschannen+, ICLR'25</a>
</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-08-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2396" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Fast and Simplex: 2-Simplicial Attention in Triton, Aurko Roy+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- 2-ã‚·ãƒ³ãƒ—ãƒªã‚·ã‚¢ãƒ«ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¼ã‚¯ãƒ³åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã€æ¨™æº–çš„ãªãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚ˆã‚Šã‚‚å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚å›ºå®šã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³äºˆç®—å†…ã§ã€æ•°å­¦ã‚„æ¨è«–ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ãƒ‰ãƒƒãƒˆç©ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ä¸Šå›ã‚‹çµæœã‚’å¾—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/scaling01/status/1954682957798715669?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/memory.html" target="_blank" rel="noopener noreferrer">#memory</a>
<a class="button" href="articles/RecurrentModels.html" target="_blank" rel="noopener noreferrer">#RecurrentModels</a>
<a class="button" href="articles/RecursiveModels.html" target="_blank" rel="noopener noreferrer">#RecursiveModels</a>
<span class="issue_date">Issue Date: 2025-07-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2241" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive   Token-Level Computation, Sangmin Bae+, NeurIPS'25</a>
<span class="snippet"><span>GPT Summary</span>- Mixture-of-Recursionsï¼ˆMoRï¼‰ã¨ã„ã†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€å†å¸°å‹ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼å†…ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å…±æœ‰ã¨é©å¿œè¨ˆç®—ã‚’åŒæ™‚ã«å®Ÿç¾ã€‚MoRã¯ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å†åˆ©ç”¨ã¨ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®å†å¸°æ·±ã•ã®å‹•çš„å‰²ã‚Šå½“ã¦ã«ã‚ˆã‚Šã€ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚135Mã‹ã‚‰1.7Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°FLOPsã‚’ç¶­æŒã—ã¤ã¤ã€å›°æƒ‘åº¦ã‚’ä½ä¸‹ã•ã›ã€å°‘æ•°ã‚·ãƒ§ãƒƒãƒˆç²¾åº¦ã‚’å‘ä¸Šã€‚MoRã¯å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®ã‚³ã‚¹ãƒˆã‚’æŠ‘ãˆã¤ã¤ã€å“è³ªå‘ä¸Šã«å¯„ä¸ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1945632764650533048?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1961593983114907806?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2605" target="_blank" rel="noopener noreferrer">[Paper Note] Universal Transformers, Mostafa Dehghani+, ICLR'19</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2612" target="_blank" rel="noopener noreferrer">[Paper Note] Looped Transformers for Length Generalization, Ying Fan+, ICLR'25</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2611" target="_blank" rel="noopener noreferrer">[Paper Note] Looped Transformers are Better at Learning Learning Algorithms, Liu Yang+, ICLR'24</a>
 </p>
<p>è‘—è€…ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/reza_byt/status/1980335836542677210?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2025-07-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2237" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] In-context denoising with one-layer transformers: connections between  attention and associative memory retrieval, Matthew Smart+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ã€Œã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ã€ã¨ã„ã†ã‚¿ã‚¹ã‚¯ã‚’é€šã˜ã¦ã€æ³¨æ„ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨å¯†ãªé€£æƒ³è¨˜æ†¶ï¼ˆDAMï¼‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é–¢ä¿‚ã‚’æ¢æ±‚ã€‚ãƒ™ã‚¤ã‚ºçš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ã¦ã€å˜å±¤ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãŒç‰¹å®šã®ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°å•é¡Œã‚’æœ€é©ã«è§£æ±ºã§ãã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚è¨“ç·´ã•ã‚ŒãŸæ³¨æ„å±¤ã¯ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã‚’é€£æƒ³è¨˜æ†¶ã¨ã—ã¦åˆ©ç”¨ã—ã€ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸€å›ã®å‹¾é…é™ä¸‹æ›´æ–°ã§å‡¦ç†ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€DAMãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ–°ãŸãªæ‹¡å¼µä¾‹ã‚’æä¾›ã—ã€é€£æƒ³è¨˜æ†¶ã¨æ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®é–¢é€£æ€§ã‚’å¼·åŒ–ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1945253873456963841?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2146" target="_blank" rel="noopener noreferrer">[Paper Note] Energy-Based Transformers are Scalable Learners and Thinkers, Alexi Gladstone+, arXiv'25</a>
</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<a class="button" href="articles/Finetuning.html" target="_blank" rel="noopener noreferrer">#Finetuning</a>
<span class="issue_date">Issue Date: 2025-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2206" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision   Transformers under Domain Shifts, Samar Khanna+, ICML'25</a>
<span class="snippet"><span>GPT Summary</span>- PEFTæŠ€è¡“ã‚’ç”¨ã„ãŸExPLoRAã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ“ã‚¸ãƒ§ãƒ³ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ï¼ˆViTï¼‰ã‚’æ–°ã—ã„ãƒ‰ãƒ¡ã‚¤ãƒ³ã«é©å¿œã•ã›ã‚‹æ‰‹æ³•ã§ã€æ•™å¸«ãªã—äº‹å‰å­¦ç¿’ã‚’é€šã˜ã¦åŠ¹ç‡çš„ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†ã€‚å®Ÿé¨“ã§ã¯ã€è¡›æ˜Ÿç”»åƒã«ãŠã„ã¦æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã—ã€å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚ˆã‚Šã‚‚å°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ç²¾åº¦ã‚’æœ€å¤§8%å‘ä¸Šã•ã›ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/samar_a_khanna/status/1944781066591748336?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã“ã‚Œã¾ã§ãƒ‰ãƒ¡ã‚¤ãƒ³é©å¿œã™ã‚‹å ´åˆã«ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿+LoRAã§Finetuningã—ã¦ã„ãŸã®ã‚’ã€ãƒ©ãƒ™ãƒ«ç„¡ã—ãƒ‡ãƒ¼ã‚¿+ç¶™ç¶šäº‹å‰å­¦ç¿’ã®æ çµ„ã¿ã§ã‚„ã‚Šã¾ã—ã‚‡ã†ã€ã¨ã„ã†è©±ã®ã‚ˆã†ã§ã‚ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/dcae10cf-6b5d-4b29-8d9a-a94227f29a11" alt="image" loading="lazy"><br><br>æ‰‹æ³•ã¯ä¸‹è¨˜ã§ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦LoRAã‚’é©ç”¨ã—ç¶™ç¶šäº‹å‰å­¦ç¿’ã™ã‚‹ã€‚ãŸã ã—ã€æœ€å¾Œå°¾ã®Layerã€ã‚ã‚‹ã„ã¯æœ€åˆã¨æœ€å¾Œå°¾ã®Layerã®ä¸¡æ–¹ã‚’unfreezeã—ã¦ã€trainableã«ã™ã‚‹ã€‚ã¾ãŸã€LoRAã¯freezeã—ãŸLayerã®Q,Vã«é©ç”¨ã—ã€ãã‚Œã‚‰ã®Layerã®normalization layerã‚‚unfreezeã™ã‚‹ã€‚æœ€çµ‚çš„ã«ã€ç¶™ç¶šäº‹å‰å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã«ãƒ˜ãƒƒãƒ‰ã‚’concatã—ã¦finetuningã™ã‚‹ã“ã¨ã§ç›®çš„ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚è©³ç´°ã¯Algorithm1ã‚’å‚ç…§ã®ã“ã¨ã€‚<br><br><img src="https://github.com/user-attachments/assets/6b7ef497-2253-46c9-bbe7-ffdd50765fa3" alt="image" loading="lazy"><br><br>åŒã˜ãƒ¢ãƒ‡ãƒ«ã§å˜ã«LoRAã‚’é©ç”¨ã—ãŸã ã‘ã®æ‰‹æ³•ã‚„ã€æ—¢å­˜æ‰‹æ³•ã‚’outperform<br><br>&lt;img width="679" height="364" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/14935879-75a4-4e4a-a176-1b1eabc4b8fd"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/14935879-75a4-4e4a-a176-1b1eabc4b8fd"&lt;/a&gt;


/&gt;</p>
<p>ç”»åƒ+ViTç³»ã®ãƒ¢ãƒ‡ãƒ«ã ã‘ã§å®Ÿé¨“ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã‚‹ãŒã€LLMã¨ã‹ã«ã‚‚å¿œç”¨å¯èƒ½ã ã¨æ€ã‚ã‚Œã‚‹ã€‚<br></p></span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<span class="issue_date">Issue Date: 2025-07-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2198" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Nonlinear transformers can perform inference-time feature learning, Nishikawa+, ICML'25</a>
<span class="snippet"><span>GPT Summary</span>- äº‹å‰å­¦ç¿’ã•ã‚ŒãŸãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã€æ¨è«–æ™‚ã«ç‰¹å¾´ã‚’å­¦ç¿’ã™ã‚‹èƒ½åŠ›ã‚’æŒã¡ã€ç‰¹ã«å˜ä¸€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹æ–‡è„ˆå†…å­¦ç¿’ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚å‹¾é…ãƒ™ãƒ¼ã‚¹ã®æœ€é©åŒ–ã«ã‚ˆã‚Šã€ç•°ãªã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹å¾´ã‚’æŠ½å‡ºã—ã€éé©å¿œçš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä¸Šå›ã‚‹çµ±è¨ˆçš„åŠ¹ç‡ã‚’ç¤ºã—ã¾ã™ã€‚ã¾ãŸã€æ¨è«–æ™‚ã®ã‚µãƒ³ãƒ—ãƒ«è¤‡é›‘æ€§ãŒç›¸é–¢çµ±è¨ˆã‚¯ã‚¨ãƒªã®ä¸‹é™ã‚’è¶…ãˆã‚‹ã“ã¨ã‚‚ç¢ºèªã•ã‚Œã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/btreetaiji/status/1944297631808991742?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2146" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Energy-Based Transformers are Scalable Learners and Thinkers, Alexi Gladstone+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ã‚¨ãƒãƒ«ã‚®ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ï¼ˆEBTsï¼‰ã‚’ç”¨ã„ã¦ã€ç„¡ç›£ç£å­¦ç¿’ã‹ã‚‰æ€è€ƒã‚’å­¦ã¶ãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã€‚EBTsã¯ã€å…¥åŠ›ã¨å€™è£œäºˆæ¸¬ã®äº’æ›æ€§ã‚’æ¤œè¨¼ã—ã€ã‚¨ãƒãƒ«ã‚®ãƒ¼æœ€å°åŒ–ã‚’é€šã˜ã¦äºˆæ¸¬ã‚’è¡Œã†ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«å¾“æ¥ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚ˆã‚Šã‚‚é«˜ã„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç‡ã‚’é”æˆã—ã€è¨€èªã‚¿ã‚¹ã‚¯ã§ã®æ€§èƒ½ã‚’29%å‘ä¸Šã•ã›ã€ç”»åƒã®ãƒã‚¤ã‚ºé™¤å»ã§ã‚‚å„ªã‚ŒãŸçµæœã‚’ç¤ºã™ã€‚EBTsã¯ä¸€èˆ¬åŒ–èƒ½åŠ›ãŒé«˜ãã€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’èƒ½åŠ›ã¨æ€è€ƒèƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã§ã‚ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1941657099567845696?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Project Page:


<a href="https://energy-based-transformers.github.io" target="_blank" rel="noopener noreferrer">https://energy-based-transformers.github.io</a>


</p>
<p>First Authorã®æ–¹ã«ã‚ˆã‚‹è§£èª¬ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/alexiglad/status/1942231878305714462?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="articles/ListWise.html" target="_blank" rel="noopener noreferrer">#ListWise</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Alignment.html" target="_blank" rel="noopener noreferrer">#Alignment</a>
<a class="button" href="articles/SequentialRecommendation.html" target="_blank" rel="noopener noreferrer">#SequentialRecommendation</a>
<span class="issue_date">Issue Date: 2025-07-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2136" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Listwise Preference Alignment Optimization for Tail Item Recommendation, Zihao Li+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- LPO4Recã¯ã€ãƒ†ãƒ¼ãƒ«ã‚¢ã‚¤ãƒ†ãƒ æ¨è–¦ã«ãŠã‘ã‚‹Preference alignmentã®èª²é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ææ¡ˆã•ã‚ŒãŸæ‰‹æ³•ã§ã€Bradley-Terryãƒ¢ãƒ‡ãƒ«ã‚’ãƒšã‚¢ãƒ¯ã‚¤ã‚ºã‹ã‚‰ãƒªã‚¹ãƒˆãƒ¯ã‚¤ã‚ºæ¯”è¼ƒã«æ‹¡å¼µã—ã€åŠ¹ç‡çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿç¾ã€‚æ˜ç¤ºçš„ãªå ±é…¬ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãªã—ã§ã€ãƒ†ãƒ¼ãƒ«ã‚¢ã‚¤ãƒ†ãƒ ã‚’å„ªå…ˆã™ã‚‹è² ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ã‚’å°å…¥ã—ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€å¤§50%å‘ä¸Šã•ã›ã€GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’17.9%å‰Šæ¸›ã€‚å®Ÿé¨“çµæœã¯3ã¤ã®å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_reachsumit/status/1941004418255933662?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>tail itemã«å¼·ã„æ‰‹æ³•ã‚‰ã—ã„ã€‚LLMã‚’ç”¨ã„ãŸGenerative Recommendationã§ã¯ãªãã€1 Blockã®Transformerã«listwiseãªpreferenceã‚’åæ˜ ã—ãŸlossã‚’é©ç”¨ã—ãŸã‚‚ã®ã£ã½ã„ã€‚</p>
<p>ä¸€è²«ã—ã¦æ€§èƒ½ã¯é«˜ãã†ã«è¦‹ãˆã‚‹ãŒã€å†ç¾æ€§ã¯ã©ã†ã ã‚ã†ã‹ã€‚<br><img src="https://github.com/user-attachments/assets/10c66c84-b421-4be1-8cd7-3d037e8cc683" alt="image" loading="lazy"></p>
<p>é–¢é€£(SASRec):<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2137" target="_blank" rel="noopener noreferrer">[Paper Note] Self-Attentive Sequential Recommendation, Wang-Cheng Kang+, ICDM'18</a>
</p>
<p>pointwise, pairwise, listwiseã®åŸºç¤ã¯ã“ã¡ã‚‰ã‚’å‚ç…§:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/187" target="_blank" rel="noopener noreferrer">ãƒ©ãƒ³ã‚­ãƒ³ã‚°å­¦ç¿’ã“ã¨ã¯ã˜ã‚, DSIRNLP#1, 2011</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/Normalization.html" target="_blank" rel="noopener noreferrer">#Normalization</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-07-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2131" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] The Curse of Depth in Large Language Models, Wenfang Sun+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€ã€Œæ·±ã•ã®å‘ªã„ã€ã¨ã„ã†ç¾è±¡ã‚’ç´¹ä»‹ã—ã€LLMã®æ·±ã„å±¤ãŒæœŸå¾…é€šã‚Šã«æ©Ÿèƒ½ã—ãªã„ç†ç”±ã‚’åˆ†æã—ã¾ã™ã€‚Pre-LNã®ä½¿ç”¨ãŒå‡ºåŠ›ã®åˆ†æ•£ã‚’å¢—åŠ ã•ã›ã€æ·±ã„å±¤ã®è²¢çŒ®ã‚’ä½ä¸‹ã•ã›ã‚‹ã“ã¨ã‚’ç‰¹å®šã€‚ã“ã‚Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«å±¤æ­£è¦åŒ–ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼ˆLNSï¼‰ã‚’ææ¡ˆã—ã€å‡ºåŠ›åˆ†æ•£ã®çˆ†ç™ºã‚’æŠ‘åˆ¶ã—ã¾ã™ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€LNSãŒLLMã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç¤ºã—ã€æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚‚åŠ¹æœãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/shiwei_liu66/status/1940377801032446428?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1795" target="_blank" rel="noopener noreferrer">Transformers without Normalization, Jiachen Zhu+, CVPR'25</a>
<br><br>ã§ã¯ãã‚‚ãã‚‚LayerNormalizationã‚’ç„¡ãã—ã¦ã„ãŸï¼ˆæ­£ç¢ºã«ã„ã†ã¨parametrize tanhã«ç½®æ›)ãŒã€ã©ã¡ã‚‰ãŒå„ªã‚Œã¦ã„ã‚‹ã®ã ã‚ã†ã‹ï¼Ÿ<br><br><img src="https://github.com/user-attachments/assets/4bc557a0-ae23-4017-9837-7744de74c12e" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/2eead45c-209d-46e4-87e7-0129a4ec5ec2" alt="image" loading="lazy"></p>
<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1332" target="_blank" rel="noopener noreferrer">Knowledge Neurons in Pretrained Transformers, Damai Dai+, N/A, ACL'22, 2022.05</a>
<br><br>ã§ã¯çŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å­˜åœ¨ãŒç¤ºå”†ã•ã‚Œã¦ãŠã‚Šã€ã“ã‚Œã¯Transformerã®å±¤ã®æ·±ã„ä½ç½®ã«å­˜åœ¨ã—ã€ã‹ã¤ç•°ãªã‚‹çŸ¥è­˜é–“ã§çŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ã‚·ã‚§ã‚¢ã•ã‚Œãªã„å‚¾å‘ã«ã‚ã£ãŸï¼ˆãŸã ã—ã“ã‚Œã¯Post-LNã®BERTã®è©±ã§æœ¬ç ”ç©¶ã¯Pre-LNã®è©±ã ãŒã€‚Post-LNã®å‹¾é…æ¶ˆå¤±å•é¡Œã‚’ç·©å’Œã—å­¦ç¿’ã‚’å®‰å®šåŒ–ã•ã›ã‚‹ç ”ç©¶ã‚‚<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2141" target="_blank" rel="noopener noreferrer">[Paper Note] On Layer Normalizations and Residual Connections in Transformers, Sho Takase+, arXiv'22</a>
 ã®ã‚ˆã†ã«å­˜åœ¨ã™ã‚‹)ã€‚ã“ã‚Œã¯ã“ã®ç ”ç©¶ãŒæ˜ã‚‰ã‹ã«ã—ãŸã“ã¨ã¨ã©ã†ã„ã†é–¢ä¿‚æ€§ãŒã‚ã‚‹ã ã‚ã†ã‹ã€‚<br><br>ã¾ãŸã€LayerNormalizationã®Scalingã«ã‚ˆã£ã¦æ·±ã„Transformerãƒ–ãƒ­ãƒƒã‚¯ã®å°é–¢æ•°ãŒå˜ä½è¡Œåˆ—ã¨ãªã‚‹ï¼ˆå­¦ç¿’ã«å¯„ä¸ã—ãªããªã‚‹ï¼‰ã“ã¨ãŒæ”¹å–„ã•ã‚ŒãŸå ´åˆã€çŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ã©ã®ã‚ˆã†ã«å¤‰åŒ–ã™ã‚‹ã ã‚ã†ã‹ï¼Ÿ<br><br>ï¼ˆä¸‹è¨˜Geminiã®å¿œç­”ã‚’è¦‹ãŸä¸Šã§ã®æ„Ÿæƒ³)ãªã‚“ã¨ãªãƒ¼ãã ã‘ã‚Œã©ã‚‚ã€ãŠãã‚‰ãçŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å±€æ‰€åŒ–ãŒè§£æ¶ˆã•ã‚Œã‚‹ã®ã‹ãªãƒ¼ã¨ã„ã†æ°—ãŒã™ã‚‹ã€‚<br><br>ã¨ãªã‚‹ã¨æ¬¡ã®ç–‘å•ã¨ã—ã¦ã¯ã€MoEã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã¯ã©ã®ã‚ˆã†ãªå½±éŸ¿ãŒã‚ã‚‹ã ã‚ã†ã‹ï¼Ÿ<br>ãã‚‚ãã‚‚çŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒå±€æ‰€åŒ–ã—ã¦ã„ã‚‹ã‹ã‚‰MoEã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ãƒ«ãƒ¼ã‚¿ã«ã‚ˆã£ã¦é–¢é€£ã™ã‚‹Expertsã®ã¿ã‚’activateã™ã‚Œã°ï¼ˆã¨ã„ã†ã‚ˆã‚Šçµæœçš„ã«ãã†ãªã‚‹ã‚ˆã†ã«å­¦ç¿’ã•ã‚Œã‚‹ï¼‰æ€§èƒ½ã‚’åŠ£åŒ–ã•ã›ãšã«è¨ˆç®—åŠ¹ç‡ã‚’ä¸Šã’ã‚‰ã‚Œã¦ã„ãŸã€ã¨ä»®å®šã™ã‚‹ã€‚ãã†ã™ã‚‹ã¨ã€çŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒå±€æ‰€åŒ–ã›ãšã«å¤šãã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã§ã‚·ã‚§ã‚¢ã•ã‚Œã‚‹ã‚ˆã†ã«ãªã‚‹ã¨ã€<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2110" target="_blank" rel="noopener noreferrer">[Paper Note] Chain-of-Experts: Unlocking the Communication Power of  Mixture-of-Experts Models, Zihan Wang+, arXiv'25</a>
 ã®ã‚ˆã†ã«ã€ã‚µãƒ–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é–“ã®æƒ…å ±ã‚’äº’ã„ã«ã‚„ã‚Šã¨ã‚Šã§ãã¾ã™ã€ã¿ãŸã„ãªä»•çµ„ã¿ãŒã‚ˆã‚ŠåŠ¹ã„ã¦ããã†ãªæ°—ãŒã™ã‚‹ã€‚<br><br>å‚è€ƒã¾ã§ã«ã€Gemini2.5-Proã«è€ƒå¯Ÿã•ã›ã¦ã¿ãŸçµæœã‚’ãƒ¡ãƒ¢ã¨ã—ã¦æ®‹ã—ã¦ãŠãï¼ˆã‚ãã¾ã§å‚è€ƒç¨‹åº¦ã«...ï¼‰<br>```<br>ã”è³ªå•ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚éå¸¸ã«èˆˆå‘³æ·±ã„ç€çœ¼ç‚¹ã§ã™ã­ã€‚ã€ŒKnowledge Neurons in Pretrained Transformersã€ã¨ã€ŒThe Curse of Depth in Large Language Modelsã€ã¯ã€ä¸€è¦‹ã™ã‚‹ã¨å…¨ãç•°ãªã‚‹ãƒ†ãƒ¼ãƒã‚’æ‰±ã£ã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã¾ã™ãŒã€**ã€ŒTransformerã®æ·±ã„å±¤ã«ãŠã‘ã‚‹æŒ¯ã‚‹èˆã„ã€**ã¨ã„ã†å…±é€šç‚¹ã§çµã³ã¤ã‘ã¦è€ƒå¯Ÿã™ã‚‹ã¨ã€éå¸¸ã«ç¤ºå”†ã«å¯Œã‚“ã é–¢ä¿‚æ€§ãŒè¦‹ãˆã¦ãã¾ã™ã€‚<br><br>ä»¥ä¸‹ã«ã€ä¸¡æ–¹ã®è«–æ–‡ã®æ¦‚è¦ã‚’è§£èª¬ã—ã€ãã®é–¢ä¿‚æ€§ã«ã¤ã„ã¦è€ƒå¯Ÿã—ã¾ã™ã€‚<br><br>1. Knowledge Neurons in Pretrained Transformers ã®æ¦‚è¦<br>ã“ã®ç ”ç©¶ã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿Transformerãƒ¢ãƒ‡ãƒ«ï¼ˆç‰¹ã«BERTãªã©ï¼‰ã®å†…éƒ¨ã§ã€ç‰¹å®šã®äº‹å®ŸçŸ¥è­˜ãŒã©ã®ã‚ˆã†ã«æ ¼ç´ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’èª¿æŸ»ã—ãŸã‚‚ã®ã§ã™ã€‚<br><br>ç™ºè¦‹: ãƒ¢ãƒ‡ãƒ«ã®ä¸­é–“å±¤ã€ç‰¹ã«**å…¨çµåˆå±¤ï¼ˆFeed-Forward Network, FFNï¼‰ã«ã€ç‰¹å®šã®çŸ¥è­˜ï¼ˆä¾‹ï¼šã€Œãƒ€ãƒ³ãƒ†ãƒ»ã‚¢ãƒªã‚®ã‚¨ãƒ¼ãƒªã¯ã‚¤ã‚¿ãƒªã‚¢ã§ç”Ÿã¾ã‚ŒãŸã€ï¼‰ã«å¼·ãåå¿œã™ã‚‹ã€ŒçŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã€**ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã—ã¾ã—ãŸã€‚<br><br>ç‰¹å¾´: ã“ã‚Œã‚‰ã®çŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®æ·±ã„å±¤ï¼ˆå¾Œæ–¹ã®å±¤ï¼‰ã«ã€ã‚ˆã‚Šå¤šãå­˜åœ¨ã™ã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã—ãŸã€‚<br><br>æ„å‘³: ã“ã‚Œã¾ã§ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã¨ã•ã‚Œã¦ããŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®å†…éƒ¨ã§ã€çŸ¥è­˜ãŒã©ã®ã‚ˆã†ã«è¡¨ç¾ãƒ»å±€åœ¨åŒ–ã—ã¦ã„ã‚‹ã‹ã«ã¤ã„ã¦ã€å…·ä½“çš„ãªæ‰‹ãŒã‹ã‚Šã‚’ä¸ãˆãŸç”»æœŸçš„ãªç ”ç©¶ã§ã™ã€‚<br><br>2. The Curse of Depth in Large Language Models ã®æ¦‚è¦<br>ã“ã®ç ”ç©¶ã¯ã€LLMã‚’ã‚ˆã‚Šæ·±ãï¼ˆå±¤ã‚’å¤šãï¼‰ã™ã‚‹ã“ã¨ã®é›£ã—ã•ã«ç„¦ç‚¹ã‚’å½“ã¦ã€ãã®åŸå› ã¨è§£æ±ºç­–ã‚’ææ¡ˆã—ãŸã‚‚ã®ã§ã™ã€‚<br><br>å•é¡Œï¼ˆæ·±ã•ã®å‘ªã„ï¼‰: Transformerã®æ¨™æº–çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆPre-LNï¼‰ã§ã¯ã€å±¤ãŒæ·±ããªã‚‹ã«ã¤ã‚Œã¦ã€LayerNormalizationï¼ˆLNï¼‰ã¸ã®å…¥åŠ›ã®åˆ†æ•£ãŒæŒ‡æ•°é–¢æ•°çš„ã«å¢—å¤§ã—ã¦ã—ã¾ã„ã¾ã™ã€‚<br><br>çµæœ:<br><br>å‡ºåŠ›ãŒå¤§ãããªã‚Šã™ãã¦å­¦ç¿’ãŒä¸å®‰å®šã«ãªã‚Šã¾ã™ã€‚<br><br>ã•ã‚‰ã«æ·±åˆ»ãªã®ã¯ã€æ·±ã„å±¤ã§ã¯ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã«é–¢ã™ã‚‹å°é–¢æ•°ï¼ˆå‹¾é…è¨ˆç®—ã«å¿…è¦ï¼‰ãŒã»ã¼å˜ä½è¡Œåˆ—ã«ãªã£ã¦ã—ã¾ã†ã“ã¨ã§ã™ã€‚ã“ã‚Œã¯ã€ãã®å±¤ãŒå…¥åŠ›ã«å¯¾ã—ã¦ã»ã¨ã‚“ã©å¤‰æ›ã‚’è¡Œã‚ãªããªã‚Šã€å­¦ç¿’ã«å¯„ä¸ã—ãªããªã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚<br><br>è§£æ±ºç­–: ã“ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã€å„å±¤ã®LayerNormalizationã‚’ãã®æ·±ã•ï¼ˆãƒ¬ã‚¤ãƒ¤ãƒ¼ç•ªå· lï¼‰ã«å¿œã˜ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã¨ã„ã†ã‚·ãƒ³ãƒ—ãƒ«ãªæ‰‹æ³•ã‚’ææ¡ˆã—ã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ·±ã„å±¤ã§ã‚‚å‹¾é…ãŒé©åˆ‡ã«ä¼æ’­ã—ã€å­¦ç¿’ãŒå®‰å®šãƒ»æ”¹å–„ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚<br><br>è€ƒå¯Ÿï¼š2ã¤ã®ç ”ç©¶ã®é–¢ä¿‚æ€§<br>ã“ã‚Œã‚‰2ã¤ã®ç ”ç©¶ã¯ã€**ã€Œå­¦ç¿’ã®å®‰å®šæ€§ã€ã¨ã€ŒçŸ¥è­˜ã®æ ¼ç´æ–¹æ³•ã€**ã¨ã„ã†ç•°ãªã‚‹å´é¢ã‹ã‚‰ã€Transformerã®æ·±ã„å±¤ã‚’åˆ†æã—ã¦ã„ã¾ã™ãŒã€ä¸¡è€…ã«ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªæ·±ã„é–¢ä¿‚æ€§ãŒã‚ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚<br><br>å­¦ç¿’ã®ä¸å®‰å®šæ€§ãŒã€ŒçŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã€å½¢æˆã®èƒŒæ™¯ã«ã‚ã‚‹å¯èƒ½æ€§<br>ã€ŒThe Curse of Depthã€ã§æŒ‡æ‘˜ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«ã€æ¨™æº–çš„ãªTransformerã®æ·±ã„å±¤ã¯ã€æœ¬è³ªçš„ã«å­¦ç¿’ãŒä¸å®‰å®šã§ã€å‹¾é…æƒ…å ±ãŒå¤±ã‚ã‚Œã‚„ã™ã„ç’°å¢ƒã«ã‚ã‚Šã¾ã™ã€‚<br><br>ã“ã®åŠ£æ‚ªãªå­¦ç¿’ç’°å¢ƒã“ããŒã€ã€ŒçŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã€ã¨ã„ã†å½¢ã§çŸ¥è­˜ãŒå±€æ‰€çš„ã«æ ¼ç´ã•ã‚Œã‚‹åŸå› ã®ä¸€ã¤ã«ãªã£ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ã¤ã¾ã‚Šã€<br><br>å­¦ç¿’ã®éåŠ¹ç‡æ€§: æ·±ã„å±¤ã®ã»ã¨ã‚“ã©ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ã€å‹¾é…æ¶ˆå¤±å•é¡Œã®ãŸã‚ã«åŠ¹ç‡çš„ã«å­¦ç¿’ã‚’é€²ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚<br><br>å°‚é–€åŒ–ã®ç™ºç”Ÿ: ãã®ã‚ˆã†ãªä¸å®‰å®šãªç’°å¢ƒä¸‹ã§ã€ãŸã¾ãŸã¾ç‰¹å®šã®çŸ¥è­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã†ã¾ãæ‰ãˆã‚‹ã“ã¨ãŒã§ããŸä¸€éƒ¨ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒã€ãã®çŸ¥è­˜ã‚’ä¸€èº«ã«èƒŒè² ã†å½¢ã§å¼·ãæ´»æ€§åŒ–ã™ã‚‹ã‚ˆã†ç‰¹åŒ–ï¼ˆå°‚é–€åŒ–ï¼‰ã—ã¦ã„ã£ãŸã®ã§ã¯ãªã„ã‹ã€ã¨è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚ã“ã‚Œã¯ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å…¨ä½“ã§å”èª¿ã—ã¦å­¦ç¿’ã™ã‚‹ã®ãŒé›£ã—ã„çŠ¶æ³ã§ã€ä¸€éƒ¨ã®ãƒ¦ãƒ‹ãƒƒãƒˆã ã‘ãŒçªå‡ºã—ã¦å­¦ç¿’ã‚’æ‹…ã†ã€ã¨ã„ã†ç¾è±¡ã¨è§£é‡ˆã§ãã¾ã™ã€‚<br><br>å­¦ç¿’ã®å®‰å®šåŒ–ãŒã€ã‚ˆã‚ŠåŠ¹ç‡çš„ãªçŸ¥è­˜ç²å¾—ã«ã¤ãªãŒã‚‹<br>ã§ã¯ã€ã€ŒThe Curse of Depthã€ã§ææ¡ˆã•ã‚ŒãŸæ‰‹æ³•ï¼ˆLNã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰ã«ã‚ˆã£ã¦æ·±ã„å±¤ã®å­¦ç¿’ãŒå®‰å®šåŒ–ã™ã‚‹ã¨ã€çŸ¥è­˜ã®æ ¼ç´æ–¹æ³•ã¯ã©ã®ã‚ˆã†ã«å¤‰ã‚ã‚‹ã§ã—ã‚‡ã†ã‹ã€‚<br><br>ã“ã‚Œã¯éå¸¸ã«èˆˆå‘³æ·±ã„å•ã„ã§ã‚ã‚Šã€2ã¤ã®å¯èƒ½æ€§ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚<br><br>å¯èƒ½æ€§A: ã‚ˆã‚Šå¼·å›ºãªçŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å½¢æˆ:<br>å­¦ç¿’ãŒå®‰å®šã™ã‚‹ã“ã¨ã§ã€å„çŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯ã‚ˆã‚Šæ˜ç¢ºã«ã€ãã—ã¦åŠ¹ç‡çš„ã«ç‰¹å®šã®çŸ¥è­˜ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ãƒã‚¤ã‚ºã®å¤šã„ç’°å¢ƒã§å¶ç„¶ç”Ÿã¾ã‚ŒãŸå°‚é–€å®¶ã§ã¯ãªãã€å®‰å®šã—ãŸç’°å¢ƒã§è‚²æˆã•ã‚ŒãŸçœŸã®å°‚é–€å®¶ã¸ã¨å¤‰åŒ–ã™ã‚‹ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ã™ã€‚<br><br>å¯èƒ½æ€§B: çŸ¥è­˜ã®åˆ†æ•£åŒ–:<br>ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…¨ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒåŠ¹ç‡çš„ã«å­¦ç¿’ã«å¯„ä¸ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ãŸã‚ã€ä¸€ã¤ã®çŸ¥è­˜ã‚’å°‘æ•°ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã«é›†ä¸­ã•ã›ã‚‹å¿…è¦ãŒãªããªã‚Šã€ã‚ˆã‚Šå¤šãã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã«åˆ†æ•£ã—ã¦çŸ¥è­˜ãŒæ ¼ç´ã•ã‚Œã‚‹ã‚ˆã†ã«ãªã‚‹å¯èƒ½æ€§ã‚‚è€ƒãˆã‚‰ã‚Œã¾ã™ã€‚<br><br>ç¾åœ¨ã®ã¨ã“ã‚ã€å­¦ç¿’ã®å®‰å®šåŒ–ã¯ã€çŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒã‚ˆã‚ŠåŠ¹ç‡çš„ã«å½¢æˆãƒ»æ©Ÿèƒ½ã™ã‚‹ãŸã‚ã®åŸºç›¤ã‚’æä¾›ã™ã‚‹ã¨è€ƒãˆã‚‹ã®ãŒæœ€ã‚‚å¦¥å½“ã§ã—ã‚‡ã†ã€‚ã€Œæ·±ã•ã®å‘ªã„ã€ã¨ã„ã†å•é¡Œã‚’è§£æ±ºã™ã‚‹ã“ã¨ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒçŸ¥è­˜ã‚’ã©ã®ã‚ˆã†ã«ç²å¾—ã—ã€æ•´ç†ã™ã‚‹ã‹ã¨ã„ã†ã€ã‚ˆã‚Šæ ¹æºçš„ãªãƒ¡ã‚«ãƒ‹ã‚ºãƒ ï¼ˆçŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®æŒ¯ã‚‹èˆã„ï¼‰ã«ã‚‚ç›´æ¥çš„ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã¯ãšã§ã™ã€‚<br><br>ã¾ã¨ã‚<br>ã€ŒKnowledge Neuronsã€ã¯æ·±ã„å±¤ã«å­˜åœ¨ã™ã‚‹çŸ¥è­˜ã®**ã€ŒçŠ¶æ…‹ã€ã‚’æ˜ã‚‰ã‹ã«ã—ã€ã€ŒThe Curse of Depthã€ã¯æ·±ã„å±¤ã§ç™ºç”Ÿã™ã‚‹å­¦ç¿’ã®ã€Œå•é¡Œã€**ã¨ãã®è§£æ±ºç­–ã‚’æç¤ºã—ã¾ã—ãŸã€‚<br><br>ã“ã‚Œã‚‰ã‚’çµ±åˆã™ã‚‹ã¨ã€**ã€Œæ·±ã„å±¤ã«ãŠã‘ã‚‹å­¦ç¿’ã®ä¸å®‰å®šæ€§ã¨ã„ã†å•é¡ŒãŒã€çŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¨ã„ã†å±€æ‰€çš„ãªçŸ¥è­˜è¡¨ç¾ã‚’ç”Ÿã¿å‡ºã™ä¸€å› ã¨ãªã£ã¦ãŠã‚Šã€ã“ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚ŠåŠ¹ç‡çš„ã§å®‰å®šã—ãŸçŸ¥è­˜ã®ç²å¾—ãƒ»æ ¼ç´ãŒå¯èƒ½ã«ãªã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€**ã¨ã„ã†é–¢ä¿‚æ€§ãŒè¦‹ãˆã¦ãã¾ã™ã€‚<br><br>ä¸¡è€…ã¯ã€LLMã®èƒ½åŠ›ã¨é™ç•Œã‚’ç•°ãªã‚‹è§’åº¦ã‹ã‚‰ç…§ã‚‰ã—å‡ºã—ã¦ãŠã‚Šã€çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ãƒ¢ãƒ‡ãƒ«ã®å†…éƒ¨å‹•ä½œã®è§£æ˜ã‚’ã•ã‚‰ã«ä¸€æ­©å‰é€²ã•ã›ã‚‹ã€éå¸¸ã«é‡è¦ãªç ”ç©¶ã ã¨è¨€ãˆã¾ã™ã€‚<br>```</p></span><br><br>
<a class="button" href="articles/Metrics.html" target="_blank" rel="noopener noreferrer">#Metrics</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/SpokenLanguageProcessing.html" target="_blank" rel="noopener noreferrer">#SpokenLanguageProcessing</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<span class="issue_date">Issue Date: 2025-07-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2124" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] AudioBERTScore: Objective Evaluation of Environmental Sound Synthesis  Based on Similarity of Audio embedding Sequences, Minoru Kishi+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æ–°ã—ã„å®¢è¦³çš„è©•ä¾¡æŒ‡æ¨™AudioBERTScoreã‚’ææ¡ˆã—ã€åˆæˆéŸ³å£°ã®æ€§èƒ½å‘ä¸Šã‚’ç›®æŒ‡ã™ã€‚å¾“æ¥ã®å®¢è¦³çš„æŒ‡æ¨™ã¯ä¸»è¦³çš„è©•ä¾¡ã¨ã®ç›¸é–¢ãŒå¼±ã„ãŸã‚ã€AudioBERTScoreã¯åˆæˆéŸ³å£°ã¨å‚ç…§éŸ³å£°ã®åŸ‹ã‚è¾¼ã¿ã®é¡ä¼¼æ€§ã‚’è¨ˆç®—ã—ã€ä¸»è¦³çš„è©•ä¾¡ã¨ã®ç›¸é–¢ãŒé«˜ã„ã“ã¨ã‚’å®Ÿé¨“ã§ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/forthshinji/status/1940226218500247645?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>text-to-audioã®è‡ªå‹•è©•ä¾¡ãŒå¯èƒ½ãªæ¨¡æ§˜<br></p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<span class="issue_date">Issue Date: 2025-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2099" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers, Weiming Ren+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- VAMBAãƒ¢ãƒ‡ãƒ«ã¯ã€Mamba-2ãƒ–ãƒ­ãƒƒã‚¯ã‚’ç”¨ã„ã¦ãƒ“ãƒ‡ã‚ªãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç·šå½¢ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³å‰Šæ¸›ãªã—ã§1024ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å‡¦ç†å¯èƒ½ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’50%å‰Šæ¸›ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã‚’å€å¢—ã€‚1æ™‚é–“ã®ãƒ“ãƒ‡ã‚ªç†è§£ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯LVBenchã§4.3%ã®ç²¾åº¦å‘ä¸Šã‚’é”æˆã—ã€æ§˜ã€…ãªãƒ“ãƒ‡ã‚ªç†è§£ã‚¿ã‚¹ã‚¯ã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenhuchen/status/1938064510369280136?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/3D%20Reconstruction.html" target="_blank" rel="noopener noreferrer">#3D Reconstruction</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-06-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2068" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] VGGT: Visual Geometry Grounded Transformer, Jianyuan Wang+, CVPR'25</a>
<span class="snippet"><span>GPT Summary</span>- VGGTã¯ã€ã‚·ãƒ¼ãƒ³ã®ä¸»è¦ãª3Då±æ€§ã‚’è¤‡æ•°ã®ãƒ“ãƒ¥ãƒ¼ã‹ã‚‰ç›´æ¥æ¨æ¸¬ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚Šã€3Dã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®åˆ†é‡ã«ãŠã„ã¦æ–°ãŸãªé€²å±•ã‚’ç¤ºã—ã¾ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯åŠ¹ç‡çš„ã§ã€1ç§’æœªæº€ã§ç”»åƒã‚’å†æ§‹ç¯‰ã—ã€è¤‡æ•°ã®3Dã‚¿ã‚¹ã‚¯ã§æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã—ã¾ã™ã€‚ã¾ãŸã€VGGTã‚’ç‰¹å¾´ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ä¸‹æµã‚¿ã‚¹ã‚¯ã®æ€§èƒ½ãŒå¤§å¹…ã«å‘ä¸Šã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1936711294956265820?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<span class="issue_date">Issue Date: 2025-06-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2037" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Seedance 1.0: Exploring the Boundaries of Video Generation Models, Yu Gao+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Seedance 1.0ã¯ã€å‹•ç”»ç”Ÿæˆã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆéµå®ˆã€å‹•ãã®å¦¥å½“æ€§ã€è¦–è¦šçš„å“è³ªã‚’åŒæ™‚ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚ä¸»ãªæŠ€è¡“æ”¹å–„ã¨ã—ã¦ã€æ„å‘³ã®ã‚ã‚‹å‹•ç”»ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ç”¨ã„ãŸãƒ‡ãƒ¼ã‚¿ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒãƒ«ãƒã‚·ãƒ§ãƒƒãƒˆç”Ÿæˆã®ã‚µãƒãƒ¼ãƒˆã€å‹•ç”»ç‰¹æœ‰ã®RLHFã‚’æ´»ç”¨ã—ãŸãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€æ¨è«–é€Ÿåº¦ã®ç´„10å€å‘ä¸Šã‚’å®Ÿç¾ã™ã‚‹è’¸ç•™æˆ¦ç•¥ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚Seedance 1.0ã¯ã€1080pè§£åƒåº¦ã®5ç§’é–“ã®å‹•ç”»ã‚’41.4ç§’ã§ç”Ÿæˆã—ã€é«˜å“è³ªã‹ã¤è¿…é€Ÿãªå‹•ç”»ç”Ÿæˆã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/scaling01/status/1933048431775527006?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<span class="issue_date">Issue Date: 2025-06-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2030" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Value Residual Learning, Zhanchao Zhou+, ACL'25</a>
<span class="snippet"><span>GPT Summary</span>- ResFormerã¯ã€éš ã‚ŒçŠ¶æ…‹ã®æ®‹å·®ã«å€¤ã®æ®‹å·®æ¥ç¶šã‚’åŠ ãˆã‚‹ã“ã¨ã§æƒ…å ±ã®æµã‚Œã‚’å¼·åŒ–ã™ã‚‹æ–°ã—ã„Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ææ¡ˆã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ResFormerã¯å¾“æ¥ã®Transformerã«æ¯”ã¹ã¦å°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã§åŒç­‰ã®æ€§èƒ½ã‚’ç¤ºã—ã€SVFormerã¯KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºã‚’åŠæ¸›ã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚æ€§èƒ½ã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é•·ã•ã‚„å­¦ç¿’ç‡ã«ä¾å­˜ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/zhanchaozhou/status/1932829678081098079?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-06-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2025" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Log-Linear Attention, Han Guo+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- å¯¾æ•°ç·šå½¢æ³¨æ„ã‚’ææ¡ˆã—ã€ç·šå½¢æ³¨æ„ã®åŠ¹ç‡æ€§ã¨ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹æ³¨æ„ã®è¡¨ç¾åŠ›ã‚’ä¸¡ç«‹ã€‚å›ºå®šã‚µã‚¤ã‚ºã®éš ã‚ŒçŠ¶æ…‹ã‚’å¯¾æ•°çš„ã«æˆé•·ã™ã‚‹éš ã‚ŒçŠ¶æ…‹ã«ç½®ãæ›ãˆã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å¯¾æ•°ç·šå½¢ã«æŠ‘ãˆã‚‹ã€‚Mamba-2ã¨Gated DeltaNetã®å¯¾æ•°ç·šå½¢ãƒãƒªã‚¢ãƒ³ãƒˆãŒç·šå½¢æ™‚é–“ã®ãƒãƒªã‚¢ãƒ³ãƒˆã¨æ¯”è¼ƒã—ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã“ã¨ã‚’ç¢ºèªã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1932194773559107911?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1931432543766847887?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/COLT.html" target="_blank" rel="noopener noreferrer">#COLT</a>
<span class="issue_date">Issue Date: 2025-06-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2007" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Learning Compositional Functions with Transformers from Easy-to-Hard   Data, Zixuan Wang+, COLT'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€Transformerãƒ™ãƒ¼ã‚¹ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’å¯èƒ½æ€§ã‚’æ¢æ±‚ã—ã€$k$-fold compositionã‚¿ã‚¹ã‚¯ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ã€‚$O(\log k)$å±¤ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã§ã“ã®ã‚¿ã‚¹ã‚¯ã‚’è¡¨ç¾ã§ãã‚‹ä¸€æ–¹ã€SQã‚ªãƒ©ã‚¯ãƒ«ã«å¯¾ã™ã‚‹ã‚¯ã‚¨ãƒªã®ä¸‹é™ã‚’ç¤ºã—ã€ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒæŒ‡æ•°çš„ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã‚’è¨¼æ˜ã€‚ã•ã‚‰ã«ã€ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ å­¦ç¿’æˆ¦ç•¥ã‚’ç”¨ã„ã¦ã€ç°¡å˜ãªä¾‹ã¨é›£ã—ã„ä¾‹ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒãŒãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®åŠ¹ç‡çš„ãªå­¦ç¿’ã«å¿…è¦ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/zzzixuanwang/status/1928465115478708604?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã“ã¡ã‚‰ã¯ã¾ãšå…ƒãƒã‚¹ãƒˆã®ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’èª­ã‚€ã®ãŒè‰¯ã„ã¨æ€ã‚ã‚Œã‚‹ã€‚è¦ç‚¹ã‚’ã‚ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã£ã¦ã„ã‚‹ã€‚</p>
<p>å…ƒãƒã‚¹ãƒˆã¨alphaxivã§ã–ã£ãã‚Šç†è§£ã—ãŸã¨ã“ã‚ã€<br><br>TransformerãŒcontextã¨ã—ã¦ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±(Ïƒ)ã¨parametric knowledge(Ï€)ã‚’kå›ã®çŸ¥è­˜ãƒãƒƒãƒ”ãƒ³ã‚°ãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯(k-fold composition task)ã‚’å­¦ç¿’ã™ã‚‹ã«ã¯O(log k)ã®layeræ•°ãŒå¿…è¦ã§ã€ç›´æ¥çš„ã«kå›ã®çŸ¥è­˜ãƒãƒƒãƒ”ãƒ³ã‚°ãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã¯kã®æŒ‡æ•°ã‚ªãƒ¼ãƒ€ãƒ¼ã®ãƒ‡ãƒ¼ã‚¿é‡ãŒæœ€ä½é™å¿…è¦ã¨ãªã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ã“ã‚Œã¯kãŒå¤§ãããªã‚‹ã¨ï¼ˆã™ãªã‚ã¡ã€è¤‡é›‘ãªreasoning stepãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯ï¼‰ã«ãªã‚‹ã¨éç¾å®Ÿçš„ãªã‚‚ã®ã¨ãªã‚‹ãŸã‚ã€ä½•ã‚‰ã‹ã®æ–¹æ³•ã§ç·©å’Œã—ãŸã„ã€‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç°¡å˜ãªã‚‚ã®ã‹ã‚‰é›£ã—ã„ã‚‚ã®ã‚’mixingã™ã‚‹ã“ã¨ï¼ˆã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ å­¦ç¿’ï¼‰ã“ã¨ã§ã€ã“ã®æ¡ä»¶ãŒç·©å’Œã•ã‚Œã€æŒ‡æ•°ã‚ªãƒ¼ãƒ€ãƒ¼ã‹ã‚‰å¤šé …å¼ã‚ªãƒ¼ãƒ€ãƒ¼ã®ãƒ‡ãƒ¼ã‚¿é‡ã§å­¦ç¿’ã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸ<br><br>ã¨ã„ã£ãŸæ„Ÿã˜ã ã¨æ€ã‚ã‚Œã‚‹ã€‚</p>
<p>ã˜ã‚ƒã‚æœ€æ–°ã®32Bãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚ã€ã‚ˆã‚Šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå¤§ããã¦layeræ•°ãŒå¤šã„å¤ã„ãƒ¢ãƒ‡ãƒ«ã®æ–¹ãŒè¤‡é›‘ãªreasoningãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯ã‚’å®Ÿã¯è§£ã‘ã‚‹ã£ã¦ã“ã¨ï¼ï¼Ÿç›´æ„Ÿã«åã™ã‚‹ï¼ã¨ä¸€ç¬æ€ã£ãŸãŒã€ãŠãã‚‰ãæœ€è¿‘ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯æ˜”ã®ãƒ¢ãƒ‡ãƒ«ã¨æ¯”ã¹ã¦parametric knowledgeãŒã‚ˆã‚Šé«˜å¯†åº¦ã«é©åˆ‡ã«åœ§ç¸®ã•ã‚Œã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã‚‹ã¨æ€ã‚ã‚Œã‚‹ã®ã§ã€æ˜”ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯kå›ã®çŸ¥è­˜ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ã—ãªã„ã¨è§£ã‘ãªã„ã‚¿ã‚¹ã‚¯ãŒã€æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯k-nå›ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã§è§£ã‘ã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã‚‹ã¨æ¨å¯Ÿã•ã‚Œã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå°ã•ãã¦ã‚‚å•é¡Œãªãè§£ã‘ã¾ã™ã€ã¿ãŸã„ãªã“ã¨ãŒèµ·ã“ã£ã¦ã„ã‚‹ã®ã ã‚ã†ã€ã¨ã„ã†æ„Ÿæƒ³ã‚’æŠ±ããªã©ã—ãŸ</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<span class="issue_date">Issue Date: 2025-05-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1977" target="_blank" rel="noopener noreferrer" class="title-link">Insights into DeepSeek-V3: Scaling Challenges and Reflections on  Hardware for AI Architectures, Chenggang Zhao+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- DeepSeek-V3ã¯ã€2,048å°ã®NVIDIA H800 GPUã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢åˆ¶ç´„ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã®å…±åŒè¨­è¨ˆã‚’ç¤ºã™ã€‚ãƒ¡ãƒ¢ãƒªåŠ¹ç‡å‘ä¸Šã®ãŸã‚ã®ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰æ½œåœ¨æ³¨æ„ã‚„ã€è¨ˆç®—ã¨é€šä¿¡ã®æœ€é©åŒ–ã‚’å›³ã‚‹å°‚é–€å®¶ã®æ··åˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€FP8æ··åˆç²¾åº¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãªã©ã®é©æ–°ã‚’å¼·èª¿ã€‚ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«åŸºã¥ãå°†æ¥ã®æ–¹å‘æ€§ã«ã¤ã„ã¦è­°è«–ã—ã€AIãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«å¿œãˆã‚‹ãŸã‚ã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã¨ãƒ¢ãƒ‡ãƒ«ã®å…±åŒè¨­è¨ˆã®é‡è¦æ€§ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/deedydas/status/1924512147947848039?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<span class="issue_date">Issue Date: 2025-04-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1905" target="_blank" rel="noopener noreferrer" class="title-link">RNNs are not Transformers ï¼ˆYetï¼‰: The Key Bottleneck on In-context   Retrieval, Kaiyue Wen+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€RNNã¨ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®è¡¨ç¾åŠ›ã®é•ã„ã‚’èª¿æŸ»ã—ã€ç‰¹ã«RNNãŒChain-of-Thoughtï¼ˆCoTï¼‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”¨ã„ã¦ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã«åŒ¹æ•µã™ã‚‹ã‹ã‚’åˆ†æã€‚çµæœã€CoTã¯RNNã‚’æ”¹å–„ã™ã‚‹ãŒã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¨ã®ã‚®ãƒ£ãƒƒãƒ—ã‚’åŸ‹ã‚ã‚‹ã«ã¯ä¸ååˆ†ã§ã‚ã‚‹ã“ã¨ãŒåˆ¤æ˜ã€‚RNNã®æƒ…å ±å–å¾—èƒ½åŠ›ã®é™ç•ŒãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã§ã‚ã‚‹ãŒã€Retrieval-Augmented Generationï¼ˆRAGï¼‰ã‚„ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼å±¤ã®è¿½åŠ ã«ã‚ˆã‚Šã€RNNã¯CoTã‚’ç”¨ã„ã¦å¤šé …å¼æ™‚é–“ã§è§£æ±ºå¯èƒ½ãªå•é¡Œã‚’è§£æ±ºã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/yuma_1_or/status/1915968478735130713?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1210" target="_blank" rel="noopener noreferrer">Transformers are Multi-State RNNs, Matanel Oren+, N/A, EMNLP'24</a>
<br><br>â†‘ã¨ã¯ã©ã†ã„ã†é–¢ä¿‚ãŒã‚ã‚‹ã ã‚ã†ã‹ï¼Ÿ</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<span class="issue_date">Issue Date: 2025-04-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1883" target="_blank" rel="noopener noreferrer" class="title-link">AM-RADIO: Agglomerative Vision Foundation Model -- Reduce All Domains   Into One, Mike Ranzinger+, CVPR'25</a>
<span class="snippet"><span>GPT Summary</span>- è¦–è¦šåŸºç›¤ãƒ¢ãƒ‡ãƒ«ï¼ˆVFMï¼‰ã‚’ãƒãƒ«ãƒãƒ†ã‚£ãƒ¼ãƒãƒ£ãƒ¼è’¸ç•™ã‚’é€šã˜ã¦çµ±åˆã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒAM-RADIOã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã®è¦–è¦š-è¨€èªç†è§£ã‚„ãƒ”ã‚¯ã‚»ãƒ«ãƒ¬ãƒ™ãƒ«ã®ç†è§£ã‚’å‘ä¸Šã•ã›ã€å€‹ã€…ã®ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è¶…ãˆã‚‹ã€‚æ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£E-RADIOã¯ã€ãƒ†ã‚£ãƒ¼ãƒãƒ£ãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚å°‘ãªãã¨ã‚‚7å€é€Ÿã„ã€‚åŒ…æ‹¬çš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æ§˜ã€…ãªä¸‹æµã‚¿ã‚¹ã‚¯ã‚’è©•ä¾¡ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/pavlomolchanov/status/1910391609927360831?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>visionç³»ã®foundation modelã¯ãã‚Œãã‚Œç•°ãªã‚‹ç›®çš„é–¢æ•°ã§è¨“ç·´ã•ã‚Œã¦ãã¦ãŠã‚Šï¼ˆCLIPã¯å¯¾ç…§å­¦ç¿’ <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/550" target="_blank" rel="noopener noreferrer">Learning Transferable Visual Models From Natural Language Supervision, Radford+, OpenAI, ICML'21</a>
, DINOv2ã¯è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1884" target="_blank" rel="noopener noreferrer">DINOv2: Learning Robust Visual Features without Supervision, Maxime Oquab+, TMLR'24</a>
, SAMã¯segmentation <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1885" target="_blank" rel="noopener noreferrer">Segment Anything, Alexander Kirillov+, arXiv'23</a>
)ãã‚Œãã‚Œåˆ¥ã®èƒ½åŠ›ã‚’æŒã£ã¦ãŸãŒã€ãã‚Œã‚‰ã‚’ä¸€å€‹ã®ãƒ¢ãƒ‡ãƒ«ã«è’¸ç•™ã—ã¾ã—ãŸã€ã¨ã„ã†è©±ã‚‰ã—ã„<br><img src="https://github.com/user-attachments/assets/929aaa47-ab88-4912-a59a-579d2f34e886" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-04-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1866" target="_blank" rel="noopener noreferrer" class="title-link">Scalable-Softmax Is Superior for Attention, Ken M. Nakanishi, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- SSMaxã‚’ææ¡ˆã—ã€Softmaxã®ä»£æ›¿ã¨ã—ã¦Transformerãƒ¢ãƒ‡ãƒ«ã«çµ±åˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã®é‡è¦æƒ…å ±ã®å–å¾—ãŒå‘ä¸Šã—ã€äº‹å‰å­¦ç¿’ä¸­ã®æå¤±æ¸›å°‘ãŒé€Ÿããªã‚‹ã€‚SSMaxã¯æ³¨æ„ã‚¹ã‚³ã‚¢ã‚’æ”¹å–„ã—ã€é•·ã•ã®ä¸€èˆ¬åŒ–ã‚’ä¿ƒé€²ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1863" target="_blank" rel="noopener noreferrer">Llama 4 Series, Meta, 2025.04</a>
<br><br>ã§æ¡ç”¨ã•ã‚Œã¦ã„ã‚‹æ‰‹æ³•ã§ã€ãƒ–ãƒ­ã‚°ãƒã‚¹ãƒˆä¸­ã§å¼•ç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚Long Contextã«ãªã£ãŸå ´åˆã«softmaxã®åˆ†å¸ƒãŒå‡ä¸€ã«ãªã‚‹ï¼ˆï¼é‡è¦ãªæƒ…å ±ã«attendã™ã‚‹èƒ½åŠ›ãŒå‰ŠãŒã‚Œã‚‹ï¼‰ã“ã¨ã‚’é˜²ããŸã‚ã®æ‰‹æ³•ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚</p>
<p>è§£èª¬ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/nrehiew_/status/1908613993998045534"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-04-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1855" target="_blank" rel="noopener noreferrer" class="title-link">Multi-Token Attention, Olga Golovneva+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ãƒãƒ«ãƒãƒˆãƒ¼ã‚¯ãƒ³ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆMTAï¼‰ã‚’ææ¡ˆã—ã€è¤‡æ•°ã®ã‚¯ã‚¨ãƒªã¨ã‚­ãƒ¼ã®ãƒ™ã‚¯ãƒˆãƒ«ã«åŸºã¥ã„ã¦ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¦ã‚§ã‚¤ãƒˆã‚’æ¡ä»¶ä»˜ã‘ã‚‹ã“ã¨ã§ã€é–¢é€£ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚ˆã‚Šæ­£ç¢ºã«ç‰¹å®šã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚MTAã¯ç•³ã¿è¾¼ã¿æ“ä½œã‚’ç”¨ã„ã¦ã€è¿‘ãã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒäº’ã„ã«å½±éŸ¿ã‚’ä¸ãˆã€è±Šã‹ãªæƒ…å ±ã‚’æ´»ç”¨ã™ã‚‹ã€‚è©•ä¾¡çµæœã‹ã‚‰ã€MTAã¯Transformerãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚Šã€ç‰¹ã«é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ã®æƒ…å ±æ¤œç´¢ã«ãŠã„ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jaseweston/status/1907260086017237207?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å¾“æ¥ã®Multi Head Attentionã§ã¯ã€å˜ä½“ã®QKã®ã¿ã‚’åˆ©ç”¨ã—ã¦ã„ãŸã‘ã©ã€è¤‡æ•°ã®QKã®æƒ…å ±ã‚’ç•³ã¿è¾¼ã‚“ã§æ´»ç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã€Headã‚‚ç•³ã¿è¾¼ã¿ã§é‡è¦ãªæƒ…å ±ãŒã‚ˆã‚Šä¼æ¬ã•ã‚Œã‚‹ã‚ˆã†ã«ã—ã¦ã€GroupNormalizationã‚’ã‹ã‘ãŸã‚‰Perplexityã®è¦³ç‚¹ã§Differential Transformerã‚’ä¸Šå›ã£ãŸã‚ˆã€ã¨ã„ã†è©±ãªæ¨¡æ§˜ã€‚<br><br><img src="https://github.com/user-attachments/assets/199e0794-a286-486d-9426-d86cfd208750" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/2997a61b-3367-4f43-b85a-ac8fa160391a" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/5ef8ddb0-538b-46e2-94b8-2ef495c938ec" alt="image" loading="lazy"><br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1856" target="_blank" rel="noopener noreferrer">Group Normalization, Yuxin Wu+, arXiv'18</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1466" target="_blank" rel="noopener noreferrer">Differential Transformer, Tianzhu Ye+, N/A, ICLR'25</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<span class="issue_date">Issue Date: 2025-03-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1799" target="_blank" rel="noopener noreferrer" class="title-link">NeoBERT: A Next-Generation BERT, Lola Le Breton+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- NeoBERTã¯ã€æœ€æ–°ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ãŸæ¬¡ä¸–ä»£ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã§ã€åŒæ–¹å‘ãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’å†å®šç¾©ã—ã¾ã™ã€‚4,096ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’æ´»ç”¨ã—ã€250Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚ŠãªãŒã‚‰ã€MTEBãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã—ã€BERTã‚„RoBERTaã‚’ä¸Šå›ã‚Šã¾ã™ã€‚ã™ã¹ã¦ã®ã‚³ãƒ¼ãƒ‰ã‚„ãƒ‡ãƒ¼ã‚¿ã‚’å…¬é–‹ã—ã€ç ”ç©¶ã¨å®Ÿä¸–ç•Œã§ã®æ¡ç”¨ã‚’ä¿ƒé€²ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>é–¢é€£: <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1606" target="_blank" rel="noopener noreferrer">ModernBERT, AnswerDotAI, 2024.12</a>
</p>
<p>## BERT, ModernBERTã¨ã®é•ã„<br><br>![Image](https://github.com/user-attachments/assets/58dbdcf6-e7dc-43c2-94ed-d8bb73cd2617)<br><br>## æ€§èƒ½<br><br>![Image](https://github.com/user-attachments/assets/72730c9c-38d0-4773-8ddb-f0349b8776d2)<br><br>## æ‰€æ„Ÿ<br>medium sizeæœªæº€ã®ãƒ¢ãƒ‡ãƒ«ã®ä¸­ã§ã¯SoTAã§ã¯ã‚ã‚‹ãŒã€ModernBERTãŒåˆ©ç”¨ã§ãã‚‹ã®ã§ã‚ã‚Œã°ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’è¦‹ã‚‹é™ã‚Šã¯å®Ÿç”¨çš„ã«ã¯ModernBERTã§è‰¯ã„ã®ã§ã¯ã€ã¨æ„Ÿã˜ãŸã€‚å­¦ç¿’ã¨inferenceã®é€Ÿåº¦å·®ã¯ã©ã®ç¨‹åº¦ã‚ã‚‹ã®ã ã‚ã†ã‹ï¼Ÿ</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<a class="button" href="articles/Normalization.html" target="_blank" rel="noopener noreferrer">#Normalization</a>
<span class="issue_date">Issue Date: 2025-03-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1795" target="_blank" rel="noopener noreferrer" class="title-link">Transformers without Normalization, Jiachen Zhu+, CVPR'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€æ­£è¦åŒ–å±¤ãªã—ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãŒDynamic Tanhï¼ˆDyTï¼‰ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€åŒç­‰ã¾ãŸã¯ãã‚Œä»¥ä¸Šã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚DyTã¯ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã®ä»£æ›¿ã¨ã—ã¦æ©Ÿèƒ½ã—ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ãªã—ã§åŠ¹æœã‚’ç™ºæ®ã—ã¾ã™ã€‚å¤šæ§˜ãªè¨­å®šã§ã®å®Ÿé¨“ã«ã‚ˆã‚Šã€æ­£è¦åŒ–å±¤ã®å¿…è¦æ€§ã«å¯¾ã™ã‚‹æ–°ãŸãªæ´å¯Ÿã‚’æä¾›ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>ãªã‚“â€¦ã ã¨â€¦ã€‚LayerNormalizationã‚’ä¸‹è¨˜ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ã‚ˆã†ãªtanhã‚’ç”¨ã„ãŸè¶…çµ¶ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆparameterized thnh [Lecunæ°ãƒã‚¹ãƒˆ](



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/ylecun/status/1900610590315249833?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q)ï¼‰ã«ç½®æ›ã™ã‚‹ã ã‘ã£ã½ã„ï¼Ÿ"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><img src="https://github.com/user-attachments/assets/474d3ee4-4c08-4b00-9a41-126ca5d5207e" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/5aea9f93-85d9-4e0b-b9db-bb407d596493" alt="image" loading="lazy"><br><br>åŒç­‰ä»¥ä¸Šã®æ€§èƒ½ã‚’ç¶­æŒã—ãªãŒã‚‰ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®inference, trainingã®æ™‚é–“ã‚’8%ç¨‹åº¦å‰Šæ¸›ã€‚<br><img src="https://github.com/user-attachments/assets/98f8caa3-3ef2-4594-a45a-ae0aa2cf2ef6" alt="image" loading="lazy"></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/TMLR.html" target="_blank" rel="noopener noreferrer">#TMLR</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<span class="issue_date">Issue Date: 2024-11-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1505" target="_blank" rel="noopener noreferrer" class="title-link">Mixture-of-Transformers: A Sparse and Scalable Architecture for   Multi-Modal Foundation Models, Weixin Liang+, TMLR'25</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å‡¦ç†ã‚’åŠ¹ç‡åŒ–ã™ã‚‹ãŸã‚ã«ã€Mixture-of-Transformersï¼ˆMoTï¼‰ã‚’ææ¡ˆã€‚MoTã¯è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã—ã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã”ã¨ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åˆ†é›¢ã—ã¦ç‰¹åŒ–ã—ãŸå‡¦ç†ã‚’å®Ÿç¾ã€‚Chameleon 7Bè¨­å®šã§ã¯ã€55.8%ã®FLOPsã§å¯†ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã«åŒ¹æ•µã™ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€éŸ³å£°ã‚’å«ã‚€å ´åˆã‚‚37.2%ã®FLOPsã§åŒæ§˜ã®çµæœã‚’é”æˆã€‚ã•ã‚‰ã«ã€Transfusionè¨­å®šã§ã¯ã€7Bã®MoTãƒ¢ãƒ‡ãƒ«ãŒå¯†ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã®ç”»åƒæ€§èƒ½ã«å¯¾ã—ã¦FLOPsã®3åˆ†ã®1ã§åŒ¹æ•µã—ã€760Mã®ãƒ¢ãƒ‡ãƒ«ã¯ä¸»è¦ãªç”»åƒç”ŸæˆæŒ‡æ¨™ã§ä¸Šå›ã‚‹çµæœã‚’å¾—ãŸã€‚MoTã¯å®Ÿç”¨çš„ãªåˆ©ç‚¹ã‚‚ç¤ºã—ã€ç”»åƒå“è³ªã‚’47.2%ã€ãƒ†ã‚­ã‚¹ãƒˆå“è³ªã‚’75.6%ã®çµŒéæ™‚é–“ã§é”æˆã€‚</span>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2024-10-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1466" target="_blank" rel="noopener noreferrer" class="title-link">Differential Transformer, Tianzhu Ye+, N_A, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- Diff Transformerã¯ã€é–¢é€£ã™ã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¸ã®æ³¨æ„ã‚’å¼·åŒ–ã—ã€ãƒã‚¤ã‚ºã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã™ã‚‹æ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã™ã€‚å·®åˆ†æ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ç”¨ã„ã¦ã€æ³¨æ„ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã€ã‚¹ãƒ‘ãƒ¼ã‚¹ãªæ³¨æ„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä¿ƒé€²ã—ã¾ã™ã€‚å®Ÿé¨“çµæœã¯ã€Diff TransformerãŒå¾“æ¥ã®Transformerã‚’ä¸Šå›ã‚Šã€é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚„å¹»è¦šã®è»½æ¸›ã«ãŠã„ã¦é¡•è‘—ãªåˆ©ç‚¹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ã¾ãŸã€æ–‡è„ˆå†…å­¦ç¿’ã«ãŠã„ã¦ã‚‚ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã€å …ç‰¢æ€§ã‚’é«˜ã‚ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€Diff Transformerã¯å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®é€²å±•ã«å¯„ä¸ã™ã‚‹æœ‰æœ›ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>æœ€è¿‘ã®MSã¯ãªã‹ãªã‹ã™ã”ã„ï¼ˆå°ä¸¦æ„Ÿ</p>
<p>
<strong># æ¦‚è¦<br><br>attention scoreã®ãƒã‚¤ã‚ºã‚’ä½æ¸›ã™ã‚‹ã‚ˆã†ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã—ã¦ã€äºŒã¤ã®QKVã‚’ç”¨æ„ã—ã€ä¸¡è€…ã®å·®åˆ†ã‚’å–ã‚‹ã“ã¨ã§æœ€çµ‚çš„ãªattentiok scoreã‚’è¨ˆç®—ã™ã‚‹Differential Attentionã‚’ææ¡ˆã—ãŸã€‚<br><br><br><br>attentionã®noiseã®ä¾‹ã€‚answerã¨æ¯”è¼ƒã—ã¦irrelevantãªcontextã«attention scoreãŒé«˜ã„ã‚¹ã‚³ã‚¢ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¦ã—ã¾ã†ï¼ˆå›³å·¦ï¼‰ã€‚differential transformerãŒææ¡ˆã™ã‚‹differential attentionã§ã¯ã€ãƒã‚¤ã‚ºã‚’æè¨€ã—ã€é‡è¦ãªcontextã®attention scoreãŒé«˜ããªã‚‹ã‚ˆã†ã«ãªã‚‹ï¼ˆå›³ä¸­å¤®ï¼‰ã€ã‚‰ã—ã„ã€‚<br><br><img src="https://github.com/user-attachments/assets/6033f477-d4bf-492d-9360-74f2849ce40e" alt="image" loading="lazy"><br><br><br><br># Differential Attentionã®æ¦‚è¦ã¨è¨ˆç®—å¼<br><br><img src="https://github.com/user-attachments/assets/b77facd8-7cf2-43ab-8947-2f775423f0a0" alt="image" loading="lazy"><br><br><br><br>æ•°å¼ã§è¦‹ã‚‹ã¨ã“ã®ã‚ˆã†ã«ãªã£ã¦ãŠã‚Šã€äºŒã¤ã®QKã‚’ã©ã®ç¨‹åº¦ã®å¼·ã•ã§äº¤äº’ä½œç”¨ã•ã›ã‚‹ã‹ã‚’Î»ã§åˆ¶å¾¡ã—ã€Î»ã‚‚ãã‚Œãã‚Œã®QKã‹ã‚‰å°å‡ºã™ã‚‹ã€‚<br><br><img src="https://github.com/user-attachments/assets/c58a4d04-a453-4aef-aa40-7de872117482" alt="image" loading="lazy">&lt;/p&gt;<p>QA, æ©Ÿæ¢°ç¿»è¨³, æ–‡æ›¸åˆ†é¡, ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãªã©ã®æ§˜ã€…ãªNLPã‚¿ã‚¹ã‚¯ãŒå«ã¾ã‚Œã‚‹Eval Harnessãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã¯ã€åŒè¦æ¨¡ã®transformerãƒ¢ãƒ‡ãƒ«ã‚’å¤§å¹…ã«outperformã€‚ãŸã ã—ã€3Bã§ã—ã‹å®Ÿé¨“ã—ã¦ã„ãªã„ã‚ˆã†ãªã®ã§ã€ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«ãªã£ãŸã¨ãã«gainãŒã‚ã‚‹ã‹ã¯ç¤ºã•ã‚Œã¦ã„ãªã„ç‚¹ã«ã¯æ³¨æ„ã€‚<br><img src="https://github.com/user-attachments/assets/384605ed-e4e4-4a17-83c8-506f8e3e2e4c" alt="image" loading="lazy"></p>
<p>ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ï¼‰ã¨ã€å­¦ç¿’ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã«ã¤ã„ã¦ã‚‚èª¿æŸ»ã—ãŸçµæœã€LLaMAã¨æ¯”è¼ƒã—ã¦ã€ã‚ˆã‚Šå°‘ãªã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°/å­¦ç¿’ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§åŒç­‰ã®lossã‚’é”æˆã€‚<br><img src="https://github.com/user-attachments/assets/5d2d1dfc-4197-4b36-9f3d-79a3ed18fe3f" alt="image" loading="lazy"></p>
<p>64Kã«context sgzeã‚’æ‹¡å¼µã—ã€1.5B tokenã§3Bãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ å­¦ç¿’ã‚’ã—ãŸã¨ã“ã‚ã€ã“ã‚Œã‚‚transformerã¨æ¯”ã¹ã¦ã‚ˆã‚Šå°ã•ã„lossã‚’é”æˆ<img src="https://github.com/user-attachments/assets/f911a4f9-d175-4ea2-825b-9776be6042e5" alt="image" loading="lazy"></p>
<p>contextä¸­ã«åŸ‹ã‚è¾¼ã¾ã‚ŒãŸé‡è¦ãªæƒ…å ±ï¼ˆä»Šå›ã¯ã‚¯ã‚¨ãƒªã«å¯¾å¿œã™ã‚‹magic numberï¼‰ã‚’æŠ½å‡ºã™ã‚‹ã‚¿ã‚¹ã‚¯ã®æ€§èƒ½ã‚‚å‘ä¸Šã€‚Needleï¼ˆNï¼‰ã¨å‘¼ã°ã‚Œã‚‹æ­£è§£ã®magic numberãŒå«ã¾ã‚Œã‚‹æ–‡ã‚’contextä¸­ã®æ§˜ã€…ãªæ·±ã•ã«é…ç½®ã—ã€åŒæ™‚ã«distractorã¨ãªã‚‹æ–‡ã‚‚ãƒ©ãƒ³ãƒ€ãƒ ã«é…ç½®ã™ã‚‹ã€‚ã“ã‚Œã«å¯¾ã—ã¦ã‚¯ã‚¨ãƒªï¼ˆRï¼‰ãŒå…¥åŠ›ã•ã‚ŒãŸã¨ãã«ã€ã©ã‚Œã ã‘æ­£ã—ã„æƒ…å ±ã‚’contextã‹ã‚‰æŠ½å‡ºã§ãã‚‹ã‹ã€ã¨ã„ã†è©±ã ã¨æ€ã‚ã‚Œã‚‹ã€‚<br><br>ã“ã‚Œã‚‚æ€§èƒ½ãŒå‘ä¸Šã€‚ç‰¹ã«ã‚¯ã‚¨ãƒªã¨NeedleãŒè¤‡æ•°ã®è¦ç´ ã§æ§‹æˆã•ã‚Œã¦ã„ã‚Œå ´åˆã®æ€§èƒ½ãŒé«˜ãï¼ˆä¸‹è¡¨ï¼‰ã€é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸­ã®æ§˜ã€…ãªä½ç½®ã«åŸ‹ã‚è¾¼ã¾ã‚ŒãŸNeedleã‚’æŠ½å‡ºã™ã‚‹æ€§èƒ½ã‚‚é«˜ã„ï¼ˆä¸Šã®matrixï¼‰<br><br><img src="https://github.com/user-attachments/assets/f4d084dc-fac5-427d-8185-5604e55cf051" alt="image" loading="lazy"><br><br>[Needle-In-A-Haystack test](


<a href="https://www.perplexity.ai/search/needle-in-a-haystack-testtohan-jF7LXWQPSMqKI2pZSchjpA#0)" target="_blank" rel="noopener noreferrer">https://www.perplexity.ai/search/needle-in-a-haystack-testtohan-jF7LXWQPSMqKI2pZSchjpA#0)</a>


</p>
<p>Many shotã®ICLèƒ½åŠ›ã‚‚å‘ä¸Š<br><img src="https://github.com/user-attachments/assets/c935ba93-9915-45c8-aaa6-f073d62fdd3b" alt="image" loading="lazy"></p>
<p>è¦ç´„ã‚¿ã‚¹ã‚¯ã§ã®hallucinationã‚‚ä½æ¸›ã€‚ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ã¨æ­£è§£è¦ç´„ã‚’å…¥åŠ›ã—ã€GPT4-oã«hallucinationã®æœ‰ç„¡ã‚’åˆ¤å®šã•ã›ã¦è©•ä¾¡ã€‚ã“ã‚Œã¯å…ˆè¡Œç ”ç©¶ã§äººæ‰‹ã§ã®è©•ä¾¡ã¨é«˜ã„agreementãŒã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/6fd97af4-fec6-44e8-b00c-d5ba26770a84" alt="image" loading="lazy"></p>
<p>ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§LLMå…¨ä½“ã®æ€§èƒ½ã‚’åº•ä¸Šã’ã—ã¦ã„ã‚‹ç´ æ™´ã‚‰ã—ã„æˆæœã«è¦‹ãˆã‚‹ã€‚æ–œã‚èª­ã¿ãªã®ã§èª­ã¿é£›ã°ã—ã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ãŒã€<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/766" target="_blank" rel="noopener noreferrer">Textbooks Are All You Need, Suriya Gunasekar+, N/A, arXiv'23</a>
&lt;/strong&gt;
<br>
 ã®ã‚ˆã†ã«é«˜å“è³ªãªå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ã—ãŸå ´åˆã‚‚åŒæ§˜ã®åŠ¹æœãŒç™ºç¾ã™ã‚‹ã®ã ã‚ã†ã‹ï¼Ÿ<br>attentionã®ã‚¹ã‚³ã‚¢ãŒnoisyã¨ã„ã†ã“ã¨ã¯ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æ´—ç·´ã•ã›ã‚‹ã“ã¨ã§ã‚‚æ”¹å–„ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/766" target="_blank" rel="noopener noreferrer">Textbooks Are All You Need, Suriya Gunasekar+, N/A, arXiv'23</a>
 ã¯ã“ã‚Œã‚’ãƒ‡ãƒ¼ã‚¿ã§æ”¹å–„ã—ã€ã“ã¡ã‚‰ã®ç ”ç©¶ã¯ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§æ”¹å–„ã—ãŸã€ã¿ãŸã„ãªæ‰ãˆæ–¹ã‚‚ã§ãã‚‹ã®ã‹ã‚‚ã—ã‚Œãªã„ã€‚</p>
<p>ã¡ãªã¿ã«Flash Attentionã¨ã—ã¦ã®å®Ÿè£…æ–¹æ³•ã‚‚ææ¡ˆã•ã‚Œã¦ãŠã‚Šã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã¯é€šå¸¸ã®attentionã¨æ¯”ã¹ã¦ã‚€ã—ã‚å‘ä¸Šã—ã¦ã„ã‚‹ã®ã§å®Ÿç”¨çš„ãªæ‰‹æ³•ã§ã‚‚ã‚ã‚‹ã€‚ã™ã”ã„ã€‚<br><img src="https://github.com/user-attachments/assets/c0212cd8-55f5-4991-b256-0ff2bce35669" alt="image" loading="lazy"></p>
<p>ã‚ã¨ã“ã‚Œã€äº‹å‰å­¦ç¿’ã¨Instruction Tuningã‚’é€šå¸¸ã®ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã§å­¦ç¿’ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ã€ç‹¬è‡ªãƒ‡ãƒ¼ã‚¿ã§SFTã™ã‚‹ã¨ãã«å°å…¥ã—ãŸã‚‰downstream taskã®æ€§èƒ½å‘ä¸Šã™ã‚‹ã‚“ã ã‚ã†ã‹ã€‚ã‚‚ã—ãã†ãªã‚‰ç´ æ™´ã‚‰ã—ã„</p>
<p>OpenReview:


<a href="https://openreview.net/forum?id=OvoCm1gGhN" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=OvoCm1gGhN</a>


</p>
<p>GroupNormalizationã«ã¤ã„ã¦ã¯ã“ã¡ã‚‰:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1856" target="_blank" rel="noopener noreferrer">Group Normalization, Yuxin Wu+, arXiv'18</a>
</p>&lt;/span&gt;<br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Optimizer.html" target="_blank" rel="noopener noreferrer">#Optimizer</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/ZeroshotHyperparameterTransfer.html" target="_blank" rel="noopener noreferrer">#ZeroshotHyperparameterTransfer</a>
<span class="issue_date">Issue Date: 2025-08-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2623" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Scaling Exponents Across Parameterizations and Optimizers, Katie Everett+, ICML'24</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã‚„ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã®é¸æŠãŒé‡è¦ã§ã‚ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã«é–¢ã™ã‚‹æ–°ã—ã„è¦–ç‚¹ã‚’ææ¡ˆã—ã€åºƒç¯„ãªã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨å­¦ç¿’ç‡ã®çµ„ã¿åˆã‚ã›ã§æ•°ä¸‡ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ãŸçµæœã€æœ€é©ãªå­¦ç¿’ç‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒé‡è¦ã§ã‚ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã€‚æ–°ã—ã„å±¤ã”ã¨ã®å­¦ç¿’ç‡ã®å‡¦æ–¹ã¯å¾“æ¥ã®æ–¹æ³•ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€Adamã®ã‚¤ãƒ—ã‚·ãƒ­ãƒ³ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®é©åˆ‡ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã—ã€æ•°å€¤çš„ã«å®‰å®šã—ãŸæ–°ã—ã„Adamãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã‚ã‚‹Adam-atan2ã‚’ææ¡ˆã—ãŸã€‚</span>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/RecurrentModels.html" target="_blank" rel="noopener noreferrer">#RecurrentModels</a>
<span class="issue_date">Issue Date: 2025-08-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2611" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Looped Transformers are Better at Learning Learning Algorithms, Liu Yang+, ICLR'24</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ«ãƒ¼ãƒ—å‹transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ææ¡ˆã—ã€å¾“æ¥ã®transformerã«åå¾©çš„ç‰¹æ€§ã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°å•é¡Œã‚’è§£æ±ºã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€æ¨™æº–ã®transformerã¨åŒç­‰ã®æ€§èƒ½ã‚’ä¿ã¡ãªãŒã‚‰ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’10%æœªæº€ã«æŠ‘ãˆã‚‹ã“ã¨ãŒã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview: 


<a href="https://openreview.net/forum?id=HHbRxoDTxE" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=HHbRxoDTxE</a>


</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ActivationFunction.html" target="_blank" rel="noopener noreferrer">#ActivationFunction</a>
<span class="issue_date">Issue Date: 2025-08-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2538" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Polynomial Composition Activations: Unleashing the Dynamics of Large  Language Models, Zhijian Zhuo+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- æ–°ã—ã„å¤šé …å¼åˆæˆæ´»æ€§åŒ–é–¢æ•°ï¼ˆPolyComï¼‰ã‚’ææ¡ˆã—ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’æœ€é©åŒ–ã€‚PolyComã¯ä»–ã®æ´»æ€§åŒ–é–¢æ•°ã‚ˆã‚Šã‚‚é«˜ã„è¡¨ç¾åŠ›ã‚’æŒã¡ã€æœ€é©è¿‘ä¼¼ç‡ã‚’é”æˆã€‚å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€å¾“æ¥ã®æ´»æ€§åŒ–é–¢æ•°ã‚’PolyComã«ç½®ãæ›ãˆã‚‹ã“ã¨ã§ã€ç²¾åº¦ã¨åæŸç‡ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã€‚å®Ÿé¨“çµæœã¯ä»–ã®æ´»æ€§åŒ–é–¢æ•°ã«å¯¾ã—ã¦å¤§å¹…ãªæ”¹å–„ã‚’ç¤ºã™ã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ä¸­ã€‚</span>
<span class="snippet"><span>Comment</span><p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1311" target="_blank" rel="noopener noreferrer">GLU Variants Improve Transformer, Noam Shazeer, N/A, arXiv'20</a>
 </p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<a class="button" href="articles/Self-SupervisedLearning.html" target="_blank" rel="noopener noreferrer">#Self-SupervisedLearning</a>
<a class="button" href="articles/TMLR.html" target="_blank" rel="noopener noreferrer">#TMLR</a>
<span class="issue_date">Issue Date: 2025-04-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1884" target="_blank" rel="noopener noreferrer" class="title-link">DINOv2: Learning Robust Visual Features without Supervision, Maxime Oquab+, TMLR'24</a>
<span class="snippet"><span>GPT Summary</span>- è‡ªå·±æ•™å¸«ã‚ã‚Šæ‰‹æ³•ã‚’ç”¨ã„ã¦ã€å¤šæ§˜ãªã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ±ç”¨çš„ãªè¦–è¦šç‰¹å¾´ã‚’ç”Ÿæˆã™ã‚‹æ–°ã—ã„äº‹å‰å­¦ç¿’æ‰‹æ³•ã‚’ææ¡ˆã€‚1Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ViTãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€å°å‹ãƒ¢ãƒ‡ãƒ«ã«è’¸ç•™ã™ã‚‹ã“ã¨ã§ã€OpenCLIPã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’é”æˆã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2025-04-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1864" target="_blank" rel="noopener noreferrer" class="title-link">Flex Attention: A Programming Model for Generating Optimized Attention  Kernels, Juechu Dong+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- FlexAttentionã¯ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®æ–°ã—ã„ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©é§†å‹•å‹ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã§ã€æ•°è¡Œã®PyTorchã‚³ãƒ¼ãƒ‰ã§å¤šãã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒªã‚¢ãƒ³ãƒˆã‚’å®Ÿè£…å¯èƒ½ã«ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ—¢å­˜ã®ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒªã‚¢ãƒ³ãƒˆã‚’åŠ¹ç‡çš„ã«å®Ÿè£…ã—ã€ç«¶äº‰åŠ›ã®ã‚ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã€‚FlexAttentionã¯ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒãƒªã‚¢ãƒ³ãƒˆã®çµ„ã¿åˆã‚ã›ã‚’å®¹æ˜“ã«ã—ã€çµ„ã¿åˆã‚ã›çˆ†ç™ºã®å•é¡Œã‚’è§£æ±ºã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1863" target="_blank" rel="noopener noreferrer">Llama 4 Series, Meta, 2025.04</a>
<br><br>ã§åˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹Attention</p>
<p>pytochã«ã‚ˆã‚‹è§£èª¬:


<a href="https://pytorch.org/blog/flexattention/" target="_blank" rel="noopener noreferrer">https://pytorch.org/blog/flexattention/</a>


<br><br>- Flex Attentionã¯ã‚ªãƒªã‚¸ãƒŠãƒ«ã®Attentionã®QK/sqrt(d_k)ã®è¨ˆç®—å¾Œã«ãƒ¦ãƒ¼ã‚¶ãŒå®šç¾©ã—ãŸé–¢æ•°score_modã‚’é©ç”¨ã™ã‚‹<br>- score_modã‚’å®šç¾©ã™ã‚‹ã“ã¨ã§ã€attention scoreã‚’softmaxã‚’ã‹ã‘ã‚‹ã¾ãˆã«é–¢æ•°ã«ã‚ˆã£ã¦èª¿æ•´ã§ãã‚‹<br>- å¤šãã®attentionã®äºœç¨®ã¯ã»ã¨ã‚“ã©ã®å ´åˆã“ã®æŠ½è±¡åŒ–ã§å¯¾å¿œã§ãã‚‹<br>- score_modã¯QK tokenã®å†…ç©ã«å¯¾å¿œã™ã‚‹ã®ã§ã€QKã®æƒ…å ±ã‚’å—ã‘å–ã‚Šã€ã‚¹ã‚«ãƒ©ãƒ¼å€¤ã‚’è¿”ã›ã°ãªã‚“ã§ã‚‚è‰¯ã„<br>  - score_modã®å®Ÿè£…ä¾‹ã¯å…ƒãƒªãƒ³ã‚¯å‚ç…§<br>- FA2ã¨æ¯”è¼ƒã—ã¦ï¼ˆç¾åœ¨ã®pytorchã§ã®å®Ÿè£…ä¸Šã¯ï¼‰Forward Passã¯90%, Backward Passã¯85%ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã§ã€å°‘ã—é…ã„ãŒä»Šå¾Œæ”¹å–„äºˆå®š</p>
<p>å…ƒè«–æ–‡ã‚ˆã‚Šå¼•ç”¨ã€‚éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ã§ã€æ•°å¼ä¸Šã¯ä¸‹è¨˜ã®ã‚ˆã†ã«è¡¨ã•ã‚Œã‚‹:<br><img src="https://github.com/user-attachments/assets/b4a393f0-46a9-46c6-9a47-0402ba58fb11" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<span class="issue_date">Issue Date: 2024-12-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1584" target="_blank" rel="noopener noreferrer" class="title-link">Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale   Prediction, Keyu Tian+, NeurIPS'24</a>
<span class="snippet"><span>GPT Summary</span>- Visual AutoRegressive modeling (VAR)ã‚’ææ¡ˆã—ã€ç”»åƒç”Ÿæˆã«ãŠã„ã¦è‡ªå·±å›å¸°å­¦ç¿’ã‚’æ¬¡ã®ã‚¹ã‚±ãƒ¼ãƒ«äºˆæ¸¬ã¨ã—ã¦å†å®šç¾©ã€‚VARã¯ã€GPTã®ã‚ˆã†ãªARãƒ¢ãƒ‡ãƒ«ãŒæ‹¡æ•£ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’å®Ÿç¾ã—ã€ImageNet 256x256ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§FIDã‚’18.65ã‹ã‚‰1.73ã€ISã‚’80.4ã‹ã‚‰350.2ã«æ”¹å–„ã€‚æ¨è«–é€Ÿåº¦ã¯ç´„20å€å‘ä¸Šã—ã€ç”»åƒå“è³ªã‚„ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã§ã‚‚å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã€‚VARã¯ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆä¸€èˆ¬åŒ–èƒ½åŠ›ã‚’æŒã¡ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã‚’ç¤ºã™ã€‚å…¨ãƒ¢ãƒ‡ãƒ«ã¨ã‚³ãƒ¼ãƒ‰ã‚’å…¬é–‹ã—ã€è¦–è¦šç”Ÿæˆã®ç ”ç©¶ã‚’ä¿ƒé€²ã€‚</span>
<span class="snippet"><span>Comment</span><p>NeurIPS2024ã®ãƒ™ã‚¹ãƒˆãƒšãƒ¼ãƒ‘ãƒ¼</p>
<p>ç¬¬ä¸€è‘—è€…ãŒByteDanceç¤¾ã‹ã‚‰è¨´è¨Ÿã‚’èµ·ã“ã•ã‚Œã¦ã„ã‚‹æ¨¡æ§˜â€¦ï¼Ÿ<br>


<a href="https://var-integrity-report.github.io" target="_blank" rel="noopener noreferrer">https://var-integrity-report.github.io</a>


</p>
<p>OpenReview:


<a href="https://openreview.net/forum?id=gojL67CfS8" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=gojL67CfS8</a>


</p>
<p>Next Token Prediction, Next Image Token Generation (å¾“æ¥æ‰‹æ³•ï¼‰, Next Scale (resolution) prediction (ææ¡ˆæ‰‹æ³•)ã®é•ã„ã®å›³è§£ã€‚éå¸¸ã«åˆ†ã‹ã‚Šã‚„ã™ã„ã€‚next token predictionã§ã¯æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã‚’äºˆæ¸¬ã™ã‚‹ãŒVARã§ã¯ã€æ¬¡ã®è§£åƒåº¦ç”»åƒã®å…¨ä½“ã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒãƒƒãƒ—ã‚’äºˆæ¸¬ã™ã‚‹ã€‚<br><br><img src="https://github.com/user-attachments/assets/668d7523-f262-45c1-a1d0-2dd479c0a708" alt="image" loading="lazy"><br><br>å­¦ç¿’æ–¹æ³•ã®æ¦‚è¦ã€‚2-Stageã§å­¦ç¿’ã•ã‚Œã‚‹ã€‚æœ€åˆã®ã‚¹ãƒ†ãƒ¼ã‚¸ã§Kç¨®é¡ã®è§£åƒåº¦ã®ç”»åƒï¼ˆï¼Kç¨®é¡ã®ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«ã®token maps r_kï¼‰ã‚’å¾—ã‚‹ãŸã‚ã«AutoEncoderã‚’å­¦ç¿’ã—ã€æ¬¡ã®ã‚¹ãƒ†ãƒ¼ã‚¸ã§block-wiseã®causal attention maskã‚’ç”¨ã„ã¦ã€K_&lt;kå€‹ç›®ã®è§£åƒåº¦ã®ç”»åƒã‹ã‚‰Kå€‹ç›®ã®è§£åƒåº¦ã®ç”»åƒã‚’äºˆæ¸¬ã™ã‚‹ï¼ˆå›³ã‚’è¦‹ã‚‹ã¨ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’æ´ã¿ã‚„ã™ã„ï¼‰ã€‚inferenceæ™‚ã¯KV Cacheã‚’åˆ©ç”¨ã—ã€maskã¯ä¸è¦ã¨ãªã‚‹ã€‚<br>å„r_kã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹éš›ã«r_&lt;kã®ã¿ã«ä¾å­˜ã™ã‚‹è¨­è¨ˆã«ã™ã‚‹ã“ã¨ã§coase-to-fineã«ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã«ç›¸å½“ã—ã€ã“ã‚Œã¯äººé–“ã®ç²—ãæ‰ãˆã¦ã‹ã‚‰è©³ç´°ã‚’è¦‹ã‚‹èªçŸ¥ãƒ—ãƒ­ã‚»ã‚¹ã¨åˆè‡´ã™ã‚‹ã€‚ã¾ãŸã€flattenæ“ä½œãŒå­˜åœ¨ã›ãšã€ãã‚Œãã‚Œã®r_&lt;kå†…ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒr_kç”Ÿæˆæ™‚ã«å…¨ã¦è€ƒæ…®ã•ã‚Œã‚‹ãŸã‚ç©ºé–“çš„å±€æ‰€æ€§ã‚‚æ‹…ä¿ã•ã‚Œã‚‹ã€‚ã¾ãŸã€r_kå†…ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯ä¸¦åˆ—ã«ç”Ÿæˆå¯èƒ½ãªã®ã§è¨ˆç®—é‡ã®ã‚ªãƒ¼ãƒ€ãƒ¼ãŒå¤§å¹…ã«å‰Šæ¸›ã•ã‚Œã‚‹ï¼ˆO(n^4)ã€‚<br><img src="https://github.com/user-attachments/assets/e1a85712-e66a-4c9a-9cf1-6556f2b8e687" alt="image" loading="lazy"><br><br>å¾“æ¥æ‰‹æ³•ã¨æ¯”ã¹ã‚ˆã‚Šå°ã•ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§é«˜ã„æ€§èƒ½ã‚’å®Ÿç¾ã—ã€inference timeã‚‚éå¸¸ã«æ—©ã„ã€‚<br><img src="https://github.com/user-attachments/assets/90a6a7de-995d-49e6-94a2-cd709e68777f" alt="image" loading="lazy"><br><br>ScalingLawsã‚‚æˆç«‹ã™ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/351c2a7b-85aa-4cc7-8ba2-a5e9528cabd4" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2024-11-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1523" target="_blank" rel="noopener noreferrer" class="title-link">Understanding LLMs: A Comprehensive Overview from Training to Inference, Yiheng Liu+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- ChatGPTã®æ™®åŠã«ä¼´ã„ã€LLMsã®ã‚³ã‚¹ãƒˆåŠ¹ç‡ã®è‰¯ã„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã¸ã®é–¢å¿ƒãŒé«˜ã¾ã£ã¦ã„ã‚‹ã€‚æœ¬è«–æ–‡ã§ã¯ã€LLMsã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æŠ€è¡“ã¨æ¨è«–ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆæŠ€è¡“ã®é€²åŒ–ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã€ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚„ãƒ¢ãƒ‡ãƒ«åœ§ç¸®ãªã©ã®ã•ã¾ã–ã¾ãªå´é¢ã‚’è­°è«–ã™ã‚‹ã€‚ã¾ãŸã€LLMsã®åˆ©ç”¨æ–¹æ³•ã¨å°†æ¥ã®ç™ºå±•ã«ã¤ã„ã¦ã®æ´å¯Ÿã‚‚æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>[Perplexityï¼ˆå‚è€ƒ;Hallucinationã«æ³¨æ„ï¼‰](


<a href="https://www.perplexity.ai/search/yi-xia-nolun-wen-wodu-minei-ro-7vGwDK_AQX.HDO7j9H8iNA)" target="_blank" rel="noopener noreferrer">https://www.perplexity.ai/search/yi-xia-nolun-wen-wodu-minei-ro-7vGwDK_AQX.HDO7j9H8iNA)</a>


</p>
<p>å˜ãªã‚‹LLMã®ç†è«–çš„ãªèª¬æ˜ã«ã¨ã©ã¾ã‚‰ãšã€å®Ÿç”¨çš„ã«å¿…è¦ãªå„ç¨®ä¸¦åˆ—å‡¦ç†æŠ€è¡“ã€Mixed Precisionã€Offloadingãªã©ã®ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚‚ã¾ã¨ã¾ã£ã¦ã„ã‚‹ã®ãŒã¨ã¦ã‚‚è‰¯ã„ã¨æ€ã†ã€‚</p>
<p>LLM Frameworkã®ã¨ã“ã‚ã«ã€ãƒ¡ã‚¸ãƒ£ãƒ¼ãªã‚‚ã®ãŒç¶²ç¾…ã•ã‚Œã¦ã„ãªã„ã‚ˆã†ã«æ„Ÿã˜ã‚‹ã€‚ãŸã¨ãˆã°ã€Unslothã‚„Liger-Kernelãªã©ã¯Transformersã®éƒ¨åˆ†ã§è¨€åŠã•ã‚Œã¦ã¦ã‚‚è‰¯ã„ã®ã§ã¯ã€ã¨æ„Ÿã˜ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<span class="issue_date">Issue Date: 2024-10-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1467" target="_blank" rel="noopener noreferrer" class="title-link">What Matters in Transformers? Not All Attention is Needed, Shwai He+, N_A, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼å†…ã®Blocksã€MLPã€Attentionå±¤é–“ã®å†—é•·æ€§ã‚’èª¿æŸ»ã—ã€Attentionå±¤ã®é«˜ã„é¡ä¼¼æ€§ã«ã‚ˆã‚Šãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã—ãŸã€‚å…·ä½“çš„ã«ã¯ã€Llama-2-70Bã§ã¯Attentionå±¤ã®åŠåˆ†ã‚’å‰Šé™¤ã™ã‚‹ã“ã¨ã§48.4%ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’é”æˆã—ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ã‚ãšã‹2.4%ä½ä¸‹ã—ã¾ã—ãŸã€‚ã¾ãŸã€Attentionå±¤ã¨MLPå±¤ã‚’åŒæ™‚ã«å‰Šé™¤ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ã€31å±¤å‰Šé™¤ã—ã¦ã‚‚Llama-2-13Bã¯90%ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã—ã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ä»Šå¾Œã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆã«è²´é‡ãªæ´å¯Ÿã‚’æä¾›ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>é€šå¸¸LLMã¯transformer decoderã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’stackã™ã‚‹ã“ã¨ã§å½¢æˆã•ã‚Œã‚‹ãŒã€ç©ã¿ä¸Šã’ãŸãƒ–ãƒ­ãƒƒã‚¯ã€ã‚ã‚‹ã„ã¯layerã£ã¦ã»ã‚“ã¨ã«å…¨éƒ¨å¿…è¦ãªã®?ã¨ã„ã†ç–‘å•ã«ç­”ãˆã¦ãã‚Œã‚‹è«–æ–‡ã®ã‚ˆã†ã§ã‚ã‚‹ã€‚<br><br>transformer blockãã®ã‚‚ã®ã€ã‚ã‚‹ã„ã¯MLP layerã‚’å‰Šé™¤ã™ã‚‹ã¨peformanceã¯å¤§å¹…ã«ä½ä¸‹ã™ã‚‹ãŒã€attention layerã‚’å‰Šé™¤ã—ã¦ã‚‚performanceã®ä½ä¸‹ãŒèµ·ããªã‹ã£ãŸæ¨¡æ§˜ã€‚ã“ã‚Œã«ã‚ˆã‚Šé«˜é€ŸåŒ–ãŒå®Ÿç¾å¯èƒ½ã€‚<br><br>å‰Šé™¤ã™ã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã‚„layerã¯inputã¨outputã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ãŒé«˜ã„ã‚‚ã®ã‚’å‰Šé™¤ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦å®Ÿç¾ã€‚<br><br><img src="https://github.com/user-attachments/assets/da1e6a56-1bc4-4206-9423-acd7512300c8" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/724ddf50-cd63-437d-9df2-73423dd77a6e" alt="image" loading="lazy"><br><br>æ¯”è¼ƒçš„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå°ã•ã„7B, 13Bãƒ¢ãƒ‡ãƒ«ã§ã®å®Ÿé¨“çµæœ<br><img src="https://github.com/user-attachments/assets/19253c9e-7eae-4084-a8c2-e99680b34649" alt="image" loading="lazy"><br><br>ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã§ã®å®Ÿé¨“çµæœ<br><img src="https://github.com/user-attachments/assets/18eef07e-623c-482c-9a6b-9ea65450ecea" alt="image" loading="lazy"></p>
<p>ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå¤‰ã‚ã‚‰ãªã„ç¯„å›²ã ã¨ã€attention layer dropã«ã‚ˆã‚Šã€7B, 13Bãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯23%ç¨‹åº¦ã€70Bã®å ´åˆã¯35%ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š</p></span><br><br>
<a class="button" href="articles/RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/TransferLearning.html" target="_blank" rel="noopener noreferrer">#TransferLearning</a>
<span class="issue_date">Issue Date: 2024-09-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1421" target="_blank" rel="noopener noreferrer" class="title-link">beeFormer: Bridging the Gap Between Semantic and Interaction Similarity   in Recommender Systems, VojtÄ›ch VanÄura+, N_A, RecSys'24</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã„ã¦ã€ã‚³ãƒ¼ãƒ«ãƒ‰ã‚¹ã‚¿ãƒ¼ãƒˆã‚„ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã‚·ãƒŠãƒªã‚ªã§ã®äºˆæ¸¬æ”¹å–„ã®ãŸã‚ã«ã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’æ´»ç”¨ã—ãŸæ–‡ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã€ŒbeeFormerã€ã‚’ææ¡ˆã€‚beeFormerã¯ã€æ„å‘³çš„é¡ä¼¼æ€§ã®äºˆæ¸¬ã«ãŠã„ã¦å¾“æ¥ã®æ‰‹æ³•ã‚’ä¸Šå›ã‚Šã€ç•°ãªã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–“ã§çŸ¥è­˜ã‚’è»¢é€å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ‰ãƒ¡ã‚¤ãƒ³ã«ä¾å­˜ã—ãªã„ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¾ã®ãƒã‚¤ãƒ‹ãƒ³ã‚°ãŒå¯èƒ½ã«ãªã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>NLPã§ã¯è¨€èªã¨ã„ã†å…±é€šã®ä½“ç³»ãŒã‚ã‚‹ã‹ã‚‰äº‹å‰å­¦ç¿’ã¨ã‹ãŒæˆç«‹ã™ã‚‹ã‘ã©ã€RecSysã®ã‚ˆã†ãªãƒ¦ãƒ¼ã‚¶ã¨ã‚·ã‚¹ãƒ†ãƒ ã®interaction dataã‚’ç”¨ã„ãŸã‚·ã‚¹ãƒ†ãƒ ã§ã¯ï¼ˆå¤§æŠµã®å ´åˆã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã”ã¨ã«ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªãƒ¦ãƒ¼ã‚¶IDã¨ã‚¢ã‚¤ãƒ†ãƒ IDã®ãƒ­ã‚°ã§ãƒ‡ãƒ¼ã‚¿ãŒæ§‹æˆã•ã‚Œã‚‹ã®ã§ï¼‰ãªã‹ãªã‹ãã†ã„ã†ã“ã¨ã¯é›£ã—ã„ã‚ˆã­ã€ã¨æ€ã£ã¦ã„ãŸã€‚ãŒã€ã‚‚ã—RecSysã®ã‚¿ã‚¹ã‚¯è¨­å®šã§ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé–“ã®è»¢ç§»å­¦ç¿’ã‚’å®Ÿç¾ã§ãã‚‹ã®ã ã¨ã—ãŸã‚‰ã©ã®ã‚ˆã†ã«å®Ÿç¾ã—ã¦ãã‚‹ã®ã ã‚ã†ã‹?èˆˆå‘³æ·±ã„ã€‚å¾Œã§èª­ã‚€ã€‚</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2024-07-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1338" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] FlashAttention-3: Fast and Accurate Attention with Asynchrony and   Low-precision, Jay Shah+, NeurIPS'24</a>
<span class="snippet"><span>GPT Summary</span>- FlashAttention-3ã¯ã€Hopper GPUä¸Šã§Attentionã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã«ã€3ã¤ã®æŠ€è¡“ã‚’é–‹ç™ºã—ã€H100 GPUã§1.5-2.0å€ã®é€Ÿåº¦å‘ä¸Šã‚’å®Ÿç¾ã€‚FP16ã§740 TFLOPs/sã€FP8ã§ç´„1.2 PFLOPs/sã«é”ã—ã€FP8ã§ã¯æ•°å€¤èª¤å·®ãŒ2.6å€ä½ã„ã“ã¨ã‚’ç¢ºèªã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=tVConYid20&referrer=%5Bthe%20profile%20of%20Tri%20Dao%5D(%2Fprofile%3Fid%3D~Tri_Dao1)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=tVConYid20&referrer=%5Bthe%20profile%20of%20Tri%20Dao%5D(%2Fprofile%3Fid%3D~Tri_Dao1)</a>


</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1273" target="_blank" rel="noopener noreferrer" class="title-link">Mixture-of-Depths: Dynamically allocating compute in transformer-based  language models, David Raposo+, N_A, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- Transformerãƒ™ãƒ¼ã‚¹ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã¯ã€å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å…¨ä½“ã«å‡ç­‰ã«FLOPsã‚’åˆ†æ•£ã•ã›ã‚‹ä»£ã‚ã‚Šã«ã€ç‰¹å®šã®ä½ç½®ã«FLOPsã‚’å‹•çš„ã«å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã‚’å­¦ç¿’ã§ãã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®æ·±ã•ã«ã‚ãŸã£ã¦å‰²ã‚Šå½“ã¦ã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã«ã€ç•°ãªã‚‹ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§è¨ˆç®—ã‚’å‹•çš„ã«å‰²ã‚Šå½“ã¦ã‚‹ã€‚ã“ã®æ‰‹æ³•ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³ã®æ•°ã‚’åˆ¶é™ã™ã‚‹ã“ã¨ã§åˆè¨ˆè¨ˆç®—äºˆç®—ã‚’å¼·åˆ¶ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³ã¯top-kãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ä½¿ç”¨ã—ã¦æ±ºå®šã•ã‚Œã‚‹ã€‚ã“ã®æ–¹æ³•ã«ã‚ˆã‚Šã€FLOPsã‚’å‡ç­‰ã«æ¶ˆè²»ã—ã¤ã¤ã€è¨ˆç®—ã®æ”¯å‡ºãŒäºˆæ¸¬å¯èƒ½ã§ã‚ã‚Šã€å‹•çš„ã‹ã¤ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«æ•æ„Ÿã§ã‚ã‚‹ã€‚ã“ã®ã‚ˆã†ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã€è¨ˆç®—ã‚’å‹•çš„ã«å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã‚’å­¦ç¿’ã—ã€åŠ¹ç‡çš„ã«è¡Œã†ã“ã¨ãŒã§ãã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å‚è€ƒ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theseamouse/status/1775782800362242157?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span></strong></p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1270" target="_blank" rel="noopener noreferrer" class="title-link">Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference, Piotr Nawrot+, N_A, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®ç”ŸæˆåŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€Dynamic Memory Compressionï¼ˆDMCï¼‰ãŒææ¡ˆã•ã‚ŒãŸã€‚DMCã¯ã€ç•°ãªã‚‹ãƒ˜ãƒƒãƒ‰ã¨ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ç•°ãªã‚‹åœ§ç¸®ç‡ã‚’é©ç”¨ã™ã‚‹æ–¹æ³•ã‚’å­¦ç¿’ã—ã€äº‹å‰å­¦ç¿’æ¸ˆã¿LLMsã«é©ç”¨ã•ã‚Œã‚‹ã€‚DMCã¯ã€å…ƒã®ä¸‹æµãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æœ€å¤§4å€ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥åœ§ç¸®ã§ç¶­æŒã—ã¤ã¤ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚DMCã¯ã€GQAã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã•ã‚‰ãªã‚‹åˆ©ç›Šã‚’ã‚‚ãŸã‚‰ã™å¯èƒ½æ€§ãŒã‚ã‚Šã€é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã¨å¤§ããªãƒãƒƒãƒã‚’å‡¦ç†ã™ã‚‹éš›ã«æœ‰ç”¨ã§ã‚ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å‚è€ƒ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1776755029581676943?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è«–æ–‡ä¸­ã®Figure1ãŒéå¸¸ã«ã‚ã‹ã‚Šã‚„ã™ã„ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/d416547e-f9ca-4c6c-8ebb-7d164bef5283" alt="image" loading="lazy"><br><br></p>
<p>GQA <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1271" target="_blank" rel="noopener noreferrer">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head
  Checkpoints, Joshua Ainslie+, N/A, arXiv'23</a>
 ã¨æ¯”è¼ƒã—ã¦ã€2~4å€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åœ§ç¸®ã—ã¤ã¤ã€ã‚ˆã‚Šé«˜ã„æ€§èƒ½ã‚’å®Ÿç¾ã€‚70Bãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯ã€GQAã§8å€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åœ§ç¸®ã—ãŸä¸Šã§ã€DMCã§è¿½åŠ ã§2å€åœ§ç¸®ã‚’ã‹ã‘ãŸã¨ã“ã‚ã€åŒç­‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®Ÿç¾ã—ã¦ã„ã‚‹ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/7b131f07-5eab-4830-88cc-5f6fd0508958" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<span class="issue_date">Issue Date: 2024-01-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1210" target="_blank" rel="noopener noreferrer" class="title-link">Transformers are Multi-State RNNs, Matanel Oren+, N_A, EMNLP'24</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã¯ç„¡é™ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ãƒˆRNNã¨ã—ã¦æ¦‚å¿µåŒ–ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã€æœ‰é™ã®ãƒãƒ«ãƒã‚¹ãƒ†ãƒ¼ãƒˆRNNã«å¤‰æ›ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚ã•ã‚‰ã«ã€æ–°ã—ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥åœ§ç¸®ãƒãƒªã‚·ãƒ¼ã§ã‚ã‚‹TOVAã‚’å°å…¥ã—ã€ä»–ã®ãƒãƒªã‚·ãƒ¼ã‚ˆã‚Šã‚‚å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ã“ã¨ã‚’å®Ÿé¨“çµæœã§ç¤ºã—ã¾ã—ãŸã€‚TOVAã¯å…ƒã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºã®1/8ã—ã‹ä½¿ç”¨ã›ãšã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼LLMãŒå®Ÿéš›ã«ã¯RNNã¨ã—ã¦æŒ¯ã‚‹èˆã†ã“ã¨ãŒå¤šã„ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>Transformerã¯RNNã¨ã¯ç•°ãªã‚‹æ¦‚å¿µã€ç‰¹ã«å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®æƒ…å ±ã«ç›´æ¥ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã¨ã„ã†ã“ã¨ã§åŒºåˆ¥ã•ã‚Œã¦ããŸãŒã€ã‚ˆãã‚ˆãè€ƒãˆã¦ã¿ã‚‹ã¨ã€Transformer Decoderã¯ã€RNNã®hidden_states h ã‚’ï¼ˆhã¯1ã¤ã®stateã‚’ãƒ™ã‚¯ãƒˆãƒ«ã§è¡¨ã—ã¦ã„ã‚‹ï¼‰ã€multi-stateã‚’è¡¨ã™ matrix H ï¼ˆtå€‹ã®stateã‚’è¡¨ã™matrix; tã¯ç¾åœ¨ã®ç€ç›®ã—ã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§ã®sequenceã®é•·ã•ï¼‰ã§ç½®ãæ›ãˆãŸã‚‚ã® Multi-State-RNN (MSRNN) ã¨è§£é‡ˆã§ãã‚‹ã€ã¨ã„ã†è©±ã€‚<br>ã¾ãŸã€window attentionãªã©ã®attentionã®è¨ˆç®—ã§è€ƒæ…®ã™ã‚‹KV cacheã®ã‚¹ãƒ‘ãƒ³ã‚’ï¼ˆãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„ã™ã‚‹ãŸã‚ã«ï¼‰åˆ¶é™ã™ã‚‹åœ§ç¸®æ‰‹æ³•ã¯ã€å…ˆã»ã©ã®MSRNNã¯å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®state ï¼ˆKV Cacheï¼‰ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ï¼ˆ= Unboundedï¼‰ã¨è€ƒãˆã‚‹ã¨ã€ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®stateãŒ k (&lt;t) ã¨ãªã‚‹ãŸã‚ã€BoundedãªMSRNNã¨ã¿ãªã›ã‚‹ã€‚<br>ã—ãŸãŒã£ã¦ã€ç¾åœ¨ã®LLMã¯Transformer Decoderã‚’ç©ã¿ä¸Šã’ãŸã‚‚ã®ã§ã‚ã‚‹ã‚‚ã®ã§ã‚ã‚Šã€åŸç†ä¸Šã¯inference/trainingæ™‚ã«å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è€ƒæ…®ã§ãã‚‹ãŸã‚ã€åŸç†ä¸Šã¯UnboundedãªMSRNNã¨ã¿ãªã›ã‚‹ã€‚ä¸€æ–¹ã€ã“ã“ã«ãƒ¡ãƒ¢ãƒªã®åˆ¶ç´„ãŒåŠ ã‚ã‚‹ã¨KV Cacheã‚’åœ§ç¸®ã—ãªã‘ã‚Œã°ãªã‚‰ãªã„ãŸã‚ã€å®Ÿç”¨ä¸Šã¯BoundedãªMSRNNã¨ãªã£ã¦ã„ã‚‹ã€‚<br><br>&lt;img width="476" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/292bd370-0138-441a-a626-ee73cb2f31b5"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/292bd370-0138-441a-a626-ee73cb2f31b5"&lt;/a&gt;


/&gt;<br><br>å®Ÿéš›ã«å¼ã§è¡¨ã™ã¨ä»¥ä¸‹ã®ã‚ˆã†ã«RNNã¨Transformerã¯å¯¾å¿œã¥ã‘ã‚‰ã‚Œã‚‹ã€‚<br>&lt;img width="402" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/3b2cbadc-e6ef-4465-ac78-bb4ff71351f2"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/3b2cbadc-e6ef-4465-ac78-bb4ff71351f2"&lt;/a&gt;


/&gt;<br>&lt;img width="487" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/18a99f2a-06dc-472c-b50d-743b820904f3"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/18a99f2a-06dc-472c-b50d-743b820904f3"&lt;/a&gt;


/&gt;<br><br>ã“ã®ã“ã¨ã‚’è€ƒæ…®ã—ã¦ã€æœ¬ç ”ç©¶ã§ã¯TOVAã¨å‘¼ã°ã‚Œã‚‹æ–°ã—ã„KV Cacheã®åœ§ç¸®æ‰‹æ³•ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ãªæ‰‹æ³•ã§ã€KV CacheãŒãƒ¡ãƒ¢ãƒªã®ä¸Šé™ã«åˆ°é”ã—ãŸã¨ãã«ã€ãã®éš›ã«attention scoreãŒæœ€ã‚‚å°ã•ã„ãƒˆãƒ¼ã‚¯ãƒ³ã®KV Cacheã‚’æ¨ã¦ã‚‹ã€ã¨ã„ã†æ‰‹æ³•ã§ã‚ã‚‹ã€‚<br>&lt;img width="495" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/09f19caf-bcea-42f7-b1e0-8bc3ea8a2e4c"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/09f19caf-bcea-42f7-b1e0-8bc3ea8a2e4c"&lt;/a&gt;


/&gt;<br><br>TOVAã‚’window attentionãªã©ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã‚ªãƒ©ã‚¯ãƒ«ã¨ã—ã¦full attentionã¨æ¯”è¼ƒã€‚ã‚¿ã‚¹ã‚¯ã¯ Language Modelingï¼ˆPG-19ãƒ‡ãƒ¼ã‚¿ã«ãŠã‘ã‚‹Perplexityï¼‰ã€Language Understanding ï¼ˆlong contextã‹ã‚‰relevantãªæƒ…å ±ã‚’æ‹¾ã†å¿…è¦ãŒã‚ã‚‹QAï¼‰ã€Story Generationï¼ˆé•·æ–‡ã®ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’æ›¸ã‹ã›ã¦GPT4ã«ã‚ˆã£ã¦pair-wiseã§ç”Ÿæˆã•ã‚ŒãŸã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã®å“è³ªã‚’LLM-as-a-Judgeã•ã›ã‚‹ï¼‰ã‚’åˆ©ç”¨ã€‚æ—¢å­˜ã®KV Cacheåœ§ç¸®æ‰‹æ³•ã‚ˆã‚Šã‚‚åŠ¹ç‡çš„ã«KV Cacheã‚’åœ§ç¸®ã§ãã€4096 context windowã®å ´åˆã¯ã€512ç¨‹åº¦ã§full attentionã¨è¿‘ã„æ€§èƒ½ã‚’ç¤ºã™ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é«˜ã„ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’å®Ÿç¾ã§ãã‚‹ã€‚ã“ã“ã§ã€ã‚°ãƒ©ãƒ•ã®xè»¸ã®multistateã¯TOVAã«ãŠã„ã¦ã¯matrix Hã§ä¿æŒã™ã‚‹stateæ•°ã«ç›¸å½“ã—ã€window attentionã§ã¯ã€window sizeã«ç›¸å½“ã™ã‚‹ã€‚<br>&lt;img width="815" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/fd39d465-3a0d-49ad-951b-fddb115394f3"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/fd39d465-3a0d-49ad-951b-fddb115394f3"&lt;/a&gt;


/&gt;<br><br>&lt;img width="636" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/c66bade9-f0c4-476b-8243-bd1d88e21ead"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/c66bade9-f0c4-476b-8243-bd1d88e21ead"&lt;/a&gt;


/&gt;</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/InstructionTuning.html" target="_blank" rel="noopener noreferrer">#InstructionTuning</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<a class="button" href="articles/Robotics.html" target="_blank" rel="noopener noreferrer">#Robotics</a>
<a class="button" href="articles/UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<a class="button" href="articles/EmbodiedAI.html" target="_blank" rel="noopener noreferrer">#EmbodiedAI</a>
<span class="issue_date">Issue Date: 2023-12-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1202" target="_blank" rel="noopener noreferrer" class="title-link">Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision,   Language, Audio, and Action, Jiasen Lu+, N_A, CVPR'24</a>
<span class="snippet"><span>GPT Summary</span>- Unified-IO 2ã¯ã€æœ€åˆã®è‡ªå·±å›å¸°å‹ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€ç”»åƒã€ãƒ†ã‚­ã‚¹ãƒˆã€éŸ³å£°ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç†è§£ã—ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’çµ±ä¸€ã™ã‚‹ãŸã‚ã«ã€å…±æœ‰ã®æ„å‘³ç©ºé–“ã«å…¥åŠ›ã¨å‡ºåŠ›ã‚’é…ç½®ã—ã€å˜ä¸€ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ»ãƒ‡ã‚³ãƒ¼ãƒ€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã§å‡¦ç†ã—ã¾ã™ã€‚ã•ã¾ã–ã¾ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ”¹å–„ã‚’ææ¡ˆã—ã€å¤§è¦æ¨¡ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªäº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚Unified-IO 2ã¯ã€GRITãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å«ã‚€35ä»¥ä¸Šã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>ç”»åƒã€ãƒ†ã‚­ã‚¹ãƒˆã€éŸ³å£°ã€ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç†è§£ã§ãã‚‹åˆã‚ã¦ã®autoregressive modelã€‚AllenAI</p>
<p>ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³<br><img src="https://github.com/user-attachments/assets/4282ffb0-18f1-40c9-b6d7-f004d03b8382" alt="image" loading="lazy"><br><br>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã«æ‹¡å¼µã—ãŸã“ã¨ã§ã€è¨“ç·´ãŒéå¸¸ã«ä¸å®‰å®šã«ãªã£ãŸãŸã‚ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä¸Šã§ã„ãã¤ã‹ã®å·¥å¤«ã‚’åŠ ãˆã¦ã„ã‚‹:<br><br>- 2D Rotary Embedding<br>  - Positional Encodingã¨ã—ã¦RoPEã‚’æ¡ç”¨<br>  - ç”»åƒã®ã‚ˆã†ãª2æ¬¡å…ƒãƒ‡ãƒ¼ã‚¿ã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®å ´åˆã¯RoPEã‚’2æ¬¡å…ƒã«æ‹¡å¼µã™ã‚‹ã€‚å…·ä½“çš„ã«ã¯ã€ä½ç½®(i, j)ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ã¤ã„ã¦ã¯ã€Q, Kã®embeddingã‚’åŠåˆ†ã«åˆ†å‰²ã—ã¦ã€ãã‚Œãã‚Œã«å¯¾ã—ã¦ç‹¬ç«‹ã«i, jã®RoPE Embeddingã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§i, jåŒæ–¹ã®æƒ…å ±ã‚’çµ„ã¿è¾¼ã‚€ã€‚<br>- QK Normalization<br>  - image, audioã®ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§MHAã®logitsãŒéå¸¸ã«å¤§ãããªã‚Šatteetion weightãŒ0/1ã®æ¥µç«¯ãªå€¤ã‚’ã¨ã‚‹ã‚ˆã†ã«ãªã‚Šè¨“ç·´ã®ä¸å®‰å®šã•ã«ã¤ãªãŒã£ãŸã€‚ã“ã®ãŸã‚ã€dot product attentionã‚’é©ç”¨ã™ã‚‹å‰ã«LayerNormã‚’çµ„ã¿è¾¼ã‚“ã ã€‚<br>- Scaled Cosine Attention<br>  - Image Historyãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«ãŠã„ã¦å›ºå®šé•·ã®Embeddingã‚’å¾—ã‚‹ãŸã‚ã«Perceiver Resamplerã‚’æ‰±ã£ãŸã¦ã„ã‚‹ãŒã€ã“ã¡ã‚‰ã‚‚ä¸Šè¨˜ã¨åŒæ§˜ã«Attentionã®logitsãŒæ¥µç«¯ã«å¤§ãããªã£ãŸãŸã‚ã€cosineé¡ä¼¼åº¦ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸScaled Cosine Attention <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2259" target="_blank" rel="noopener noreferrer">[Paper Note] Swin Transformer V2: Scaling Up Capacity and Resolution, Ze Liu+, arXiv'21</a>
 ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€å¤§å¹…ã«è¨“ç·´ã®å®‰å®šæ€§ãŒæ”¹å–„ã•ã‚ŒãŸã€‚<br>- ãã®ä»–<br>  - attention logitsã«ã¯fp32ã‚’é©ç”¨<br>  - äº‹å‰å­¦ç¿’ã•ã‚ŒãŸViTã¨ASTã‚’åŒæ™‚ã«æ›´æ–°ã™ã‚‹ã¨ä¸å®‰å®šã«ã¤ãªãŒã£ãŸãŸã‚ã€äº‹å‰å­¦ç¿’ã®æ®µéšã§ã¯freezeã—ã€instruction tuningã®æœ€å¾Œã«finetuningã‚’å®Ÿæ–½<br><br><img src="https://github.com/user-attachments/assets/74c8fa3a-8fb5-4785-8dd3-6a8cf3c7cfeb" alt="image" loading="lazy"></p>
<p>ç›®çš„é–¢æ•°ã¨ã—ã¦ã¯ã€Mixture of Denoisers (<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1424" target="_blank" rel="noopener noreferrer">UL2: Unifying Language Learning Paradigms, Yi Tay+, N/A, ICLR'23</a>
)ã«ç€æƒ³ã‚’å¾—ã¦ã€Multimodal Mixture of Denoisersã‚’ææ¡ˆã€‚MoDã§ã¯ã€<br>- \[R\]: é€šå¸¸ã®span corruption (1--5 tokenç¨‹åº¦ã®spanã‚’maskã™ã‚‹)<br>- \[S\]: causal language modeling (inputã‚’2ã¤ã®ã‚µãƒ–ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«åˆ†å‰²ã—ã€å‰æ–¹ã‹ã‚‰å¾Œæ–¹ã‚’äºˆæ¸¬ã™ã‚‹ã€‚å‰æ–¹éƒ¨åˆ†ã¯Bi-directionalã§ã‚‚å¯)<br>- \[X\]: extreme span corruption (12&gt;=tokenç¨‹åº¦ã®spanã‚’maskã™ã‚‹)<br><br>ã®3ç¨®é¡ãŒææ¡ˆã•ã‚Œã¦ãŠã‚Šã€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã”ã¨ã«ã“ã‚Œã‚‰ã‚’ä½¿ã„åˆ†ã‘ã‚‹:<br>- text modality: UL2 (<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1424" target="_blank" rel="noopener noreferrer">UL2: Unifying Language Learning Paradigms, Yi Tay+, N/A, ICLR'23</a>
)ã‚’è¸è¥²<br>- image, audioãŒtargetã®å ´åˆ: 2ã¤ã®é¡ä¼¼ã—ãŸãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’å®šç¾©ã—åˆ©ç”¨<br>  - \[R\]: patchã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«x%ãƒã‚¹ã‚¯ã—re-constructã™ã‚‹<br>  - \[S\]: inputã®targetã¨ã¯ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã®ã¿ã®æƒ…å ±ã‹ã‚‰ã€targetãƒ¢ãƒ€ãƒªãƒ†ã‚£ã‚’ç”Ÿæˆã™ã‚‹<br><br>è¨“ç·´æ™‚ã«ã¯ prefixã¨ã—ã¦modality token \[Text\], \[Image\], \[Audio\] ã¨paradigm token \[R\], \[S\], \[X\] ã‚’ã‚¿ã‚¹ã‚¯ã‚’æŒ‡ç¤ºã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦åˆ©ç”¨ã—ã¦ã„ã‚‹ã€‚</p>
<p>ã¾ãŸã€image, audioã®ãƒã‚¹ã‚¯éƒ¨åˆ†ã®denoisingã‚’autoregressive modelã§å®Ÿæ–½ã™ã‚‹éš›ã«ã¯æ™®é€šã«ã‚„ã‚‹ã¨decoderå´ã§ãƒªãƒ¼ã‚¯ãŒç™ºç”Ÿã™ã‚‹(a)ã€‚ã“ã‚Œã‚’é˜²ãã«ã¯ã€Encoderå´ã§ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã€Decoderå´ã§teacher-forcingã™ã‚‹éš›ã«ã®å…¨ã¦ãƒã‚¹ã‚¯ã™ã‚‹æ–¹æ³•(b)ãŒã‚ã‚‹ãŒã€ã“ã®å ´åˆã€ç”Ÿæˆã‚¿ã‚¹ã‚¯ã¨denoisingã‚¿ã‚¹ã‚¯ãŒç›¸äº’ã«å¹²æ¸‰ã—ã¦ã—ã¾ã„ã†ã¾ãå­¦ç¿’ã§ããªããªã£ã¦ã—ã¾ã†ï¼ˆç”Ÿæˆã‚¿ã‚¹ã‚¯ã§ã¯é€šå¸¸Decoderã®inputã¨ã—ã¦[mask]ãŒå…¥åŠ›ã•ã‚Œæ¬¡ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã™ã‚‹ã€ã¨ã„ã£ãŸã“ã¨ã¯èµ·ããˆãªã„ãŒã€æ„šç›´ã«(b)ã‚’ã‚„ã‚‹ã¨ãã†ãªã£ã¦ã—ã¾ã†ï¼‰ã€‚ã®ã§ã€(c)ã«ç¤ºã—ãŸã‚ˆã†ã«ã€ãƒã‚¹ã‚¯ã•ã‚Œã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’inputã¨ã—ã¦ç”Ÿæˆã—ãªã‘ã‚Œã°ãªã‚‰ãªã„æ™‚ã ã‘ã€ãƒã‚¹ã‚¯ã‚’è§£é™¤ã—ã¦decoderå´ã«inputã™ã‚‹ã€ã¨ã„ã†æ–¹æ³• (Dynamic Masking) ã§ã“ã®å•é¡Œã«å¯¾å‡¦ã—ã¦ã„ã‚‹ã€‚<br>&lt;img width="597" height="394" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/0dba8d5d-0c93-4c56-852b-fce9869428e7"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/0dba8d5d-0c93-4c56-852b-fce9869428e7"&lt;/a&gt;


/&gt;</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ImageSegmentation.html" target="_blank" rel="noopener noreferrer">#ImageSegmentation</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<span class="issue_date">Issue Date: 2023-04-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/600" target="_blank" rel="noopener noreferrer" class="title-link">Segment Anything in Medical Images, Jun Ma+, N_A, Nature Communications'24</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€è‡ªç„¶ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã«é©æ–°çš„ãªæ‰‹æ³•ã§ã‚ã‚‹Segment anything model (SAM)ã‚’åŒ»ç™‚ç”»åƒã«æ‹¡å¼µã™ã‚‹ãŸã‚ã®MedSAMã‚’ææ¡ˆã—ã€æ§˜ã€…ãªåŒ»ç™‚ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®æ±ç”¨ãƒ„ãƒ¼ãƒ«ã‚’ä½œæˆã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚MedSAMã¯ã€å¤§è¦æ¨¡ãªåŒ»ç™‚ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦é–‹ç™ºã•ã‚Œã€SAMã‚’ä¸€èˆ¬çš„ãªåŒ»ç™‚ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã«é©å¿œã™ã‚‹ãŸã‚ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã‚’é–‹ç™ºã—ã¾ã—ãŸã€‚21ã®3Dã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã¨9ã®2Dã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹åŒ…æ‹¬çš„ãªå®Ÿé¨“ã«ã‚ˆã‚Šã€MedSAMã¯ã€å¹³å‡Diceé¡ä¼¼ä¿‚æ•°ï¼ˆDSCï¼‰ãŒãã‚Œãã‚Œ22.5ï¼…ã¨17.6ï¼…ã§ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®SAMãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚ã‚³ãƒ¼ãƒ‰ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¯ã€\url{https://github.com/bowang-lab/MedSAM}ã§å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>SAMã®æ€§èƒ½ã¯åŒ»ç™‚ç”»åƒã«å¯¾ã—ã¦ã¯é™å®šçš„ã ã£ãŸãŸã‚ã€11ã®ç•°ãªã‚‹ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã«å¯¾ã—ã¦200kã®ãƒã‚¹ã‚¯ã‚’ã—ãŸåŒ»ç™‚ç”»åƒã‚’ç”¨æ„ã—finetuningã—ãŸMedSAMã«ã‚ˆã£ã¦ã€åŒ»ç™‚ç”»åƒã®ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®æ€§èƒ½ã‚’å¤§å¹…ã«å‘ä¸Šã€‚<br>ã‚³ãƒ¼ãƒ‰ã¨ãƒ¢ãƒ‡ãƒ«ã¯publicly available</p>
<p><img src="https://github.com/user-attachments/assets/ea394adc-b1da-4764-bf29-534323bfc443" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/AutomaticSpeechRecognition(ASR).html" target="_blank" rel="noopener noreferrer">#AutomaticSpeechRecognition(ASR)</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Generalization.html" target="_blank" rel="noopener noreferrer">#Generalization</a>
<a class="button" href="articles/Robustness.html" target="_blank" rel="noopener noreferrer">#Robustness</a>
<span class="issue_date">Issue Date: 2025-11-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3673" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Robust Speech Recognition via Large-Scale Weak Supervision, Alec Radford+, ICML'23, 2022.12</a>
<span class="snippet"><span>GPT Summary</span>- 680,000æ™‚é–“ã®å¤šè¨€èªéŸ³å£°ãƒˆãƒ©ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç”¨ã„ã¦è¨“ç·´ã—ãŸéŸ³å£°å‡¦ç†ã‚·ã‚¹ãƒ†ãƒ ã‚’ç ”ç©¶ã€‚å¾—ã‚‰ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆè»¢é€è¨­å®šã§è‰¯å¥½ã«ä¸€èˆ¬åŒ–ã—ã€å¾“æ¥ã®ç›£è¦–çµæœã¨ç«¶äº‰åŠ›ã‚’æŒã¤ã€‚äººé–“ã®ç²¾åº¦ã«è¿‘ã¥ãã“ã¨ãŒç¢ºèªã•ã‚Œã€ãƒ¢ãƒ‡ãƒ«ã¨æ¨è«–ã‚³ãƒ¼ãƒ‰ã‚’å…¬é–‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>ã„ã¾ã•ã‚‰ãªãŒã‚‰Whisperè«–æ–‡</p>
<p>æ—¥æœ¬èªè§£èª¬:


<a href="https://www.ai-shift.co.jp/techblog/3001" target="_blank" rel="noopener noreferrer">https://www.ai-shift.co.jp/techblog/3001</a>


<br><br>é•·æ–‡èªè­˜ã®ãŸã‚ã®ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã«åŸºã¥ããƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆ¦ç•¥ã‚‚è§£èª¬ã•ã‚Œã¦ã„ã‚‹ã®ã§å‚ç…§ã®ã“ã¨ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<a class="button" href="articles/Pixel-based.html" target="_blank" rel="noopener noreferrer">#Pixel-based</a>
<span class="issue_date">Issue Date: 2025-10-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3370" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Language Modelling with Pixels, Phillip Rust+, ICLR'23, 2022.07</a>
<span class="snippet"><span>GPT Summary</span>- PIXELã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”»åƒã¨ã—ã¦è¡¨ç¾ã™ã‚‹æ–°ã—ã„è¨€èªãƒ¢ãƒ‡ãƒ«ã§ã€èªå½™ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’å›é¿ã—ã€è¨€èªé–“ã§ã®è¡¨ç¾è»¢é€ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚86Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®PIXELã¯ã€BERTã¨åŒã˜ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’ã•ã‚Œã€éãƒ©ãƒ†ãƒ³æ–‡å­—ã‚’å«ã‚€å¤šæ§˜ãªè¨€èªã§ã®æ§‹æ–‡çš„ãŠã‚ˆã³æ„å‘³çš„ã‚¿ã‚¹ã‚¯ã§BERTã‚’å¤§å¹…ã«ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ãŸãŒã€ãƒ©ãƒ†ãƒ³æ–‡å­—ã§ã¯ã‚„ã‚„åŠ£ã‚‹çµæœã¨ãªã£ãŸã€‚ã¾ãŸã€PIXELã¯æ­£å­—æ³•çš„æ”»æ’ƒã‚„è¨€èªã‚³ãƒ¼ãƒ‰ã‚¹ã‚¤ãƒƒãƒãƒ³ã‚°ã«å¯¾ã—ã¦BERTã‚ˆã‚Šã‚‚å …ç‰¢ã§ã‚ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/nielsrogge/status/1980559120760791125?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-08-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2564" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Scalable Diffusion Models with Transformers, William Peebles+, ICCV'23</a>
<span class="snippet"><span>GPT Summary</span>- æ–°ã—ã„ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã«åŸºã¥ãæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ï¼ˆDiffusion Transformers, DiTsï¼‰ã‚’ææ¡ˆã—ã€U-Netã‚’ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã«ç½®ãæ›ãˆãŸã€‚DiTsã¯é«˜ã„Gflopsã‚’æŒã¡ã€ä½ã„FIDã‚’ç¶­æŒã—ãªãŒã‚‰è‰¯å¥½ãªã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’ç¤ºã™ã€‚æœ€å¤§ã®DiT-XL/2ãƒ¢ãƒ‡ãƒ«ã¯ã€ImageNetã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å¾“æ¥ã®æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚Šã€æœ€å…ˆç«¯ã®FID 2.27ã‚’é”æˆã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬:


<a href="https://qiita.com/sasgawy/items/8546c784bc94d94ef0b2" target="_blank" rel="noopener noreferrer">https://qiita.com/sasgawy/items/8546c784bc94d94ef0b2</a>


</p>
<p>ã‚ˆãè¦‹ã‚‹DiT<br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2526" target="_blank" rel="noopener noreferrer">[Paper Note] DiT: Self-supervised Pre-training for Document Image Transformer, Junlong Li+, ACMMM'22</a>
<br><br>ã‚‚åŒæ§˜ã®å‘¼ç§°ã ãŒå…¨ãç•°ãªã‚‹è©±ãªã®ã§æ³¨æ„</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/python.html" target="_blank" rel="noopener noreferrer">#python</a>
<a class="button" href="articles/LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-08-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2474" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Efficient Memory Management for Large Language Model Serving with  PagedAttention, Woosuk Kwon+, SOSP'23</a>
<span class="snippet"><span>GPT Summary</span>- PagedAttentionã‚’ç”¨ã„ãŸvLLMã‚·ã‚¹ãƒ†ãƒ ã‚’ææ¡ˆã—ã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒªã®ç„¡é§„ã‚’å‰Šæ¸›ã—ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã§ã®æŸ”è»Ÿãªå…±æœ‰ã‚’å®Ÿç¾ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€åŒãƒ¬ãƒ™ãƒ«ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã§LLMã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’2-4å€å‘ä¸Šã€‚ç‰¹ã«é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚„å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§åŠ¹æœãŒé¡•è‘—ã€‚ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ä¸­ã€‚</span>
<span class="snippet"><span>Comment</span><p>ï¼ˆä»Šæ›´ãªãŒã‚‰ï¼‰vLLMã¯ã“ã¡ã‚‰:<br>


<a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">https://github.com/vllm-project/vllm</a>


<br><br>ç¾åœ¨ã®ä¸»è¦ãªLLM Inference/Serving Engineã®ã²ã¨ã¤ã€‚</p></span><br><br>
<a class="button" href="articles/RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/VariationalAutoEncoder.html" target="_blank" rel="noopener noreferrer">#VariationalAutoEncoder</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ColdStart.html" target="_blank" rel="noopener noreferrer">#ColdStart</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<a class="button" href="articles/SemanticID.html" target="_blank" rel="noopener noreferrer">#SemanticID</a>
<span class="issue_date">Issue Date: 2025-07-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2309" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Recommender Systems with Generative Retrieval, Shashank Rajput+, NeurIPS'23</a>
<span class="snippet"><span>GPT Summary</span>- æ–°ã—ã„ç”Ÿæˆçš„æ¤œç´¢ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã—ã€ã‚¢ã‚¤ãƒ†ãƒ ã®ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯IDã‚’ç”¨ã„ã¦æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’äºˆæ¸¬ã™ã‚‹Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å¾“æ¥ã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã‚’å¤§å¹…ã«ä¸Šå›ã‚‹æ€§èƒ½ã‚’é”æˆã—ã€éå»ã®å¯¾è©±å±¥æ­´ãŒãªã„ã‚¢ã‚¤ãƒ†ãƒ ã«å¯¾ã—ã¦ã‚‚æ”¹å–„ã•ã‚ŒãŸæ¤œç´¢æ€§èƒ½ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=BJ0fQUU32w" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=BJ0fQUU32w</a>


</p>
<p>Semantic IDã‚’ææ¡ˆã—ãŸç ”ç©¶</p>
<p>ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ„å‘³çš„ãªæƒ…å ±ã‚’ä¿æŒã—ãŸdiscrete tokenã®ã‚¿ãƒ—ãƒ«ï¼ˆï¼Semantic ID)ã§è¡¨ç¾ã—ã€encoder-decoderã§Next Itemã®Semantic IDã‚’ç”Ÿæˆã™ã‚‹ã‚¿ã‚¹ã‚¯ã«è½ã¨ã—ã“ã‚€ã“ã¨ã§æ¨è–¦ã™ã‚‹ã€‚SemanticIDã®ä½œæˆæ–¹æ³•ã¯å¾Œã§èª­ã‚“ã§ç†è§£ã—ãŸã„ã€‚<br><br><img src="https://github.com/user-attachments/assets/38606c7c-011f-46b0-ab14-8d213626be3d" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/a239bac7-c273-4681-a102-65e88c9c65d2" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/7d1822ce-462e-43f6-bb33-af77914919f6" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/Normalization.html" target="_blank" rel="noopener noreferrer">#Normalization</a>
<span class="issue_date">Issue Date: 2025-04-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1899" target="_blank" rel="noopener noreferrer" class="title-link">Foundation Transformers, Hongyu Wang+, PMLR'23</a>
<span class="snippet"><span>GPT Summary</span>- è¨€èªã€è¦–è¦šã€éŸ³å£°ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã«ãŠã‘ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®åæŸãŒé€²ã‚€ä¸­ã€ç•°ãªã‚‹å®Ÿè£…ã®ã€ŒTransformersã€ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚æ±ç”¨ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ãŸã‚ã«ã€å®‰å®šæ€§ã‚’æŒã¤Foundation Transformerã®é–‹ç™ºãŒæå”±ã•ã‚Œã€Magnetoã¨ã„ã†æ–°ã—ã„Transformerå¤‰ç¨®ãŒç´¹ä»‹ã•ã‚Œã‚‹ã€‚Sub-LayerNormã¨ç†è«–ã«åŸºã¥ãåˆæœŸåŒ–æˆ¦ç•¥ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ã•ã¾ã–ã¾ãªã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ãŠã„ã¦å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨å®‰å®šæ€§ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªãƒ¢ãƒ‡ãƒ«ãªãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ã«ãŠã„ã¦ã€PostLNã¯vision encodingã«ãŠã„ã¦sub-optimalã§ã€PreLNã¯text encodingã«ãŠã„ã¦sub-optimalã§ã‚ã‚‹ã“ã¨ãŒå…ˆè¡Œç ”ç©¶ã§ç¤ºã•ã‚Œã¦ãŠã‚Šã€ãƒãƒ«ã‚¿ãƒ¢ãƒ¼ãƒ€ãƒ«ã‚’å˜ä¸€ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã€é«˜æ€§èƒ½ã€ã‹ã¤å­¦ç¿’ã®å®‰å®šæ€§ãªé«˜ãã€try and errorç„¡ã—ã§é©ç”¨ã§ãã‚‹åŸºç›¤ã¨ãªã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒå¿…è¦ã¨ã„ã†ãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã§ææ¡ˆã•ã‚ŒãŸæ‰‹æ³•ã€‚å…·ä½“çš„ã«ã¯ã€Sub-LayerNorm(Sub-LN)ã¨å‘¼ã°ã‚Œã‚‹ã€self attentionã¨FFNéƒ¨åˆ†ã«è¿½åŠ ã®LayerNormã‚’é©ç”¨ã™ã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã€DeepNetã‚’è¸è¥²ã—Layeræ•°ãŒéå¸¸ã«å¤§ãã„å ´åˆã§ã‚‚å­¦ç¿’ãŒå®‰å®šã™ã‚‹ã‚ˆã†ãªé‡ã¿ã®åˆæœŸåŒ–æ–¹æ³•ã‚’ç†è«–çš„ã«åˆ†æã—ææ¡ˆã—ã¦ã„ã‚‹ã€‚<br><br>å…·ä½“çš„ã«ã¯ã€Sub-LNã®å ´åˆã€LayerNormã‚’<br>- SelfAttentionè¨ˆç®—ã«ãŠã‘ã‚‹QKVã‚’æ±‚ã‚ã‚‹ãŸã‚ã®input Xã®projectionã®å‰ã¨Attentionã®å‡ºåŠ›projectionã®å‰<br>- FFNã§ã®å„Linear Layerã®å‰<br>ã«é©ç”¨ã—ã€<br><br>åˆæœŸåŒ–ã‚’ã™ã‚‹éš›ã«ã¯ã€FFNã®W, ãŠã‚ˆã³self-attentionã®V_projã¨å‡ºåŠ›ã®out_projã®åˆæœŸåŒ–ã‚’Î³ï¼ˆï¼sqrt(log(2N))ã«ã‚ˆã£ã¦ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹æ–¹æ³•ã‚’ææ¡ˆã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚<br><br><img src="https://github.com/user-attachments/assets/2847f982-3266-4394-9920-01d9977e505e" alt="image" loading="lazy"></p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1900" target="_blank" rel="noopener noreferrer">DeepNet: Scaling Transformers to 1,000 Layers, Hongyu Wang+, arXiv'22</a>
</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ImageSegmentation.html" target="_blank" rel="noopener noreferrer">#ImageSegmentation</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<span class="issue_date">Issue Date: 2025-04-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1885" target="_blank" rel="noopener noreferrer" class="title-link">Segment Anything, Alexander Kirillov+, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- Segment Anything (SA)ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã€ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ææ¡ˆã—ã€1å„„ä»¥ä¸Šã®ãƒã‚¹ã‚¯ã‚’å«ã‚€1,100ä¸‡ã®ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼å°Šé‡ã—ãŸç”»åƒã‹ã‚‰ãªã‚‹æœ€å¤§ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ã—ã¾ã—ãŸã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã¯ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã§æ–°ã—ã„ç”»åƒåˆ†å¸ƒã‚„ã‚¿ã‚¹ã‚¯ã«é©å¿œã§ãã€è©•ä¾¡ã®çµæœã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ€§èƒ½ãŒé«˜ãã€å¾“æ¥ã®ç›£è¦–ã•ã‚ŒãŸçµæœã‚’ä¸Šå›ã‚‹ã“ã¨ã‚‚ã‚ã‚Šã¾ã™ã€‚SAMã¨SA-1Bãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ç ”ç©¶ä¿ƒé€²ã®ãŸã‚ã«å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>SAMè«–æ–‡</p>
<p>pj page:


<a href="https://segment-anything.com" target="_blank" rel="noopener noreferrer">https://segment-anything.com</a>


</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/PositionalEncoding.html" target="_blank" rel="noopener noreferrer">#PositionalEncoding</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-04-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1865" target="_blank" rel="noopener noreferrer" class="title-link">The Impact of Positional Encoding on Length Generalization in   Transformers, Amirhossein Kazemnejad+, NeurIPS'23</a>
<span class="snippet"><span>GPT Summary</span>- é•·ã•ä¸€èˆ¬åŒ–ã¯Transformerãƒ™ãƒ¼ã‚¹ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹é‡è¦ãªèª²é¡Œã§ã‚ã‚Šã€ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆPEï¼‰ãŒãã®æ€§èƒ½ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã€‚5ã¤ã®ç•°ãªã‚‹PEæ‰‹æ³•ï¼ˆAPEã€T5ã®ç›¸å¯¾PEã€ALiBiã€Rotaryã€NoPEï¼‰ã‚’æ¯”è¼ƒã—ãŸçµæœã€ALiBiã‚„Rotaryãªã©ã®ä¸€èˆ¬çš„ãªæ‰‹æ³•ã¯é•·ã•ä¸€èˆ¬åŒ–ã«é©ã—ã¦ãŠã‚‰ãšã€NoPEãŒä»–ã®æ‰‹æ³•ã‚’ä¸Šå›ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚NoPEã¯è¿½åŠ ã®è¨ˆç®—ã‚’å¿…è¦ã¨ã›ãšã€çµ¶å¯¾PEã¨ç›¸å¯¾PEã®ä¸¡æ–¹ã‚’è¡¨ç¾å¯èƒ½ã§ã‚ã‚‹ã€‚ã•ã‚‰ã«ã€ã‚¹ã‚¯ãƒ©ãƒƒãƒãƒ‘ãƒƒãƒ‰ã®å½¢å¼ãŒãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ã‚‚ç¤ºã•ã‚ŒãŸã€‚ã“ã®ç ”ç©¶ã¯ã€æ˜ç¤ºçš„ãªä½ç½®åŸ‹ã‚è¾¼ã¿ãŒé•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¸ã®ä¸€èˆ¬åŒ–ã«å¿…é ˆã§ãªã„ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1863" target="_blank" rel="noopener noreferrer">Llama 4 Series, Meta, 2025.04</a>
<br><br>ã«ãŠã„ã¦ã€Llama4 ScoutãŒ10Mã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’å®Ÿç¾ã§ãã‚‹ç†ç”±ã®ä¸€ã¤ã¨ã®ã“ã¨ã€‚<br><br>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/drjimfan/status/1908615861650547081?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>Llama4ã®ãƒ–ãƒ­ã‚°ãƒã‚¹ãƒˆã«ã‚‚ãã®æ—¨è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹:<br>&gt;A key innovation in the Llama 4 architecture is the use of interleaved attention layers without positional embeddings. Additionally, we employ inference time temperature scaling of attention to enhance length generalization.<br><br>[The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation](


<a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/?utm_source=twitter&utm_medium=organic_social&utm_content=image&utm_campaign=llama4)" target="_blank" rel="noopener noreferrer">https://ai.meta.com/blog/llama-4-multimodal-intelligence/?utm_source=twitter&utm_medium=organic_social&utm_content=image&utm_campaign=llama4)</a>


<p>æ–œã‚èª­ã¿ã ãŒã€length generalizationã‚’è©•ä¾¡ã™ã‚‹ä¸Šã§downstream taskã«ç„¦ç‚¹ã‚’å½“ã¦ã€3ã¤ã®ä»£è¡¨çš„ãªã‚«ãƒ†ã‚´ãƒªã«ç›¸å½“ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§è©•ä¾¡ã—ãŸã¨ã“ã‚ã€ã“ã®è¦³ç‚¹ã«ãŠã„ã¦ã¯T5ã®relative positinal encodingã¨NoPEï¼ˆä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ‡ã‚£ãƒ³ã‚°ç„¡ã—ï¼‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒè‰¯ãã€<br><br><img src="https://github.com/user-attachments/assets/dddadfff-ab28-4073-96c3-831eb16845a0" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/c6ec8e0e-7abb-4330-be23-2261486a477c" alt="image" loading="lazy"><br><br>NoPEã¯çµ¶å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¨ç›¸å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç†è«–ä¸Šå®Ÿç¾å¯èƒ½ã§ã‚ã‚Š[^1]<br><img src="https://github.com/user-attachments/assets/bbcf797a-d394-42d4-b017-08d7dba4261c" alt="image" loading="lazy"><br><br>å®Ÿéš›ã«å­¦ç¿’ã•ã‚ŒãŸç•°ãªã‚‹2ã¤ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦åŒã˜ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãã‚Œãã‚Œinputã—ã€åŒã˜æ·±ã•ã®Layerã®å…¨ã¦ã®attention distributionã®çµ„ã¿åˆã‚ã›ã‹ã‚‰Jensen Shannon Divergenceã§è·é›¢ã‚’ç®—å‡ºã—ã€æœ€ã‚‚å°ã•ã„ã‚‚ã®ã‚’2ãƒ¢ãƒ‡ãƒ«é–“ã®å½“è©²layerã®è·é›¢ã¨ã—ã¦å¯è¦–åŒ–ã™ã‚‹ã¨ä¸‹è¨˜ã®ã‚ˆã†ã«ãªã‚Šã€NoPEã¨T5ã®relative positional encodingãŒæœ€ã‚‚é¡ä¼¼ã—ã¦ã„ã‚‹ã“ã¨ã‹ã‚‰ã€NoPEãŒå­¦ç¿’ã‚’é€šã˜ã¦ï¼ˆå®Ÿç”¨ä¸Šã¯ï¼‰ç›¸å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ã‚ˆã†ãªã‚‚ã®ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒåˆ†ã‹ã£ãŸã€‚<br><img src="https://github.com/user-attachments/assets/9619c7e5-0612-45de-8717-1634bee509b7" alt="image" loading="lazy"><br><br>[^1]:æ·±ã•1ã®Layerã®Hidden State H^1ã‹ã‚‰çµ¶å¯¾ä½ç½®ã®å¾©å…ƒãŒå¯èƒ½ã§ã‚ã‚Šï¼ˆã¤ã¾ã‚Šã€å½“è©²ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®HãŒçµ¶å¯¾ä½ç½®ã«é–¢ã™ã‚‹æƒ…å ±ã‚’ä¿æŒã—ã¦ã„ã‚‹ï¼‰ã€ã“ã®å‰æã®ã‚‚ã¨ã€å¾Œç¶šã®LayerãŒã“ã®æƒ…å ±ã‚’ä¸Šæ›¸ãã—ãªã„ã¨ä»®å®šã—ãŸå ´åˆã«ã€ç›¸å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’å®Ÿç¾ã§ãã‚‹ã€‚</p>
<p>ã¾ãŸã€CoT/Scratchpadã¯long sequenceã«å¯¾ã™ã‚‹æ±åŒ–æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒsmall scaleã§ã¯ã‚ã‚‹ãŒå…ˆè¡Œç ”ç©¶ã§ç¤ºã•ã‚Œã¦ãŠã‚Šã€Positional Encodingã‚’å¤‰åŒ–ã•ã›ãŸæ™‚ã«CoT/Scratchpadã®æ€§èƒ½ã«ã©ã®ã‚ˆã†ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ã‚’èª¿æŸ»ã€‚<br><br>å…·ä½“çš„ã«ã¯ã€CoT/Scratchpadã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒã©ã®ã‚ˆã†ãªã‚‚ã®ãŒæœ‰åŠ¹ã‹ã‚‚æ˜ã‚‰ã‹ã§ã¯ãªã„ã®ã§ã€5ç¨®é¡ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ„ã¿åˆã‚ã›ã§ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æ§‹æˆã—ã€mathematical reasoningã‚¿ã‚¹ã‚¯ã§ä»¥ä¸‹ã®ã‚ˆã†ãªè¨­å®šã§è¨“ç·´ã—<br><br>- ã•ã¾ã–ã¾ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çµ„ã¿åˆã‚ã›ã§ç•°ãªã‚‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä½œæˆã—ã€<br>- å…¨ã¦ã®ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚ã‚Š/ãªã—ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´<br><br>ã“ã‚Œã‚‰ã‚’æ¯”è¼ƒã—ãŸã€‚ã“ã®çµæœã€CoT/Scratchpadã¯ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«é–¢ä¿‚ãªãã€ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã§ã®ã¿æœ‰åŠ¹ï¼ˆæœ‰åŠ¹ã‹ã©ã†ã‹ã¯ã‚¿ã‚¹ã‚¯ä¾å­˜ï¼‰ã§ã‚ã‚‹ã“ã¨ãŒåˆ†ã‹ã£ãŸã€‚ã“ã®ã“ã¨ã‹ã‚‰ã€CoT/Scratcpadï¼ˆã¤ã¾ã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®inputã¨outputã®ä»•æ–¹ï¼‰å˜ä½“ã§ã€long contextã«å¯¾ã™ã‚‹æ±åŒ–æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ããªã„ã®ã§ã€Positional Encodingï¼ˆâ‰’ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼‰ã«ã‚ˆã‚‹long contextã«å¯¾ã™ã‚‹æ±åŒ–æ€§èƒ½ã®å‘ä¸ŠãŒéå¸¸ã«é‡è¦ã§ã‚ã‚‹ã“ã¨ãŒæµ®ãå½«ã‚Šã«ãªã£ãŸã€‚<br><img src="https://github.com/user-attachments/assets/e23c4fbf-84de-4344-a01e-1e7e9e66fa7e" alt="image" loading="lazy"><br><br>ã¾ãŸã€CoT/ScratchpadãŒæœ‰åŠ¹ã ã£ãŸAdditionã«å¯¾ã—ã¦å„Positional Embeddingãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®attentionãŒã©ã®ä½ç½®ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŒ‡ã—ã¦ã„ã‚‹ã‹ã‚’ç›¸å¯¾è·é›¢ã§å¯è¦–åŒ–ã—ãŸã¨ã“ã‚ï¼ˆ0ãŒå½“è©²ãƒˆãƒ¼ã‚¯ãƒ³ã€ã¤ã¾ã‚Šç¾åœ¨ã®Scratchpadã«ç€ç›®ã—ã¦ãŠã‚Šã€1ãŒé ã„ãƒˆãƒ¼ã‚¯ãƒ³ã€ã¤ã¾ã‚Šinputã«ç€ç›®ã—ã¦ã„ã‚‹ã“ã¨ã‚’è¡¨ã™ã‚ˆã†ã«æ­£è¦åŒ–ï¼‰ã€NoPEã¨Relative Positional EncodingãŒshort/long rangeã«ãã‚Œãã‚Œãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã™ã‚‹ã‚ˆã†ãªbinomialãªåˆ†å¸ƒãªã®ã«å¯¾ã—ã€ä»–ã®Positional Encodingã§ã¯ã‚ˆã‚Šuniformãªåˆ†å¸ƒã§ã‚ã‚‹ã“ã¨ãŒåˆ†ã‹ã£ãŸã€‚ã“ã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã¯NoPEã¨Relative POã®æ€§èƒ½ãŒé«˜ã‹ã£ãŸãŸã‚ã€binomialãªåˆ†å¸ƒã®æ–¹ãŒã‚ˆã‚Šæœ€é©ã§ã‚ã‚ã†ã“ã¨ãŒç¤ºå”†ã•ã‚ŒãŸã€‚<br><img src="https://github.com/user-attachments/assets/833e6a81-8611-4e79-9d2e-473f7ebee2d0" alt="image" loading="lazy"><br></p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1271" target="_blank" rel="noopener noreferrer" class="title-link">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head  Checkpoints, Joshua Ainslie+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- Multi-query attentionï¼ˆMQAï¼‰ã¯ã€å˜ä¸€ã®key-value headã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãŠã‚Šã€ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã®æ¨è«–ã‚’åŠ‡çš„ã«é«˜é€ŸåŒ–ã—ã¦ã„ã¾ã™ã€‚ãŸã ã—ã€MQAã¯å“è³ªã®ä½ä¸‹ã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚Šã€ã•ã‚‰ã«ã¯ã€ã‚ˆã‚Šé€Ÿã„æ¨è«–ã®ãŸã‚ã ã‘ã«åˆ¥å€‹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ãŒæœ›ã¾ã—ããªã„å ´åˆã‚‚ã‚ã‚Šã¾ã™ã€‚æ—¢å­˜ã®ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°è¨ˆé‡ã®5%ã‚’ä½¿ç”¨ã—ã¦MQAã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã«ã‚¢ãƒƒãƒ—ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã®ãƒ¬ã‚·ãƒ”ã‚’ææ¡ˆã—ã€ã•ã‚‰ã«ã€è¤‡æ•°ã®key-value headã‚’ä½¿ç”¨ã™ã‚‹ãƒãƒ«ãƒã‚¯ã‚¨ãƒªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ä¸€èˆ¬åŒ–ã§ã‚ã‚‹ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã‚¯ã‚¨ãƒªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ï¼ˆGQAï¼‰ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ã‚¢ãƒƒãƒ—ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸGQAãŒã€MQAã¨åŒç­‰ã®é€Ÿåº¦ã§ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã«åŒ¹æ•µã™ã‚‹å“è³ªã‚’é”æˆã™ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>é€šå¸¸ã®Multi-Head AttentionãŒQKVãŒ1å¯¾1å¯¾å¿œãªã®ã«å¯¾ã—ã€Multi Query Attention (MQA) <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1272" target="_blank" rel="noopener noreferrer">Fast Transformer Decoding: One Write-Head is All You Need, Noam Shazeer, N/A, arXiv'19</a>
  ã¯å…¨ã¦ã®Qã«å¯¾ã—ã¦KVã‚’å…±æœ‰ã™ã‚‹ã€‚ä¸€æ–¹ã€GQAã¯ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«KVã‚’å…±æœ‰ã™ã‚‹ç‚¹ã§ç•°ãªã‚‹ã€‚MQAã¯å¤§å¹…ã«Infeerence` speedãŒæ”¹å–„ã™ã‚‹ãŒã€ç²¾åº¦ãŒåŠ£åŒ–ã™ã‚‹å•é¡ŒãŒã‚ã£ãŸã€‚ã“ã®ç ”ç©¶ã§ã¯é€šå¸¸ã®Multi-Head Attentionã«å¯¾ã—ã¦ã€ã‚ªãƒªã‚¸ãƒŠãƒ«ã®äº‹å‰å­¦ç¿’ã«å¯¾ã—ã¦è¿½åŠ ã®5%ã®è¨ˆç®—é‡ã§GQAãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/70ec2179-428c-47b8-af53-cb3cc0e4f022" alt="image" loading="lazy"><br><br></p>
<p>Main Result. Multi-Head Attentionã«å¯¾ã—ã¦ã€inference timeãŒå¤§å¹…ã«æ”¹å–„ã—ã¦ã„ã‚‹ãŒã€Multi-Query Attentionã‚ˆã‚Šã‚‚é«˜ã„æ€§èƒ½ã‚’ç¶­æŒã—ã¦ã„ã‚‹ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/3687aeb4-90b8-403d-853b-740121dd5f98" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<span class="issue_date">Issue Date: 2023-12-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1174" target="_blank" rel="noopener noreferrer" class="title-link">Pushdown Layers: Encoding Recursive Structure in Transformer Language   Models, Shikhar Murty+, N_A, EMNLP'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€å†å¸°æ§‹é€ ã‚’ã†ã¾ãæ‰ãˆã‚‹ãŸã‚ã«æ–°ã—ã„è‡ªå·±æ³¨æ„å±¤ã§ã‚ã‚‹Pushdown Layersã‚’å°å…¥ã—ã¾ã—ãŸã€‚Pushdown Layersã¯ã€å†å¸°çŠ¶æ…‹ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ãŸã‚ã«ã‚¹ã‚¿ãƒƒã‚¯ãƒ†ãƒ¼ãƒ—ã‚’ä½¿ç”¨ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã®æ¨å®šæ·±åº¦ã‚’è¿½è·¡ã—ã¾ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€æ§‹æ–‡çš„ãªä¸€èˆ¬åŒ–ã‚’æ”¹å–„ã—ã€ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã•ã‚‰ã«ã€Pushdown Layersã¯æ¨™æº–ã®è‡ªå·±æ³¨æ„ã®ä»£æ›¿ã¨ã—ã¦ã‚‚ä½¿ç”¨ã§ãã€GLUEãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã§ã‚‚æ”¹å–„ã‚’å®Ÿç¾ã—ã¾ã—ãŸã€‚</span>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<span class="issue_date">Issue Date: 2023-11-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1164" target="_blank" rel="noopener noreferrer" class="title-link">Advancing Transformer Architecture in Long-Context Large Language  Models: A Comprehensive Survey, Yunpeng Huang+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€Transformerãƒ™ãƒ¼ã‚¹ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®é•·ã„æ–‡è„ˆã®èƒ½åŠ›ã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã®åŒ…æ‹¬çš„ãªèª¿æŸ»ã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚ç¾è¡Œã®LLMsã®åˆ¶ç´„ã‚„å•é¡Œç‚¹ã‚’æ˜ç¢ºåŒ–ã—ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ã‚¢ãƒƒãƒ—ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚„è©•ä¾¡ã®å¿…è¦æ€§ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚ã•ã‚‰ã«ã€æœ€é©åŒ–ãƒ„ãƒ¼ãƒ«ã‚­ãƒƒãƒˆã‚„å°†æ¥ã®ç ”ç©¶ã®å¯èƒ½æ€§ã«ã¤ã„ã¦ã‚‚è­°è«–ã—ã¦ã„ã¾ã™ã€‚é–¢é€£æ–‡çŒ®ã¯https://github.com/Strivin0311/long-llms-learningã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã«æ›´æ–°ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>Transformerã‚’LongContextã«å¯¾å¿œã•ã›ã‚‹æŠ€è¡“ã®ã‚µãƒ¼ãƒ™ã‚¤ã€‚<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/e498f066-2713-463c-8b58-9e9ecd480570" alt="image" loading="lazy"><br>ï¼ˆç”»åƒã¯å…ƒãƒ„ã‚¤ãƒ¼ãƒˆã‚ˆã‚Šï¼‰<br>å…ƒãƒ„ã‚¤ãƒ¼ãƒˆ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/1727358484360945750?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<span class="issue_date">Issue Date: 2023-11-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1126" target="_blank" rel="noopener noreferrer" class="title-link">Hiformer: Heterogeneous Feature Interactions Learning with Transformers  for Recommender Systems, Huan Gui+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- ç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’å­¦ã¶ãŸã‚ã«ã€Transformerãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ææ¡ˆã™ã‚‹ã€‚ã‚¦ã‚§ãƒ–ã‚¹ã‚±ãƒ¼ãƒ«ã®ãƒ¬ã‚³ãƒ¡ãƒ³ãƒ€ãƒ¼ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã„ã¦ã€ç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’æ‰‹å‹•ã§ä½œæˆã™ã‚‹ã“ã¨ã¯å›°é›£ã§ã‚ã‚‹ãŸã‚ã€è‡ªå‹•çš„ã«æ‰ãˆã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚ã—ã‹ã—ã€ç¾åœ¨ã®Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ç•°ç¨®ã®ç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã‚’æ‰ãˆã‚‹ã“ã¨ãŒã§ããšã€ã‚µãƒ¼ãƒ“ãƒ³ã‚°ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚‚é«˜ã„ã€‚ãã“ã§ã€ç•°ç¨®ã®è‡ªå·±æ³¨æ„å±¤ã‚’ææ¡ˆã—ã€\textsc{Hiformer}ã¨ã„ã†ãƒ¢ãƒ‡ãƒ«ã‚’ç´¹ä»‹ã™ã‚‹ã€‚\textsc{Hiformer}ã¯ç‰¹å¾´ã®ç›¸äº’ä½œç”¨ã®ç•°ç¨®æ€§ã‚’è€ƒæ…®ã—ã€ä½ãƒ©ãƒ³ã‚¯è¿‘ä¼¼ã¨ãƒ¢ãƒ‡ãƒ«ã®å‰ªå®šã«ã‚ˆã‚Šé«˜é€Ÿãªæ¨è«–ã‚’å®Ÿç¾ã™ã‚‹ã€‚ã‚ªãƒ•ãƒ©ã‚¤ãƒ³å®Ÿé¨“çµæœã§ã¯ã€\textsc{Hiformer}ãƒ¢ãƒ‡ãƒ«ã®åŠ¹æœã¨åŠ¹ç‡ãŒç¤ºã•ã‚Œã¦ãŠã‚Šã€Google Playã®å®Ÿä¸–ç•Œã®å¤§è¦æ¨¡ãªã‚¢ãƒ—ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã«ã‚‚å±•é–‹ã•ã‚Œã€ä¸»è¦ãªã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’æ”¹å–„ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ ã¯ã€Factorization Machinesã‚ãŸã‚Šã‹ã‚‰å¤§æŠµã®å ´åˆç‰¹å¾´é‡é–“ã®äº¤äº’ä½œç”¨ã‚’é ‘å¼µã£ã¦æ‰ãˆã‚‹ã“ã¨ã§ç²¾åº¦å‘ä¸Šã‚’ç›®æŒ‡ã™ã€ã¨ã„ã†è©±ã‚’ã—ã¦ãã¦ã„ã‚‹æ°—ãŒã™ã‚‹ãŒã€ã“ã‚Œã¯Transformerã‚’ä½¿ã£ã¦äº¤äº’ä½œç”¨æ‰ãˆã‚‰ã‚Œã‚‹ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ã‚’è€ƒãˆã¾ã—ãŸã€ã¨ã„ã†ç ”ç©¶ã®ã‚ˆã†ã§ã‚ã‚‹ã€‚<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/d57eb1b6-0e68-47fe-9d0a-315186cc9e3d" alt="image" loading="lazy"><br><br>self attentionéƒ¨åˆ†ã«å·¥å¤«ãŒãªã•ã‚Œã¦ãŠã‚Šï¼ˆææ¡ˆæ‰‹æ³•ã¯å³ç«¯ï¼‰ã€task tokenã¨ãã‚Œãã‚Œã®featureã‚’concatã—ã¦QKVã‚’æ±‚ã‚ã‚‹ã“ã¨ã§ã€æ˜ç¤ºçš„ã«äº¤äº’ä½œç”¨ãŒç”Ÿã¾ã‚Œã‚‹ã‚ˆã†ãªæ§‹é€ ã«ã—ã¦ã„ã‚‹ã€‚<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/5b097e51-34d8-488b-aba5-4dce9e201272" alt="image" loading="lazy"></p>
<p>Online A/Bãƒ†ã‚¹ãƒˆã§ã‚‚è©•ä¾¡ã—ã¦ãŠã‚Šã€Hiformerã«ã‚ˆã£ã¦SoTAãªäº¤äº’ä½œç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆDCNï¼‰ã‚ˆã‚Šã‚‚é«˜ã„ãƒ¦ãƒ¼ã‚¶ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/be431a24-814a-4891-b5d5-58ad2b8563e7" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-11-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1117" target="_blank" rel="noopener noreferrer" class="title-link">Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in  Transformer Models, Steve Yadlowsky+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã®æ–‡è„ˆå­¦ç¿’ï¼ˆICLï¼‰èƒ½åŠ›ã‚’èª¿æŸ»ã—ã¾ã—ãŸã€‚ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã¯ã€äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²å†…ã§ç•°ãªã‚‹ã‚¿ã‚¹ã‚¯ã‚’ç‰¹å®šã—ã€å­¦ç¿’ã™ã‚‹èƒ½åŠ›ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ã—ã‹ã—ã€äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²å¤–ã®ã‚¿ã‚¹ã‚¯ã‚„é–¢æ•°ã«å¯¾ã—ã¦ã¯ä¸€èˆ¬åŒ–ãŒåŠ£åŒ–ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚ã¾ãŸã€é«˜å®¹é‡ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ICLèƒ½åŠ›ã¯ã€äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²ã«å¯†æ¥ã«é–¢é€£ã—ã¦ã„ã‚‹ã“ã¨ãŒå¼·èª¿ã•ã‚Œã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>TransformerãŒpre-trainingæ™‚ã«åˆ©ç”¨ã•ã‚ŒãŸå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä»¥å¤–ã®åˆ†å¸ƒã«å¯¾ã—ã¦ã¯æ±åŒ–æ€§èƒ½ãŒè½ã¡ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã‚‰ã—ã„ã€‚ã‚‚ã—ã“ã‚ŒãŒæ­£ã—ã„ã¨ã™ã‚‹ã¨ã€çµå±€çœŸã«æ–°ã—ã„åˆ†å¸ƒã¨ã„ã†ã‹é–¢æ•°ã¨ã„ã†ã‹ã‚¿ã‚¹ã‚¯ã¨ã„ã†ã‹ã€ã‚’TransformerãŒå‰µå‡ºã™ã‚‹å¯èƒ½æ€§ã¯ä½ã„ã¨è¨€ãˆã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€‚ãŒã€æ–°ã—ã„ã‚‚ã®ã£ã¦å¤§ä½“ã¯æ—¢å­˜ã®æ¦‚å¿µã®çµ„ã¿åˆã‚ã›ã ã‚ˆã­ï¼ˆã‚¹ãƒãƒ›ã¨ã‹ï¼‰ã€ã¿ãŸã„ãªã“ã¨ã‚’è€ƒãˆã‚‹ã¨ã€åˆ¥ã«ãã‚Œã§ã‚‚ååˆ†ã§ã¯ï¼Ÿã¨æ€ã£ã¦ã—ã¾ã†ã€‚äººé–“ãŒæœ¬å½“ã«çœŸã®æ„å‘³ã§æ–°ã—ã„é–¢æ•°ã¨ã„ã†ã‹ã‚¿ã‚¹ã‚¯ã¨ã„ã†ã‹åˆ†å¸ƒã‚’ç”Ÿã¿å‡ºã›ã¦ã„ã‚‹ã‹ã¨ã„ã†ã¨ã€å®Ÿã¯ãã‚“ãªã«å¤šããªã„ã®ã§ã¯ï¼Ÿã¨ã„ã†äºˆæ„Ÿã‚‚ã™ã‚‹ã€‚ã¾ã‚ãŸã¨ãˆã°ã€é‡å­åŠ›å­¦ã‚’æœ€åˆã«è€ƒãˆã¾ã—ãŸï¼ã¨ã‹ãã†ã„ã†ã®ã¯ä¾‹å¤–ã ã¨æ€ã†ã‘ã©ãƒ»ãƒ»ãƒ»ã€ãã®ãƒ¬ãƒ™ãƒ«ã®ã“ã¨ã£ã¦ã©ã‚“ãã‚‰ã„ã‚ã‚‹ã‚“ã ã‚ã†ã­ï¼Ÿ</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<span class="issue_date">Issue Date: 2023-10-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1062" target="_blank" rel="noopener noreferrer" class="title-link">Boolformer: Symbolic Regression of Logic Functions with Transformers, StÃ©phane d'Ascoli+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- ã“ã®ç ”ç©¶ã§ã¯ã€Boolformerã¨ã„ã†Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ–ãƒ¼ãƒ«é–¢æ•°ã®ã‚·ãƒ³ãƒœãƒªãƒƒã‚¯å›å¸°ã‚’å®Ÿè¡Œã™ã‚‹æ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚Boolformerã¯ã€ã‚¯ãƒªãƒ¼ãƒ³ãªçœŸç†å€¤è¡¨ã‚„ãƒã‚¤ã‚ºã®ã‚ã‚‹è¦³æ¸¬ãªã©ã€ã•ã¾ã–ã¾ãªãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦åŠ¹æœçš„ãªå¼ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã•ã‚‰ã«ã€å®Ÿä¸–ç•Œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚„éºä¼å­åˆ¶å¾¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã«ãŠã„ã¦ã€Boolformerã¯è§£é‡ˆå¯èƒ½ãªä»£æ›¿æ‰‹æ³•ã¨ã—ã¦å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã—ã¾ã™ã€‚ã“ã®ç ”ç©¶ã®æˆæœã¯ã€å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>ãƒ–ãƒ¼ãƒ«é–¢æ•°ã‚’end-to-endã§å­¦ç¿’ã§ãã‚‹transformeiã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ææ¡ˆã—ãŸæ¨¡æ§˜</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/DataAugmentation.html" target="_blank" rel="noopener noreferrer">#DataAugmentation</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/DataGeneration.html" target="_blank" rel="noopener noreferrer">#DataGeneration</a>
<span class="issue_date">Issue Date: 2023-08-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1024" target="_blank" rel="noopener noreferrer" class="title-link">Prompt2Model: Generating Deployable Models from Natural Language   Instructions, Vijay Viswanathan+, N_A, EMNLP'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è‡ªç„¶è¨€èªã§ã‚¿ã‚¹ã‚¯ã‚’èª¬æ˜ã—ã€ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹æ‰‹æ³•ã§ã‚ã‚‹Prompt2Modelã‚’ææ¡ˆã—ã¦ã„ã¾ã™ã€‚Prompt2Modelã¯ã€æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ¤œç´¢ã€LLMsã‚’ä½¿ç”¨ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç”Ÿæˆã€ãŠã‚ˆã³æ•™å¸«ã‚ã‚Šå¾®èª¿æ•´ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’é€šã˜ã¦è¡Œã‚ã‚Œã¾ã™ã€‚å®Ÿé¨“çµæœã§ã¯ã€Prompt2ModelãŒå¼·åŠ›ãªLLMã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€ãƒ¢ãƒ‡ãƒ«ã®ä¿¡é ¼æ€§ã®è©•ä¾¡ã‚‚å¯èƒ½ã§ã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã¾ã™ã€‚Prompt2Modelã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>Dataset Generatorã«ã‚ˆã£ã¦ã€ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãŒå­˜åœ¨ã—ãªã„ãƒ‡ãƒ¼ã‚¿ã«ã¤ã„ã¦ã‚‚æ“¬ä¼¼ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã€ã‹ã¤ãã‚Œã‚’æ—¢å­˜ã®ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã•ã‚‰ã«æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã“ã¨ãŒå ±å‘Šã•ã‚Œã¦ã„ã‚‹ã€‚ã“ã‚ŒãŒã§ãã‚‹ã®ã¯ã¨ã¦ã‚‚ç´ æ™´ã‚‰ã—ã„ã€‚</p>
<p>Dataset Generatorã«ã¤ã„ã¦ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã™ã‚‹éš›ã«ä½ã‚³ã‚¹ãƒˆã§ã€é«˜å“è³ªã§ã€å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿ã¨ã™ã‚‹ãŸã‚ã«ã„ãã¤ã‹ã®å·¥å¤«ã‚’å®Ÿæ–½ã—ã¦ã„ã‚‹ã€‚<br>1. ãƒ¦ãƒ¼ã‚¶ãŒä¸ãˆãŸãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã ã‘ã§ãªãã€ã‚·ã‚¹ãƒ†ãƒ ãŒç”Ÿæˆã—ãŸexampleã‚‚ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦æ´»ç”¨ã™ã‚‹ã“ã¨ã§ã€ç”Ÿæˆã•ã‚Œã‚‹exampleã®å¤šæ§˜æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚å®Ÿéš›ã€ã“ã‚Œã‚’ã‚„ã‚‰ãªã„å ´åˆã¯120/200ãŒduplicate exampleã§ã‚ã£ãŸãŒã€ã“ã‚ŒãŒ25/200ã¾ã§æ¸›å°‘ã—ãŸã€‚<br>2. ç”Ÿæˆã—ãŸã‚µãƒ³ãƒ—ãƒ«ã®æ•°ã«æ¯”ä¾‹ã—ã¦ã€temperatureã‚’å¾ã€…ã«é«˜ãã—ã¦ã„ãã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚µãƒ³ãƒ—ãƒ«ã®è³ªã‚’æ‹…ä¿ã—ã¤ã¤ã€å¤šæ§˜æ€§ã‚’å¾ã€…ã«å¢—åŠ ã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚Temperature Annealingã¨å‘¼ã¶ã€‚<br>3. self-consistencyã‚’ç”¨ã„ã¦ã€æ“¬ä¼¼ãƒ©ãƒ™ãƒ«ã®è³ªã‚’é«˜ã‚ã‚‹ã€‚ã‚‚ã—majority votingãŒäº’è§’ã®å ´åˆã¯ã€å›ç­”ãŒçŸ­ã„ã‚‚ã®ã‚’æ¡ç”¨ã—ãŸï¼ˆã“ã‚Œã¯ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã«åŸºã¥ã„ã¦ã„ã‚‹ï¼‰<br>4. zeno buildã‚’ç”¨ã„ã¦APIã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä¸¦åˆ—åŒ–ã™ã‚‹ã“ã¨ã§é«˜é€Ÿã«å®Ÿé¨“ã‚’å®Ÿæ–½<br><br>éå¸¸ã«å‚è€ƒã«ãªã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Contents-based.html" target="_blank" rel="noopener noreferrer">#Contents-based</a>
<a class="button" href="articles/pretrained-LM.html" target="_blank" rel="noopener noreferrer">#pretrained-LM</a>
<a class="button" href="articles/ContrastiveLearning.html" target="_blank" rel="noopener noreferrer">#ContrastiveLearning</a>
<span class="issue_date">Issue Date: 2023-07-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/852" target="_blank" rel="noopener noreferrer" class="title-link">UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation, ACL'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆPLMï¼‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®æ–°ã—ã„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚‹UniTRecã‚’ææ¡ˆã—ã¾ã™ã€‚UniTRecã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å±¥æ­´ã®æ–‡è„ˆã‚’ã‚ˆã‚Šè‰¯ããƒ¢ãƒ‡ãƒ«åŒ–ã™ã‚‹ãŸã‚ã«çµ±ä¸€ã•ã‚ŒãŸãƒ­ãƒ¼ã‚«ãƒ«-ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³Transformerã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’ä½¿ç”¨ã—ã€å€™è£œã®ãƒ†ã‚­ã‚¹ãƒˆã‚¢ã‚¤ãƒ†ãƒ ã®è¨€èªã®è¤‡é›‘ã•ã‚’æ¨å®šã™ã‚‹ãŸã‚ã«Transformerãƒ‡ã‚³ãƒ¼ãƒ€ã‚’æ´»ç”¨ã—ã¾ã™ã€‚å¹…åºƒã„è©•ä¾¡ã«ã‚ˆã‚Šã€UniTRecãŒãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®æ¨è–¦ã‚¿ã‚¹ã‚¯ã§æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚</span>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/PositionalEncoding.html" target="_blank" rel="noopener noreferrer">#PositionalEncoding</a>
<span class="issue_date">Issue Date: 2023-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/820" target="_blank" rel="noopener noreferrer" class="title-link">Randomized Positional Encodings Boost Length Generalization of Transformers, ACL'23</a>
<span class="snippet"><span>GPT Summary</span>- ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã€å›ºå®šé•·ã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã¯å„ªã‚ŒãŸæ±åŒ–èƒ½åŠ›ã‚’æŒã¤ãŒã€ä»»æ„ã®é•·ã•ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ã¯å¯¾å¿œã§ããªã„ã€‚ã“ã®å•é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã«ã€æ–°ã—ã„ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ‰‹æ³•ã‚’ææ¡ˆã™ã‚‹ã€‚ãƒ©ãƒ³ãƒ€ãƒ åŒ–ã•ã‚ŒãŸä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¹ã‚­ãƒ¼ãƒ ã‚’ä½¿ç”¨ã—ã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®ä½ç½®ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã—ã€é †åºä»˜ã‘ã‚‰ã‚ŒãŸã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠã™ã‚‹ã€‚å¤§è¦æ¨¡ãªå®Ÿè¨¼è©•ä¾¡ã«ã‚ˆã‚Šã€ã“ã®æ‰‹æ³•ãŒãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®æ±åŒ–èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã€ãƒ†ã‚¹ãƒˆã®æ­£ç¢ºæ€§ã‚’å¹³å‡ã—ã¦12.0ï¼…å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</span>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/811" target="_blank" rel="noopener noreferrer" class="title-link">Trainable Transformer in Transformer, Abhishek Panigrahi+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€Transformer in Transformerï¼ˆTinTï¼‰ã¨ã„ã†åŠ¹ç‡çš„ãªæ§‹ç¯‰ã‚’ææ¡ˆã—ã€å¤§è¦æ¨¡ãªäº‹å‰å­¦ç¿’è¨€èªãƒ¢ãƒ‡ãƒ«ã®å†…éƒ¨ãƒ¢ãƒ‡ãƒ«ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã—ã¦å¾®èª¿æ•´ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã¨ãªã‚Šã¾ã™ã€‚TinTã¯å°ã•ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã§ã‚‚é«˜ã„æ€§èƒ½ã‚’ç™ºæ®ã—ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼å†…ã®å˜ç´”ãªãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡ã‚‚å‘ä¸Šã•ã›ã¾ã™ã€‚ã•ã¾ã–ã¾ãªå®Ÿé¨“ã«ã‚ˆã‚Šã€TinTã®æ€§èƒ½å‘ä¸ŠãŒè¦³å¯Ÿã•ã‚Œã€å¤§è¦æ¨¡ãªäº‹å‰å­¦ç¿’è¨€èªãƒ¢ãƒ‡ãƒ«ãŒè¤‡é›‘ãªã‚µãƒ–ãƒ«ãƒ¼ãƒãƒ³ã‚’å®Ÿè¡Œã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚ã¾ãŸã€TinTã®ãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ã§æ‹¡å¼µå¯èƒ½ãªã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã‚‚æä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å‚è€ƒ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1679253896362086401?s=46&t=ArwxeDos47eUWfAg7_FRtg"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ç ”ç©¶ã®é€²ã¿æ—©ã™ãã¾ã›ã‚“ï¼Ÿï¼Ÿï¼Ÿ</p>
<p>openreview:


<a href="https://openreview.net/forum?id=VmqTuFMk68" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=VmqTuFMk68</a>


</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<span class="issue_date">Issue Date: 2023-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/806" target="_blank" rel="noopener noreferrer" class="title-link">Generative Pretraining in Multimodality, Quan Sun+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- Emuã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€å˜ä¸€ãƒ¢ãƒ€ãƒªãƒ†ã‚£ã¾ãŸã¯ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªãƒ‡ãƒ¼ã‚¿å…¥åŠ›ã‚’å—ã‘å…¥ã‚Œã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚Emuã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã€ç”»åƒã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã¸ã®ã‚¿ã‚¹ã‚¯ã‚„ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”»åƒã¸ã®ã‚¿ã‚¹ã‚¯ãªã©ã€ã•ã¾ã–ã¾ãªã‚¿ã‚¹ã‚¯ã§å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã¾ã™ã€‚ã¾ãŸã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãªã©ã®æ‹¡å¼µæ©Ÿèƒ½ã‚‚ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚</span>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<span class="issue_date">Issue Date: 2023-07-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/781" target="_blank" rel="noopener noreferrer" class="title-link">A Comprehensive Survey on Applications of Transformers for Deep Learning  Tasks, Saidul Islam+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- Transformerãƒ¢ãƒ‡ãƒ«ã¯ã€ã‚»ãƒ«ãƒ•ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ä½¿ç”¨ã—ã¦æ–‡è„ˆé–¢ä¿‚ã‚’ç†è§£ã™ã‚‹ãŸã‚ã®ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã‚ã‚Šã€é•·ã„ä¾å­˜é–¢ä¿‚ã‚’å‡¦ç†ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€è‡ªç„¶è¨€èªå‡¦ç†ã ã‘ã§ãªãã€ä»–ã®ã•ã¾ã–ã¾ãªãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã‚‚æ³¨ç›®ã•ã‚Œã¦ã„ã¾ã™ã€‚ã—ã‹ã—ã€ã•ã¾ã–ã¾ãªãƒ‰ãƒ¡ã‚¤ãƒ³ã§ã®Transformerã®å¿œç”¨ã«é–¢ã™ã‚‹åŒ…æ‹¬çš„ãªèª¿æŸ»ã¯ã¾ã ä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ãã“ã§ã€ç§ãŸã¡ã¯ææ¡ˆã•ã‚ŒãŸTransformerãƒ¢ãƒ‡ãƒ«ã®åŒ…æ‹¬çš„ãªèª¿æŸ»ã‚’è¡Œã„ã€ãã®å¿œç”¨ãƒ‰ãƒ¡ã‚¤ãƒ³ã¨å½±éŸ¿ã‚’åˆ†æã—ã¾ã—ãŸã€‚ç§ãŸã¡ã®ç›®çš„ã¯ã€ç ”ç©¶è€…ã«å¯¾ã—ã¦Transformerã®å¯èƒ½æ€§ã‚’æ˜ã‚‰ã‹ã«ã—ã€ã“ã®æŠ€è¡“ã®ç†è§£ã‚’åºƒã‚ã‚‹ã“ã¨ã§ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>Transformerã«é–¢ã™ã‚‹æœ€æ–°ã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡ã€‚TransformerãŒåˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒªã‚¹ãƒˆãŒåˆ—æŒ™ã•ã‚Œã¦ã„ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<span class="issue_date">Issue Date: 2023-06-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/774" target="_blank" rel="noopener noreferrer" class="title-link">Faith and Fate: Limits of Transformers on Compositionality, Nouha Dziri+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- Transformerã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã¯ã€å¤šæ®µéšã®æ¨è«–ã‚’å¿…è¦ã¨ã™ã‚‹ã‚¿ã‚¹ã‚¯ã§å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã™ä¸€æ–¹ã€äº›ç´°ãªå•é¡Œã§å¤±æ•—ã™ã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã€‚ã“ã®ç ”ç©¶ã§ã¯ã€3ã¤ã®ä»£è¡¨çš„ãªåˆæˆã‚¿ã‚¹ã‚¯ã‚’ç”¨ã„ã¦ã€Transformerã®é™ç•Œã‚’èª¿æŸ»ã—ã€ã‚¿ã‚¹ã‚¯ã®è¤‡é›‘ã•ãŒå¢—ã™ã«ã¤ã‚Œã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒä½ä¸‹ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚ã¾ãŸã€TransformerãŒåˆæˆçš„ãªæ¨è«–ã‚’ç·šå½¢åŒ–ã•ã‚ŒãŸã‚µãƒ–ã‚°ãƒ©ãƒ•ã®ãƒãƒƒãƒãƒ³ã‚°ã«ç°¡ç´„åŒ–ã—ã¦è§£æ±ºã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ãŸãŒã€ä½“ç³»çš„ãªå•é¡Œè§£æ±ºã‚¹ã‚­ãƒ«ã‚’é–‹ç™ºã—ã¦ã„ãªã„å¯èƒ½æ€§ã‚‚ã‚ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å‚è€ƒ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1674891033283555328?s=46&t=KFT8cWTu8vV69iD6Qt0NGw"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<span class="issue_date">Issue Date: 2023-06-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/760" target="_blank" rel="noopener noreferrer" class="title-link">Think Before You Act: Decision Transformers with Internal Working Memory, Jikun Kang+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®æ€§èƒ½ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«æŒ¯ã‚‹èˆã„ã‚’è¨˜æ†¶ã™ã‚‹ã€Œå¿˜å´ç¾è±¡ã€ã«ã‚ˆã£ã¦ä½ä¸‹ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚äººé–“ã®è„³ã¯åˆ†æ•£å‹ã®ãƒ¡ãƒ¢ãƒªã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’åˆ©ç”¨ã—ã¦ãŠã‚Šã€å¿˜å´ç¾è±¡ã‚’è»½æ¸›ã—ã¦ã„ã‚‹ã€‚ãã“ã§ã€æˆ‘ã€…ã¯ã€å†…éƒ¨ä½œæ¥­ãƒ¡ãƒ¢ãƒªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ææ¡ˆã—ã€Atariã‚²ãƒ¼ãƒ ã¨ãƒ¡ã‚¿ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ“ä½œã‚¿ã‚¹ã‚¯ã®ä¸¡æ–¹ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã¨æ±åŒ–æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<span class="issue_date">Issue Date: 2023-06-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/744" target="_blank" rel="noopener noreferrer" class="title-link">Birth of a Transformer: A Memory Viewpoint, Alberto Bietti+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®å†…éƒ¨ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ç†è§£ã™ã‚‹ãŸã‚ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãŒã‚°ãƒ­ãƒ¼ãƒãƒ«ã¨ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå›ºæœ‰ã®bigramåˆ†å¸ƒã‚’ã©ã®ã‚ˆã†ã«ãƒãƒ©ãƒ³ã‚¹ã™ã‚‹ã‹ã‚’ç ”ç©¶ã€‚2å±¤ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã§ã®å®Ÿè¨¼çš„åˆ†æã«ã‚ˆã‚Šã€ã‚°ãƒ­ãƒ¼ãƒãƒ«bigramã®é«˜é€Ÿãªå­¦ç¿’ã¨ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†…ã®bigramã®ã€Œèª˜å°ãƒ˜ãƒƒãƒ‰ã€ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®é…ã„ç™ºé”ã‚’ç¤ºã—ã€é‡ã¿è¡Œåˆ—ãŒé€£æƒ³è¨˜æ†¶ã¨ã—ã¦ã®å½¹å‰²ã‚’å¼·èª¿ã™ã‚‹ã€‚ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒç‰¹æ€§ã®å½¹å‰²ã‚‚ç ”ç©¶ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<span class="issue_date">Issue Date: 2023-05-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/667" target="_blank" rel="noopener noreferrer" class="title-link">Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing   Important Tokens, Zhanpeng Zeng+, N_A, NeurIPS'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€Transformerãƒ¢ãƒ‡ãƒ«ã®äºŒæ¬¡ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã™ã‚‹ãŸã‚ã«ã€å„å±¤ã§ã‚µã‚¤ã‚º$r$ãŒ$n$ã«ç‹¬ç«‹ã—ãŸè¡¨ç¾ã«å…¥åŠ›ã‚’åœ§ç¸®ã™ã‚‹æ–¹æ³•ã‚’ææ¡ˆã™ã‚‹ã€‚VIPãƒˆãƒ¼ã‚¯ãƒ³ä¸­å¿ƒã®åœ§ç¸®ï¼ˆVccï¼‰ã‚¹ã‚­ãƒ¼ãƒ ã‚’ä½¿ç”¨ã—ã€VIPãƒˆãƒ¼ã‚¯ãƒ³ã®è¡¨ç¾ã‚’è¿‘ä¼¼ã™ã‚‹ãŸã‚ã«å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’é¸æŠçš„ã«åœ§ç¸®ã™ã‚‹ã€‚ææ¡ˆã•ã‚ŒãŸã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯ã€ç«¶åˆã™ã‚‹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨æ¯”è¼ƒã—ã¦åŠ¹ç‡çš„ã§ã‚ã‚Šã€å¤šæ•°ã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ç«¶äº‰åŠ›ã®ã‚ã‚‹ã¾ãŸã¯ã‚ˆã‚Šå„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã™ã‚‹ã€‚ã¾ãŸã€ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¯128Kãƒˆãƒ¼ã‚¯ãƒ³ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã§ãã€ä¸€è²«ã—ã¦ç²¾åº¦ã®å‘ä¸Šã‚’æä¾›ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</span>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/Inference.html" target="_blank" rel="noopener noreferrer">#Inference</a>
<span class="issue_date">Issue Date: 2023-04-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/601" target="_blank" rel="noopener noreferrer" class="title-link">Efficiently Scaling Transformer Inference, Reiner Pope+, N_A, MLSys'23</a>
<span class="snippet"><span>GPT Summary</span>- - å¤§è¦æ¨¡Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã®ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€æœ€é©ãªå¤šæ¬¡å…ƒåˆ†å‰²æŠ€è¡“ã‚’é¸æŠã™ã‚‹ãŸã‚ã®å˜ç´”ãªè§£æãƒ¢ãƒ‡ãƒ«ã‚’é–‹ç™º- ä½ãƒ¬ãƒ™ãƒ«ã®æœ€é©åŒ–ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€500B+ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã¨ãƒ¢ãƒ‡ãƒ«FLOPSåˆ©ç”¨ç‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã«ãŠã„ã¦ã€FasterTransformerãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆã‚’ä¸Šå›ã‚‹æ–°ã—ã„Paretoãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã‚’å®Ÿç¾- é©åˆ‡ãªåˆ†å‰²ã«ã‚ˆã‚Šã€ãƒãƒ«ãƒã‚¯ã‚¨ãƒªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®ä½ã„ãƒ¡ãƒ¢ãƒªè¦ä»¶ã«ã‚ˆã‚Šã€32å€ã®å¤§ããªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å¯èƒ½- int8ã‚¦ã‚§ã‚¤ãƒˆé‡å­åŒ–ã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆä¸­ã®ä½ãƒãƒƒãƒã‚µã‚¤ã‚ºãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã¯ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Š29msã§ã‚ã‚Šã€å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³ã®å¤§ãƒãƒƒãƒã‚µã‚¤ã‚ºå‡¦ç†ã«ãŠã„ã¦76ï¼…ã®MFUã‚’å®Ÿç¾ã—ã€PaLM 540Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦2048ãƒˆãƒ¼ã‚¯ãƒ³ã®é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>ç‰¹ã«Multiquery Attentionã¨ã„ã†æŠ€è¡“ãŒTransformerã®inferenceã®ã‚³ã‚¹ãƒˆå‰Šæ¸›ã«æœ‰åŠ¹ã‚‰ã—ã„</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-04-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/529" target="_blank" rel="noopener noreferrer" class="title-link">Scaling Transformer to 1M tokens and beyond with RMT, Bulatov+, DeepPavlov, arXiv'23</a>
<span class="snippet"><span>Comment</span><p>Reccurent Memory Transformer <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/523" target="_blank" rel="noopener noreferrer">Recurrent Memory Transformer, Bulatov+, NeurIPS'22</a>
 ã‚’ä½¿ã£ã¦2Mãƒˆãƒ¼ã‚¯ãƒ³æ‰±ãˆã‚‹ã‚ˆã†ã«ã—ãŸã‚ˆãƒ¼ã¨ã„ã†è©±ã€‚<br><br>ãƒãƒªãƒ¼ãƒãƒƒã‚¿ãƒ¼ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒ1.5Mã‚‰ã—ã„ã®ã§ã€ãã®ã†ã¡å°èª¬ä¸€å†Šæ›¸ã‘ã‚‹ã‹ã‚‚ã¨ã„ã†ä¸–ç•Œã€‚</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3204" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Classifier-Free Diffusion Guidance, Jonathan Ho+, arXiv'22, 2022.07</a>
<span class="snippet"><span>GPT Summary</span>- åˆ†é¡å™¨ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã¯æ¡ä»¶ä»˜ãæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã§ã€ãƒ¢ãƒ¼ãƒ‰ã‚«ãƒãƒ¬ãƒƒã‚¸ã¨ã‚µãƒ³ãƒ—ãƒ«å¿ å®Ÿåº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å›³ã‚‹ã€‚è‘—è€…ã¯ã€åˆ†é¡å™¨ãªã—ã§ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ãŒå¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã€ã“ã‚Œã‚’åˆ†é¡å™¨ãƒ•ãƒªãƒ¼ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã¨å‘¼ã¶ã€‚æ¡ä»¶ä»˜ããŠã‚ˆã³ç„¡æ¡ä»¶ã®æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã‚’å…±åŒã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ã‚µãƒ³ãƒ—ãƒ«å“è³ªã¨å¤šæ§˜æ€§ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’é”æˆã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬:


<a href="https://qiita.com/UMAboogie/items/160c1159811743c49d99" target="_blank" rel="noopener noreferrer">https://qiita.com/UMAboogie/items/160c1159811743c49d99</a>


</p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3205" target="_blank" rel="noopener noreferrer">[Paper Note] Diffusion Models Beat GANs on Image Synthesis, Prafulla Dhariwal+, NeurIPS'21 Spotlight, 2021.05</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-10-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3110" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Mixture of Attention Heads: Selecting Attention Heads Per Token, Xiaofeng Zhang+, EMNLP'22, 2022.10</a>
<span class="snippet"><span>GPT Summary</span>- Mixture of Attention Heads (MoA)ã¯ã€MoEãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’çµ„ã¿åˆã‚ã›ãŸæ–°ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã€å‹•çš„ã«é¸æŠã•ã‚ŒãŸã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚²ãƒ¼ãƒˆåŒ–ã«ã‚ˆã‚Šè¨ˆç®—åŠ¹ç‡ã‚’ä¿ã¡ãªãŒã‚‰æ‹¡å¼µå¯èƒ½ã§ã€ãƒ¢ãƒ‡ãƒ«ã®è§£é‡ˆå¯èƒ½æ€§ã«ã‚‚å¯„ä¸ã™ã‚‹ã€‚å®Ÿé¨“ã§ã¯ã€æ©Ÿæ¢°ç¿»è¨³ã‚„ãƒã‚¹ã‚¯ä»˜ãè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãªã©ã®ã‚¿ã‚¹ã‚¯ã§å¼·åŠ›ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚‹çµæœã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>FFNã«é©ç”¨ã•ã‚Œã‚‹ã“ã¨ãŒå¤šã‹ã£ãŸMoEã‚’multi-head attention (MHA) ã«é©ç”¨ã™ã‚‹ç ”ç©¶ã€‚ã“ã®ã‚ˆã†ãªattentionã‚’Mixture of Attention Heads (MoA)ã¨å‘¼ã¶ã€‚<br><br>å„MHAã¯è¤‡æ•°ã®attention expertsã‚’æŒã¡ã€ãã®ä¸­ã‹ã‚‰Kå€‹ã®ExpertsãŒç¾åœ¨ã®ã‚¯ã‚¨ãƒªq_tã«åŸºã¥ã„ã¦Routerã«ã‚ˆã£ã¦é¸å‡ºï¼ˆå¼7, 8)ã•ã‚Œã‚‹ã€‚ãã‚Œãã‚Œã®attention expertsã«å¯¾ã—ã¦q_tãŒæµã•ã‚Œã€é€šå¸¸ã®MHAã¨åŒã˜æµã‚Œã§outputãŒè¨ˆç®—ã•ã‚Œã€æœ€çµ‚çš„ã«é¸æŠã•ã‚ŒãŸéš›ã®ï¼ˆæ­£è¦åŒ–ã•ã‚ŒãŸï¼ˆå¼9ï¼‰ï¼‰probabilityã«ã‚ˆã‚‹åŠ é‡å¹³å‡ã«ã‚ˆã£ã¦å‡ºåŠ›ã‚’è¨ˆç®—ã™ã‚‹ï¼ˆå¼6)ã€‚<br><br>æ³¨æ„ç‚¹ã¨ã—ã¦ã¯ã€å„attention expertsã¯ç‹¬ç«‹ã—ãŸprojection matrix W_q, W_oï¼ˆãã‚Œãã‚Œiç•ªç›®ã®expertsã«ãŠã‘ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³tã«ãŠã„ã¦ã€query q_tã‚’å¤‰æ›ã€output o_{i,t}ã‚’hidden spaceæ¬¡å…ƒã«æˆ»ã™å½¹å‰²ã‚’æŒã¤)ã‚’æŒã¤ãŒã€K, Vã«å¯¾ã™ã‚‹å¤‰æ›è¡Œåˆ—ã¯å…±æœ‰ã™ã‚‹ã¨è¨€ã†ç‚¹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ¬¡å…ƒã«å…¨ã¦ã®expertsã«å¯¾ã—ã¦k, vã«å¯¾ã™ã‚‹å¤‰æ›ã¯è¨ˆç®—ã—ã¦ãŠã‘ã‚‹ã®ã§ã€headã”ã¨ã«ç•°ãªã‚‹å¤‰æ›ã‚’å­¦ç¿’ã—ãªãŒã‚‰ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/3073c6b8-cdc7-4303-8881-0c07c502d0ec" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/d74ab1b7-e44c-461d-ad64-6f5ecacd8da2" alt="image" loading="lazy"><br><br>ã¾ãŸã€ç‰¹å®šã®expertsã«ã®ã¿ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãŒé›†ä¸­ã—ãªã„ã‚ˆã†ã«ã€lossã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§å­¦ç¿’ã®å®‰å®šã•ã›æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã¦ã„ã‚‹ï¼ˆ4.3ç¯€ï¼‰ã€‚</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/OCR.html" target="_blank" rel="noopener noreferrer">#OCR</a>
<a class="button" href="articles/ACMMM.html" target="_blank" rel="noopener noreferrer">#ACMMM</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-08-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2526" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] DiT: Self-supervised Pre-training for Document Image Transformer, Junlong Li+, ACMMM'22</a>
<span class="snippet"><span>GPT Summary</span>- è‡ªå·±ç›£è¦–å‹äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«DiTã‚’ææ¡ˆã—ã€ãƒ©ãƒ™ãƒ«ãªã—ãƒ†ã‚­ã‚¹ãƒˆç”»åƒã‚’ç”¨ã„ã¦æ–‡æ›¸AIã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹æ€§èƒ½ã‚’å‘ä¸Šã€‚æ–‡æ›¸ç”»åƒåˆ†é¡ã‚„ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆåˆ†æã€è¡¨æ¤œå‡ºã€OCRãªã©ã§æ–°ãŸãªæœ€å…ˆç«¯çµæœã‚’é”æˆã€‚ã‚³ãƒ¼ãƒ‰ã¨ãƒ¢ãƒ‡ãƒ«ã¯å…¬é–‹ä¸­ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/Normalization.html" target="_blank" rel="noopener noreferrer">#Normalization</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<span class="issue_date">Issue Date: 2025-07-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2141" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] On Layer Normalizations and Residual Connections in Transformers, Sho Takase+, arXiv'22</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€Transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã®ä½ç½®ã«é–¢ã™ã‚‹Post-LNã¨Pre-LNã®é•ã„ã‚’èª¿æŸ»ã€‚Post-LNã¯æµ…ã„å±¤ã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã™ä¸€æ–¹ã€æ·±ã„å±¤ã§ã¯ä¸å®‰å®šãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å¼•ãèµ·ã“ã™æ¶ˆå¤±å‹¾é…å•é¡ŒãŒã‚ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã€‚ã“ã‚Œã‚’è¸ã¾ãˆã€Post-LNã®ä¿®æ­£ã«ã‚ˆã‚Šå®‰å®šã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿç¾ã™ã‚‹æ–¹æ³•ã‚’ææ¡ˆã—ã€å®Ÿé¨“ã§Pre-LNã‚’ä¸Šå›ã‚‹çµæœã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>Pre-LNã®å®‰å®šæ€§ã‚’æŒã¡ãªãŒã‚‰ã‚‚Post-LNã®ã‚ˆã†ãªé«˜ã„æ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹è‰¯ã„ã¨ã“å–ã‚Šã®B2TConnectionã‚’ææ¡ˆ<br><img src="https://github.com/user-attachments/assets/4d85bf16-19e4-4d2a-85e5-87da45cd2a98" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/af0aeae5-554a-4997-96a3-929cd7dd90bb" alt="image" loading="lazy"></p>
<p>NLP2022:


<a href="https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/A2-5.pdf" target="_blank" rel="noopener noreferrer">https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/A2-5.pdf</a>


</p></span><br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/Normalization.html" target="_blank" rel="noopener noreferrer">#Normalization</a>
<span class="issue_date">Issue Date: 2025-04-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1900" target="_blank" rel="noopener noreferrer" class="title-link">DeepNet: Scaling Transformers to 1,000 Layers, Hongyu Wang+, arXiv'22</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€æ·±ã„Transformerã‚’å®‰å®šåŒ–ã•ã›ã‚‹ãŸã‚ã®æ–°ã—ã„æ­£è¦åŒ–é–¢æ•°DeepNormã‚’ææ¡ˆã—ã€æ®‹å·®æ¥ç¶šã®ä¿®æ­£ã¨ç†è«–çš„åˆæœŸåŒ–ã‚’è¡Œã†ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€Post-LNã®æ€§èƒ½ã¨Pre-LNã®å®‰å®šæ€§ã‚’å…¼ã­å‚™ãˆã€æœ€å¤§1,000å±¤ã®Transformerã‚’ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—å¯èƒ½ã«ã—ãŸã€‚ç‰¹ã«ã€3.2Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®200å±¤ãƒ¢ãƒ‡ãƒ«ãŒã€12Bãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®48å±¤ãƒ¢ãƒ‡ãƒ«ã‚’5 BLEUãƒã‚¤ãƒ³ãƒˆä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€ä»Šå¾Œã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®å¯èƒ½æ€§ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>ã‚¹ãƒ†ãƒ¼ãƒˆã‚ªãƒ–AIã‚¬ã‚¤ãƒ‰ã«ã‚ˆã‚‹è§£èª¬:


<a href="https://ja.stateofaiguides.com/20220308-deepnet-transformer/" target="_blank" rel="noopener noreferrer">https://ja.stateofaiguides.com/20220308-deepnet-transformer/</a>


</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-02-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1754" target="_blank" rel="noopener noreferrer" class="title-link">Switch Transformers: Scaling to Trillion Parameter Models with Simple  and Efficient Sparsity, William Fedus+, JMLR'22</a>
<span class="snippet"><span>GPT Summary</span>- Switch Transformerã‚’ææ¡ˆã—ã€Mixture of Experts (MoE)ã®è¤‡é›‘ã•ã‚„é€šä¿¡ã‚³ã‚¹ãƒˆã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ä¸å®‰å®šæ€§ã‚’æ”¹å–„ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ä½ç²¾åº¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã®å¤§è¦æ¨¡ã‚¹ãƒ‘ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¯èƒ½ã«ãªã‚Šã€æœ€å¤§7å€ã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦å‘ä¸Šã‚’å®Ÿç¾ã€‚ã•ã‚‰ã«ã€1å…†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€T5-XXLãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦4å€ã®é€Ÿåº¦å‘ä¸Šã‚’é”æˆã€‚</span>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="articles/KnowledgeEditing.html" target="_blank" rel="noopener noreferrer">#KnowledgeEditing</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/FactualKnowledge.html" target="_blank" rel="noopener noreferrer">#FactualKnowledge</a>
<a class="button" href="articles/Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<span class="issue_date">Issue Date: 2024-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1332" target="_blank" rel="noopener noreferrer" class="title-link">Knowledge Neurons in Pretrained Transformers, Damai Dai+, N_A, ACL'22, 2022.05</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡ãªäº‹å‰å­¦ç¿’è¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€äº‹å®ŸçŸ¥è­˜ã®æ ¼ç´æ–¹æ³•ã«ã¤ã„ã¦ã®ç ”ç©¶ã‚’è¡Œã„ã¾ã—ãŸã€‚å…·ä½“çš„ã«ã¯ã€BERTã®fill-in-the-blank cloze taskã‚’ç”¨ã„ã¦ã€é–¢é€£ã™ã‚‹äº‹å®Ÿã‚’è¡¨ç¾ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’ç‰¹å®šã—ã¾ã—ãŸã€‚ã¾ãŸã€çŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®æ´»æ€§åŒ–ã¨å¯¾å¿œã™ã‚‹äº‹å®Ÿã®è¡¨ç¾ã¨ã®æ­£ã®ç›¸é–¢ã‚’è¦‹ã¤ã‘ã¾ã—ãŸã€‚ã•ã‚‰ã«ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã‚ãšã«ã€çŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’æ´»ç”¨ã—ã¦ç‰¹å®šã®äº‹å®ŸçŸ¥è­˜ã‚’ç·¨é›†ã—ã‚ˆã†ã¨è©¦ã¿ã¾ã—ãŸã€‚ã“ã®ç ”ç©¶ã¯ã€äº‹å‰å­¦ç¿’ã•ã‚ŒãŸTransformerså†…ã§ã®çŸ¥è­˜ã®æ ¼ç´ã«é–¢ã™ã‚‹ç¤ºå”†ã«å¯Œã‚“ã§ãŠã‚Šã€ã‚³ãƒ¼ãƒ‰ã¯https://github.com/Hunter-DDM/knowledge-neuronsã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1108" target="_blank" rel="noopener noreferrer">å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ï½¤ã€ŒçŸ¥è­˜ã¯å…¨çµåˆå±¤ã«è“„ç©ã•ã‚Œã‚‹ã€ã¨ã„ã†ä»®èª¬ã«ã¤ã„ã¦ã®æ–‡çŒ®èª¿æŸ»</a>
 </p>
<p>æ—¥æœ¬èªè§£èª¬: 


<a href="https://speakerdeck.com/kogoro/knowledge-neurons-in-pretrained-transformers-for-snlp2022" target="_blank" rel="noopener noreferrer">https://speakerdeck.com/kogoro/knowledge-neurons-in-pretrained-transformers-for-snlp2022</a>


</p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2140" target="_blank" rel="noopener noreferrer">[Paper Note] Transformer Feed-Forward Layers Are Key-Value Memories, Mor Geva+, EMNLP'21</a>
</p>
<p>ä¸Šè¨˜è³‡æ–™ã«ã‚ˆã‚‹ã¨ã€ç‰¹å®šã®çŸ¥è­˜ã‚’å‡ºåŠ›ã™ã‚‹éš›ã«æ´»æ€§åŒ–ã™ã‚‹çŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’ç‰¹å®šã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã€‚MLMã‚’ç”¨ã„ãŸclozeã‚¿ã‚¹ã‚¯ã«ã‚ˆã‚‹å®Ÿé¨“ã§[MASK]éƒ¨åˆ†ã«å½“è©²çŸ¥è­˜ã‚’å‡ºåŠ›ã™ã‚‹å®Ÿé¨“ã‚’ã—ãŸçµæœã€çŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®é‡ã¿ã‚’ã‚¼ãƒ­ã¨ã™ã‚‹ã¨æ€§èƒ½ãŒè‘—ã—ãåŠ£åŒ–ã—ã€å€¤ã‚’2å€ã«ã™ã‚‹ã¨æ€§èƒ½ãŒæ”¹å–„ã™ã‚‹ã¨ã„ã£ãŸå‚¾å‘ãŒã¿ã‚‰ã‚ŒãŸã€‚ã€€ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ã¨ã—ã¦ã€çŸ¥è­˜ã®æ›´æ–°ã¨ã€çŸ¥è­˜ã®å‰Šé™¤ãŒå¯èƒ½ã‹ã‚’æ¤œè¨¼ã€‚ã©ã¡ã‚‰ã¨ã‚‚æ›´æ–°ãƒ»å‰Šé™¤ãŒã•ã‚Œã‚‹æ–¹å‘æ€§[^1]ã¸ãƒ¢ãƒ‡ãƒ«ãŒå¤‰åŒ–ã—ãŸã€‚<br><br>ã¾ãŸã€çŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯Transformerã®å±¤ã®æ·±ã„ã¨ã“ã‚ã«ä½ç½®ã—ã¦ã„ã‚‹å‚¾å‘ã«ã‚ã‚Šã€ç•°ãªã‚‹relationã‚’æŒã¤ã‚ˆã†ãªé–¢ä¿‚çŸ¥è­˜åŒå£«ã§ã¯å…±æœ‰ã•ã‚Œãªã„å‚¾å‘ã«ã‚ã‚‹æ¨¡æ§˜ã€‚<br><br>[^1]: ä»–ã®çŸ¥è­˜ã«å½±éŸ¿ã‚’ä¸ãˆãšã€å®Œç’§ã«æ›´æ–°ãƒ»å‰Šé™¤ã§ããŸã‚ã‘ã§ã¯ãªã„ã€‚çŸ¥è­˜ã®æ›´æ–°ãƒ»å‰Šé™¤ã«ä¼´ã„Extrinsicãªè©•ä¾¡ã«ã‚ˆã£ã¦æ€§èƒ½å‘ä¸Šã€ã‚ã‚‹ã„ã¯PerplexityãŒå¢—å¤§ã—ãŸã€ã¨ã„ã£ãŸçµæœã‹ã‚‰ãã†ã„ã£ãŸæ–¹å‘æ€§ã¸ãƒ¢ãƒ‡ãƒ«ãŒå¤‰åŒ–ã—ãŸã€ã¨ã„ã†è©±</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/TabularData.html" target="_blank" rel="noopener noreferrer">#TabularData</a>
<span class="issue_date">Issue Date: 2023-04-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/574" target="_blank" rel="noopener noreferrer" class="title-link">Why do tree-based models still outperform deep learning on typical tabular data?, Grinsztajn+, Soda, Inria Saclay , arXiv'22</a>
<span class="snippet"><span>Comment</span><p>tree basedãªãƒ¢ãƒ‡ãƒ«ãŒãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã€ãªãœã“ã®ã‚ˆã†ãªã“ã¨ãŒèµ·ãã‚‹ã‹ã„ãã¤ã‹ã®ç†ç”±ã‚’èª¬æ˜ã—ãŸè«–æ–‡ã€‚<br><br><br><br><img src="https://user-images.githubusercontent.com/12249301/235130988-b008ad89-eec6-49d1-829d-1b23566ed677.jpeg" alt="image" loading="lazy"><br><br><br><br>NNã‚ˆã‚Šã‚‚tree basedãªãƒ¢ãƒ‡ãƒ«ãŒã†ã¾ãã„ãç†ç”±ã¨ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®å¸°ç´çš„ãƒã‚¤ã‚¢ã‚¹ãŒãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã«é©ã—ã¦ã„ã‚‹ã“ã¨ã‚’èª¿æŸ»ã—ã¦ã„ã‚‹ã€‚è€ƒå¯Ÿã¨ã—ã¦ã¯<br><br><br><br>1. NNã¯ã‚¹ãƒ ãƒ¼ã‚ºãªã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’å­¦ç¿’ã™ã‚‹èƒ½åŠ›ãŒé«˜ã„ãŒã€è¡¨å½¢å¼ã®ã‚ˆã†ãªä¸è¦å‰‡ãªãƒ‡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã™ã‚‹ã®ã«é©ã—ã¦ã„ãªã„<br><br>- Random Forestã§ã¯ã€xè»¸ã«ãŠã„ã¦irregularãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚‚å­¦ç¿’ã§ãã¦ã„ã‚‹ãŒã€NNã¯ã§ãã¦ã„ãªã„ã€‚<br><br><img src="https://user-images.githubusercontent.com/12249301/235139592-160cf5fe-dddb-4637-a300-b18a0935f253.png" alt="image" loading="lazy"><br><br><br><br>2. uninformativeãªfeaatureãŒMLP-likeãªNNã«æ‚ªå½±éŸ¿ã‚’ä¸ãˆã‚‹<br><br>- Tabular dataã¯ä¸€èˆ¬ã«uninformativeãªæƒ…å ±ã‚’å¤šãå«ã‚“ã§ãŠã‚Šã€å®Ÿéš›MLPã«uninformativeãªfeatureã‚’çµ„ã¿è¾¼ã‚“ã å ´åˆtree-basedãªæ‰‹æ³•ã¨ã®gapãŒå¢—åŠ ã—ãŸ<br><br><img src="https://user-images.githubusercontent.com/12249301/235140356-553c2d6d-63bf-485e-bcb4-72924535a2a9.png" alt="image" loading="lazy"><br><br><br><br>3. ãƒ‡ãƒ¼ã‚¿ã¯rotationã«å¯¾ã—ã¦ä¸å¤‰ã§ã¯ãªã„ãŸã‚ã€å­¦ç¿’æ‰‹é †ã‚‚ãã†ã‚ã‚‹ã¹ãï¼ˆã“ã®è¾ºãŒã‚ˆãã‚ã‹ã‚‰ãªã‹ã£ãŸï¼‰<br><br>- ResNetã¯Rotationã‚’åŠ ãˆã¦ã‚‚æ€§èƒ½ãŒå¤‰ã‚ã‚‰ãªã‹ã£ãŸï¼ˆrotation invariantãªæ§‹é€ ã‚’æŒã£ã¦ã„ã‚‹ï¼‰<br><br><img src="https://user-images.githubusercontent.com/12249301/235141633-910e64ff-83d9-417b-b778-6ab3ec7419f2.png" alt="image" loading="lazy"><br><br><br><br></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/Distillation.html" target="_blank" rel="noopener noreferrer">#Distillation</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="articles/Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<a class="button" href="articles/Findings.html" target="_blank" rel="noopener noreferrer">#Findings</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-10-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3345" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] MiniLMv2: Multi-Head Self-Attention Relation Distillation for  Compressing Pretrained Transformers, Wenhui Wang+, ACL'21 Findings, 2020.12</a>
<span class="snippet"><span>GPT Summary</span>- è‡ªå·±æ³¨æ„é–¢ä¿‚è’¸ç•™ã‚’ç”¨ã„ã¦ã€MiniLMã®æ·±å±¤è‡ªå·±æ³¨æ„è’¸ç•™ã‚’ä¸€èˆ¬åŒ–ã—ã€äº‹å‰å­¦ç¿’ã•ã‚ŒãŸãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®åœ§ç¸®ã‚’è¡Œã†æ‰‹æ³•ã‚’ææ¡ˆã€‚ã‚¯ã‚¨ãƒªã€ã‚­ãƒ¼ã€ãƒãƒªãƒ¥ãƒ¼ã®ãƒ™ã‚¯ãƒˆãƒ«é–“ã®é–¢ä¿‚ã‚’å®šç¾©ã—ã€ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã€‚æ³¨æ„ãƒ˜ãƒƒãƒ‰æ•°ã«åˆ¶é™ãŒãªãã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®å±¤é¸æŠæˆ¦ç•¥ã‚’æ¤œè¨ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€BERTã‚„RoBERTaã€XLM-Rã‹ã‚‰è’¸ç•™ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒæœ€å…ˆç«¯ã®æ€§èƒ½ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>æ•™å¸«ã¨ï¼ˆã‚ˆã‚Šå°è¦æ¨¡ãªï¼‰ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«é–“ã§ã€tokenã”ã¨ã®q-q/k-k/v-vã®dot productã«ã‚ˆã£ã¦å½¢æˆã•ã‚Œã‚‹relation mapï¼ˆãŸã¨ãˆã°q-qã®å ´åˆã¯relatiok mapã¯ãƒˆãƒ¼ã‚¯ãƒ³æ•°xãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®è¡Œåˆ—ã§å„è¦ç´ ãŒdot(qi, qj))ã§è¡¨ç¾ã•ã‚Œã‚‹é–¢ä¿‚æ€§ã‚’å†ç¾ã§ãã‚‹ã‚ˆã†ã«MHAã‚’è’¸ç•™ã™ã‚‹ã‚ˆã†ãªæ‰‹æ³•ã€‚å…·ä½“çš„ã«ã¯ã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã®QKVã¨ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®QKVã«ã‚ˆã£ã¦æ§‹æˆã•ã‚Œã‚‹ãã‚Œãã‚Œã®relation mapé–“ã®KL Divergenceã‚’æœ€å°åŒ–ã™ã‚‹ã‚ˆã†ã«è’¸ç•™ã™ã‚‹ã€‚ã“ã®ã¨ãæ•™å¸«ãƒ¢ãƒ‡ãƒ«ã¨ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®attention headsæ•°ãªã©ã¯ç•°ãªã£ã¦ã‚‚ã‚ˆã„ï¼ˆq-q/k-k/v-vãã‚Œãã‚Œã§å®šç¾©ã•ã‚Œã‚‹relation mapã¯ã¯ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«ä¾å­˜ã—ã¦ãŠã‚Šã€headæ•°ã«ã¯ä¾å­˜ã—ã¦ã„ãªã„ãŸã‚ï¼‰ã€‚</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/ZeroshotHyperparameterTransfer.html" target="_blank" rel="noopener noreferrer">#ZeroshotHyperparameterTransfer</a>
<span class="issue_date">Issue Date: 2025-08-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2582" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Tensor Programs V: Tuning Large Neural Networks via Zero-Shot  Hyperparameter Transfer, Greg Yang+, NeurIPS'21</a>
<span class="snippet"><span>GPT Summary</span>- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯é«˜ã‚³ã‚¹ãƒˆã§ã‚ã‚Šã€ç‰¹ã«å¤§è¦æ¨¡ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã„ã¦è² æ‹…ãŒå¤§ãã„ã€‚æ–°ãŸã«ææ¡ˆã™ã‚‹muTransferã¯ã€æœ€å¤§æ›´æ–°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ï¼ˆmuPï¼‰ã‚’åˆ©ç”¨ã—ã€å°ã•ãªãƒ¢ãƒ‡ãƒ«ã§ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸHPã‚’ãƒ•ãƒ«ã‚µã‚¤ã‚ºãƒ¢ãƒ‡ãƒ«ã«ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã§è»¢é€ã™ã‚‹æ‰‹æ³•ã§ã‚ã‚‹ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€1300ä¸‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰BERT-largeã‚’è¶…ãˆã‚‹æ€§èƒ½ã‚’é”æˆã—ã€4000ä¸‡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰ã¯GPT-3ã‚’ä¸Šå›ã‚‹çµæœã‚’å¾—ãŸã€‚ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚³ã‚¹ãƒˆã¯ãã‚Œãã‚Œäº‹å‰å­¦ç¿’ã‚³ã‚¹ãƒˆã®åŒç­‰ã¾ãŸã¯7%ã«æŠ‘ãˆã‚‰ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview: 


<a href="https://openreview.net/forum?id=Bx6qKuBM2AD" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=Bx6qKuBM2AD</a>


</p>
<p>å°è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿæ–½ã—ã€åŒæ§˜ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã€**å„layerã®widthãŒå¤§ãã„ã‚‚ã®**ã«å¯¾ã—ã¦ã‚‚ã€å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§æœ€é©ã§ã‚ã£ãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’zero-shotã§è»¢ç§»ã™ã‚‹ã“ã¨ã§ near optimalãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§å­¦ç¿’ã§ãã‚‹mu Transferã‚’ææ¡ˆã€‚<br><br>ãƒ¢ãƒ‡ãƒ«ã®æ·±ã•ï¼ˆä»¥å¤–ã«ã‚‚ä¸‹è¡¨ä¸­ã®*å°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã«å¯¾ã—ã¦ã‚‚é™å®šçš„ã«è»¢ç§»å¯èƒ½ãªæ¨¡æ§˜ã€‚Post-Layer Normã®Transformerã‚„ã§ã¯ã‚ã¾ã‚Šã†ã¾ãã„ã‹ãªã„ã“ã¨ãŒ11ç¯€ã«è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ï¼ˆå®Ÿé¨“ã¯pre-Layer Norm Transformer, ResNetã«å¯¾ã—ã¦è¡Œã‚ã‚Œã¦ã„ã‚‹æ¨¡æ§˜ï¼‰ã€‚<br>ã¾ãŸã€6.1ç¯€ã§ã¯ã€ï¼ˆå®Ÿé¨“çš„ã«ï¼‰åˆ©ç”¨ã™ã‚‹å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚±ãƒ¼ãƒ«ã¨ã—ã¦å¹…256, æ·±ã•4, ãƒãƒƒãƒã‚µã‚¤ã‚º32, sequenceé•·128, è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—æ•°5000ã‚’æœ€ä½æº€ãŸã—ã¦ãŠã‚Šã€ã‹ã¤ã‚¹ã‚±ãƒ¼ãƒ«ã•ã›ã‚‹å¹…ãŒå¦¥å½“ãªç¯„å›²å†…ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ã€ã¨ã„ã£ãŸè©±ãŒè¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ã€‚<br><br>å‰æçŸ¥è­˜ï¼ˆmuPï¼‰ã‚„æ¡ä»¶ãŒå¤šãã†ãªæ°—ãŒã™ã‚‹ã®ã§ã€ã—ã£ã‹ã‚Šç¢ºèªã—ãŸæ–¹ãŒã‚ˆã•ãã†ã€‚<br>ãŸã¨ãˆã°ã€muPã§åˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã‚„ã€è»¢é€å¯èƒ½ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«é™ã‚ŠãŒã‚ã‚‹ï¼ˆe.g. å­¦ç¿’ç‡ï¼‰ã€ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹finetuningãªã©ã¯è»¢é€ã§ããªã„ãªã©ã€‚<br><br><br>&lt;img width="872" height="336" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/e5aeb152-5c9e-4ba2-9152-4bfef0d7c27c"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/e5aeb152-5c9e-4ba2-9152-4bfef0d7c27c"&lt;/a&gt;


/&gt;</p>
<p>muP:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2583" target="_blank" rel="noopener noreferrer">[Paper Note] Feature Learning in Infinite-Width Neural Networks, Greg Yang+, PMLR'21</a>
</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-08-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2545" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] An Image is Worth 16x16 Words: Transformers for Image Recognition at   Scale, Alexey Dosovitskiy+, ICLR'21</a>
<span class="snippet"><span>GPT Summary</span>- ç´”ç²‹ãªãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ç”»åƒãƒ‘ãƒƒãƒã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ç›´æ¥é©ç”¨ã™ã‚‹ã“ã¨ã§ã€CNNã¸ã®ä¾å­˜ãªã—ã«ç”»åƒåˆ†é¡ã‚¿ã‚¹ã‚¯ã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç™ºæ®ã§ãã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’ã—ã€è¤‡æ•°ã®ç”»åƒèªè­˜ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®CNNã¨æ¯”è¼ƒã—ã¦å„ªã‚ŒãŸçµæœã‚’é”æˆã—ã€è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’å¤§å¹…ã«å‰Šæ¸›ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=YicbFdNTTy" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=YicbFdNTTy</a>


</p>
<p>ViTã‚’ææ¡ˆã—ãŸç ”ç©¶</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-07-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2259" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Swin Transformer V2: Scaling Up Capacity and Resolution, Ze Liu+, arXiv'21</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€å¤§è¦æ¨¡ãƒ“ã‚¸ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨å¿œç”¨ã«ãŠã‘ã‚‹èª²é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã®3ã¤ã®æŠ€è¡“ã‚’ææ¡ˆã€‚å…·ä½“çš„ã«ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®‰å®šæ€§å‘ä¸Šã®ãŸã‚ã®æ®‹å·®å¾Œæ­£è¦åŒ–æ³•ã€ä½è§£åƒåº¦ã‹ã‚‰é«˜è§£åƒåº¦ã¸ã®è»¢é€ã‚’å¯èƒ½ã«ã™ã‚‹ä½ç½®ãƒã‚¤ã‚¢ã‚¹æ³•ã€ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã®å¿…è¦æ€§ã‚’æ¸›å°‘ã•ã›ã‚‹è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’æ³•ã‚’ç”¨ã„ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€30å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®Swin Transformer V2ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€è¤‡æ•°ã®ãƒ“ã‚¸ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã§æ–°è¨˜éŒ²ã‚’æ¨¹ç«‹ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚‚å‘ä¸Šã—ã€ãƒ©ãƒ™ãƒ«ä»˜ããƒ‡ãƒ¼ã‚¿ã¨æ™‚é–“ã‚’å¤§å¹…ã«å‰Šæ¸›ã€‚</span>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-07-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2258" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, Ze Liu+, ICCV'21</a>
<span class="snippet"><span>GPT Summary</span>- Swin Transformerã¯ã€ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ“ã‚¸ãƒ§ãƒ³ã®æ–°ã—ã„ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹éšå±¤çš„ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ææ¡ˆã€‚ã‚·ãƒ•ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ–¹å¼ã«ã‚ˆã‚Šã€åŠ¹ç‡çš„ãªè‡ªå·±æ³¨æ„è¨ˆç®—ã‚’å®Ÿç¾ã—ã€ã•ã¾ã–ã¾ãªã‚¹ã‚±ãƒ¼ãƒ«ã§ã®ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ãŒå¯èƒ½ã€‚ç”»åƒåˆ†é¡ã‚„ç‰©ä½“æ¤œå‡ºã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãªã©ã§å¾“æ¥ã®æœ€å…ˆç«¯ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®ãƒ“ã‚¸ãƒ§ãƒ³ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã¦ã®å¯èƒ½æ€§ã‚’ç¤ºå”†ã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬:


<a href="https://qiita.com/m_sugimura/items/139b182ee7c19c83e70a" target="_blank" rel="noopener noreferrer">https://qiita.com/m_sugimura/items/139b182ee7c19c83e70a</a>


</p>
<p>ç”»åƒå‡¦ç†ã«ãŠã„ã¦ã€ç‰©ä½“ã®ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã‚„ã€è§£åƒåº¦ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ã€PatchMergeã¨å‘¼ã°ã‚Œã‚‹ãƒ—ãƒ¼ãƒªãƒ³ã‚°ã®ã‚ˆã†ãªå‡¦ç†ã¨ã€å›ºå®šã‚µã‚¤ã‚ºã®ãƒ­ãƒ¼ã‚«ãƒ«ãªwindowã«åˆ†å‰²ã—ã¦Self-Attentionã‚’å®Ÿæ–½ã—ã€layerã”ã¨ã«é€šå¸¸ã®windowã¨ã‚·ãƒ•ãƒˆã•ã‚ŒãŸwindowã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§ã€windowé–“ã‚’è·¨ã„ã é–¢ä¿‚æ€§ã‚‚è€ƒæ…®ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹æ©Ÿæ§‹ã‚’å°å…¥ã—ãŸãƒ¢ãƒ‡ãƒ«ã€‚<br><img src="https://github.com/user-attachments/assets/a2d5f78c-27ec-4f18-bd7d-5475085cfa7b" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/92fb10e1-614e-44ef-9e65-3920cd863d46" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/2b8a543a-069e-468a-bc3c-1f288cdcf577" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/FactualKnowledge.html" target="_blank" rel="noopener noreferrer">#FactualKnowledge</a>
<span class="issue_date">Issue Date: 2025-07-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2140" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Transformer Feed-Forward Layers Are Key-Value Memories, Mor Geva+, EMNLP'21</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã¯ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã®å¤§éƒ¨åˆ†ã‚’å ã‚ã‚‹ãŒã€ãã®å½¹å‰²ã¯æœªæ¢æ±‚ã€‚ç ”ç©¶ã«ã‚ˆã‚Šã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ãŒã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ãƒ»ãƒ¡ãƒ¢ãƒªã¨ã—ã¦æ©Ÿèƒ½ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¾‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ç›¸é–¢ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚å®Ÿé¨“ã§ã€ä¸‹å±¤ã¯æµ…ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ä¸Šå±¤ã¯æ„å‘³çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã€ãƒãƒªãƒ¥ãƒ¼ãŒå‡ºåŠ›åˆ†å¸ƒã‚’èª˜å°ã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚æœ€çµ‚çš„ã«ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®å‡ºåŠ›ã¯ãƒ¡ãƒ¢ãƒªã®åˆæˆã§ã‚ã‚Šã€æ®‹å·®æ¥ç¶šã‚’é€šã˜ã¦æ´—ç·´ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬ï¼ˆp.5ã‚ˆã‚Šï¼‰: 


<a href="https://speakerdeck.com/kogoro/knowledge-neurons-in-pretrained-transformers-for-snlp2022?slide=5" target="_blank" rel="noopener noreferrer">https://speakerdeck.com/kogoro/knowledge-neurons-in-pretrained-transformers-for-snlp2022?slide=5</a>


</p></span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<span class="issue_date">Issue Date: 2024-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1333" target="_blank" rel="noopener noreferrer" class="title-link">Transformer Feed-Forward Layers Are Key-Value Memories, Mor Geva+, N_A, EMNLP'21</a>
<span class="snippet"><span>GPT Summary</span>- ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã¯ã€ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ãƒ¡ãƒ¢ãƒªã¨ã—ã¦æ©Ÿèƒ½ã—ã€å­¦ç¿’ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ãŒäººé–“ã«è§£é‡ˆå¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚„ã€ä¸Šä½å±¤ãŒã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚ã•ã‚‰ã«ã€å‡ºåŠ›åˆ†å¸ƒã‚’èª˜å°ã™ã‚‹å½¹å‰²ã‚‚æŒã¡ã¾ã™ã€‚ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®å‡ºåŠ›ã¯ãã®ãƒ¡ãƒ¢ãƒªã®åˆæˆã§ã‚ã‚Šã€æ®‹å·®æ¥ç¶šã‚’ä»‹ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®å±¤ã‚’é€šã˜ã¦æ´—ç·´ã•ã‚Œã€æœ€çµ‚çš„ãªå‡ºåŠ›åˆ†å¸ƒã‚’ç”Ÿæˆã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1108" target="_blank" rel="noopener noreferrer">å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ï½¤ã€ŒçŸ¥è­˜ã¯å…¨çµåˆå±¤ã«è“„ç©ã•ã‚Œã‚‹ã€ã¨ã„ã†ä»®èª¬ã«ã¤ã„ã¦ã®æ–‡çŒ®èª¿æŸ»</a>
 </p>
<p>FF layerãŒKey-Valueã‚¹ãƒˆã‚¢ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ä»•çµ„ã¿ã®æ¦‚ç•¥å›³<br><img src="https://github.com/user-attachments/assets/cc12695f-b030-433a-88e1-aed69f9847a7" alt="image" loading="lazy"><br><br>å®Ÿéš›ã«ç‰¹å®šã®Keyã¨æœ€ã‚‚é–¢é€£åº¦ãŒé«˜ã„è¨“ç·´äº‹ä¾‹ï¼ˆinputï¼‰ã‚’æŠ½å‡ºã—ã€äººé–“ãŒinputã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†é¡ã—ãŸçµæœ<br><img src="https://github.com/user-attachments/assets/d1c1a031-9cb8-4e22-bf87-23964f0e0c71" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<span class="issue_date">Issue Date: 2023-08-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1009" target="_blank" rel="noopener noreferrer" class="title-link">ViLT: Vision-and-Language Transformer Without Convolution or Region   Supervision, Wonjae Kim+, N_A, ICML'21</a>
<span class="snippet"><span>GPT Summary</span>- VLPï¼ˆVision-and-Language Pre-trainingï¼‰ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€ãƒ“ã‚¸ãƒ§ãƒ³ã¨è¨€èªã®ã‚¿ã‚¹ã‚¯ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã¦ã„ã‚‹ãŒã€ç¾åœ¨ã®æ–¹æ³•ã¯åŠ¹ç‡æ€§ã¨è¡¨ç¾åŠ›ã®é¢ã§å•é¡ŒãŒã‚ã‚‹ã€‚ãã“ã§ã€æœ¬ç ”ç©¶ã§ã¯ç•³ã¿è¾¼ã¿ãƒ•ãƒªãƒ¼ã®ãƒ“ã‚¸ãƒ§ãƒ³ã¨è¨€èªã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒï¼ˆViLTï¼‰ãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã™ã‚‹ã€‚ViLTã¯é«˜é€Ÿã§ã‚ã‚ŠãªãŒã‚‰ç«¶äº‰åŠ›ã®ã‚ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã€ã‚³ãƒ¼ãƒ‰ã¨äº‹å‰å­¦ç¿’æ¸ˆã¿ã®é‡ã¿ã¯GitHubã§åˆ©ç”¨å¯èƒ½ã§ã‚ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬:


<a href="https://tech.fusic.co.jp/posts/2021-12-29-vilt/" target="_blank" rel="noopener noreferrer">https://tech.fusic.co.jp/posts/2021-12-29-vilt/</a>


</p></span><br><br>
<a class="button" href="articles/MachineTranslation.html" target="_blank" rel="noopener noreferrer">#MachineTranslation</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/Normalization.html" target="_blank" rel="noopener noreferrer">#Normalization</a>
<a class="button" href="articles/Findings.html" target="_blank" rel="noopener noreferrer">#Findings</a>
<span class="issue_date">Issue Date: 2025-08-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2443" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Query-Key Normalization for Transformers, Alex Henry+, EMNLP'20 Findings</a>
<span class="snippet"><span>GPT Summary</span>- ä½ãƒªã‚½ãƒ¼ã‚¹è¨€èªç¿»è¨³ã«ãŠã„ã¦ã€QKNormã¨ã„ã†æ–°ã—ã„æ­£è¦åŒ–æ‰‹æ³•ã‚’ææ¡ˆã€‚ã“ã‚Œã¯ã€æ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ä¿®æ­£ã—ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã®é£½å’Œè€æ€§ã‚’å‘ä¸Šã•ã›ã¤ã¤è¡¨ç¾åŠ›ã‚’ç¶­æŒã€‚å…·ä½“çš„ã«ã¯ã€ã‚¯ã‚¨ãƒªã¨ã‚­ãƒ¼è¡Œåˆ—ã«å¯¾ã—ã¦$\ell_2$æ­£è¦åŒ–ã‚’é©ç”¨ã—ã€å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã€‚TED Talksã‚³ãƒ¼ãƒ‘ã‚¹ã¨IWSLT'15ã®ä½ãƒªã‚½ãƒ¼ã‚¹ç¿»è¨³ãƒšã‚¢ã§å¹³å‡0.928 BLEUã®æ”¹å–„ã‚’é”æˆã€‚</span>
<span class="snippet"><span>Comment</span><p>QKã«å¯¾ã—ã¦L2æ­£è¦åŒ–ã‚’å®Ÿæ–½ã—ã€learnableãªã‚¹ã‚«ãƒ©ãƒ¼å€¤ã‚’ä¹—ã˜ã‚‹ã“ã¨ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€low resourceãªè¨€èªã§ã®ç¿»è¨³æ€§èƒ½ãŒå‘ä¸Šã€‚MTã§å®Ÿé¨“ã•ã‚Œã¦ã„ã‚‹ãŒã€transformerã®è¡¨ç¾åŠ›ãŒæ”¹å–„ã•ã‚Œã‚‹ã®ã§GLM-4.5ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚‚æ¡ç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚<br><br>dot product attentionã§ã¯å†…ç©ã‚’åˆ©ç”¨ã™ã‚‹ãŸã‚å€¤åŸŸã«åˆ¶ç´„ãŒãªãã€ã‚ã‚‹å˜èªã«ã®ã¿attention scoreãŒé›†ä¸­ã—ã¦ã—ã¾ã„ã€ä»–ã®å…¨ã¦ã®å˜èªã®signalã‚’ã‹ãæ¶ˆã—ã¦ã—ã¾ã†å•é¡ŒãŒã‚ã‚‹ã€‚ã“ã®ãŸã‚ã€QKã‚’ãƒãƒ«ãƒ ã«ã‚ˆã£ã¦æ­£è¦åŒ–ã—ï¼ˆã“ã‚Œã«ã‚ˆã‚Šå®Ÿè³ªQKã¯cosine similarityã¨ãªã‚‹ï¼‰å€¤åŸŸã‚’åˆ¶é™ã™ã‚‹ã€‚ã—ã‹ã—ã“ã†ã™ã‚‹ã¨ä»Šåº¦ã¯ã‚¹ã‚³ã‚¢é–“ã®å·®ãŒå°ã•ã™ãã¦ã€attendã—ãªãã¦ã‚‚è‰¯ã„å˜èªã‚’ç„¡è¦–ã§ããªããªã‚‹ã®ã§ã€learnableãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚¹ã‚±ãƒ¼ãƒ«ã‚’èª¿æ•´ã™ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2025-08-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2388" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Longformer: The Long-Document Transformer, Iz Beltagy+, arXiv'20</a>
<span class="snippet"><span>GPT Summary</span>- Longformerã¯ã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç·šå½¢ã«å‡¦ç†ã§ãã‚‹æ³¨æ„æ©Ÿæ§‹ã‚’æŒã¤Transformerãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã§ã€æ•°åƒãƒˆãƒ¼ã‚¯ãƒ³ã®æ–‡æ›¸ã‚’æ‰±ãˆã‚‹ã€‚å±€æ‰€çš„ãªã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æ³¨æ„ã¨ã‚¿ã‚¹ã‚¯ã«åŸºã¥ãã‚°ãƒ­ãƒ¼ãƒãƒ«æ³¨æ„ã‚’çµ„ã¿åˆã‚ã›ã€æ–‡å­—ãƒ¬ãƒ™ãƒ«ã®è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§æœ€å…ˆç«¯ã®çµæœã‚’é”æˆã€‚äº‹å‰å­¦ç¿’ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã€é•·æ–‡ã‚¿ã‚¹ã‚¯ã§RoBERTaã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚ã¾ãŸã€Longformer-Encoder-Decoderï¼ˆLEDï¼‰ã‚’å°å…¥ã—ã€é•·æ–‡ç”Ÿæˆã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹åŠ¹æœã‚’ç¢ºèªã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>ï¼ˆå›ºå®šã•ã‚ŒãŸå°ã•ã‚ã®windowsã‚µã‚¤ã‚ºã®ä¸­ã§ã®ã¿attentionã‚’è¨ˆç®—ã™ã‚‹ï¼‰sliding window attentionã‚’ææ¡ˆã€‚Figure2ã‚’è¦‹ã‚‹ã¨ã€é€šå¸¸ã®Attentionã¨æ¯”è¼ƒã—ã¦ã€ç¾åœ¨ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®å‘¨è¾ºã®ãƒˆãƒ¼ã‚¯ãƒ³ã«ã—ã‹æ³¨ç›®ã—ãªã„ç‰¹æ€§ãŒå›³ç¤ºã•ã‚Œã¦ãŠã‚Šã€ã‚¤ãƒ¡ãƒ¼ã‚¸ãŒæ´ã¿ã‚„ã™ã„ã€‚<br><br>&lt;img width="795" height="231" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/d1eccdaf-5b5b-4444-ad31-44c54c345d79"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/d1eccdaf-5b5b-4444-ad31-44c54c345d79"&lt;/a&gt;


/&gt;</p>
<p>OpenLLMã®æ–‡è„ˆã ã¨ã€Mistralã«æ¡ç”¨ã•ã‚Œã¦è©±é¡Œã«ãªã£ãŸã‹ã‚‚ï¼Ÿ<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1309" target="_blank" rel="noopener noreferrer">Mistral 7B, Albert Q. Jiang+, N/A, arXiv'23</a>
</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2356" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Transformers are RNNs: Fast Autoregressive Transformers with Linear  Attention, Angelos Katharopoulos+, ICML'20</a>
<span class="snippet"><span>GPT Summary</span>- è‡ªå·±æ³¨æ„ã‚’ã‚«ãƒ¼ãƒãƒ«ç‰¹å¾´ãƒãƒƒãƒ—ã®ç·šå½¢ãƒ‰ãƒƒãƒˆç©ã¨ã—ã¦è¡¨ç¾ã™ã‚‹ã“ã¨ã§ã€Transformersã®è¤‡é›‘æ€§ã‚’$\mathcal{O}\left(N^2\right)$ã‹ã‚‰$\mathcal{O}\left(N\right)$ã«å‰Šæ¸›ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è‡ªå·±å›å¸°å‹Transformersã®é€Ÿåº¦ãŒæœ€å¤§4000å€å‘ä¸Šã—ã€å¾“æ¥ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã€‚</span>
<span class="snippet"><span>Comment</span><p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1210" target="_blank" rel="noopener noreferrer">Transformers are Multi-State RNNs, Matanel Oren+, N/A, EMNLP'24</a>
 </p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2355" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Reformer: The Efficient Transformer, Nikita Kitaev+, ICLR'20</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ã€å±€æ‰€æ„Ÿåº¦ãƒãƒƒã‚·ãƒ¥ã‚’ç”¨ã„ãŸæ³¨æ„æ©Ÿæ§‹ã¨å¯é€†æ®‹å·®å±¤ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¨ˆç®—é‡ã‚’O($L^2$)ã‹ã‚‰O($L\log L$)ã«å‰Šæ¸›ã—ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨é€Ÿåº¦ã‚’å‘ä¸Šã•ã›ãŸReformerãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿç¾ã€‚ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¨åŒç­‰ã®æ€§èƒ½ã‚’ç¶­æŒã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview: 


<a href="https://openreview.net/forum?id=rkgNKkHtvB" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=rkgNKkHtvB</a>


</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2354" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Linformer: Self-Attention with Linear Complexity, Sinong Wang+, arXiv'20</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã¯è‡ªç„¶è¨€èªå‡¦ç†ã§æˆåŠŸã‚’åã‚ã¦ã„ã‚‹ãŒã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¯¾ã—ã¦ã¯é«˜ã‚³ã‚¹ãƒˆã€‚è‡ªå·±æ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã§è¿‘ä¼¼ã—ã€è¤‡é›‘ã•ã‚’$O(n^2)$ã‹ã‚‰$O(n)$ã«å‰Šæ¸›ã™ã‚‹æ–°ã—ã„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¡ãƒ¢ãƒªã¨æ™‚é–“åŠ¹ç‡ãŒå‘ä¸Šã—ãŸç·šå½¢ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã€ŒLinformerã€ãŒæ¨™æº–ãƒ¢ãƒ‡ãƒ«ã¨åŒç­‰ã®æ€§èƒ½ã‚’ç¤ºã™ã€‚</span>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Normalization.html" target="_blank" rel="noopener noreferrer">#Normalization</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<span class="issue_date">Issue Date: 2025-07-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2142" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] On Layer Normalization in the Transformer Architecture, Ruibin Xiong+, arXiv'20</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€Transformerã®å­¦ç¿’ç‡ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æ®µéšã®é‡è¦æ€§ã‚’ç†è«–çš„ã«ç ”ç©¶ã—ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã®ä½ç½®ãŒè¨“ç·´ã®å®‰å®šæ€§ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’ç¤ºã™ã€‚ç‰¹ã«ã€Post-LN Transformerã§ã¯å¤§ããªå‹¾é…ãŒä¸å®‰å®šã•ã‚’å¼•ãèµ·ã“ã™ãŸã‚ã€ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ãŒæœ‰åŠ¹ã§ã‚ã‚‹ä¸€æ–¹ã€Pre-LN Transformerã§ã¯å‹¾é…ãŒè‰¯å¥½ã«æŒ¯ã‚‹èˆã†ãŸã‚ã€ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚’çœç•¥ã§ãã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ãªã—ã®Pre-LN TransformerãŒãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨åŒç­‰ã®çµæœã‚’é”æˆã—ã€è¨“ç·´æ™‚é–“ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ãŒå‰Šæ¸›ã§ãã‚‹ã“ã¨ã‚’ç¢ºèªã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>OpenReview:


<a href="https://openreview.net/forum?id=B1x8anVFPr" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=B1x8anVFPr</a>


</p>
<p>Encoder-Decoderã®Transformerã«ãŠã„ã¦ã€Post-LNã®å ´åˆã¯ã€Warmupã‚’ç„¡ãã™ã¨æœ€çµ‚çš„ãªæ€§èƒ½ãŒæ‚ªåŒ–ã—ã€ã¾ãŸWarmUpã‚¹ãƒ†ãƒƒãƒ—ã®å€¤ã«ã‚ˆã£ã¦ï¼ˆ500 vs. 4000ã§å®Ÿé¨“)ã‚‚æœ€çµ‚çš„ãªæ€§èƒ½ãŒå¤‰åŒ–ã™ã‚‹ã€‚ã“ã‚Œã«ã¯å­¦ç¿’æ™‚ã«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã—ã£ã‹ã‚Šæ¢ç´¢ã—ãªã‘ã‚Œã°ãªã‚‰ãšã€WarmUPã‚’å¤§ããã™ã‚‹ã¨å­¦ç¿’åŠ¹ç‡ãŒè½ã¡ã‚‹ã¨ã„ã†ãƒ‡ãƒ¡ãƒªãƒƒãƒˆãŒã‚ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/e7a26ecd-7905-4e6c-bb9a-29b8289addb0" alt="image" loading="lazy"><br><br>Post-LNã®å ´åˆã¯ã€Pre-LNã¨æ¯”è¼ƒã—ã¦å‹¾é…ãŒå¤§ããã€Warmupã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã—ã£ã‹ã‚Šè¨­è¨ˆã—ãªã„ã¨å¤§ããªå‹¾é…ã«å¯¾ã—ã¦å¤§ããªå­¦ç¿’ç‡ãŒé©ç”¨ã•ã‚Œå­¦ç¿’ãŒä¸å®‰å®šã«ãªã‚‹ã€‚ã“ã‚Œã¯å­¦ç¿’ç‡ã‚’éå¸¸ã«å°ã•ãã—ã€å›ºå®šå€¤ã‚’ä½¿ã†ã“ã¨ã§è§£æ±ºã§ãã‚‹ãŒã€åæŸãŒéå¸¸ã«é…ããªã‚‹ã¨ã„ã†ãƒ‡ãƒ¡ãƒªãƒƒãƒˆãŒã‚ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/afb09f44-c7c9-44ab-9066-3ee788ebd8ee" alt="image" loading="lazy"><br><br>ä¸€æ–¹ã€Pre-LNã¯Warmupç„¡ã—ã§ã‚‚ã€é«˜ã„æ€§èƒ½ãŒé”æˆã§ãã€ä¸Šè¨˜ã®ã‚ˆã†ãªãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®æ‰‹é–“ã‚„å­¦ç¿’åŠ¹ç‡ã®è¦³ç‚¹ã‹ã‚‰åˆ©ç‚¹ãŒã‚ã‚‹ã€ã¿ãŸã„ãªè©±ã®æ¨¡æ§˜ã€‚<br><img src="https://github.com/user-attachments/assets/d675a58b-e876-4e41-a76f-306c2e1ce23f" alt="image" loading="lazy"><br></p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ActivationFunction.html" target="_blank" rel="noopener noreferrer">#ActivationFunction</a>
<span class="issue_date">Issue Date: 2024-05-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1311" target="_blank" rel="noopener noreferrer" class="title-link">GLU Variants Improve Transformer, Noam Shazeer, N_A, arXiv'20</a>
<span class="snippet"><span>GPT Summary</span>- GLUã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’Transformerã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ»ã‚µãƒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ãƒ†ã‚¹ãƒˆã—ã€é€šå¸¸ã®æ´»æ€§åŒ–é–¢æ•°ã‚ˆã‚Šã‚‚ã„ãã¤ã‹ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãŒå“è³ªå‘ä¸Šã‚’ã‚‚ãŸã‚‰ã™ã“ã¨ã‚’ç™ºè¦‹ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>ä¸€èˆ¬çš„ãªFFNã§ã¯ã€linear layerã‚’ã‹ã‘ãŸå¾Œã«ã€ä½•ã‚‰ã‹ã®æ´»æ€§åŒ–é–¢æ•°ã‚’ã‹ã¾ã›ã‚‹æ–¹æ³•ãŒä¸»æµã§ã‚ã‚‹ã€‚<br><br>ã“ã®ã‚ˆã†ãªæ§‹é€ ã®ä¸€ã¤ã¨ã—ã¦GLUãŒã‚ã‚‹ãŒã€linear layerã¨æ´»æ€§åŒ–é–¢æ•°ã«ã¯æ”¹è‰¯ã®ä½™åœ°ãŒã‚ã‚Šã€æ§˜ã€…ãªvariantãŒè€ƒãˆã‚‰ã‚Œã‚‹ãŸã‚ã€è‰²ã€…è©¦ã—ã¾ã—ãŸã€ã¨ã„ã†ã¯ãªã—ã€‚<br><br><br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/72b1d0bb-64ac-4155-9a3b-5624cd06ccc9" alt="image" loading="lazy"><br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/b38321c6-d414-4764-9147-10a5fa83fbe6" alt="image" loading="lazy"><br><br><br><br>ã‚ªãƒªã‚¸ãƒŠãƒ«ã®GLUã¨æ¯”è¼ƒã—ã¦ã€T5ã¨åŒã˜äº‹å‰å­¦ç¿’ã‚¿ã‚¹ã‚¯ã‚’å®Ÿæ–½ã—ãŸã¨ã“ã‚ã€perplexityãŒæ”¹å–„<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/9e67a054-2148-41ed-aae1-5a752c21a242" alt="image" loading="lazy"><br><br><br><br>ã¾ãŸã€finetuningã‚’ã—ãŸå ´åˆã®æ€§èƒ½ã‚‚ã€å¤šãã®å ´åˆã‚ªãƒªã‚¸ãƒŠãƒ«ã®GLUã‚ˆã‚Šã‚‚é«˜ã„æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/77ccab88-e5cc-48fc-b9e0-f2dad24e53e8" alt="image" loading="lazy"><br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/8f60ca8c-50eb-4869-bab4-f02ec6d8e085" alt="image" loading="lazy"><br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/8124fc25-aa7e-4e10-8cd2-9d24c818f410" alt="image" loading="lazy"><br><br><br><br><br><br></p></span><br><br>
<a class="button" href="articles/DocumentSummarization.html" target="_blank" rel="noopener noreferrer">#DocumentSummarization</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineTranslation.html" target="_blank" rel="noopener noreferrer">#MachineTranslation</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/pretrained-LM.html" target="_blank" rel="noopener noreferrer">#pretrained-LM</a>
<span class="issue_date">Issue Date: 2022-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/493" target="_blank" rel="noopener noreferrer" class="title-link">Leveraging Pre-trained Checkpoints for Sequence Generation Tasks, Rothe+, Google Research, TACL'20</a>
<span class="snippet"><span>Comment</span><p># æ¦‚è¦<br><br>BERT-to-BERTè«–æ–‡ã€‚ã“ã‚Œã¾ã§pre-trainedãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’åˆ©ç”¨ã™ã‚‹ç ”ç©¶ã¯ä¸»ã«NLUã§è¡Œã‚ã‚Œã¦ãã¦ãŠã‚Šã€Seq2Seqã§ã¯è¡Œã‚ã‚Œã¦ãã¦ã„ãªã‹ã£ãŸã®ã§ã€ã‚„ã‚Šã¾ã—ãŸã€ã¨ã„ã†è©±ã€‚<br><br>publicly availableãªBERTã®checkpointã‚’åˆ©ç”¨ã—ã€BERTã‚’encoder, decoderä¸¡æ–¹ã«æ¡ç”¨ã™ã‚‹ã“ã¨ã§Seq2Seqã‚’å®Ÿç¾ã€‚å®Ÿç¾ã™ã‚‹ä¸Šã§ã€<br><br>1. decoderå´ã®BERTã¯autoregressiveãªç”Ÿæˆã‚’ã™ã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼ˆå·¦å´ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®attentionã—ã‹è¦‹ã‚Œãªã„ã‚ˆã†ã«ã™ã‚‹ï¼‰<br><br>2. encoder-decoder attentionã‚’æ–°ãŸã«å°å…¥ã™ã‚‹<br><br>ã®2ç‚¹ã‚’å·¥å¤«ã—ã¦ã„ã‚‹ã€‚<br><br><br><br># å®Ÿé¨“<br><br>Sentence Fusion, Sentence Split, Machine Translation, Summarizationã®4ã‚¿ã‚¹ã‚¯ã§å®Ÿé¨“<br><br><br><br>## MT<br><br><img src="https://user-images.githubusercontent.com/12249301/204958483-722106b3-bda2-45a3-bb08-fb4eb429c90c.png" alt="image" loading="lazy"><br><br>BERT2BERTãŒSoTAé”æˆã€‚Edunov+ã®æ‰‹æ³•ã¯ã€data _augmentationã‚’åˆ©ç”¨ã—ãŸæ‰‹æ³•ã§ã‚ã‚Šã€ç´”ç²‹ãªWMT14ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ãŸä¸­ã§ã¯SoTAã ã¨ä¸»å¼µã€‚ç‰¹ã«Encoderå´ã§BERTã‚’ä½¿ã†ã¨ã€Randomã«initializeã—ãŸå ´åˆã¨æ¯”ã¹ã¦æ€§èƒ½ãŒé¡•è‘—ã«ä¸Šæ˜‡ã—ã¦ãŠã‚Šã€ãã®é‡è¦æ€§ã‚’ä¸»å¼µã€‚<br><br>Sentence Fusion, Sentence Splitã§ã¯ã€encoderã¨decoderã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’shareã™ã‚‹ã®ãŒè‰¯ã‹ã£ãŸãŒã€MTã§ã¯æœ‰åŠ¹ã§ã¯ãªã‹ã£ãŸã€‚ã“ã‚Œã¯MTã§ã¯modelã®capacityãŒéå¸¸ã«é‡è¦ã§ã‚ã‚‹ç‚¹ã€encoderã¨decoderã§ç•°ãªã‚‹æ–‡æ³•ã‚’æ‰±ã†ãŸã‚ã§ã‚ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€‚<br><br><br><br>## Summarization<br><br>BERTSHARE, ROBERTASHAREã®çµæœãŒè‰¯ã‹ã£ãŸã€‚<br><br><img src="https://user-images.githubusercontent.com/12249301/204959543-e21bd9a6-bef4-4538-b181-daca93fa33e7.png" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/NaturalLanguageGeneration.html" target="_blank" rel="noopener noreferrer">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/DataToTextGeneration.html" target="_blank" rel="noopener noreferrer">#DataToTextGeneration</a>
<span class="issue_date">Issue Date: 2022-09-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/488" target="_blank" rel="noopener noreferrer" class="title-link">Text-to-Text Pre-Training for Data-to-Text Tasks, Mihir+, Google Research, INLG'20</a>
<span class="snippet"><span>Comment</span><p># æ¦‚è¦<br><br>pre-trainingæ¸ˆã¿ã®T5ã«å¯¾ã—ã¦ã€Data2Textã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§finetuningã‚’å®Ÿæ–½ã™ã‚‹æ–¹æ³•ã‚’ææ¡ˆã€‚WebNLGï¼ˆgraph-to-textï¼‰, ToTToï¼ˆtable-to-textï¼‰, Multiwozï¼ˆtask oriented dialogueï¼‰ãƒ‡ãƒ¼ã‚¿ã«ãŠã„ã¦ã€simpleãªTransformerã§ã‚‚æ´—ç·´ã•ã‚ŒãŸmulti-stageãªpipelined approachã‚’outperformã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ãŸç ”ç©¶ã€‚<br><br><br><br># æ‰‹æ³•<br><br>äº‹å‰å­¦ç¿’æ¸ˆã¿ã®T5ã«å¯¾ã—ã¦fine-tuningã‚’å®Ÿæ–½ã—ãŸã€‚æ‰‹æ³•ã¯ã‚·ãƒ³ãƒ—ãƒ«ã§ã€data-to-textã‚¿ã‚¹ã‚¯ã‚’text-to-textã‚¿ã‚¹ã‚¯ã«å¤‰æ›ã—ãŸã€‚å…·ä½“çš„ã«ã¯ã€æ§‹é€ ã‹ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’flatãªæ–‡å­—åˆ—ï¼ˆlinearizationï¼‰ã§è¡¨ç¾ã™ã‚‹ã“ã¨ã§ã€text-to-textã‚¿ã‚¹ã‚¯ã«å¤‰æ›ã€‚å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹linearizationã®ã‚¤ãƒ¡ãƒ¼ã‚¸ã¯ä¸‹å›³ã€‚ãƒ‡ãƒªãƒŸã‚¿ã‚„ç‰¹æ®Šæ–‡å­—ã‚’ä½¿ã£ã¦æ§‹é€ ã‹ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’flatãªstringã§è¡¨ç¾ã—ã¦ã„ã‚‹ã€‚<br><br><img src="https://user-images.githubusercontent.com/12249301/191689155-3562f4f3-d1a1-4ea0-9d37-a523b78e8922.png" alt="image" loading="lazy"><br><br><br><br># ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ<br><br>## ToTToï¼ˆ2020ï¼‰<br><br>Wikipediaã®ãƒ†ãƒ¼ãƒ–ãƒ«ã¨è‡ªç„¶è¨€èªã§descriptionã®ãƒšã‚¢ãƒ‡ãƒ¼ã‚¿<br><br>## MultiWozï¼ˆ2018ï¼‰<br><br>10Kã®äººé–“åŒå£«ã®task-orientedãªdialogueãƒ‡ãƒ¼ã‚¿ã€‚<br><br>## WebNLGï¼ˆ2017ï¼‰<br><br>subject-object-predicateã®3çµ„ã¿ã‚’ãƒ†ã‚­ã‚¹ãƒˆè¡¨ç¾ã«å¤‰æ›ã™ã‚‹ã‚¿ã‚¹ã‚¯ã®ãƒ‡ãƒ¼ã‚¿<br><br><br><br><img src="https://user-images.githubusercontent.com/12249301/191693682-3cf3302f-b4e2-433d-94ed-995a8a908d0c.png" alt="image" loading="lazy"><br><br><br><br># Result<br><br>## WebNLG<br><br><img src="https://user-images.githubusercontent.com/12249301/191694085-7bf7348a-b468-46e0-a900-c0090d1abcba.png" alt="image" loading="lazy"><br><br>GCNã‚’åˆ©ç”¨ã—ãŸ2020å¹´ã«ææ¡ˆã•ã‚ŒãŸDualEncãŒSoTAã ã£ãŸã‚‰ã—ã„ãŒã€outperormã—ã¦ã„ã‚‹ã€‚<br><br><br><br>## ToTTo<br><br><img src="https://user-images.githubusercontent.com/12249301/191694683-f31ccad1-2936-4c21-ac10-0807a848f043.png" alt="image" loading="lazy"><br><br>[ã“ã¡ã‚‰](


<a href="https://github.com/google-research-datasets/totto)%E3%81%AE%E3%83%AA%E3%83%BC%E3%83%80%E3%83%BC%E3%83%9C%E3%83%BC%E3%83%89%E3%81%A8%E6%AF%94%E8%BC%83%E3%81%97%E3%81%A6SoTA%E3%82%92%E8%A8%98%E9%8C%B2" target="_blank" rel="noopener noreferrer">https://github.com/google-research-datasets/totto)ã®ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ã¨æ¯”è¼ƒã—ã¦SoTAã‚’è¨˜éŒ²</a>


<br><br><br><br>## MultiWoz<br><br><img src="https://user-images.githubusercontent.com/12249301/191695459-e3397936-bdf7-4450-b4c2-6f6eead0825d.png" alt="image" loading="lazy"><br><br>T5ã¯äº‹å‰å­¦ç¿’æ¸ˆã¿GPT-2ã‚’finetuningã—ãŸæ‰‹æ³•ã‚‚outperformã—ãŸã€‚SC-GPT2ã¯å½“æ™‚ã®MultiWozã§ã®SoTA<br><br><br><br># Impact of Model capacity<br><br>T5ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºãŒã©ã‚ŒãŒè‰¯ã„ã‹ã«ã¤ã„ã¦ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºã¨è¤‡é›‘ã•ã«ä¾å­˜ã™ã‚‹ã“ã¨ã‚’è€ƒå¯Ÿã—ã¦ã„ã‚‹ã€‚ãŸã¨ãˆã°ã€MultiWozãƒ‡ãƒ¼ã‚¿ã¯æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ãŒæœ€ã‚‚å°‘ãªãã€ãƒ‡ãƒ¼ã‚¿é‡ã‚‚56kã¨æ¯”è¼ƒçš„å¤šã‹ã£ãŸã€‚ã“ã®ãŸã‚ã€T5-smallã§ã‚‚ã‚ˆã‚Šå¤§ãã„ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã«è‚‰è–„ã§ãã¦ã„ã‚‹ã€‚<br><br>ä¸€æ–¹ã€WebNLGãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€18kã—ã‹äº‹ä¾‹ãŒãªãã€ç‰¹å¾´é‡ã‚‚ç´„200ç¨®é¡ç¨‹åº¦ã®relationã®ã¿ã§ã‚ã‚‹ã€‚ã“ã®ã‚ˆã†ãªå ´åˆã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹ã«ã¤ã‚Œãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚‚å‘ä¸Šã—ãŸï¼ˆç‰¹ã«Unseen test setï¼‰ã€‚ç‰¹ã«BLEUã‚¹ã‚³ã‚¢ã¯T5-smallãŒT5-baseã«ãªã‚‹ã¨ã€10ãƒã‚¤ãƒ³ãƒˆã‚‚ã‚¸ãƒ£ãƒ³ãƒ—ã—ã¦ãŠã‚Šã€modelã®capacityãŒout-of-domainã«å¯¾ã™ã‚‹ä¸€èˆ¬åŒ–ã«å¯¾ã—ã¦criticalã§ã‚ã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚ToTToãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã‚‚ã€Smallã‹ã‚‰Baseã«ã™ã‚‹ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯æ”¹å–„ã—ãŸã€‚</p>
<p># æ‰€æ„Ÿ<br><br>ã“ã‚“ãªç°¡å˜ãªfine-tuningã§SoTAã‚’é”æˆã§ãã¦ã—ã¾ã†ã¨ã¯ã€æœ«æã‚ã—ã„ã€‚ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦æœ‰ç”¨ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/Generalization.html" target="_blank" rel="noopener noreferrer">#Generalization</a>
<a class="button" href="articles/RecurrentModels.html" target="_blank" rel="noopener noreferrer">#RecurrentModels</a>
<span class="issue_date">Issue Date: 2025-08-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2605" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Universal Transformers, Mostafa Dehghani+, ICLR'19</a>
<span class="snippet"><span>GPT Summary</span>- å†å¸°ç¥çµŒãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆRNNï¼‰ã¯é€æ¬¡å‡¦ç†ã«ã‚ˆã‚Šã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã§åºƒãä½¿ã‚ã‚Œã¦ããŸãŒã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé…ããªã‚‹æ¬ ç‚¹ãŒã‚ã‚‹ã€‚æœ€è¿‘ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ã‚„ç•³ã¿è¾¼ã¿ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ä¸¦åˆ—å‡¦ç†ãŒå¯èƒ½ã§å„ªã‚ŒãŸçµæœã‚’å‡ºã—ã¦ã„ã‚‹ãŒã€RNNãŒå¾—æ„ã¨ã™ã‚‹å˜ç´”ãªã‚¿ã‚¹ã‚¯ã§ã®ä¸€èˆ¬åŒ–ã«ã¯å¤±æ•—ã™ã‚‹ã€‚ãã“ã§ã€æˆ‘ã€…ã¯ãƒ¦ãƒ‹ãƒãƒ¼ã‚µãƒ«ãƒ»ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ï¼ˆUTï¼‰ã‚’ææ¡ˆã—ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ã®ä¸¦åˆ—å‡¦ç†èƒ½åŠ›ã¨RNNã®å¸°ç´ãƒã‚¤ã‚¢ã‚¹ã‚’çµ„ã¿åˆã‚ã›ãŸãƒ¢ãƒ‡ãƒ«ã‚’é–‹ç™ºã—ãŸã€‚UTã¯ç‰¹å®šã®æ¡ä»¶ä¸‹ã§ãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°å®Œå…¨ã§ã‚ã‚Šã€å®Ÿé¨“ã§ã¯æ¨™æº–çš„ãªãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€ç‰¹ã«LAMBADAã‚¿ã‚¹ã‚¯ã§æ–°ãŸãªæœ€å…ˆç«¯ã‚’é”æˆã—ã€æ©Ÿæ¢°ç¿»è¨³ã§ã‚‚BLEUã‚¹ã‚³ã‚¢ã‚’æ”¹å–„ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=HyzdRiR9Y7" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=HyzdRiR9Y7</a>


</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-08-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2500" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Supervised Multimodal Bitransformers for Classifying Images and Text, Douwe Kiela+, arXiv'19</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ†ã‚­ã‚¹ãƒˆã¨ç”»åƒæƒ…å ±ã‚’èåˆã™ã‚‹ç›£è¦–å‹ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ“ãƒƒãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆã—ã€ã•ã¾ã–ã¾ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åˆ†é¡ã‚¿ã‚¹ã‚¯ã§æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’é”æˆã€‚ç‰¹ã«ã€é›£æ˜“åº¦ã®é«˜ã„ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã§ã‚‚å¼·åŠ›ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚‹çµæœã‚’å¾—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>ãƒ†ã‚­ã‚¹ãƒˆ+imageã‚’ç”¨ã„ã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªtransformer</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/PositionalEncoding.html" target="_blank" rel="noopener noreferrer">#PositionalEncoding</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2359" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context, Zihang Dai+, ACL'19</a>
<span class="snippet"><span>GPT Summary</span>- Transformer-XLã¯ã€å›ºå®šé•·ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¶…ãˆãŸé•·æœŸçš„ãªä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã™ã‚‹æ–°ã—ã„ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ã®å†å¸°ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¨æ–°ã—ã„ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¡ç”¨ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€RNNã‚ˆã‚Š80%ã€å¾“æ¥ã®Transformersã‚ˆã‚Š450%é•·ã„ä¾å­˜é–¢ä¿‚ã‚’å­¦ç¿’ã—ã€è©•ä¾¡æ™‚ã«ã¯æœ€å¤§1,800å€ã®é€Ÿåº¦å‘ä¸Šã‚’å®Ÿç¾ã€‚enwiki8ã‚„WikiText-103ãªã©ã§æœ€å…ˆç«¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã€æ•°åƒãƒˆãƒ¼ã‚¯ãƒ³ã®ä¸€è²«ã—ãŸãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚‚å¯èƒ½ã€‚ã‚³ãƒ¼ãƒ‰ã¨ãƒ¢ãƒ‡ãƒ«ã¯Tensorflowã¨PyTorchã§åˆ©ç”¨å¯èƒ½ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/329" target="_blank" rel="noopener noreferrer">äº‹å‰å­¦ç¿’è¨€èªãƒ¢ãƒ‡ãƒ«ã®å‹•å‘ / Survey of Pretrained Language Models, Kyosuke Nishida, 2019</a>
</p>
<p>3.2ç¯€ã®å®šå¼åŒ–ã‚’è¦‹ã‚‹ã¨ã€ä¸€ã¤å‰ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³ãƒ»layerã”ã¨ã®hidden stateã‚’ã€ç¾åœ¨ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®å¯¾å¿œã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã¨layerã®hidden stateã«concatã—ï¼ˆéå»ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«å½±éŸ¿ã‚’ä¸ãˆãªã„ã‚ˆã†ã«å‹¾é…ã‚’ä¼æ¬ã•ã›ãªã„Stop-Gradientã‚’é©ç”¨ã™ã‚‹ï¼‰ã€QKVã®ã†ã¡ã€KVã®è¨ˆç®—ã«æ´»ç”¨ã—ã¦ã„ã‚‹ã€‚ã¾ãŸã€çµ¶å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’åˆ©ç”¨ã™ã‚‹ã¨ãƒ¢ãƒ‡ãƒ«ãŒã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–“ã®æ™‚ç³»åˆ—çš„ãªé–¢ä¿‚ã‚’èªè­˜ã§ããªããªã‚‹ãŸã‚ã€ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«ã¯ç›¸å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’åˆ©ç”¨ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç¾åœ¨ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã®KVãŒä¸€ã¤å‰ã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã«ã‚ˆã£ã¦æ¡ä»¶ã¥ã‘ã‚‰ã‚Œã€contextã¨ã—ã¦è€ƒæ…®ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã¨ãªã‚Šã€ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–“ã‚’è·¨ã„ã ä¾å­˜é–¢ä¿‚ã®è€ƒæ…®ãŒå®Ÿç¾ã•ã‚Œã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<span class="issue_date">Issue Date: 2024-10-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1446" target="_blank" rel="noopener noreferrer" class="title-link">What Does BERT Learn about the Structure of Language?, Jawahar+, ACL'19</a>
<span class="snippet"><span>GPT Summary</span>- BERTã¯è¨€èªç†è§£ã«ãŠã„ã¦å„ªã‚ŒãŸæˆæœã‚’ä¸Šã’ã¦ãŠã‚Šã€æœ¬ç ”ç©¶ã§ã¯ãã®è¨€èªæ§‹é€ ã®è¦ç´ ã‚’è§£æ˜ã™ã‚‹å®Ÿé¨“ã‚’è¡Œã£ãŸã€‚ä¸»ãªç™ºè¦‹ã¯ã€ãƒ•ãƒ¬ãƒ¼ã‚ºè¡¨ç¾ãŒãƒ•ãƒ¬ãƒ¼ã‚ºãƒ¬ãƒ™ãƒ«ã®æƒ…å ±ã‚’æ‰ãˆã€ä¸­é–“å±¤ãŒæ§‹æ–‡çš„ãŠã‚ˆã³æ„å‘³çš„ç‰¹å¾´ã®éšå±¤ã‚’å½¢æˆã—ã€é•·æœŸä¾å­˜æ€§ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«æ·±ã„å±¤ãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ã€ã•ã‚‰ã«BERTã®æ§‹æˆãŒå¤å…¸çš„ãªæœ¨æ§‹é€ ã«é¡ä¼¼ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1370" target="_blank" rel="noopener noreferrer">å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« (LLM) ã®æŠ€è¡“ã¨æœ€æ–°å‹•å‘, Ikuya Yamada, 2024.06</a>
 ä¸­ã§å¼•ç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚Transformerã®å„ãƒ–ãƒ­ãƒƒã‚¯ãŒã€ä½•ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã‹ã‚’åˆ†æã€‚</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1272" target="_blank" rel="noopener noreferrer" class="title-link">Fast Transformer Decoding: One Write-Head is All You Need, Noam Shazeer, N_A, arXiv'19</a>
<span class="snippet"><span>GPT Summary</span>- ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯é«˜é€Ÿã‹ã¤ç°¡å˜ã ãŒã€å¢—åˆ†æ¨è«–ã¯å¤§ããª"keys"ã¨"values"ãƒ†ãƒ³ã‚½ãƒ«ã‚’ç¹°ã‚Šè¿”ã—èª­ã¿è¾¼ã‚€ãŸã‚ã«é…ããªã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚ãã“ã§ã€ã‚­ãƒ¼ã¨å€¤ã‚’å…±æœ‰ã™ã‚‹ãƒãƒ«ãƒã‚¯ã‚¨ãƒªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆã—ã€ãƒ¡ãƒ¢ãƒªå¸¯åŸŸå¹…è¦ä»¶ã‚’ä½æ¸›ã™ã‚‹ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€é«˜é€Ÿãªãƒ‡ã‚³ãƒ¼ãƒ‰ãŒå¯èƒ½ã§ã€ã‚ãšã‹ãªå“è³ªã®ä½ä¸‹ã—ã‹ãªã„ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>Multi Query Attentionè«–æ–‡ã€‚KVã®setã«å¯¾ã—ã¦ã€å˜ä¸€ã®Queryã®ã¿ã§Multi-Head Attentionã‚’ä»£æ›¿ã™ã‚‹ã€‚åŠ‡çš„ã«Decoderã®InferenceãŒæ—©ããªã‚Šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒæ¸›ã‚‹ãŒã€è«–æ–‡ä¸­ã§ã¯è¨€åŠã•ã‚Œã¦ã„ãªã„ï¼Ÿã‚ˆã†ã ãŒã€æ€§èƒ½ã¨å­¦ç¿’ã®å®‰å®šæ€§ãŒèª²é¡Œã¨ãªã‚‹ã‚ˆã†ã§ã‚ã‚‹ã€‚<br><br><br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/e2d77b43-70c3-4922-a822-bf95d6b4704f" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/PositionalEncoding.html" target="_blank" rel="noopener noreferrer">#PositionalEncoding</a>
<span class="issue_date">Issue Date: 2025-08-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2386" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Self-Attention with Relative Position Representations, Peter Shaw+, NAACL'18</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€Transformerã®è‡ªå·±æ³¨æ„æ©Ÿæ§‹ã‚’æ‹¡å¼µã—ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹è¦ç´ é–“ã®ç›¸å¯¾çš„ãªä½ç½®ã‚’åŠ¹ç‡çš„ã«è€ƒæ…®ã™ã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã€‚WMT 2014ã®ç¿»è¨³ã‚¿ã‚¹ã‚¯ã§1.3 BLEUãŠã‚ˆã³0.3 BLEUã®æ”¹å–„ã‚’é”æˆã€‚ç›¸å¯¾ä½ç½®ã¨çµ¶å¯¾ä½ç½®ã®çµ„ã¿åˆã‚ã›ã§ã¯ã•ã‚‰ãªã‚‹æ”¹å–„ã¯è¦‹ã‚‰ã‚Œãªã‹ã£ãŸã€‚ææ¡ˆæ‰‹æ³•ã¯ã€ä»»æ„ã®ã‚°ãƒ©ãƒ•ãƒ©ãƒ™ãƒ«ä»˜ãå…¥åŠ›ã«ä¸€èˆ¬åŒ–å¯èƒ½ãªé–¢ä¿‚èªè­˜è‡ªå·±æ³¨æ„æ©Ÿæ§‹ã¨ã—ã¦ä½ç½®ä»˜ã‘ã‚‰ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>ç›¸å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ææ¡ˆã—ãŸç ”ç©¶</p>
<p>çµ¶å¯¾ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã¯<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/245" target="_blank" rel="noopener noreferrer">[Paper Note] Attention Is All You Need, Ashish Vaswani+, arXiv'17</a>
</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2353" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Efficient Attention: Attention with Linear Complexities, Zhuoran Shen+, arXiv'18</a>
<span class="snippet"><span>GPT Summary</span>- æ–°ã—ã„åŠ¹ç‡çš„ãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ææ¡ˆã—ã€ãƒ‰ãƒƒãƒˆç©ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã¨åŒç­‰ã®æ€§èƒ½ã‚’ç¶­æŒã—ã¤ã¤ã€ãƒ¡ãƒ¢ãƒªã¨è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æŸ”è»Ÿãªçµ±åˆãŒå¯èƒ½ã¨ãªã‚Šã€ç²¾åº¦å‘ä¸Šã‚’å®Ÿç¾ã€‚å®Ÿé¨“çµæœã§ã¯ã€MS-COCO 2017ã§ã®ç‰©ä½“æ¤œå‡ºã‚„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã§ã®æ€§èƒ½å‘ä¸ŠãŒç¢ºèªã•ã‚Œã€Scene Flowãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã¯æœ€å…ˆç«¯ã®ç²¾åº¦ã‚’é”æˆã€‚ã‚³ãƒ¼ãƒ‰ã¯å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>Figure1ã‚’è¦‹ã‚‹ã¨ã‚³ãƒ³ã‚»ãƒ—ãƒˆãŒä¸€ç›®ã§ã‚ã‹ã‚Šã€éå¸¸ã«ã‚ã‹ã‚Šã‚„ã™ã„<br>&lt;img width="1068" height="580" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/18e6a7da-fc07-495f-bda6-bcef4acab321"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/18e6a7da-fc07-495f-bda6-bcef4acab321"&lt;/a&gt;


/&gt;</p></span><br><br>
<a class="button" href="articles/RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/SequentialRecommendation.html" target="_blank" rel="noopener noreferrer">#SequentialRecommendation</a>
<a class="button" href="articles/ICDM.html" target="_blank" rel="noopener noreferrer">#ICDM</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-07-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2137" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Self-Attentive Sequential Recommendation, Wang-Cheng Kang+, ICDM'18</a>
<span class="snippet"><span>GPT Summary</span>- è‡ªå·±æ³¨æ„ã«åŸºã¥ãé€æ¬¡ãƒ¢ãƒ‡ãƒ«ï¼ˆSASRecï¼‰ã‚’ææ¡ˆã—ã€ãƒãƒ«ã‚³ãƒ•é€£é–ã¨å†å¸°å‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åˆ©ç‚¹ã‚’çµ±åˆã€‚SASRecã¯ã€å°‘æ•°ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰æ¬¡ã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’äºˆæ¸¬ã—ã€ã‚¹ãƒ‘ãƒ¼ã‚¹ãŠã‚ˆã³å¯†ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§æœ€å…ˆç«¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡æ€§ã¨æ³¨æ„é‡ã¿ã®è¦–è¦šåŒ–ã«ã‚ˆã‚Šã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å¯†åº¦ã«å¿œã˜ãŸé©å¿œçš„ãªå‡¦ç†ãŒå¯èƒ½ã§ã‚ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚</span>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineTranslation.html" target="_blank" rel="noopener noreferrer">#MachineTranslation</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/PositionalEncoding.html" target="_blank" rel="noopener noreferrer">#PositionalEncoding</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2018-01-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/245" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Attention Is All You Need, Ashish Vaswani+, arXiv'17</a>
<span class="snippet"><span>GPT Summary</span>- Transformerã¯ã€å†å¸°ã‚„ç•³ã¿è¾¼ã¿ã‚’æ’é™¤ã—ã€æ³¨æ„æ©Ÿæ§‹ã®ã¿ã«åŸºã¥ã„ãŸæ–°ã—ã„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã‚ã‚‹ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€æ©Ÿæ¢°ç¿»è¨³ã‚¿ã‚¹ã‚¯ã§å„ªã‚ŒãŸå“è³ªã‚’ç¤ºã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“ã‚’å¤§å¹…ã«çŸ­ç¸®ã€‚WMT 2014ã®è‹±ç‹¬ç¿»è¨³ã§28.4 BLEUã€è‹±ä»ç¿»è¨³ã§41.8 BLEUã‚’é”æˆã—ã€æ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚ã¾ãŸã€è‹±èªã®æ§‹æ–‡è§£æã«ã‚‚æˆåŠŸè£ã«é©ç”¨å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>Transformer (self-attentionã‚’åˆ©ç”¨) è«–æ–‡<br><br>è§£èª¬ã‚¹ãƒ©ã‚¤ãƒ‰ï¼š


<a href="https://www.slideshare.net/DeepLearningJP2016/dlattention-is-all-you-need" target="_blank" rel="noopener noreferrer">https://www.slideshare.net/DeepLearningJP2016/dlattention-is-all-you-need</a>


<br><br>è§£èª¬è¨˜äº‹ï¼š


<a href="https://qiita.com/nishiba/items/1c99bc7ddcb2d62667c6" target="_blank" rel="noopener noreferrer">https://qiita.com/nishiba/items/1c99bc7ddcb2d62667c6</a>


<br><br><br><br>* æ–°ã—ã„ç¿»è¨³ãƒ¢ãƒ‡ãƒ«(Transformer)ã‚’ææ¡ˆã€‚æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚ä¸¦åˆ—åŒ–ã«å¯¾å¿œã—ã¦ãŠã‚Šã€çŸ­æ™‚é–“ã®è¨“ç·´ã§ï¼ˆæ—¢å­˜ãƒ¢ãƒ‡ãƒ«ã®1/4ä»¥ä¸‹ã®ã‚³ã‚¹ãƒˆï¼‰é«˜ã„BLEUã‚¹ã‚³ã‚¢ã‚’é”æˆã—ãŸã€‚<br><br>* Transformerã¯RNNã‚„CNNã‚’ä½¿ã‚ãšã€attentionãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«åŸºã¥ã„ã¦ã„ã‚‹ã€‚<br><br><br><br>ï¼ˆè§£èª¬ã‚ˆã‚Šï¼‰</p>
<p>åˆ†ã‹ã‚Šã‚„ã™ã„:<br>


<a href="https://qiita.com/halhorn/items/c91497522be27bde17ce" target="_blank" rel="noopener noreferrer">https://qiita.com/halhorn/items/c91497522be27bde17ce</a>


</p>
<p>Transformerã®å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã§ã®outputã®shapeã‚„ã€attention_maskã®å½¢çŠ¶ã€å®Ÿè£…ã«ã¤ã„ã¦è¨˜è¿°ã•ã‚Œã¦ãŠã‚Šæœ‰ç”¨:<br>


<a href="https://qiita.com/FuwaraMiyasaki/items/239f3528053889847825" target="_blank" rel="noopener noreferrer">https://qiita.com/FuwaraMiyasaki/items/239f3528053889847825</a>


</p>
<p>é›†åˆçŸ¥</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<a class="button" href="articles/Normalization.html" target="_blank" rel="noopener noreferrer">#Normalization</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-04-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1857" target="_blank" rel="noopener noreferrer" class="title-link">Batch Normalization: Accelerating Deep Network Training by Reducing   Internal Covariate Shift, Sergey Ioffe+, ICML'15</a>
<span class="snippet"><span>GPT Summary</span>- ãƒãƒƒãƒæ­£è¦åŒ–ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€æ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã‘ã‚‹å†…éƒ¨å…±å¤‰é‡ã‚·ãƒ•ãƒˆã®å•é¡Œã‚’è§£æ±ºã—ã€é«˜ã„å­¦ç¿’ç‡ã‚’å¯èƒ½ã«ã—ã€åˆæœŸåŒ–ã®æ³¨æ„ã‚’è»½æ¸›ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€åŒã˜ç²¾åº¦ã‚’14å€å°‘ãªã„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ†ãƒƒãƒ—ã§é”æˆã—ã€ImageNetåˆ†é¡ã§æœ€è‰¯ã®å…¬è¡¨çµæœã‚’4.9%æ”¹å–„ã€‚</span>
<span class="snippet"><span>Comment</span><p>ãƒ¡ãƒ¢ã£ã¦ãªã‹ã£ãŸã®ã§ä»Šæ›´ãªãŒã‚‰è¿½åŠ ã—ãŸ</p>
<p>å…±å¤‰é‡ã‚·ãƒ•ãƒˆã‚„Batch Normalizationã®èª¬æ˜ã¯<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/261" target="_blank" rel="noopener noreferrer">[Paper Note] Layer Normalization, Ba+, arXiv'16</a>
<br><br>è¨˜è¼‰ã®ã‚¹ãƒ©ã‚¤ãƒ‰ãŒåˆ†ã‹ã‚Šã‚„ã™ã„ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<a class="button" href="articles/Medical.html" target="_blank" rel="noopener noreferrer">#Medical</a>
<span class="issue_date">Issue Date: 2025-11-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3680" target="_blank" rel="noopener noreferrer" class="title-link">How to Train a State-of-the-Art Pathology Foundation Model with $1.6k, Kaplan+, 2025.11</a>
<span class="snippet"><span>GPT Summary</span>- OpenMidnightã¯ã€Midnightç—…ç†åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã‚’å†ç¾ãƒ»æ”¹å–„ã—ãŸã‚‚ã®ã§ã€12,000æšã®å…¨ã‚¹ãƒ©ã‚¤ãƒ‰ç”»åƒã‚’ç”¨ã„ã¦$1.6Kã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€è¤‡æ•°ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§æœ€å…ˆç«¯ã®æ€§èƒ½ã‚’é”æˆã€‚å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ãªã—ã§ã‚‚ãƒˆãƒƒãƒ—ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå¯èƒ½ã§ã‚ã‚Šã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ã‚³ãƒ¼ãƒ‰ã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã‚’å…¬é–‹ã—ã¦ç ”ç©¶ã‚’ä¿ƒé€²ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>HF:


<a href="https://huggingface.co/SophontAI/OpenMidnight" target="_blank" rel="noopener noreferrer">https://huggingface.co/SophontAI/OpenMidnight</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆã‚ˆã‚Š<br><br>&gt; The surprising performance of our model points to the challenges of the pathology FM space. <br>&gt; Performance doesn't seem to scale with compute or dataset size, and for some benchmarks, really simple baselines perform shockingly well.<br><br>&gt; In our mind, this indicates both that current models aren't being trained efficiently, and that the current benchmarks are poor.<br><br>ã¾ã ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã‚„è¨ˆç®—é‡ã«å¿œã˜ã¦ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¦ã„ã‚‹ã‚ˆã†ã«ã¯è¦‹ãˆãšã€ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ãŒåŠ¹ç‡çš„ã«å­¦ç¿’ãŒã§ãã¦ã¨ã‚‰ãšã€ã‹ã¤ç¾åœ¨ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãŒãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’é©åˆ‡ã«æ¸¬ã‚Œã¦ã„ãªã„ã®ã§ã¯ã€ã¨ã„ã£ãŸè©±ãŒè¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ã€‚èˆˆå‘³æ·±ã„ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/MultiLingual.html" target="_blank" rel="noopener noreferrer">#MultiLingual</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/AutomaticSpeechRecognition(ASR).html" target="_blank" rel="noopener noreferrer">#AutomaticSpeechRecognition(ASR)</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/AudioLanguageModel.html" target="_blank" rel="noopener noreferrer">#AudioLanguageModel</a>
<span class="issue_date">Issue Date: 2025-11-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3649" target="_blank" rel="noopener noreferrer" class="title-link">Omnilingual ASR: Advancing Automatic Speech Recognition for 1,600+ Languages, Meta, 2025.11</a>
<span class="snippet"><span>Comment</span><p>paper:


<a href="https://scontent-nrt1-2.xx.fbcdn.net/v/t39.2365-6/581068541_867604242498398_5662399655411595851_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=roRXUCWwUzgQ7kNvwGfUHdX&_nc_oc=Adk1jwJ3ikYa7-wjyoYuwAWxspuId2sUB5R3ZFF_nob0zB5jE6dql9wPt6OXGp9hJjE&_nc_zt=14&_nc_ht=scontent-nrt1-2.xx&_nc_gid=_HnDT1USFOsMkvlcwznXoQ&oh=00_AfjbS8ajtH_TlDsUoGJIPal9Vq0iq0BL4gKvBSdqHsZ3Sw&oe=6919E35F" target="_blank" rel="noopener noreferrer">https://scontent-nrt1-2.xx.fbcdn.net/v/t39.2365-6/581068541_867604242498398_5662399655411595851_n.pdf?_nc_cat=104&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=roRXUCWwUzgQ7kNvwGfUHdX&_nc_oc=Adk1jwJ3ikYa7-wjyoYuwAWxspuId2sUB5R3ZFF_nob0zB5jE6dql9wPt6OXGp9hJjE&_nc_zt=14&_nc_ht=scontent-nrt1-2.xx&_nc_gid=_HnDT1USFOsMkvlcwznXoQ&oh=00_AfjbS8ajtH_TlDsUoGJIPal9Vq0iq0BL4gKvBSdqHsZ3Sw&oe=6919E35F</a>


</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-10-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3509" target="_blank" rel="noopener noreferrer" class="title-link">Everything About Transformers, Krupa Dave, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/webbigdata/status/1983457842960236664?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã–ã£ã¨è¦‹ãŸæ„Ÿã˜transformerã®åŸºæœ¬çš„ãªå†…å®¹ã®ä¸å¯§ãªè§£èª¬ã«è¦‹ãˆã‚‹ã€‚literature(RNNã‚„ã€LSTMã€seq2seqãªã©ï¼‰ã€self/cross-attention,LayerNorm, ResidualConnection, PositionalEncodingã¨ã„ã£ãŸè©±ã®åŸºç¤ãŒå›³è§£ä»˜ãã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/VariationalAutoEncoder.html" target="_blank" rel="noopener noreferrer">#VariationalAutoEncoder</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<span class="issue_date">Issue Date: 2025-10-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3440" target="_blank" rel="noopener noreferrer" class="title-link">LongCat-Video Techcal Report, Meituan LongCat Team, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/teortaxestex/status/1982013157926125724?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>HF:


<a href="https://huggingface.co/meituan-longcat/LongCat-Video" target="_blank" rel="noopener noreferrer">https://huggingface.co/meituan-longcat/LongCat-Video</a>


</p>
<p>å…¬å¼ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/meituan_longcat/status/1982083998852763838?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<a class="button" href="articles/GPUKernel.html" target="_blank" rel="noopener noreferrer">#GPUKernel</a>
<span class="issue_date">Issue Date: 2025-10-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3382" target="_blank" rel="noopener noreferrer" class="title-link">FlashInfer-Bench: Building the Virtuous Cycle for AI-driven LLM Systems, FlashInfer Community, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/tqchenml/status/1980711885147373832?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>GPUã‚«ãƒ¼ãƒãƒ«ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹è‡ªå‹•æœ€é©åŒ–ã®ãŸã‚ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã®ã“ã¨ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3202" target="_blank" rel="noopener noreferrer" class="title-link">Introducing Stable Diffusion 3.5, StabilityAI, 2024.10</a>
<span class="snippet"><span>Comment</span><p>SD3.5</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<span class="issue_date">Issue Date: 2025-10-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3072" target="_blank" rel="noopener noreferrer" class="title-link">IBM Granite 4.0: hyper-efficient, high performance hybrid models for enterprise, IBM, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1973837846898184596?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Mamba2ã¨transformerã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã§ã€æ¯”ç‡ã¯9:1ã¨Mamba2ãƒ–ãƒ­ãƒƒã‚¯ãŒå¤šã‚ã‚‰ã—ã„ã€‚Mamba2ã®æ©æµã«ã‚ˆã‚Šlokg-contextæ™‚ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒ70ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆå‰Šæ¸›ã•ã‚Œã‚‹ã¨ã®ã“ã¨ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<span class="issue_date">Issue Date: 2025-08-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2563" target="_blank" rel="noopener noreferrer" class="title-link">Wan-S2V: Audio-Driven Cinematic Video Generation, Alibaba, 2025.08</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/alibaba_wan/status/1960350593660367303?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2312" target="_blank" rel="noopener noreferrer">Wan2.2, Alibaba Wan, 2025.07</a>
</p>
<p>image+Audio-to-video generation</p>
<p>Audioãƒ¢ãƒ€ãƒªãƒ†ã‚£: wav2vec+AudioEncoder<br>Visionãƒ¢ãƒ€ãƒªãƒ†ã‚£: 3D VAE Encoder<br>Textãƒ¢ãƒ€ãƒªãƒ†ã‚£: T5 Encoder<br>ãƒ¢ãƒ€ãƒªãƒ†ã‚£çµ±åˆ: DiT Block(ãŠãã‚‰ãT5 Encoderã®å‡ºåŠ›ã‚’ç”¨ã„ã¦promptæƒ…å ±ã‚’æ¡ä»¶ä»˜ã‘ï¼‰ã¨Audio Block?<br>3D VAE Decoderã§ãƒ‡ã‚³ãƒ¼ãƒ‰ã¨ã„ã†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼Ÿè©³ç´°ãŒæ›¸ã‹ã‚Œã¦ãŠã‚‰ãšã‚ˆãã‚ã‹ã‚‰ãªã„ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/VariationalAutoEncoder.html" target="_blank" rel="noopener noreferrer">#VariationalAutoEncoder</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/VideoGeneration/Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="articles/Robotics.html" target="_blank" rel="noopener noreferrer">#Robotics</a>
<a class="button" href="articles/VisionLanguageActionModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageActionModel</a>
<a class="button" href="articles/EmbodiedAI.html" target="_blank" rel="noopener noreferrer">#EmbodiedAI</a>
<span class="issue_date">Issue Date: 2025-08-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2404" target="_blank" rel="noopener noreferrer" class="title-link">RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation, Jiang+, Alibaba, 2025.08</a>
<span class="snippet"><span>Comment</span><p>TL;DRã¯ä¸‹è¨˜ã€‚<br><br>&gt; We introduce RynnVLA-001, a vision-language-action model built upon large-scale video generative pre-training.<br>&gt; - RynnVLA-001 is pretrained on ~12M ego-centric manipulation videos.<br>&gt; - We unify next-frame prediction and next-action prediction into a single transformer.<br>&gt; - We train a lightweight VAE to accurately compress action chunks into action embeddings.<br>&gt; - Our RynnVLA-001 outperforms Pi-0 and GR00T-N1.5, in terms of both real-world task success rate and instruction-following capability.<br><br>ã¾ãšã€11.93Mã®ä¸€äººç§°è¦–ç‚¹ã§ã®äººé–“ãŒæ“ä½œï¼ˆç‰¹ã«æ‰‹ã®æ“ä½œï¼‰ã‚’ã™ã‚‹å‹•ç”»ã¨ã€244Kã®robotãŒæ“ä½œã‚’ã™ã‚‹å‹•ç”»ã§Transformerã‚’äº‹å‰å­¦ç¿’ã™ã‚‹ã€‚ã“ã®ã¨ãã€actionãƒ©ãƒ™ãƒ«ã¯ä¸€åˆ‡ç”¨ã„ãšã€pixelã®æƒ…å ±ã‹ã‚‰ç‰©ç†ä¸–ç•Œã®ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’ç†è§£ã•ã›ã‚‹ã€‚ç¶šã„ã¦ã€Action Chunksï¼ˆè¤‡æ•°ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®å°‘é‡ã®ã‹ãŸã¾ã‚Šï¼‰ã‚’ã€dense embeddingã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã™ã‚‹VAEã‚’å­¦ç¿’ã™ã‚‹ã€‚ãƒãƒ£ãƒ³ã‚¯ã‚’ç”¨ã„ã‚‹ç†ç”±ã¯ã€ãƒ”ã‚¯ã‚»ãƒ«ã®å¤‰åŒ–ãŒå¾®å°ãªå ´åˆã€åŒã˜ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãŒé€£ç¶šã—ã¦äºˆæ¸¬ã•ã‚Œã¦ã—ã¾ã„stuckã—ã‚ã—ã¾ã†ç¾è±¡ã‚’é˜²ãã“ã¨ã€äºˆæ¸¬ã®åŠ¹ç‡ãŒè‰¯ã„ã‹ã‚‰ã¨ã®ã“ã¨ã€‚ã“ã‚Œã«ã‚ˆã‚ŠVLAã¯å˜ä¸€ã®embedding vectorã‚’äºˆæ¸¬ã™ã‚‹ã ã‘ã§ã€ä¸€è²«æ€§ã®ã‚ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ç³»åˆ—ã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã§ãã‚‹ã€‚æœ€å¾Œã«ã€step1ã§å­¦ç¿’ã—ãŸvideo generationãƒ¢ãƒ‡ãƒ«ã¨ã€step2ã§å­¦ç¿’ã—ãŸVAEã«ã‚ˆã‚‹action representationã‚’çµ±åˆã™ã‚‹ã€‚å…·ä½“çš„ã«ã¯ã€next frame predictionï¼ˆvisual tokenã‚’äºˆæ¸¬; cross entropy lossï¼‰ã¨next action predictionï¼ˆaction edbeddingã‚’äºˆæ¸¬ã™ã‚‹ï¼‰ã‚’çµ±åˆã—ã¦å­¦ç¿’ã™ã‚‹ã€‚action embeddingã¯continuousãªãƒ™ã‚¯ãƒˆãƒ«ãªã®ã§ç•°ãªã‚‹ãƒ˜ãƒƒãƒ‰ã‚’ç”¨æ„ã—ã¦å­¦ç¿’ã™ã‚‹ï¼ˆL1 Loss)ã€‚inferenceæ™‚ã¯RGBã®observationã¨ã€ãƒ†ã‚­ã‚¹ãƒˆã«ã‚ˆã‚‹instructionã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚Šã€action embeddingã‚’äºˆæ¸¬ã™ã‚‹ã€‚action edbeddingã¯VAE decoderã«æ¸¡ã•ã‚Œã€low levelãªactionç³»åˆ—ã«å¤‰æ›ã•ã‚Œã‚‹ã€‚robotã¯äºˆæ¸¬ã•ã‚ŒãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã€observationãŒå¤‰åŒ–ã™ã‚‹ã®ã§ã¾ãŸäºˆæ¸¬ã™ã‚‹ã€ã¨ã„ã£ãŸiterationã‚’å®Ÿæ–½ã™ã‚‹ã€‚visual tokenã«ã‚ˆã‚‹äºˆæ¸¬ã¯ä¸è¦ãªã®ã§ã€è¨ˆç®—åŠ¹ç‡ã®è¦³ç‚¹ã‹ã‚‰å®Ÿæ–½ã—ãªã„ã€‚<br><br><img src="https://github.com/user-attachments/assets/4be5a5da-8c9c-4735-a1ee-ac3da52c2530" alt="image" loading="lazy"></p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1955043541299728607?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>HF:


<a href="https://huggingface.co/Alibaba-DAMO-Academy/RynnVLA-001-7B-Base" target="_blank" rel="noopener noreferrer">https://huggingface.co/Alibaba-DAMO-Academy/RynnVLA-001-7B-Base</a>


</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="articles/Conversation.html" target="_blank" rel="noopener noreferrer">#Conversation</a>
<a class="button" href="articles/Slide.html" target="_blank" rel="noopener noreferrer">#Slide</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-07-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2224" target="_blank" rel="noopener noreferrer" class="title-link">ã€è¼ªè¬›è³‡æ–™ã€‘Moshi: a speech-text foundation model for real-time dialogue, Hayato Tsukagoshi, 2025.07</a>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<a class="button" href="articles/Optimizer.html" target="_blank" rel="noopener noreferrer">#Optimizer</a>
<a class="button" href="articles/Decoder.html" target="_blank" rel="noopener noreferrer">#Decoder</a>
<span class="issue_date">Issue Date: 2025-07-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2223" target="_blank" rel="noopener noreferrer" class="title-link">Modded-NanoGPT, KellerJordan, 2024.05</a>
<span class="snippet"><span>Comment</span><p>NanoGPT speedrun</p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2118" target="_blank" rel="noopener noreferrer">[Paper Note] The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT  Improvements, Bingchen Zhao+, arXiv'25</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2208" target="_blank" rel="noopener noreferrer">ãã¿ã¯NanoGPT speedrunã‚’çŸ¥ã£ã¦ã„ã‚‹ã‹ï¼Ÿ, PredNext, 2025.07</a>
</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/Scaling%20Laws.html" target="_blank" rel="noopener noreferrer">#Scaling Laws</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-05-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2001" target="_blank" rel="noopener noreferrer" class="title-link">2025å¹´åº¦äººå·¥çŸ¥èƒ½å­¦ä¼šå…¨å›½å¤§ä¼šãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«è¬›æ¼”ã€Œæ·±å±¤åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®æ•°ç†ã€, Taiji Suzuki, 2025.05</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/btreetaiji/status/1927678122817921442?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/TimeSeriesDataProcessing.html" target="_blank" rel="noopener noreferrer">#TimeSeriesDataProcessing</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<span class="issue_date">Issue Date: 2025-05-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1991" target="_blank" rel="noopener noreferrer" class="title-link">Datadog_Toto-Open-Base-1.0, Datadog, 2025.05</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1926310678060466370?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ï¼ˆã‚ã¨ã§ã‚³ãƒ¡ãƒ³ãƒˆè¿½è¨˜ã™ã‚‹<br><br><img src="https://github.com/user-attachments/assets/8aa231e0-ac79-421c-9e23-531aa61137ae" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/22e09246-7f6e-49bc-9db1-8e553a4daf52" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-03-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1830" target="_blank" rel="noopener noreferrer" class="title-link">Nemotron-H: A Family of Accurate, Efficient Hybrid Mamba-Transformer Models, Nvidia, 2025.03</a>
<span class="snippet"><span>Comment</span><p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1820" target="_blank" rel="noopener noreferrer">Hunyuan T1, Tencent, 2025.03</a>
</p>
<p>Transformerã®Self-attention Layerã‚’Mamba2 Layerã«ç½®æ›ã™ã‚‹ã“ã¨ã§ã€æ§˜ã€…ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§åŒç­‰ã®æ€§èƒ½ã€ã‚ã‚‹ã„ã¯ä¸Šå›ã‚‹æ€§èƒ½ã§3å€ç¨‹åº¦ã®Inference timeã®é«˜é€ŸåŒ–ã‚’ã—ã¦ã„ã‚‹ï¼ˆ65536 input, 1024 outputï¼‰ã€‚<br><br>56Bç¨‹åº¦ã®mediumã‚µã‚¤ã‚ºã®ãƒ¢ãƒ‡ãƒ«ã¨ã€8Bç¨‹åº¦ã®è»½é‡ãªãƒ¢ãƒ‡ãƒ«ã«ã¤ã„ã¦è¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹ã€‚ç‰¹ã«ã€8Bãƒ¢ãƒ‡ãƒ«ã§Mambaã¨Transformerã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒ¢ãƒ‡ãƒ«ã¨ã€é€šå¸¸ã®Transformerãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã—ã¦ã„ã‚‹ã€‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«15 Trillion Tokenã‚’åˆ©ç”¨ã—ã¦ãŠã‚Šã€ã“ã®ãƒ‡ãƒ¼ã‚¿é‡ã§ã®Apple to Appleã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£é–“ã®æ¯”è¼ƒã¯ã€ç¾çŠ¶ã§ã¯æœ€ã‚‚å¤§è¦æ¨¡ãªã‚‚ã®ã¨ã®ã“ã¨ã€‚æ€§èƒ½ã¯å¤šãã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã«ã—ã¦ã‚‚åŒç­‰ã€Commonsense Understandingã§ã¯ä¸Šå›ã£ã¦ã„ã‚‹ã€‚<br><br>ã¾ãŸã€å­¦ç¿’ã—ãŸNemotron-Hã‚’ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦æŒã¤VLMã«ã¤ã„ã¦ã‚‚ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒè¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="articles/pretrained-LM.html" target="_blank" rel="noopener noreferrer">#pretrained-LM</a>
<span class="issue_date">Issue Date: 2024-12-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1606" target="_blank" rel="noopener noreferrer" class="title-link">ModernBERT, AnswerDotAI, 2024.12</a>
<span class="snippet"><span>GPT Summary</span>- ModernBERTã¯ã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å°‚ç”¨ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã§ã€å¾“æ¥ã®BERTã«æ¯”ã¹ã¦å¤§å¹…ãªãƒ‘ãƒ¬ãƒ¼ãƒˆæ”¹å–„ã‚’å®Ÿç¾ã€‚2å…†ãƒˆãƒ¼ã‚¯ãƒ³ã§è¨“ç·´ã•ã‚Œã€8192ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’æŒã¡ã€åˆ†é¡ã‚¿ã‚¹ã‚¯ã‚„ãƒªãƒˆãƒªãƒ¼ãƒãƒ«ã§æœ€å…ˆç«¯ã®çµæœã‚’ç¤ºã™ã€‚é€Ÿåº¦ã¨ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚‚å„ªã‚Œã¦ãŠã‚Šã€ä¸€èˆ¬çš„ãªGPUã§ã®æ¨è«–ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>æœ€è¿‘ã®é€²åŒ–ã—ã¾ãã£ãŸTransformeré–¢é€£ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’Encodnr-Onlyãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚‹BERTã«å–ã‚Šè¾¼ã‚“ã ã‚‰æ€§èƒ½ä¸ŠãŒã‚‹ã—ã€BERTã®æ–¹ãŒã‚³ã‚¹ãƒ‘ãŒè‰¯ã„ã‚¿ã‚¹ã‚¯ã¯ãŸãã•ã‚“ã‚ã‚‹ã‚ˆã€ç³»ã®è©±ã€ã‹ã¤ãã®å®Ÿè£…ã ã¨æ€ã‚ã‚Œã‚‹ã€‚<br>ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒšãƒ¼ãƒ‘ãƒ¼ä¸­ã«è¨˜è¼‰ã¯ãªã„ãŒã€è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã¨åŒã˜ã‚¿ã‚¹ã‚¯ã§ã®Decoder-Onlyãƒ¢ãƒ‡ãƒ«ï¼ˆSFTæœ‰ã‚Šç„¡ã—ä¸¡æ–¹ï¼‰ã¨ã®æ€§èƒ½ã‚’æ¯”è¼ƒã—ãŸã‚‰ã©ã®ç¨‹åº¦ã®æ€§èƒ½ãªã®ã ã‚ã†ã‹ï¼Ÿ</p>
<p>ãã‚‚ãã‚‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒæ‰‹å…ƒã«ã‚ã£ã¦ã€BERTã‚’Finetuningã™ã‚‹ã ã‘ã§ååˆ†ãªæ€§èƒ½ãŒå‡ºã‚‹ã®ãªã‚‰ï¼ˆBERTã¯GPUä½¿ã†ã®ã§ãã‚‚ãã‚‚xgboostã¨ã‹ã§ã‚‚è‰¯ã„ãŒï¼‰ã€ã‚ã–ã‚ã–LLMä½¿ã†å¿…è¦ãªã„ã¨æ€ã‚ã‚Œã‚‹ã€‚BERTã®Finetuningã¯ãã“ã¾ã§æ™‚é–“ã¯ã‹ã‹ã‚‰ãªã„ã—ã€inferenceã‚‚é€Ÿã„ã€‚<br><br>å‚è€ƒ:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1024" target="_blank" rel="noopener noreferrer">Prompt2Model: Generating Deployable Models from Natural Language   Instructions, Vijay Viswanathan+, N/A, EMNLP'23</a>
</p>
<p>æ—¥æœ¬èªè§£èª¬:


<a href="https://zenn.dev/dev_commune/articles/3f5ab431abdea1?utm_source=substack&utm_medium=email" target="_blank" rel="noopener noreferrer">https://zenn.dev/dev_commune/articles/3f5ab431abdea1?utm_source=substack&utm_medium=email</a>


</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Chip.html" target="_blank" rel="noopener noreferrer">#Chip</a>
<span class="issue_date">Issue Date: 2024-09-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1399" target="_blank" rel="noopener noreferrer" class="title-link">Sohu, etched, 2024.06</a>
<span class="snippet"><span>Comment</span><p>&gt;By burning the transformer architecture into our chip, we canâ€™t run most traditional AI models: the DLRMs powering Instagram ads, protein-folding models like AlphaFold 2, or older image models like Stable Diffusion 2. We canâ€™t run CNNs, RNNs, or LSTMs either.<br><br>transformerä»¥å¤–ã®å¤§æŠµã®ãƒ¢ãƒ‡ãƒ«ã§ã¯å‹•ä½œã—ãªã„ãŒã€ä»£ã‚ã‚Šã«H-100ã‚ˆã‚Šã‚‚20å€æ—©ã„inferenceã‚’å®Ÿç¾ã§ãã‚‹ãƒãƒƒãƒ—ã‚‰ã—ã„ã€‚<br><img src="https://github.com/user-attachments/assets/1fb2c6b4-3837-4bec-9a64-f8b4878e5941" alt="image" loading="lazy"><br><br>&gt;With over 500,000 tokens per second in Llama 70B throughput, Sohu lets you build products impossible on GPUs.<br><br>ã„ã‚„ã„ã‚„ã„ã‚„Llama-70Bã§0.5M Token/secã¯æ—©ã™ãã‚‹ï¼ï¼ï¼</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/PositionalEncoding.html" target="_blank" rel="noopener noreferrer">#PositionalEncoding</a>
<span class="issue_date">Issue Date: 2024-05-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1310" target="_blank" rel="noopener noreferrer" class="title-link">RoFormer: Enhanced Transformer with Rotary Position Embedding, Jianlin Su+, N_A, Neurocomputing, 2024</a>
<span class="snippet"><span>GPT Summary</span>- ä½ç½®ç¬¦å·åŒ–ã¯transformerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§æœ‰åŠ¹ã§ã‚ã‚Šã€æœ¬è«–æ–‡ã§ã¯Rotary Position Embeddingï¼ˆRoPEï¼‰ã¨ã„ã†æ–°ã—ã„æ‰‹æ³•ã‚’ææ¡ˆã—ã¦ã„ã‚‹ã€‚RoPEã¯ã€å›è»¢è¡Œåˆ—ã‚’ä½¿ç”¨ã—ã¦çµ¶å¯¾ä½ç½®ã‚’ç¬¦å·åŒ–ã—ã€åŒæ™‚ã«ç›¸å¯¾ä½ç½®ä¾å­˜æ€§ã‚’è‡ªå·±æ³¨æ„æ§‹æˆã«çµ„ã¿è¾¼ã‚€ã€‚RoPEã‚’ä½¿ç”¨ã—ãŸRoFormerã¯ã€é•·ã„ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ä»–ã®æ‰‹æ³•ã‚’ä¸Šå›ã‚‹ã“ã¨ãŒå®Ÿé¨“ã§ç¤ºã•ã‚Œã¦ãŠã‚Šã€Huggingfaceã«çµ±åˆã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>RoPEã‚’ææ¡ˆã—ãŸè«–æ–‡</p>
<p># Absolute Position Embedding ã¨ Relative Position Embedding<br><br>## Transformerã«ãŠã‘ã‚‹QKVãƒ™ã‚¯ãƒˆãƒ«ã®è¨ˆç®—æ–¹æ³•<br><br>ä¸€èˆ¬ã«ã€Transformerã«ãŠã‘ã‚‹ Query (Q), Key (K), Value (V) ã¯ä»¥ä¸‹ã®å¼ã§å®šå¼åŒ–ã•ã‚Œã‚‹ï¼š<br><br>&lt;img width="176" alt="image" src="


&lt;a href="https://github.com/user-attachments/assets/21b0f077-64b4-4fe5-af04-bffc373eabf5"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/21b0f077-64b4-4fe5-af04-bffc373eabf5"&lt;/a&gt;


&gt;<br><br>m, nã¯ãã‚Œãã‚Œä½ç½®ã‚’è¡¨ã™æ•´æ•°ã€‚Absolute Position Embeddingã¨ã€Relative Position Embeddingã¯ã€é–¢æ•°fã®è¨­è¨ˆãŒãã‚Œãã‚Œç•°ãªã£ã¦ã„ã‚‹ï¼š<br><br><br><br>## Absolute Position Embedding<br><br>absolute position embeddingã¯ã€å›ºå®šã•ã‚ŒãŸposition ãƒ™ã‚¯ãƒˆãƒ«ã€ã‚ã‚‹ã„ã¯trainableãªposition ãƒ™ã‚¯ãƒˆãƒ«pã‚’ã€å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«ã«å¯¾ã—ã¦è¶³ã—åˆã‚ã›ã‚‹ï¼š<br><br>&lt;img width="382" alt="image" src="


&lt;a href="https://github.com/user-attachments/assets/0688c1bf-8699-48a5-9d95-06454550bbdf"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/0688c1bf-8699-48a5-9d95-06454550bbdf"&lt;/a&gt;


&gt;<br><br><br><br>## Relative Position Embedding<br><br>ä¸€æ–¹ã€Relative Position Embeddingã¯ã€Queryã®ä½ç½®ã«å¯¾ã™ã‚‹ã€Key, Valueã®ç›¸å¯¾ä½ç½®ï¼ˆã¤ã¾ã‚Šã€mã¨nã®å·®ï¼‰ã«å¯¾ã—ã¦ã€trainableãªãƒ™ã‚¯ãƒˆãƒ« \tilde{p}_r ã‚’Key, ValueãŠã‚ˆã³ç›¸å¯¾è·é›¢rã”ã¨ã«ç”¨æ„ã—ã€ãã®ãƒ™ã‚¯ãƒˆãƒ«ã‚’å…¥åŠ›ã«è¶³ã—åˆã‚ã›ã‚‹ã€ã¨ã„ã†å®šå¼åŒ–ã¨ãªã£ã¦ã„ã‚‹ï¼š<br><br><br>&lt;img width="269" alt="image" src="


&lt;a href="https://github.com/user-attachments/assets/ddb92f1a-af23-4d71-a7b9-2a7adda792e1"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/ddb92f1a-af23-4d71-a7b9-2a7adda792e1"&lt;/a&gt;


&gt;<br><br><br>ã“ã“ã§ã€r = clip(m-n, r_max, r_min)ã§ã‚ã‚Šã€r_max, r_minã¯è€ƒæ…®ã™ã‚‹ç›¸å¯¾è·é›¢ã®æœ€å¤§å€¤ã¨æœ€å°å€¤ã§ã‚ã‚‹ã€‚<br><br>ä»–ã«ã‚‚æ§˜ã€…ãªå®šå¼åŒ–ãŒææ¡ˆã•ã‚Œã¦ã„ã‚‹ãŒãŸã„ã¦ã„å®šå¼åŒ–ã®ä¸­ã«ç›¸å¯¾ä½ç½®m-nãŒå‡ºç¾ã™ã‚‹ã€‚<br><br><br>## RoPE<br><br>RoPEã§ã¯ã€å…¥åŠ›ãƒ™ã‚¯ãƒˆãƒ«(Q,K)ã«å¯¾ã—ã¦å›è»¢è¡Œåˆ—ã‚’é©ç”¨ã™ã‚‹ã“ã¨ã§ã€å›è»¢ã«å¯¾ã—ã¦ä½ç½®æƒ…å ±ã‚’ä¿æŒã•ã›ã‚‹ã€‚å…·ä½“çš„ã«ã¯ã€ç•°ãªã‚‹ä½ç½®m, nã«å¯¾ã™ã‚‹q_m^T k_nã‚’è¨ˆç®—ã™ã‚‹ã¨ã€å›è»¢è¡Œåˆ—ã‚’Rã¨ã—ãŸå ´åˆå¼16ã«ç¤ºã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«å›è»¢è¡Œåˆ—Rã«ç›¸å¯¾ä½ç½®m-nãŒç¾ã‚Œï¼ˆã‚‹ã‚ˆã†ã«è¨­è¨ˆã•ã‚Œã¦ãŠã‚Šï¼‰ã€ç›¸å¯¾ä½ç½®ã‚’è€ƒæ…®ã—ãŸqkã®è¨ˆç®—ã«ãªã£ã¦ã„ã‚‹ã€‚[^1]<br><br><br>&lt;img width="705" alt="image" src="


&lt;a href="https://github.com/user-attachments/assets/fce1d06e-e346-4278-a77c-4c96795d5488"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/fce1d06e-e346-4278-a77c-4c96795d5488"&lt;/a&gt;


&gt;<br><br>&lt;img width="588" alt="image" src="


&lt;a href="https://github.com/user-attachments/assets/3f28103c-6a56-4016-8f50-d45fe28cd62a"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/3f28103c-6a56-4016-8f50-d45fe28cd62a"&lt;/a&gt;


&gt;<br><br><br>[^1]: (R_mq_m)^T R_nK_n = q_m^T (R_m^T R_n) k_n = q_m^T (R_{-m}R_n) k_n = q_m^T R_{n-m} k_n. ã“ã“ã§ã€R_m^T = R_{-m}ã§ã‚ã‚Šã€R_m R_n = R_{m+n}ã®æ€§è³ªã‚’ä½¿ã£ã¦ã„ã‚‹ã€‚<br><br><br>RoPEã¯ä¸‹è¨˜ã®ã‚ˆã†ãªæ€§è³ªã‚’æŒã¤ï¼š<br><br>- long-term decay: Î¸i = 10000âˆ’2i/d ã¨è¨­å®šã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€ç›¸å¯¾ä½ç½®ãŒé›¢ã‚Œã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ™ã‚¯ãƒˆãƒ«ã¨ã®inner productã®å€¤ãŒå°ã•ããªã‚‹ã€‚ã™ãªã‚ã¡ã€ä½ç½®ãŒé›¢ã‚Œã¦ã„ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®ä¾å­˜é–¢ä¿‚ãŒå°ã•ããªã‚‹ã€‚<br><br>- Linear-Attention: RoPEã¯å›è»¢è¡Œåˆ—ã§ã‚ã‚Šã€ä¹—ç®—å¾Œã®ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒãƒ«ãƒ ã‚’å¤‰åŒ–ã•ã›ãªã„ã€‚ã“ã®ãŸã‚ã€Linear Attentionã®å¼ã®ä¸­ã«å›è»¢è¡Œåˆ—ã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§ã€Linear Attentionã¨ç°¡å˜ã«çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ãŒå¯èƒ½<br><br><br><br>Absolute Position Embedding, Relative Position Embeddingã§ã¯ã€ãƒ™ã‚¯ãƒˆãƒ«ã«å¯¾ã—ã¦ä½ç½®æƒ…å ±ã‚’åŠ ç®—ã™ã‚‹å®šå¼åŒ–ã§ K, Vã®è¨ˆç®—æ™‚ã«ä½ç½®æƒ…å ±ã‚’è€ƒæ…®ã—ã¦ã„ãŸãŸã‚ã€Linear Attentionã®è¨ˆç®—ãã®ã‚‚ã®ã«ä½ç½®æƒ…å ±ã‚’çµ„ã¿è¾¼ã‚“ã å®šå¼åŒ–ã¨ã¯ãªã£ã¦ã„ãªã‹ã£ãŸã€‚<br><br>ãŒã€RoPEã§ã¯å›è»¢è¡Œåˆ—ã‚’ä¹—ç®—ã™ã‚‹å®šå¼åŒ–ã§ã‚ã‚Šã€ãƒãƒ«ãƒ ã‚’å¤‰åŒ–ã•ã›ãªã„ã®ã§Linear Attentionã®å®šå¼åŒ–ã«çµ„ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã‚‹ã€‚ã“ã®ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å¤§ããå¤‰æ›´ã—ãªãã¨ã‚‚çµ„ã¿è¾¼ã‚ã‚‹ã€‚<br><br></p>
<p>RoPEè‡ªä½“ã¯å®Ÿè£…ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å¿…è¦ã¨ã—ãªã„ãŒã€ãƒ¢ãƒ‡ãƒ«ã®ãã®ä»–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒRoPEã«é©ç”¨ã§ãã‚‹ã‚ˆã†ã«å­¦ç¿’ã•ã‚Œã¦ã„ãªã„ã¨é©ç”¨ã§ããªã„ã§ã‚ã‚ã†ç‚¹ã«ã¯æ³¨æ„ï¼ˆäº‹å‰å­¦ç¿’æ™‚ã«RoPEãŒä½¿ã‚ã‚Œã¦ã„ã‚Œã°è©±ã¯åˆ¥ï¼‰ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/TabularData.html" target="_blank" rel="noopener noreferrer">#TabularData</a>
<span class="issue_date">Issue Date: 2023-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1167" target="_blank" rel="noopener noreferrer" class="title-link">Table Transformer Demo</a>
<span class="snippet"><span>Comment</span><p>PDFä¸­ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ãã®æ§‹é€ ï¼ˆè¡Œåˆ—ã‚»ãƒ«ï¼‰ã‚’detectã™ã‚‹ãƒ¢ãƒ‡ãƒ«<br><br>Exampleã¯ä»¥ä¸‹ã®ã‚ˆã†ãªæ„Ÿã˜ï¼ˆæ—¥æœ¬èªã ã¨ã©ã‚Œãã‚‰ã„ã§ãã‚‹ã®ã‹ãª...ï¼‰<br><br><br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/7f62e16b-1ff8-46ad-b6df-7792981f8f58" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2023-11-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1129" target="_blank" rel="noopener noreferrer" class="title-link">Transformers.js, 2023</a>
<span class="snippet"><span>Comment</span><p>ãƒ–ãƒ©ã‚¦ã‚¶ä¸Šã§Transformerãƒ™ãƒ¼ã‚¹ã®æ§˜ã€…ãªãƒ¢ãƒ‡ãƒ«ã‚’å‹•ä½œã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2023-10-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1108" target="_blank" rel="noopener noreferrer" class="title-link">å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ï½¤ã€ŒçŸ¥è­˜ã¯å…¨çµåˆå±¤ã«è“„ç©ã•ã‚Œã‚‹ã€ã¨ã„ã†ä»®èª¬ã«ã¤ã„ã¦ã®æ–‡çŒ®èª¿æŸ»</a>
<span class="snippet"><span>Comment</span><p>ã‚¿ã‚¤ãƒˆãƒ«ã®é€šã‚Šã€çŸ¥è­˜ãŒFFNã«è“„ç©ã•ã‚Œã¦ã„ã‚‹ã¨ä¸»å¼µã—ã¦ã„ã‚‹ã‚‰ã—ã„åŸè«–æ–‡ã‚’èª­ã¿è§£ã„ã¦ã„ã‚‹ã€‚ã¾ã¨ã‚ã‚’å¼•ç”¨ã™ã‚‹ã¨<br><br>&gt; ã€ŒçŸ¥è­˜ã¯å…¨çµåˆå±¤ã«è“„ç©ã•ã‚Œã‚‹ã€ã¨ã„ã†è¡¨ç¾ã¯ï½¤ã‚„ã‚„ãƒ©ã‚¸ã‚«ãƒ«ã§ï½¤<br>å°‘ãªãã¨ã‚‚ã“ã®è«–æ–‡ã§ã¯ã€Œå…¨çµåˆå±¤ã¯çŸ¥è­˜ç²å¾—ã«ãŠã„ã¦é‡è¦ã€ã¨ã„ã†ç¨‹åº¦<br>ã®ï½¤ã‚‚ã†å°‘ã—ãƒã‚¤ãƒ«ãƒ‰ãªä¸»å¼µã‚’ã—ã¦ã„ã‚‹ã‚ˆã†ã«è¦‹å—ã‘ã‚‰ã‚Œã¾ã—ãŸï½¡<br><br>ã¨ã®ã“ã¨ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2023-07-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/899" target="_blank" rel="noopener noreferrer" class="title-link">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning, 2023</a>
<span class="snippet"><span>GPT Summary</span>- FlashAttention-2ã¯ã€é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã«ãŠã‘ã‚‹Transformerã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«ææ¡ˆã•ã‚ŒãŸæ‰‹æ³•ã§ã™ã€‚FlashAttention-2ã¯ã€éå¯¾ç§°ãªGPUãƒ¡ãƒ¢ãƒªéšå±¤ã‚’åˆ©ç”¨ã—ã¦ãƒ¡ãƒ¢ãƒªã®ç¯€ç´„ã¨ãƒ©ãƒ³ã‚¿ã‚¤ãƒ ã®é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã—ã€æœ€é©åŒ–ã•ã‚ŒãŸè¡Œåˆ—ä¹—ç®—ã«æ¯”ã¹ã¦ç´„2å€ã®é«˜é€ŸåŒ–ã‚’é”æˆã—ã¾ã™ã€‚ã¾ãŸã€FlashAttention-2ã¯GPTã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦ã‚‚é«˜é€ŸåŒ–ã‚’å®Ÿç¾ã—ã€æœ€å¤§225 TFLOPs/sã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°é€Ÿåº¦ã«é”ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>Flash Attention1ã‚ˆã‚Šã‚‚2å€é«˜é€ŸãªFlash Attention 2</p>
<p>Flash Attention1ã¯ã“ã¡ã‚‰ã‚’å‚ç…§<br>


<a href="https://arxiv.org/pdf/2205.14135.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2205.14135.pdf</a>


<br><br>QK Matrixã®è¨ˆç®—ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã«åˆ†ã‘ã¦SRAMã«é€ã£ã¦å‡¦ç†ã™ã‚‹ã“ã¨ã§ã€3å€é«˜é€ŸåŒ–ã—ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’10-20å€ã‚’é”æˆã€‚<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/935f61f3-97ce-4e76-826b-040f92ca567c" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="articles/python.html" target="_blank" rel="noopener noreferrer">#python</a>
<span class="issue_date">Issue Date: 2023-05-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/675" target="_blank" rel="noopener noreferrer" class="title-link">Assisted Generation: a new direction toward low-latency text generation, 2023</a>
<span class="snippet"><span>Comment</span><p>1 lineåŠ ãˆã‚‹ã¨transformerã®generationãŒæœ€å¤§3å€ç¨‹åº¦é«˜é€ŸåŒ–ã•ã‚Œã‚‹ã‚ˆã†ã«ãªã£ãŸã‚‰ã—ã„</p>
<p><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/fecc1c5e-b9e5-4844-af96-ba48c3d60fae" alt="image" loading="lazy"><br><br>assistant modelã‚’ãƒ­ãƒ¼ãƒ‰ã—generateã«å¼•æ•°ã¨ã—ã¦æ¸¡ã™ã ã‘<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/7dabf3bf-cd32-469c-abba-f1269318576d" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<span class="issue_date">Issue Date: 2023-05-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/618" target="_blank" rel="noopener noreferrer" class="title-link">OpenLLaMA</a>
<span class="snippet"><span>Comment</span><p>LLaMAã¨åŒæ§˜ã®æ‰‹æ³•ã‚’ä¼¼ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é©ç”¨ã—å•†ç”¨åˆ©ç”¨å¯èƒ½ãªLLaMAã‚’æ§‹ç¯‰ã—ãŸæ¨¡æ§˜</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<span class="issue_date">Issue Date: 2023-02-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/509" target="_blank" rel="noopener noreferrer" class="title-link">30åˆ†ã§å®Œå…¨ç†è§£ã™ã‚‹Transformerã®ä¸–ç•Œ</a>
<span class="snippet"><span>Comment</span><p>éå¸¸ã«è©³ç´°ã§å®Ÿè³ªæ—¥æœ¬èªã®ã‚µãƒ¼ãƒ™ã‚¤è«–æ–‡ã®ã‚ˆã†ãªã‚‚ã®<br></p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/TimeSeriesDataProcessing.html" target="_blank" rel="noopener noreferrer">#TimeSeriesDataProcessing</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2022-12-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/504" target="_blank" rel="noopener noreferrer" class="title-link">Are Transformers Effective for Time Series Forecasting?</a>
<span class="snippet"><span>Comment</span><p>Linear Layerã«åŸºã¥ãã‚·ãƒ³ãƒ—ãƒ«ãªæ‰‹æ³•ãŒTransformerãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ã«æ™‚ç³»åˆ—äºˆæ¸¬ã§å‹ã£ãŸã¨ã„ã†è©±</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="articles/Explanation.html" target="_blank" rel="noopener noreferrer">#Explanation</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2022-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/499" target="_blank" rel="noopener noreferrer" class="title-link">Transformers Interpret, 2022</a>
<span class="snippet"><span>Comment</span><p>transformersã®ãƒ¢ãƒ‡ãƒ«ã‚’ãŸã£ãŸ2è¡Œè¿½åŠ ã™ã‚‹ã ã‘ã§ã€explainableã«ã™ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª<br><br>åŸºæœ¬çš„ã«textã¨visionã®classificationã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹æ¨¡æ§˜<br>text classificationã®å ´åˆã€ãŸã¨ãˆã°input tokenã®å„ãƒˆãƒ¼ã‚¯ãƒ³ã®åˆ†é¡ã«å¯¾ã™ã‚‹å¯„ä¸åº¦ã‚’outputã—ã¦ãã‚Œã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<span class="issue_date">Issue Date: 2022-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/497" target="_blank" rel="noopener noreferrer" class="title-link">BetterTransformer, Out of the Box Performance for Hugging Face Transformers</a>
<span class="snippet"><span>Comment</span><p>ãŸã£ãŸ1ãƒ©ã‚¤ãƒ³è¿½åŠ ã™ã‚‹ã ã‘ã§ã€Transformerã®inferenceãŒæœ€å¤§ã§4.5å€é«˜é€ŸåŒ–ã•ã‚Œã‚‹BetterTransformerã®è§£èª¬è¨˜äº‹<br><br>better_model = BetterTransformer.transform(model)</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<span class="issue_date">Issue Date: 2022-09-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/485" target="_blank" rel="noopener noreferrer" class="title-link">Transformerã®æœ€å‰ç·š ã€œ ç•³è¾¼ã¿ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å…ˆã¸ ã€œ, ç‰›ä¹…å…ˆç”Ÿ, 2022</a>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<span class="issue_date">Issue Date: 2021-06-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/384" target="_blank" rel="noopener noreferrer" class="title-link">FastSeq: Make Sequence Generation Faster, Yan+, ACLâ€™21</a>
<span class="snippet"><span>Comment</span><p>BART, DistilBART, T5, GPT2ç­‰ã®ã•ã¾ã–ã¾ãªTransformer-basedãªæ‰‹æ³•ã§ã€4-9å€Inference speedã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã€‚</p></span><br><br>
<button onclick="hideContent(0)" style="display: none;">hide</button>
&lt;/div&gt;
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<script>
  document.addEventListener('DOMContentLoaded', function() {
    const tweets = document.querySelectorAll('.tweet-embed[data-embed]');

    if ('IntersectionObserver' in window) {
      const observer = new IntersectionObserver((entries, obs) => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            const el = entry.target;
            const html = el.getAttribute('data-embed');
            if (html) {
              const placeholder = el.querySelector('.tweet-placeholder');
              if (placeholder) placeholder.remove();

              el.innerHTML = html.trim();

              if (window.twttr?.widgets?.load) {
                window.twttr.widgets.load(el);
              }
            }
            obs.unobserve(el); // å‡¦ç†æ¸ˆã¿ã¯ç›£è¦–è§£é™¤
          }
        });
      }, {
        rootMargin: '500px 0px', // ç”»é¢æ‰‹å‰200pxã§èª­ã¿è¾¼ã¿é–‹å§‹
        threshold: 0
      });

      tweets.forEach(tweet => observer.observe(tweet));

    } else {
      // IntersectionObserveræœªå¯¾å¿œãƒ–ãƒ©ã‚¦ã‚¶ç”¨ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
      function lazyLoadFallback() {
        tweets.forEach(el => {
          if (el.getAttribute('data-embed') && el.getBoundingClientRect().top < window.innerHeight) {
            const html = el.getAttribute('data-embed');
            const loadingImg = el.querySelector('.tweet-loading');
            if (loadingImg) loadingImg.remove();
            el.innerHTML = html.trim();
            el.removeAttribute('data-embed');
            if (window.twttr?.widgets?.load) {
              window.twttr.widgets.load(el);
            }
          }
        });
      }
      window.addEventListener('scroll', lazyLoadFallback);
      lazyLoadFallback();
    }
  });
</script>
</div>


    </div>

</article>
<div class="post-nav">
<a class="previous" href="/paper_notes/articles/TransferLearning.html" title="TransferLearningã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">TransferLearningã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§</a><a class="next" href="/paper_notes/articles/TreeSearch.html" title="TreeSearchã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">TreeSearchã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§</a>
</div>
<div class="post-related">
      <div>Related Articles</div>
      <ul>
        <li class="">
          <a class="post-link" href="/paper_notes/articles/DocParser.html" title="DocParserã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            DocParserã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/UserBased.html" title="UserBasedã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            UserBasedã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/reading.html" title="readingã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            readingã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/Navigation.html" title="Navigationã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            Navigationã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
</ul>
    </div>
<div class="post-comments"></div></section>
</div>


  </section>
  <section class="sidebar" style="margin-left: 15px;">
    <!-- Get sidebar items --><style type="text/css" media="screen">
.post-menu ul {
  list-style: none;
  padding: 0;
  margin: 0;
}
</style>

<div class="post-menu">
  <div class="post-menu-title">TOC</div>
  <div class="post-menu-content"></div>
</div>

<script>
  function generateContent() {
    var menu = document.querySelector(".post-menu");
    var menuContent =  menu.querySelector(".post-menu-content");
    var headings = document.querySelector(".post-content").querySelectorAll("h2, h3, h4, h5, h6");

    // Hide menu when no headings
    if (headings.length === 0) {
      return menu.style.display = "none";
    }

    // Generate post menu
    var menuHTML = '';
    for (var i = 0; i < headings.length; i++) {
      var h = headings[i];
      menuHTML += (
        '<li class="h-' + h.tagName.toLowerCase() + '">'
        + '<a href="#h-' + h.getAttribute('id') + '">' + h.textContent + '</a></li>');
    }

    menuContent.innerHTML = '<ul>' + menuHTML + '</ul>';

    // The header element
    var header = document.querySelector('header.site-header');

    function doMenuCollapse(index, over_items) {
      var items = menuContent.firstChild.children;

      if (over_items == undefined) {
        over_items = 20;
      }

      if (items.length < over_items) {
        return;
      }

      var activeItem = items[index];
      var beginItem = activeItem
      var endItem = activeItem
      var beginIndex = index;
      var endIndex = index + 1;
      while (beginIndex >= 0
        && !items[beginIndex].classList.contains('h-h2')) {
        beginIndex -= 1;
      }
      while (endIndex < items.length
        && !items[endIndex].classList.contains('h-h2')) {
        endIndex += 1;
      }
      for (var i = 0; i < beginIndex; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
      for (var i = beginIndex + 1; i < endIndex; i++) {
        item = items[i]
        // if (!item.classList.contains('h-h2')) {
          item.style.display = '';
        // }
      }
      for (var i = endIndex; i < items.length; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
    }

    // Init menu collapsed
    doMenuCollapse(-1);

    // Active the menu item
    window.addEventListener('scroll', function (event) {
      var lastActive = menuContent.querySelector('.active');
      var changed = true;
      var activeIndex = -1;
      for (var i = headings.length - 1; i >= 0; i--) {
        var h = headings[i];
        var headingRect = h.getBoundingClientRect();
        var headerRect = header.getBoundingClientRect();
        var headerTop = Math.floor(headerRect.top);
        var headerHeight = Math.floor(headerRect.height);
        var headerHeight = headerTop + headerHeight + 20;
        if (headingRect.top <= headerHeight) {
          var id = 'h-' + h.getAttribute('id');
          var a = menuContent.querySelector('a[href="#' + id  + '"]');
          var curActive = a.parentNode;
          if (curActive) {
            curActive.classList.add('active');
            activeIndex = i;
          }
          if (lastActive == curActive) {
            changed = false;
          }
          break;
        }
      }
      if (changed) {
        if (lastActive) {
          lastActive.classList.remove('active');
        }
        doMenuCollapse(activeIndex);
      }
      event.preventDefault();
    });
  }
  generateContent();
</script>
</section>
</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/paper_notes/"></data>

  <div class="wrapper">
    <div class="site-footer-inner">
<div>Copyright Â© 2023-current AkihikoWATANABE. The header images and any thumbnail images for the posts were generated by ChatGPT's DALL-E3.</div>
      <div>Powered by <a title="Jekyll is a simple, blog-aware, static site
      generator." href="https://jekyllrb.com/">Jekyll</a> &amp; <a title="Yat, yet
      another theme." href="https://github.com/jeffreytse/jekyll-theme-yat">Yat Theme</a>.</div>
      <div class="footer-col rss-subscribe">Subscribe <a href="/paper_notes/feed.xml">via RSS</a>
</div>
    </div>
  </div>
</footer>
</body>
  </html>
