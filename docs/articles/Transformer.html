<!DOCTYPE html>
<html lang="ja">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-translate-customization" content="108d9124921d80c3-80e20d618ff053c8-g4f02ec6f3dba68b7-c">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Transformerに関する論文・技術記事メモの一覧 | わたしのべんきょうノート</title>
<meta name="generator" content="Jekyll v4.3.2">
<meta property="og:title" content="Transformerに関する論文・技術記事メモの一覧">
<meta name="author" content="AkihikoWATANABE">
<meta property="og:locale" content="ja">
<meta name="description" content="Transformer">
<meta property="og:description" content="Transformer">
<link rel="canonical" href="http://akihikowatanabe.github.io/paper_notes/articles/Transformer.html">
<meta property="og:url" content="http://akihikowatanabe.github.io/paper_notes/articles/Transformer.html">
<meta property="og:site_name" content="わたしのべんきょうノート">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-08-12T00:50:27+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Transformerに関する論文・技術記事メモの一覧">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"AkihikoWATANABE"},"dateModified":"2025-08-12T00:50:27+00:00","datePublished":"2025-08-12T00:50:27+00:00","description":"Transformer","headline":"Transformerに関する論文・技術記事メモの一覧","mainEntityOfPage":{"@type":"WebPage","@id":"http://akihikowatanabe.github.io/paper_notes/articles/Transformer.html"},"url":"http://akihikowatanabe.github.io/paper_notes/articles/Transformer.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="icon" href="">
  <link rel="canonical" href="http://akihikowatanabe.github.io">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/main.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/custom_button.css">
  <script src="/paper_notes/assets/js/main.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script><link type="application/atom+xml" rel="alternate" href="http://akihikowatanabe.github.io/paper_notes/feed.xml" title="わたしのべんきょうノート">
<script>
  function showMore(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "block";
          
      // このボタンの参照を取得して非表示にします
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "none";
      }
          
      // hideボタンの参照を取得して表示します
      const hideButton = contentDiv.querySelector('button[onclick^="hideContent"]');
      if (hideButton) {
        hideButton.style.display = "block";
      }
    }
  }

  function hideContent(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "none";
  
      // moreボタンの参照を取得して表示します
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "block";
      }
  
      // このボタンを隠します
      const hideButtons = document.querySelectorAll('button[onclick^="hideContent"]');
      const hideButton = hideButtons[index];
      if (hideButton) {
        hideButton.style.display = "none";
      }
    }
  }
</script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script charset="UTF-8" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/go.min.js" async></script>



















<script>
// Init highlight js
document.addEventListener('DOMContentLoaded', function(event) {
  var els = document.querySelectorAll('pre code')

  function addLangData(block) {
    var outer = block.parentElement.parentElement.parentElement;
    var lang = block.getAttribute('data-lang');
    for (var i = 0; i < outer.classList.length; i++) {
      var cls = outer.classList[i];
      if (cls.startsWith('language-')) {
        lang = cls;
        break;
      }
    }
    if (!lang) {
      cls = block.getAttribute('class');
      lang = cls ? cls.replace('hljs ', '') : '';
    }
    if (lang.startsWith('language-')) {
      lang = lang.substr(9);
    }
    block.setAttribute('class', 'hljs ' + lang);
    block.parentNode.setAttribute('data-lang', lang);
  }

  function addBadge(block) {
    var enabled = ('true' || 'true').toLowerCase();
    if (enabled == 'true') {
      var pre = block.parentElement;
      pre.classList.add('badge');
    }
  }

  function handle(block) {
    addLangData(block);
    addBadge(block)
    hljs.highlightBlock(block);
  }

  for (var i = 0; i < els.length; i++) {
    var el = els[i];
    handle(el);
  }
});
</script>

<style>
  /* code language badge */
  pre.badge::before {
    content: attr(data-lang);
    color: #fff;
    background-color: #ff4e00;
    padding: 0 .5em;
    border-radius: 0 2px;
    text-transform: uppercase;
    text-align: center;
    min-width: 32px;
    display: inline-block;
    position: absolute;
    right: 0;
  }

  /* fix wrong badge display for firefox browser */
  code > table pre::before {
    display: none;
  }
</style>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe-lightbox.umd.min.js" async></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe.umd.min.js" async></script>
<link href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/photoswipe.min.css" rel="stylesheet">
<style>
  .pswp .pswp__container .pswp__img {
    background-color: white;
  }
</style>

<script>
  function initPhotoSwipe() {
    let customOptions = {};

    try {
      const data = `{"gallery"=>"section.main", "children"=>"a.photo-swipe", "bgOpacity"=>0.8, "padding"=>{"top"=>20, "bottom"=>40, "left"=>100, "right"=>100}}`.replaceAll("=>", ":");
      customOptions = JSON.parse(data);
    } catch (e) {
      console.info("Invalid custom photo previewer options! " + e.message);
    }

    // Define object and gallery options
    const options = Object.assign(
      {
        gallery: "section.main",
        children: "a.photo-swipe",
        photo_scale: 2,
        // dynamic import is not supported in UMD version
        pswpModule: PhotoSwipe,
      },
      customOptions
    );

    const galleryEl = document.querySelector(options.gallery);
    if (!galleryEl) {
      return;
    }

    const imgEls = [];
    const els = galleryEl.querySelectorAll("img:not(.emoji)");
    els.forEach((el) => {
      if (el.src.trim() == "") {
        return;
      }
      if (!imgEls.includes(el)) {
        imgEls.push(el);
      }
    });

    if (imgEls.length === 0) {
      return;
    }

    imgEls.forEach((imgEl) => {
      imgEl.outerHTML = `
      <a class="photo-swipe"
        href="${imgEl.src}"
        data-pswp-width="${
          Math.max(imgEl.naturalWidth, imgEl.width) * options.photo_scale
        }"
        data-pswp-height="${
          Math.max(imgEl.naturalHeight, imgEl.height) * options.photo_scale
        }"
        data-pswp-caption="${imgEl.getAttribute("caption") || imgEl.alt}"
        target="_blank">
        ${imgEl.outerHTML}
      </a>`;
    });

    // Initialize PhotoSwipe 5
    var lightbox = new PhotoSwipeLightbox(options);

    lightbox.init();
  }

  window.addEventListener("load", initPhotoSwipe);
</script>
<meta name="google-site-verification" content="u_DTTPcCZ806iq51zgirHyWq3556HUKGq8AQfH91iFI">
<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>





































































































































<header class="site-header site-header-transparent" role="banner">

  <div class="wrapper">
    <div class="site-header-inner">
<span class="site-brand"><a class="site-brand-inner" rel="author" href="/paper_notes/">
  <img class="site-favicon" title="わたしのべんきょうノート" src="" onerror="this.style.display='none'">
  わたしのべんきょうノート
</a>
</span><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger">
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewbox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
              </svg>
            </span>
          </label>

          <div class="trigger">









<span class="page-link">



<div id="google_translate_element" style="display: none;">
</div>

<span class="ct-language">
  <ul class="list-unstyled ct-language-dropdown">
    
      <li>
        <a href="#" class="lang-select" data-lang="en">
          
          <img src="https://cdn.countryflags.com/thumbs/united-states-of-america/flag-400.png" title="English">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="fr">
          
          <img src="https://cdn.countryflags.com/thumbs/france/flag-400.png" title="French">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="zh-CN">
          
          <img src="https://cdn.countryflags.com/thumbs/china/flag-400.png" title="Chinese(Simple)">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ja">
          
          <img src="https://cdn.countryflags.com/thumbs/japan/flag-400.png" title="Japanese">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ko">
          
          <img src="https://cdn.countryflags.com/thumbs/south-korea/flag-400.png" title="Korean">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ru">
          
          <img src="https://cdn.countryflags.com/thumbs/russia/flag-400.png" title="Russian">
          
        </a>
      </li>
    
  </ul>
</span>

<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({
    pageLanguage: 'ja',
    autoDisplay: false,
    layout: google.translate.TranslateElement.InlineLayout.VERTICAL
  }, 'google_translate_element');

  // Links to cross-origin destinations are unsafe
  var gll = document.getElementsByClassName('goog-logo-link')[0];
  if (gll) {
    gll.setAttribute('rel', 'noopener');
  }

  function restoreLang() {
    var iframe = document.getElementsByClassName('goog-te-banner-frame')[0];
    if (!iframe) return;

    var innerDoc = iframe.contentDocument || iframe.contentWindow.document;
    var restore_el = innerDoc.getElementsByTagName("button");

    for (var i = 0; i < restore_el.length; i++) {
      if (restore_el[i].id.indexOf("restore") >= 0) {
        restore_el[i].click();
        var close_el = innerDoc.getElementsByClassName("goog-close-link");
        close_el[0].click();
        return;
      }
    }
  }

  function triggerHtmlEvent(element, eventName) {
    var event;
    if (document.createEvent) {
      event = document.createEvent('HTMLEvents');
      event.initEvent(eventName, true, true);
      element.dispatchEvent(event);
    } else {
      event = document.createEventObject();
      event.eventType = eventName;
      element.fireEvent('on' + event.eventType, event);
    }
  }

  var googleCombo = document.querySelector("select.goog-te-combo");
  var langSelect = document.querySelector('.ct-language');
  langSelect.addEventListener('click', function(event) {
    if (!event.target) {
      return;
    }

    var selected = document.querySelector('.ct-language .ct-language-selected');
    if (selected) {
      selected.classList.remove('ct-language-selected');
    }

    var target = event.target;
    while (target && target !== langSelect ) {
      if (target.matches('.lang-select')) {
        break;
      }
      target = target.parentElement;
    }

    if (target && target.matches('.lang-select')) {
      var lang = target.getAttribute('data-lang');
      if (googleCombo.value == lang) {
        restoreLang();
      } else {
        target.parentElement.classList.add('ct-language-selected');
        googleCombo.value = lang;
        triggerHtmlEvent(googleCombo, 'change');
      }
    }

    event.preventDefault();
  });
}
</script>

<script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit" async></script>
</span>
</div>
        </nav>
</div>
  </div>
</header>

<script>
  function initHeader() {
    var lastScrollY = getScrollPos().y;
    var documentElement = document.documentElement;

    function storeScrollData() {
      var y = getScrollPos().y;documentElement.setAttribute("data-header-transparent", "");var scrollStatus = "";

      if (y <= 0) {
        scrollStatus = "top";
      } else if ((window.innerHeight + y) >= document.body.offsetHeight) {
        scrollStatus = "bottom";
      } else {
        var isScrollDown = (y - lastScrollY > 0) ? true : false;
        scrollStatus = isScrollDown ? "down" : "up";
      }

      lastScrollY = y;
      documentElement.setAttribute("data-scroll-status", scrollStatus);
    }

    window.addEventListener('scroll', function(e) {
      storeScrollData();
    });

    storeScrollData();
  }
  document.addEventListener('DOMContentLoaded', initHeader);
</script>


























































































































































<style>
    html .page-banner .page-banner-img > *:first-child {
      opacity: 0.4;
    }

    html[data-theme="dark"] .page-banner .page-banner-img > *:first-child {
      opacity: 0.2872;
    }
  </style>
<section class="page-banner">
    <div class="page-banner-img">
<div style="background-image: url(/paper_notes/assets/images/banner.png)"></div>
        <img class="img-placeholder" src="/paper_notes/assets/images/banner.png">
</div>
    <div class="wrapper">
      <div class="page-banner-inner">
<header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">わたしのべんきょうノート</h1>
  <h2 class="post-subtitle">勉強した論文や技術等の情報をGithubのIssueにメモっているひとのブログ。
それなりにメモの量が蓄積されてきたので、一度整理したいなと思いブログはじめてみました！
自然言語処理(NLP), 推薦システム(RecommenderSystem), Educational Data Mining (EDM), Learning Analytics (LA)などの分野のメモが多いと思います。
最近は特にLLMの勉強が多めです :)</h2>

  <div class="post-meta">
    <time class="dt-published" datetime="2025-08-12T00:50:27+00:00" itemprop="datePublished"><i class="fa fa-calendar"></i> Aug 12, 2025
    </time><span class="post-author left-vsplit"><i class="fa fa-pencil"></i> AkihikoWATANABE</span>
    
































    <span class="post-reading-time left-vsplit"><i class="fa fa-clock-o"></i> About 3 hours 2 mins</span>
  </div></header>
</div>
    </div>
  </section><script>
  function hashLocate(hashValue) {
    hashValue = hashValue.replace(/^.*#h-/, '');
    hashValue = decodeURIComponent(hashValue);
    var element = document.getElementById(hashValue);

    if (!element) {
      return;
    }

    var header = document.querySelector('header.site-header');
    var headerRect = header.getBoundingClientRect();
    var headerTop = Math.floor(headerRect.top);
    var headerHeight = Math.floor(headerRect.height);
    var scrollPos = getScrollPos();
    var offsetY = element.offsetTop - (headerTop + headerHeight + 20);

    if (offsetY == scrollPos.y) {
      return;
    }

    if (headerTop == 0  && offsetY > scrollPos.y) {
      offsetY += headerHeight + 2;
    } else if (headerTop < 0  && offsetY < scrollPos.y) {
      offsetY -= headerHeight - 2;
    }

    smoothScrollTo(offsetY);
  }

  // The first event occurred
  window.addEventListener('load', function(event) {
    if (window.location.hash) {
      hashLocate(window.location.hash);
    }
  });

  // The first event occurred
  window.addEventListener('click', function(event) {
    if (event.target.tagName.toLowerCase() == 'a') {
      hashLocate(event.target.getAttribute('href'));
    }
  });
</script>
<div class="theme-toggle">
  <input type="checkbox" id="theme-switch">
  <label for="theme-switch">
    <div class="toggle"></div>
    <div class="names">
      <p class="light">Light</p>
      <p class="dark">Dark</p>
    </div>
  </label>
</div>




<script>
  (function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var nightModeOption = ('off' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
      storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
      var data = storage.getItem('theme');
      try {
        data = JSON.parse(data ? data : '');
      } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
      }
      return data;
    }

    function handleThemeToggle(nightShift) {
      themeData.nightShift = nightShift;
      saveThemeData(themeData);
      html.dataset.theme = nightShift ? 'dark' : 'light';
      setTimeout(function() {
        sw.checked = nightShift ? true : false;
      }, 50);
    }

    function autoThemeToggle() {
      // Next time point of theme toggle
      var now = new Date();
      var toggleAt = new Date();
      var hours = now.getHours();
      var nightShift = hours >= 19 || hours <=7;

      if (nightShift) {
        if (hours > 7) {
          toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
      } else {
        toggleAt.setHours(19);
      }

      toggleAt.setMinutes(0);
      toggleAt.setSeconds(0);
      toggleAt.setMilliseconds(0)

      var delay = toggleAt.getTime() - now.getTime();

      // auto toggle theme mode
      setTimeout(function() {
        handleThemeToggle(!nightShift);
      }, delay);

      return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
      };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
      handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
      var data = autoThemeToggle();

      // Toggle theme by local setting
      if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
      } else {
        handleThemeToggle(themeData.nightShift);
      }
    } else if (nightModeOption == 'manual') {
      handleThemeToggle(themeData.nightShift);
    } else {
      var nightShift = themeData.nightShift;
      if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
      }
      handleThemeToggle(nightShift);
    }
  })();
</script>
<div id="click-to-top" class="click-to-top">
  <i class="fa fa-arrow-up"></i>
</div>
<script>
  (function () {
    const clickToTop = document.getElementById('click-to-top');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 100) {
        clickToTop.classList.add('show')
      }else {
        clickToTop.classList.remove('show')
      }
    });
    clickToTop.addEventListener('click', () => {
      window.smoothScrollTo(0);
    });
  })();
</script>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="framework">
  <section class="main">

     <div class="post">
  <section>









<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="post-content e-content" itemprop="articleBody">

      <h2 id="transformer">Transformer</h2>

<div class="visible-content">
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>


<br>


<span class="issue_date">Issue Date: 2025-08-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2396">[Paper Note] Fast and Simplex: 2-Simplicial Attention in Triton, Aurko Roy+, arXiv'25</a>
<span class="snippet"><span>Summary</span>2-シンプリシアルトランスフォーマーを用いることで、トークン効率を向上させ、標準的なトランスフォーマーよりも優れた性能を発揮することを示す。固定されたトークン予算内で、数学や推論タスクにおいてドット積アテンションを上回る結果を得た。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/scaling01/status/1954682957798715669?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a>


<br>


<span class="issue_date">Issue Date: 2025-07-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2237">[Paper Note] In-context denoising with one-layer transformers: connections between  attention and associative memory retrieval, Matthew Smart+, arXiv'25</a>
<span class="snippet"><span>Summary</span>「インコンテキストデノイジング」というタスクを通じて、注意ベースのアーキテクチャと密な連想記憶（DAM）ネットワークの関係を探求。ベイズ的フレームワークを用いて、単層トランスフォーマーが特定のデノイジング問題を最適に解決できることを示す。訓練された注意層は、コンテキストトークンを連想記憶として利用し、デノイジングプロンプトを一回の勾配降下更新で処理。これにより、DAMネットワークの新たな拡張例を提供し、連想記憶と注意メカニズムの関連性を強化する。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/hillbig/status/1945253873456963841?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q関連:

<br>

・2146</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/PEFT(Adaptor_LoRA).html">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<a class="button" href="articles/Finetuning.html">#Finetuning</a>


<br>


<span class="issue_date">Issue Date: 2025-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2206">[Paper Note] ExPLoRA: Parameter-Efficient Extended Pre-Training to Adapt Vision   Transformers under Domain Shifts, Samar Khanna+, ICML'25</a>
<span class="snippet"><span>Summary</span>PEFT技術を用いたExPLoRAは、事前学習済みビジョントランスフォーマー（ViT）を新しいドメインに適応させる手法で、教師なし事前学習を通じて効率的にファインチューニングを行う。実験では、衛星画像において最先端の結果を達成し、従来のアプローチよりも少ないパラメータで精度を最大8%向上させた。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/samar_a_khanna/status/1944781066591748336?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qこれまでドメイン適応する場合にラベル付きデータ+LoRAでFinetuningしていたのを、ラベル無しデータ+継続事前学習の枠組みでやりましょう、という話のようである。

<br>

<img src="https://github.com/user-attachments/assets/dcae10cf-6b5d-4b29-8d9a-a94227f29a11" alt="image" loading="lazy">

<br>



<br>

手法は下記で、事前学習済みのモデルに対してLoRAを適用し継続事前学習する。ただし、最後尾のLayer、あるいは最初と最後尾のLayerの両方をunfreezeして、trainableにする。また、LoRAはfreezeしたLayerのQ,Vに適用し、それらのLayerのnormalization layerもunfreezeする。最終的に、継続事前学習したモデルにヘッドをconcatしてfinetuningすることで目的のタスクを実行できるようにする。

<br>



<br>

<img src="https://github.com/user-attachments/assets/6b7ef497-2253-46c9-bbe7-ffdd50765fa3" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/84596039-0a10-4556-896b-1fee164a153b" alt="image" loading="lazy">

<br>



<br>

同じモデルで単にLoRAを適用しただけの手法や、既存手法をoutperform

<br>



<br>

<img width="679" height="364" alt="Image" src="https://github.com/user-attachments/assets/14935879-75a4-4e4a-a176-1b1eabc4b8fd">画像+ViT系のモデルだけで実験されているように見えるが、LLMとかにも応用可能だと思われる。

<br>

</span>
</div>
<p><button onclick="showMore(0)">more</button></p>
<div class="hidden-content">
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<span class="issue_date">Issue Date: 2025-07-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2198">[Paper Note] Nonlinear transformers can perform inference-time feature learning, Nishikawa+, ICML'25</a>
<span class="snippet"><span>Summary</span>事前学習されたトランスフォーマーは、推論時に特徴を学習する能力を持ち、特に単一インデックスモデルにおける文脈内学習に焦点を当てています。勾配ベースの最適化により、異なるプロンプトからターゲット特徴を抽出し、非適応的アルゴリズムを上回る統計的効率を示します。また、推論時のサンプル複雑性が相関統計クエリの下限を超えることも確認されました。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/btreetaiji/status/1944297631808991742?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<a class="button" href="articles/VideoGeneration/Understandings.html">#VideoGeneration/Understandings</a>
<a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-07-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2146">[Paper Note] Energy-Based Transformers are Scalable Learners and Thinkers, Alexi Gladstone+, arXiv'25</a>
<span class="snippet"><span>Summary</span>エネルギーベースのトランスフォーマー（EBTs）を用いて、無監督学習から思考を学ぶモデルを提案。EBTsは、入力と候補予測の互換性を検証し、エネルギー最小化を通じて予測を行う。トレーニング中に従来のアプローチよりも高いスケーリング率を達成し、言語タスクでの性能を29%向上させ、画像のノイズ除去でも優れた結果を示す。EBTsは一般化能力が高く、モデルの学習能力と思考能力を向上させる新しいパラダイムである。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/hillbig/status/1941657099567845696?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QProject Page:https://energy-based-transformers.github.ioFirst Authorの方による解説ポスト:https://x.com/alexiglad/status/1942231878305714462?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/ListWise.html">#ListWise</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Alignment.html">#Alignment</a>
<a class="button" href="articles/SequentialRecommendation.html">#SequentialRecommendation</a>
<span class="issue_date">Issue Date: 2025-07-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2136">[Paper Note] Listwise Preference Alignment Optimization for Tail Item Recommendation, Zihao Li+, arXiv'25</a>
<span class="snippet"><span>Summary</span>LPO4Recは、テールアイテム推薦におけるPreference alignmentの課題を解決するために提案された手法で、Bradley-Terryモデルをペアワイズからリストワイズ比較に拡張し、効率的なトレーニングを実現。明示的な報酬モデリングなしで、テールアイテムを優先する負のサンプリング戦略を導入し、パフォーマンスを最大50%向上させ、GPUメモリ使用量を17.9%削減。実験結果は3つの公開データセットで示されている。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/_reachsumit/status/1941004418255933662?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qtail itemに強い手法らしい。LLMを用いたGenerative Recommendationではなく、1 BlockのTransformerにlistwiseなpreferenceを反映したlossを適用したものっぽい。一貫して性能は高そうに見えるが、再現性はどうだろうか。

<br>

<img src="https://github.com/user-attachments/assets/10c66c84-b421-4be1-8cd7-3d037e8cc683" alt="image" loading="lazy">関連(SASRec):

<br>

・2137pointwise, pairwise, listwiseの基礎はこちらを参照:

<br>

・187</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<a class="button" href="articles/Normalization.html">#Normalization</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2025-07-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2131">[Paper Note] The Curse of Depth in Large Language Models, Wenfang Sun+, arXiv'25</a>
<span class="snippet"><span>Summary</span>本論文では、「深さの呪い」という現象を紹介し、LLMの深い層が期待通りに機能しない理由を分析します。Pre-LNの使用が出力の分散を増加させ、深い層の貢献を低下させることを特定。これを解決するために層正規化スケーリング（LNS）を提案し、出力分散の爆発を抑制します。実験により、LNSがLLMの事前トレーニング性能を向上させることを示し、教師ありファインチューニングにも効果があることを確認しました。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/shiwei_liu66/status/1940377801032446428?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q・1795

<br>



<br>

ではそもそもLayerNormalizationを無くしていた（正確にいうとparametrize tanhに置換)が、どちらが優れているのだろうか？

<br>



<br>

<img src="https://github.com/user-attachments/assets/4bc557a0-ae23-4017-9837-7744de74c12e" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/2eead45c-209d-46e4-87e7-0129a4ec5ec2" alt="image" loading="lazy">・1332

<br>



<br>

では知識ニューロンの存在が示唆されており、これはTransformerの層の深い位置に存在し、かつ異なる知識間で知識ニューロンはシェアされない傾向にあった（ただしこれはPost-LNのBERTの話で本研究はPre-LNの話だが。Post-LNの勾配消失問題を緩和し学習を安定化させる研究も2141 のように存在する)。これはこの研究が明らかにしたこととどういう関係性があるだろうか。

<br>



<br>

また、LayerNormalizationのScalingによって深いTransformerブロックの導関数が単位行列となる（学習に寄与しなくなる）ことが改善された場合、知識ニューロンはどのように変化するだろうか？

<br>



<br>

（下記Geminiの応答を見た上での感想)なんとなーくだけれども、おそらく知識ニューロンの局所化が解消されるのかなーという気がする。

<br>



<br>

となると次の疑問としては、MoEアーキテクチャにはどのような影響があるだろうか？

<br>

そもそも知識ニューロンが局所化しているからMoEアーキテクチャのルータによって関連するExpertsのみをactivateすれば（というより結果的にそうなるように学習される）性能を劣化させずに計算効率を上げられていた、と仮定する。そうすると、知識ニューロンが局所化せずに多くのニューロンでシェアされるようになると、2110 のように、サブネットワーク間の情報を互いにやりとりできます、みたいな仕組みがより効いてきそうな気がする。

<br>



<br>

参考までに、Gemini2.5-Proに考察させてみた結果をメモとして残しておく（あくまで参考程度に...）

<br>

```

<br>

ご質問ありがとうございます。非常に興味深い着眼点ですね。「Knowledge Neurons in Pretrained Transformers」と「The Curse of Depth in Large Language Models」は、一見すると全く異なるテーマを扱っているように見えますが、「Transformerの深い層における振る舞い」という共通点で結びつけて考察すると、非常に示唆に富んだ関係性が見えてきます。

<br>



<br>

以下に、両方の論文の概要を解説し、その関係性について考察します。

<br>



<br>

1. Knowledge Neurons in Pretrained Transformers の概要

<br>

この研究は、事前学習済みTransformerモデル（特にBERTなど）の内部で、特定の事実知識がどのように格納されているかを調査したものです。

<br>



<br>

発見: モデルの中間層、特に全結合層（Feed-Forward Network, FFN）に、特定の知識（例：「ダンテ・アリギエーリはイタリアで生まれた」）に強く反応する「知識ニューロン」が存在することを発見しました。

<br>



<br>

特徴: これらの知識ニューロンは、モデルの深い層（後方の層）に、より多く存在する傾向がありました。

<br>



<br>

意味: これまでブラックボックスとされてきた大規模言語モデルの内部で、知識がどのように表現・局在化しているかについて、具体的な手がかりを与えた画期的な研究です。

<br>



<br>

2. The Curse of Depth in Large Language Models の概要

<br>

この研究は、LLMをより深く（層を多く）することの難しさに焦点を当て、その原因と解決策を提案したものです。

<br>



<br>

問題（深さの呪い）: Transformerの標準的なアーキテクチャ（Pre-LN）では、層が深くなるにつれて、LayerNormalization（LN）への入力の分散が指数関数的に増大してしまいます。

<br>



<br>

結果:

<br>



<br>

出力が大きくなりすぎて学習が不安定になります。

<br>



<br>

さらに深刻なのは、深い層ではモデルの出力に関する導関数（勾配計算に必要）がほぼ単位行列になってしまうことです。これは、その層が入力に対してほとんど変換を行わなくなり、学習に寄与しなくなることを意味します。

<br>



<br>

解決策: この問題を解決するため、各層のLayerNormalizationをその深さ（レイヤー番号 l）に応じてスケーリングするというシンプルな手法を提案しました。これにより、深い層でも勾配が適切に伝播し、学習が安定・改善することが示されました。

<br>



<br>

考察：2つの研究の関係性

<br>

これら2つの研究は、「学習の安定性」と「知識の格納方法」という異なる側面から、Transformerの深い層を分析していますが、両者には以下のような深い関係性があると考えられます。

<br>



<br>

学習の不安定性が「知識ニューロン」形成の背景にある可能性

<br>

「The Curse of Depth」で指摘されているように、標準的なTransformerの深い層は、本質的に学習が不安定で、勾配情報が失われやすい環境にあります。

<br>



<br>

この劣悪な学習環境こそが、「知識ニューロン」という形で知識が局所的に格納される原因の一つになっている可能性があります。つまり、

<br>



<br>

学習の非効率性: 深い層のほとんどのニューロンは、勾配消失問題のために効率的に学習を進めることができません。

<br>



<br>

専門化の発生: そのような不安定な環境下で、たまたま特定の知識パターンをうまく捉えることができた一部のニューロンが、その知識を一身に背負う形で強く活性化するよう特化（専門化）していったのではないか、と考えられます。これは、ネットワーク全体で協調して学習するのが難しい状況で、一部のユニットだけが突出して学習を担う、という現象と解釈できます。

<br>



<br>

学習の安定化が、より効率的な知識獲得につながる

<br>

では、「The Curse of Depth」で提案された手法（LNのスケーリング）によって深い層の学習が安定化すると、知識の格納方法はどのように変わるでしょうか。

<br>



<br>

これは非常に興味深い問いであり、2つの可能性が考えられます。

<br>



<br>

可能性A: より強固な知識ニューロンの形成:

<br>

学習が安定することで、各知識ニューロンはより明確に、そして効率的に特定の知識をエンコードできるようになるかもしれません。ノイズの多い環境で偶然生まれた専門家ではなく、安定した環境で育成された真の専門家へと変化するイメージです。

<br>



<br>

可能性B: 知識の分散化:

<br>

ネットワークの全ニューロンが効率的に学習に寄与できるようになるため、一つの知識を少数のニューロンに集中させる必要がなくなり、より多くのニューロンに分散して知識が格納されるようになる可能性も考えられます。

<br>



<br>

現在のところ、学習の安定化は、知識ニューロンがより効率的に形成・機能するための基盤を提供すると考えるのが最も妥当でしょう。「深さの呪い」という問題を解決することは、モデルが知識をどのように獲得し、整理するかという、より根源的なメカニズム（知識ニューロンの振る舞い）にも直接的な影響を与えるはずです。

<br>



<br>

まとめ

<br>

「Knowledge Neurons」は深い層に存在する知識の「状態」を明らかにし、「The Curse of Depth」は深い層で発生する学習の「問題」とその解決策を提示しました。

<br>



<br>

これらを統合すると、「深い層における学習の不安定性という問題が、知識ニューロンという局所的な知識表現を生み出す一因となっており、この問題を解決することで、より効率的で安定した知識の獲得・格納が可能になるかもしれない」という関係性が見えてきます。

<br>



<br>

両者は、LLMの能力と限界を異なる角度から照らし出しており、組み合わせることでモデルの内部動作の解明をさらに一歩前進させる、非常に重要な研究だと言えます。

<br>

```</span>
<a class="button" href="articles/Metrics.html">#Metrics</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/SpokenLanguageProcessing.html">#SpokenLanguageProcessing</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<span class="issue_date">Issue Date: 2025-07-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2124">[Paper Note] AudioBERTScore: Objective Evaluation of Environmental Sound Synthesis  Based on Similarity of Audio embedding Sequences, Minoru Kishi+, arXiv'25</a>
<span class="snippet"><span>Summary</span>新しい客観的評価指標AudioBERTScoreを提案し、合成音声の性能向上を目指す。従来の客観的指標は主観的評価との相関が弱いため、AudioBERTScoreは合成音声と参照音声の埋め込みの類似性を計算し、主観的評価との相関が高いことを実験で示した。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/forthshinji/status/1940226218500247645?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qtext-to-audioの自動評価が可能な模様

<br>

</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/VideoGeneration/Understandings.html">#VideoGeneration/Understandings</a>
<a class="button" href="articles/ICCV.html">#ICCV</a>
<span class="issue_date">Issue Date: 2025-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2099">[Paper Note] Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers, Weiming Ren+, arXiv'25</a>
<span class="snippet"><span>Summary</span>VAMBAモデルは、Mamba-2ブロックを用いてビデオトークンを線形にエンコードし、トークン削減なしで1024フレームを処理可能。これにより、GPUメモリ使用量を50%削減し、トレーニング速度を倍増。1時間のビデオ理解ベンチマークLVBenchで4.3%の精度向上を達成し、様々なビデオ理解タスクで優れた性能を示す。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/wenhuchen/status/1938064510369280136?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/CVPR.html">#CVPR</a>
<a class="button" href="articles/3D%20Reconstruction.html">#3D Reconstruction</a>
<span class="issue_date">Issue Date: 2025-06-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2068">[Paper Note] VGGT: Visual Geometry Grounded Transformer, Jianyuan Wang+, CVPR'25</a>
<span class="snippet"><span>Summary</span>VGGTは、シーンの主要な3D属性を複数のビューから直接推測するフィードフォワードニューラルネットワークであり、3Dコンピュータビジョンの分野において新たな進展を示します。このアプローチは効率的で、1秒未満で画像を再構築し、複数の3Dタスクで最先端の結果を達成します。また、VGGTを特徴バックボーンとして使用することで、下流タスクの性能が大幅に向上することが示されています。コードは公開されています。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/hillbig/status/1936711294956265820?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a>
<a class="button" href="articles/VideoGeneration/Understandings.html">#VideoGeneration/Understandings</a>
<span class="issue_date">Issue Date: 2025-06-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2037">[Paper Note] Seedance 1.0: Exploring the Boundaries of Video Generation Models, Yu Gao+, arXiv'25</a>
<span class="snippet"><span>Summary</span>Seedance 1.0は、動画生成の基盤モデルであり、プロンプト遵守、動きの妥当性、視覚的品質を同時に向上させることを目指しています。主な技術改善として、意味のある動画キャプションを用いたデータキュレーション、マルチショット生成のサポート、動画特有のRLHFを活用したファインチューニング、推論速度の約10倍向上を実現する蒸留戦略が挙げられます。Seedance 1.0は、1080p解像度の5秒間の動画を41.4秒で生成し、高品質かつ迅速な動画生成を実現しています。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/scaling01/status/1933048431775527006?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<span class="issue_date">Issue Date: 2025-06-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2030">[Paper Note] Value Residual Learning, Zhanchao Zhou+, ACL'25</a>
<span class="snippet"><span>Summary</span>ResFormerは、隠れ状態の残差に値の残差接続を加えることで情報の流れを強化する新しいTransformerアーキテクチャを提案。実験により、ResFormerは従来のTransformerに比べて少ないパラメータとトレーニングデータで同等の性能を示し、SVFormerはKVキャッシュサイズを半減させることができる。性能はシーケンスの長さや学習率に依存する。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/zhanchaozhou/status/1932829678081098079?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q<img src="https://github.com/user-attachments/assets/c2c97ea1-0930-4033-85bd-b3618463f87a" alt="image" loading="lazy"></span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<span class="issue_date">Issue Date: 2025-06-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2025">[Paper Note] Log-Linear Attention, Han Guo+, arXiv'25</a>
<span class="snippet"><span>Summary</span>対数線形注意を提案し、線形注意の効率性とソフトマックス注意の表現力を両立。固定サイズの隠れ状態を対数的に成長する隠れ状態に置き換え、計算コストを対数線形に抑える。Mamba-2とGated DeltaNetの対数線形バリアントが線形時間のバリアントと比較して優れた性能を示すことを確認。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/hillbig/status/1932194773559107911?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q解説ポスト:https://x.com/theturingpost/status/1931432543766847887?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<a class="button" href="articles/COLT.html">#COLT</a>
<span class="issue_date">Issue Date: 2025-06-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2007">[Paper Note] Learning Compositional Functions with Transformers from Easy-to-Hard   Data, Zixuan Wang+, COLT'25</a>
<span class="snippet"><span>Summary</span>本研究では、Transformerベースの言語モデルの学習可能性を探求し、$k$-fold compositionタスクに焦点を当てる。$O(\log k)$層のトランスフォーマーでこのタスクを表現できる一方、SQオラクルに対するクエリの下限を示し、サンプルサイズが指数的である必要があることを証明。さらに、カリキュラム学習戦略を用いて、簡単な例と難しい例を含むデータ分布がトランスフォーマーの効率的な学習に必要であることを明らかにした。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/zzzixuanwang/status/1928465115478708604?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qこちらはまず元ポストのスレッドを読むのが良いと思われる。要点をわかりやすく説明してくださっている。元ポストとalphaxivでざっくり理解したところ、

<br>



<br>

Transformerがcontextとして与えられた情報(σ)とparametric knowledge(π)をk回の知識マッピングが必要なタスク(k-fold composition task)を学習するにはO(log k)のlayer数が必要で、直接的にk回の知識マッピングが必要なタスクを学習するためにはkの指数オーダーのデータ量が最低限必要となることが示された。これはkが大きくなると（すなわち、複雑なreasoning stepが必要なタスク）になると非現実的なものとなるため、何らかの方法で緩和したい。学習データを簡単なものから難しいものをmixingすること（カリキュラム学習）ことで、この条件が緩和され、指数オーダーから多項式オーダーのデータ量で学習できることが示された

<br>



<br>

といった感じだと思われる。じゃあ最新の32Bモデルよりも、よりパラメータ数が大きくてlayer数が多い古いモデルの方が複雑なreasoningが必要なタスクを実は解けるってこと！？直感に反する！と一瞬思ったが、おそらく最近のモデルでは昔のモデルと比べてparametric knowledgeがより高密度に適切に圧縮されるようになっていると思われるので、昔のモデルではk回の知識マッピングをしないと解けないタスクが、最新のモデルではk-n回のマッピングで解けるようになっていると推察され、パラメータサイズが小さくても問題なく解けます、みたいなことが起こっているのだろう、という感想を抱くなどした</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/LLMServing.html">#LLMServing</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/SoftwareEngineering.html">#SoftwareEngineering</a>
<span class="issue_date">Issue Date: 2025-05-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1977">Insights into DeepSeek-V3: Scaling Challenges and Reflections on  Hardware for AI Architectures, Chenggang Zhao+, arXiv'25</a>
<span class="snippet"><span>Summary</span>DeepSeek-V3は、2,048台のNVIDIA H800 GPUでトレーニングされ、ハードウェア制約に対処するための共同設計を示す。メモリ効率向上のためのマルチヘッド潜在注意や、計算と通信の最適化を図る専門家の混合アーキテクチャ、FP8混合精度トレーニングなどの革新を強調。ハードウェアのボトルネックに基づく将来の方向性について議論し、AIワークロードに応えるためのハードウェアとモデルの共同設計の重要性を示す。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/deedydas/status/1924512147947848039?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Chain-of-Thought.html">#Chain-of-Thought</a>
<a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<span class="issue_date">Issue Date: 2025-04-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1905">RNNs are not Transformers （Yet）: The Key Bottleneck on In-context   Retrieval, Kaiyue Wen+, ICLR'25</a>
<span class="snippet"><span>Summary</span>本論文では、RNNとトランスフォーマーの表現力の違いを調査し、特にRNNがChain-of-Thought（CoT）プロンプトを用いてトランスフォーマーに匹敵するかを分析。結果、CoTはRNNを改善するが、トランスフォーマーとのギャップを埋めるには不十分であることが判明。RNNの情報取得能力の限界がボトルネックであるが、Retrieval-Augmented Generation（RAG）やトランスフォーマー層の追加により、RNNはCoTを用いて多項式時間で解決可能な問題を解決できることが示された。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/yuma_1_or/status/1915968478735130713?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q関連:

<br>

・1210

<br>



<br>

↑とはどういう関係があるだろうか？</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/FoundationModel.html">#FoundationModel</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<a class="button" href="articles/CVPR.html">#CVPR</a>
<span class="issue_date">Issue Date: 2025-04-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1883">AM-RADIO: Agglomerative Vision Foundation Model -- Reduce All Domains   Into One, Mike Ranzinger+, CVPR'25</a>
<span class="snippet"><span>Summary</span>視覚基盤モデル（VFM）をマルチティーチャー蒸留を通じて統合するアプローチAM-RADIOを提案。これにより、ゼロショットの視覚-言語理解やピクセルレベルの理解を向上させ、個々のモデルの性能を超える。新しいアーキテクチャE-RADIOは、ティーチャーモデルよりも少なくとも7倍速い。包括的なベンチマークで様々な下流タスクを評価。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/pavlomolchanov/status/1910391609927360831?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Qvision系のfoundation modelはそれぞれ異なる目的関数で訓練されてきており（CLIPは対照学習 550, DINOv2は自己教師あり学習 1884, SAMはsegmentation 1885)それぞれ別の能力を持ってたが、それらを一個のモデルに蒸留しました、という話らしい

<br>

<img src="https://github.com/user-attachments/assets/929aaa47-ab88-4912-a59a-579d2f34e886" alt="image" loading="lazy"></span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<span class="issue_date">Issue Date: 2025-04-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1866">Scalable-Softmax Is Superior for Attention, Ken M. Nakanishi, arXiv'25</a>
<span class="snippet"><span>Summary</span>SSMaxを提案し、Softmaxの代替としてTransformerモデルに統合。これにより、長いコンテキストでの重要情報の取得が向上し、事前学習中の損失減少が速くなる。SSMaxは注意スコアを改善し、長さの一般化を促進する。</span>
<span class="snippet"><span>Comment</span>・1863

<br>



<br>

で採用されている手法で、ブログポスト中で引用されている。Long Contextになった場合にsoftmaxの分布が均一になる（＝重要な情報にattendする能力が削がれる）ことを防ぐための手法を提案している。解説ポスト:https://x.com/nrehiew_/status/1908613993998045534</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<span class="issue_date">Issue Date: 2025-04-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1855">Multi-Token Attention, Olga Golovneva+, arXiv'25</a>
<span class="snippet"><span>Summary</span>マルチトークンアテンション（MTA）を提案し、複数のクエリとキーのベクトルに基づいてアテンションウェイトを条件付けることで、関連するコンテキストをより正確に特定できるようにする。MTAは畳み込み操作を用いて、近くのトークンが互いに影響を与え、豊かな情報を活用する。評価結果から、MTAはTransformerベースラインモデルを上回り、特に長いコンテキストでの情報検索において優れた性能を示した。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/jaseweston/status/1907260086017237207?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q従来のMulti Head Attentionでは、単体のQKのみを利用していたけど、複数のQKの情報を畳み込んで活用できるようにして、Headも畳み込みで重要な情報がより伝搬されるようにして、GroupNormalizationをかけたらPerplexityの観点でDifferential Transformerを上回ったよ、という話な模様。

<br>



<br>

<img src="https://github.com/user-attachments/assets/199e0794-a286-486d-9426-d86cfd208750" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/2997a61b-3367-4f43-b85a-ac8fa160391a" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/5ef8ddb0-538b-46e2-94b8-2ef495c938ec" alt="image" loading="lazy">

<br>



<br>

・1856

<br>

・1466</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2025-03-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1799">NeoBERT: A Next-Generation BERT, Lola Le Breton+, arXiv'25</a>
<span class="snippet"><span>Summary</span>NeoBERTは、最新のアーキテクチャとデータを統合した次世代エンコーダで、双方向モデルの能力を再定義します。4,096トークンのコンテキスト長を活用し、250Mパラメータでありながら、MTEBベンチマークで最先端の結果を達成し、BERTやRoBERTaを上回ります。すべてのコードやデータを公開し、研究と実世界での採用を促進します。</span>
<span class="snippet"><span>Comment</span>関連: 

<br>

・1606BERT, ModernBERTとの違い

<br>



<br>

![Image](https://github.com/user-attachments/assets/58dbdcf6-e7dc-43c2-94ed-d8bb73cd2617)

<br>



<br>

性能

<br>



<br>

![Image](https://github.com/user-attachments/assets/72730c9c-38d0-4773-8ddb-f0349b8776d2)

<br>



<br>

所感

<br>

medium size未満のモデルの中ではSoTAではあるが、ModernBERTが利用できるのであれば、ベンチマークを見る限りは実用的にはModernBERTで良いのでは、と感じた。学習とinferenceの速度差はどの程度あるのだろうか？</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/CVPR.html">#CVPR</a>
<a class="button" href="articles/Normalization.html">#Normalization</a>
<span class="issue_date">Issue Date: 2025-03-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1795">Transformers without Normalization, Jiachen Zhu+, CVPR'25</a>
<span class="snippet"><span>Summary</span>本研究では、正規化層なしのトランスフォーマーがDynamic Tanh（DyT）を用いることで、同等またはそれ以上のパフォーマンスを達成できることを示します。DyTは、レイヤー正規化の代替として機能し、ハイパーパラメータの調整なしで効果を発揮します。多様な設定での実験により、正規化層の必要性に対する新たな洞察を提供します。</span>
<span class="snippet"><span>Comment</span>なん…だと…。LayerNormalizationを下記アルゴリズムのようなtanhを用いた超絶シンプルなレイヤー（parameterized thnh [Lecun氏ポスト](https://x.com/ylecun/status/1900610590315249833?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q)）に置換するだけっぽい？

<br>

<img src="https://github.com/user-attachments/assets/474d3ee4-4c08-4b00-9a41-126ca5d5207e" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/5aea9f93-85d9-4e0b-b9db-bb407d596493" alt="image" loading="lazy">

<br>



<br>

同等以上の性能を維持しながらモデル全体のinference, trainingの時間を8%程度削減。

<br>

<img src="https://github.com/user-attachments/assets/98f8caa3-3ef2-4594-a45a-ae0aa2cf2ef6" alt="image" loading="lazy"></span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<span class="issue_date">Issue Date: 2024-10-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1466">Differential Transformer, Tianzhu Ye+, N_A, ICLR'25</a>
<span class="snippet"><span>Summary</span>Diff Transformerは、関連するコンテキストへの注意を強化し、ノイズをキャンセルする新しいアーキテクチャです。差分注意メカニズムを用いて、注意スコアを計算し、スパースな注意パターンを促進します。実験結果は、Diff Transformerが従来のTransformerを上回り、長いコンテキストモデリングや幻覚の軽減において顕著な利点を示しています。また、文脈内学習においても精度を向上させ、堅牢性を高めることが確認されました。これにより、Diff Transformerは大規模言語モデルの進展に寄与する有望なアーキテクチャとされています。</span>
<span class="snippet"><span>Comment</span>最近のMSはなかなかすごい（小並感概要

<br>



<br>

attention scoreのノイズを低減するようなアーキテクチャとして、二つのQKVを用意し、両者の差分を取ることで最終的なattentiok scoreを計算するDifferential Attentionを提案した。

<br>



<br>



<br>



<br>

attentionのnoiseの例。answerと比較してirrelevantなcontextにattention scoreが高いスコアが割り当てられてしまう（図左）。differential transformerが提案するdifferential attentionでは、ノイズを提言し、重要なcontextのattention scoreが高くなるようになる（図中央）、らしい。

<br>



<br>

<img src="https://github.com/user-attachments/assets/6033f477-d4bf-492d-9360-74f2849ce40e" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Differential Attentionの概要と計算式

<br>



<br>

<img src="https://github.com/user-attachments/assets/b77facd8-7cf2-43ab-8947-2f775423f0a0" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

数式で見るとこのようになっており、二つのQKをどの程度の強さで交互作用させるかをλで制御し、λもそれぞれのQKから導出する。

<br>



<br>

<img src="https://github.com/user-attachments/assets/c58a4d04-a453-4aef-aa40-7de872117482" alt="image" loading="lazy">QA, 機械翻訳, 文書分類, テキスト生成などの様々なNLPタスクが含まれるEval Harnessベンチマークでは、同規模のtransformerモデルを大幅にoutperform。ただし、3Bでしか実験していないようなので、より大きなモデルサイズになったときにgainがあるかは示されていない点には注意。

<br>

<img src="https://github.com/user-attachments/assets/384605ed-e4e4-4a17-83c8-506f8e3e2e4c" alt="image" loading="lazy">モデルサイズ（パラメータ数）と、学習トークン数のスケーラビリティについても調査した結果、LLaMAと比較して、より少ないパラメータ数/学習トークン数で同等のlossを達成。

<br>

<img src="https://github.com/user-attachments/assets/5d2d1dfc-4197-4b36-9f3d-79a3ed18fe3f" alt="image" loading="lazy">64Kにcontext sgzeを拡張し、1.5B tokenで3Bモデルを追加学習をしたところ、これもtransformerと比べてより小さいlossを達成<img src="https://github.com/user-attachments/assets/f911a4f9-d175-4ea2-825b-9776be6042e5" alt="image" loading="lazy">context中に埋め込まれた重要な情報（今回はクエリに対応するmagic number）を抽出するタスクの性能も向上。Needle（N）と呼ばれる正解のmagic numberが含まれる文をcontext中の様々な深さに配置し、同時にdistractorとなる文もランダムに配置する。これに対してクエリ（R）が入力されたときに、どれだけ正しい情報をcontextから抽出できるか、という話だと思われる。

<br>



<br>

これも性能が向上。特にクエリとNeedleが複数の要素で構成されていれ場合の性能が高く（下表）、長いコンテキスト中の様々な位置に埋め込まれたNeedleを抽出する性能も高い（上のmatrix）

<br>



<br>

<img src="https://github.com/user-attachments/assets/f4d084dc-fac5-427d-8185-5604e55cf051" alt="image" loading="lazy">

<br>



<br>

[Needle-In-A-Haystack test](https://www.perplexity.ai/search/needle-in-a-haystack-testtohan-jF7LXWQPSMqKI2pZSchjpA0)Many shotのICL能力も向上

<br>

<img src="https://github.com/user-attachments/assets/c935ba93-9915-45c8-aaa6-f073d62fdd3b" alt="image" loading="lazy">要約タスクでのhallucinationも低減。生成された要約と正解要約を入力し、GPT4-oにhallucinationの有無を判定させて評価。これは先行研究で人手での評価と高いagreementがあることが示されている。

<br>

<img src="https://github.com/user-attachments/assets/6fd97af4-fec6-44e8-b00c-d5ba26770a84" alt="image" loading="lazy">シンプルなアプローチでLLM全体の性能を底上げしている素晴らしい成果に見える。斜め読みなので読み飛ばしているかもしれないが、766 のように高品質な学習データで学習した場合も同様の効果が発現するのだろうか？

<br>

attentionのスコアがnoisyということは、学習データを洗練させることでも改善される可能性があり、766 はこれをデータで改善し、こちらの研究はモデルのアーキテクチャで改善した、みたいな捉え方もできるのかもしれない。ちなみにFlash Attentionとしての実装方法も提案されており、スループットは通常のattentionと比べてむしろ向上しているので実用的な手法でもある。すごい。

<br>

<img src="https://github.com/user-attachments/assets/c0212cd8-55f5-4991-b256-0ff2bce35669" alt="image" loading="lazy">あとこれ、事前学習とInstruction Tuningを通常のマルチヘッドアテンションで学習されたモデルに対して、独自データでSFTするときに導入したらdownstream taskの性能向上するんだろうか。もしそうなら素晴らしいOpenReview:https://openreview.net/forum?id=OvoCm1gGhNGroupNormalizationについてはこちら:

<br>

・1856</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/FoundationModel.html">#FoundationModel</a>
<a class="button" href="articles/Self-SupervisedLearning.html">#Self-SupervisedLearning</a>
<a class="button" href="articles/TMLR.html">#TMLR</a>
<span class="issue_date">Issue Date: 2025-04-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1884">DINOv2: Learning Robust Visual Features without Supervision, Maxime Oquab+, TMLR'24</a>
<span class="snippet"><span>Summary</span>自己教師あり手法を用いて、多様なキュレーションデータから汎用的な視覚特徴を生成する新しい事前学習手法を提案。1BパラメータのViTモデルを訓練し、小型モデルに蒸留することで、OpenCLIPを上回る性能を達成。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<span class="issue_date">Issue Date: 2025-04-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1864">Flex Attention: A Programming Model for Generating Optimized Attention  Kernels, Juechu Dong+, arXiv'24</a>
<span class="snippet"><span>Summary</span>FlexAttentionは、アテンションの新しいコンパイラ駆動型プログラミングモデルで、数行のPyTorchコードで多くのアテンションバリアントを実装可能にします。これにより、既存のアテンションバリアントを効率的に実装し、競争力のあるパフォーマンスを達成。FlexAttentionは、アテンションバリアントの組み合わせを容易にし、組み合わせ爆発の問題を解決します。</span>
<span class="snippet"><span>Comment</span>・1863

<br>



<br>

で利用されているAttentionpytochによる解説:https://pytorch.org/blog/flexattention/

<br>



<br>

・Flex AttentionはオリジナルのAttentionのQK/sqrt(d_k)の計算後にユーザが定義した関数score_modを適用する

<br>

・score_modを定義することで、attention scoreをsoftmaxをかけるまえに関数によって調整できる

<br>

・多くのattentionの亜種はほとんどの場合この抽象化で対応できる

<br>

・score_modはQK tokenの内積に対応するので、QKの情報を受け取り、スカラー値を返せばなんでも良い

<br>

  ・score_modの実装例は元リンク参照

<br>

・FA2と比較して（現在のpytorchでの実装上は）Forward Passは90%, Backward Passは85%のスループットで、少し遅いが今後改善予定元論文より引用。非常にシンプルで、数式上は下記のように表される:

<br>

<img src="https://github.com/user-attachments/assets/b4a393f0-46a9-46c6-9a47-0402ba58fb11" alt="image" loading="lazy"></span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<span class="issue_date">Issue Date: 2024-12-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1584">Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale   Prediction, Keyu Tian+, NeurIPS'24</a>
<span class="snippet"><span>Summary</span>Visual AutoRegressive modeling (VAR)を提案し、画像生成において自己回帰学習を次のスケール予測として再定義。VARは、GPTのようなARモデルが拡散トランスフォーマーを上回ることを実現し、ImageNet 256x256ベンチマークでFIDを18.65から1.73、ISを80.4から350.2に改善。推論速度は約20倍向上し、画像品質やデータ効率でも優れた性能を示す。VARはゼロショット一般化能力を持ち、スケーリング法則を示す。全モデルとコードを公開し、視覚生成の研究を促進。</span>
<span class="snippet"><span>Comment</span>NeurIPS2024のベストペーパー第一著者がByteDance社から訴訟を起こされている模様…？

<br>

https://var-integrity-report.github.ioOpenReview:https://openreview.net/forum?id=gojL67CfS8Next Token Prediction, Next Image Token Generation (従来手法）, Next Scale (resolution) prediction (提案手法)の違いの図解。非常に分かりやすい。next token predictionでは次トークンのみを予測するがVARでは、次の解像度画像の全体のトークンマップを予測する。

<br>



<br>

<img src="https://github.com/user-attachments/assets/668d7523-f262-45c1-a1d0-2dd479c0a708" alt="image" loading="lazy">

<br>



<br>

学習方法の概要。2-Stageで学習される。最初のステージでK種類の解像度の画像（＝K種類のマルチスケールのtoken maps r_k）を得るためにAutoEncoderを学習し、次のステージでblock-wiseのcausal attention maskを用いて、K_&lt;k個目の解像度の画像からK個目の解像度の画像を予測する（図を見るとイメージを掴みやすい）。inference時はKV Cacheを利用し、maskは不要となる。

<br>

各r_kをデコードする際にr_&lt;kのみに依存する設計にすることでcoase-to-fineに画像を生成することに相当し、これは人間の粗く捉えてから詳細を見る認知プロセスと合致する。また、flatten操作が存在せず、それぞれのr_&lt;k内のトークンがr_k生成時に全て考慮されるため空間的局所性も担保される。また、r_k内のトークンは並列に生成可能なので計算量のオーダーが大幅に削減される（O(n^4)。

<br>

<img src="https://github.com/user-attachments/assets/e1a85712-e66a-4c9a-9cf1-6556f2b8e687" alt="image" loading="lazy">

<br>



<br>

従来手法と比べより小さいパラメータで高い性能を実現し、inference timeも非常に早い。

<br>

<img src="https://github.com/user-attachments/assets/90a6a7de-995d-49e6-94a2-cd709e68777f" alt="image" loading="lazy">

<br>



<br>

ScalingLawsも成立する。

<br>

<img src="https://github.com/user-attachments/assets/351c2a7b-85aa-4cc7-8ba2-a5e9528cabd4" alt="image" loading="lazy"></span>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<span class="issue_date">Issue Date: 2024-11-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1523">Understanding LLMs: A Comprehensive Overview from Training to Inference, Yiheng Liu+, arXiv'24</a>
<span class="snippet"><span>Summary</span>ChatGPTの普及に伴い、LLMsのコスト効率の良いトレーニングとデプロイメントへの関心が高まっている。本論文では、LLMsのトレーニング技術と推論デプロイメント技術の進化をレビューし、データ前処理やモデル圧縮などのさまざまな側面を議論する。また、LLMsの利用方法と将来の発展についての洞察も提供する。</span>
<span class="snippet"><span>Comment</span>[Perplexity（参考;Hallucinationに注意）](https://www.perplexity.ai/search/yi-xia-nolun-wen-wodu-minei-ro-7vGwDK_AQX.HDO7j9H8iNA)単なるLLMの理論的な説明にとどまらず、実用的に必要な各種並列処理技術、Mixed Precision、Offloadingなどのテクニックもまとまっているのがとても良いと思う。LLM Frameworkのところに、メジャーなものが網羅されていないように感じる。たとえば、UnslothやLiger-KernelなどはTransformersの部分で言及されてても良いのでは、と感じる。</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/SpeechProcessing.html">#SpeechProcessing</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<span class="issue_date">Issue Date: 2024-11-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1505">Mixture-of-Transformers: A Sparse and Scalable Architecture for  Multi-Modal Foundation Models, Weixin Liang+, arXiv'24</a>
<span class="snippet"><span>Summary</span>大規模言語モデル（LLMs）のマルチモーダル処理を効率化するために、Mixture-of-Transformers（MoT）を提案。MoTは計算コストを削減し、モダリティごとにパラメータを分離して特化した処理を実現。Chameleon 7B設定では、55.8%のFLOPsで密なベースラインに匹敵する性能を示し、音声を含む場合も37.2%のFLOPsで同様の結果を達成。さらに、Transfusion設定では、7BのMoTモデルが密なベースラインの画像性能に対してFLOPsの3分の1で匹敵し、760Mのモデルは主要な画像生成指標で上回る結果を得た。MoTは実用的な利点も示し、画像品質を47.2%、テキスト品質を75.6%の経過時間で達成。</span>
<span class="snippet"><span>Comment</span><img src="https://github.com/user-attachments/assets/340ab176-7b17-467a-8731-20d1594d6951" alt="image" loading="lazy"></span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2024-10-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1467">What Matters in Transformers? Not All Attention is Needed, Shwai He+, N_A, arXiv'24</a>
<span class="snippet"><span>Summary</span>本研究では、トランスフォーマー内のBlocks、MLP、Attention層間の冗長性を調査し、Attention層の高い類似性によりプルーニングが可能であることを示しました。具体的には、Llama-2-70BではAttention層の半分を削除することで48.4%のスピードアップを達成し、パフォーマンスはわずか2.4%低下しました。また、Attention層とMLP層を同時に削除する手法を提案し、31層削除してもLlama-2-13Bは90%のパフォーマンスを維持しました。これにより、今後のネットワークアーキテクチャ設計に貴重な洞察を提供します。</span>
<span class="snippet"><span>Comment</span>通常LLMはtransformer decoderのブロックをstackすることで形成されるが、積み上げたブロック、あるいはlayerってほんとに全部必要なの?という疑問に答えてくれる論文のようである。

<br>



<br>

transformer blockそのもの、あるいはMLP layerを削除するとpeformanceは大幅に低下するが、attention layerを削除してもperformanceの低下が起きなかった模様。これにより高速化が実現可能。

<br>



<br>

削除するブロックやlayerはinputとoutputのコサイン類似度が高いものを削除することによって実現。

<br>



<br>

<img src="https://github.com/user-attachments/assets/da1e6a56-1bc4-4206-9423-acd7512300c8" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/724ddf50-cd63-437d-9df2-73423dd77a6e" alt="image" loading="lazy">

<br>



<br>

比較的パラメータサイズが小さい7B, 13Bモデルでの実験結果

<br>

<img src="https://github.com/user-attachments/assets/19253c9e-7eae-4084-a8c2-e99680b34649" alt="image" loading="lazy">

<br>



<br>

より大きなモデルでの実験結果

<br>

<img src="https://github.com/user-attachments/assets/18eef07e-623c-482c-9a6b-9ea65450ecea" alt="image" loading="lazy">パフォーマンスが変わらない範囲だと、attention layer dropにより、7B, 13Bモデルの場合は23%程度、70Bの場合は35%のスループット向上</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/TransferLearning.html">#TransferLearning</a>
<span class="issue_date">Issue Date: 2024-09-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1421">beeFormer: Bridging the Gap Between Semantic and Interaction Similarity   in Recommender Systems, Vojtěch Vančura+, N_A, RecSys'24</a>
<span class="snippet"><span>Summary</span>レコメンダーシステムにおいて、コールドスタートやゼロショットシナリオでの予測改善のために、インタラクションデータを活用した文のトランスフォーマーモデル「beeFormer」を提案。beeFormerは、意味的類似性の予測において従来の手法を上回り、異なるドメインのデータセット間で知識を転送可能であることを示した。これにより、ドメインに依存しないテキスト表現のマイニングが可能になる。</span>
<span class="snippet"><span>Comment</span>NLPでは言語という共通の体系があるから事前学習とかが成立するけど、RecSysのようなユーザとシステムのinteraction dataを用いたシステムでは（大抵の場合はデータセットごとにユニークなユーザIDとアイテムIDのログでデータが構成されるので）なかなかそういうことは難しいよね、と思っていた。が、もしRecSysのタスク設定で、データセット間の転移学習を実現できるのだとしたらどのように実現してきるのだろうか?興味深い。後で読む。</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1273">Mixture-of-Depths: Dynamically allocating compute in transformer-based  language models, David Raposo+, N_A, arXiv'24</a>
<span class="snippet"><span>Summary</span>Transformerベースの言語モデルは、入力シーケンス全体に均等にFLOPsを分散させる代わりに、特定の位置にFLOPsを動的に割り当てることを学習できることを示す。モデルの深さにわたって割り当てを最適化するために、異なるレイヤーで計算を動的に割り当てる。この手法は、トークンの数を制限することで合計計算予算を強制し、トークンはtop-kルーティングメカニズムを使用して決定される。この方法により、FLOPsを均等に消費しつつ、計算の支出が予測可能であり、動的かつコンテキストに敏感である。このようにトレーニングされたモデルは、計算を動的に割り当てることを学習し、効率的に行うことができる。</span>
<span class="snippet"><span>Comment</span>参考: https://x.com/theseamouse/status/1775782800362242157?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1270">Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference, Piotr Nawrot+, N_A, arXiv'24</a>
<span class="snippet"><span>Summary</span>トランスフォーマーの生成効率を向上させるために、Dynamic Memory Compression（DMC）が提案された。DMCは、異なるヘッドとレイヤーで異なる圧縮率を適用する方法を学習し、事前学習済みLLMsに適用される。DMCは、元の下流パフォーマンスを最大4倍のキャッシュ圧縮で維持しつつ、スループットを向上させることができる。DMCは、GQAと組み合わせることでさらなる利益をもたらす可能性があり、長いコンテキストと大きなバッチを処理する際に有用である。</span>
<span class="snippet"><span>Comment</span>参考: https://x.com/hillbig/status/1776755029581676943?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q論文中のFigure1が非常にわかりやすい。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/d416547e-f9ca-4c6c-8ebb-7d164bef5283" alt="image" loading="lazy">

<br>



<br>

GQA 1271 と比較して、2~4倍キャッシュを圧縮しつつ、より高い性能を実現。70Bモデルの場合は、GQAで8倍キャッシュを圧縮した上で、DMCで追加で2倍圧縮をかけたところ、同等のパフォーマンスを実現している。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/7b131f07-5eab-4830-88cc-5f6fd0508958" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/EMNLP.html">#EMNLP</a>
<span class="issue_date">Issue Date: 2024-01-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1210">Transformers are Multi-State RNNs, Matanel Oren+, N_A, EMNLP'24</a>
<span class="snippet"><span>Summary</span>本研究では、トランスフォーマーのデコーダーは無限マルチステートRNNとして概念化できることを示し、有限のマルチステートRNNに変換することも可能であることを示します。さらに、新しいキャッシュ圧縮ポリシーであるTOVAを導入し、他のポリシーよりも優れた性能を示すことを実験結果で示しました。TOVAは元のキャッシュサイズの1/8しか使用せず、トランスフォーマーデコーダーLLMが実際にはRNNとして振る舞うことが多いことを示しています。</span>
<span class="snippet"><span>Comment</span>TransformerはRNNとは異なる概念、特に全てのトークンの情報に直接アクセスできるということで区別されてきたが、よくよく考えてみると、Transformer Decoderは、RNNのhidden_states h を（hは1つのstateをベクトルで表している）、multi-stateを表す matrix H （t個のstateを表すmatrix; tは現在の着目しているトークンまでのsequenceの長さ）で置き換えたもの Multi-State-RNN (MSRNN) と解釈できる、という話。

<br>

また、window attentionなどのattentionの計算で考慮するKV cacheのスパンを（メモリを節約するために）制限する圧縮手法は、先ほどのMSRNNは全トークンのstate （KV Cache）にアクセスできる（= Unbounded）と考えると、アクセスできるトークンのstateが k (&lt;t) となるため、BoundedなMSRNNとみなせる。

<br>

したがって、現在のLLMはTransformer Decoderを積み上げたものであるものであり、原理上はinference/training時に全てのトークンを考慮できるため、原理上はUnboundedなMSRNNとみなせる。一方、ここにメモリの制約が加わるとKV Cacheを圧縮しなければならないため、実用上はBoundedなMSRNNとなっている。

<br>



<br>

<img width="476" alt="Image" src="https://github.com/user-attachments/assets/292bd370-0138-441a-a626-ee73cb2f31b5">

<br>



<br>

実際に式で表すと以下のようにRNNとTransformerは対応づけられる。

<br>

<img width="402" alt="Image" src="https://github.com/user-attachments/assets/3b2cbadc-e6ef-4465-ac78-bb4ff71351f2">

<br>

<img width="487" alt="Image" src="https://github.com/user-attachments/assets/18a99f2a-06dc-472c-b50d-743b820904f3">

<br>



<br>

このことを考慮して、本研究ではTOVAと呼ばれる新しいKV Cacheの圧縮手法を提案している。非常にシンプルな手法で、KV Cacheがメモリの上限に到達したときに、その際にattention scoreが最も小さいトークンのKV Cacheを捨てる、という手法である。

<br>

<img width="495" alt="Image" src="https://github.com/user-attachments/assets/09f19caf-bcea-42f7-b1e0-8bc3ea8a2e4c">

<br>



<br>

TOVAをwindow attentionなどのベースラインとオラクルとしてfull attentionと比較。タスクは Language Modeling（PG-19データにおけるPerplexity）、Language Understanding （long contextからrelevantな情報を拾う必要があるQA）、Story Generation（長文のストーリーを書かせてGPT4によってpair-wiseで生成されたストーリーの品質をLLM-as-a-Judgeさせる）を利用。既存のKV Cache圧縮手法よりも効率的にKV Cacheを圧縮でき、4096 context windowの場合は、512程度でfull attentionと近い性能を示すことが示された。これにより、高いメモリ効率とスループットを実現できる。ここで、グラフのx軸のmultistateはTOVAにおいてはmatrix Hで保持するstate数に相当し、window attentionでは、window sizeに相当する。

<br>

<img width="815" alt="Image" src="https://github.com/user-attachments/assets/fd39d465-3a0d-49ad-951b-fddb115394f3">

<br>



<br>

<img width="636" alt="Image" src="https://github.com/user-attachments/assets/c66bade9-f0c4-476b-8243-bd1d88e21ead"></span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/InstructionTuning.html">#InstructionTuning</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/SpeechProcessing.html">#SpeechProcessing</a>
<a class="button" href="articles/CVPR.html">#CVPR</a>
<a class="button" href="articles/Encoder-Decoder.html">#Encoder-Decoder</a>
<a class="button" href="articles/Robotics.html">#Robotics</a>
<span class="issue_date">Issue Date: 2023-12-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1202">Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision,   Language, Audio, and Action, Jiasen Lu+, N_A, CVPR'24</a>
<span class="snippet"><span>Summary</span>Unified-IO 2は、最初の自己回帰型のマルチモーダルモデルであり、画像、テキスト、音声、アクションを理解し生成することができます。異なるモダリティを統一するために、共有の意味空間に入力と出力を配置し、単一のエンコーダ・デコーダトランスフォーマーモデルで処理します。さまざまなアーキテクチャの改善を提案し、大規模なマルチモーダルな事前トレーニングコーパスを使用してモデルをトレーニングします。Unified-IO 2は、GRITベンチマークを含む35以上のベンチマークで最先端のパフォーマンスを発揮します。</span>
<span class="snippet"><span>Comment</span>画像、テキスト、音声、アクションを理解できる初めてのautoregressive model。AllenAI<img src="https://github.com/user-attachments/assets/fa54f7bf-6689-4346-a1ca-031e4e5516ea" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/4282ffb0-18f1-40c9-b6d7-f004d03b8382" alt="image" loading="lazy">マルチモーダルに拡張したことで、訓練が非常に不安定になったため、アーキテクチャ上でいくつかの工夫を加えている:

<br>



<br>

・2D Rotary Embedding

<br>

  ・Positional EncodingとしてRoPEを採用

<br>

  ・画像のような2次元データのモダリティの場合はRoPEを2次元に拡張する。具体的には、位置(i, j)のトークンについては、Q, Kのembeddingを半分に分割して、それぞれに対して独立にi, jのRoPE Embeddingを適用することでi, j双方の情報を組み込む。

<br>

・QK Normalization

<br>

  ・image, audioのモダリティを組み込むことでMHAのlogitsが非常に大きくなりatteetion weightが0/1の極端な値をとるようになり訓練の不安定さにつながった。このため、dot product attentionを適用する前にLayerNormを組み込んだ。

<br>

・Scaled Cosine Attention

<br>

  ・Image Historyモダリティにおいて固定長のEmbeddingを得るためにPerceiver Resamplerを扱ったているが、こちらも上記と同様にAttentionのlogitsが極端に大きくなったため、cosine類似度をベースとしたScaled Cosine Attention 2259 を利用することで、大幅に訓練の安定性が改善された。

<br>

・その他

<br>

  ・attention logitsにはfp32を適用

<br>

  ・事前学習されたViTとASTを同時に更新すると不安定につながったため、事前学習の段階ではfreezeし、instruction tuningの最後にfinetuningを実施

<br>



<br>

<img src="https://github.com/user-attachments/assets/74c8fa3a-8fb5-4785-8dd3-6a8cf3c7cfeb" alt="image" loading="lazy">目的関数としては、Mixture of Denoisers (1424)に着想を得て、Multimodal Mixture of Denoisersを提案。MoDでは、

<br>

・\[R\]: 通常のspan corruption (1--5 token程度のspanをmaskする)

<br>

・\[S\]: causal language modeling (inputを2つのサブシーケンスに分割し、前方から後方を予測する。前方部分はBi-directionalでも可)

<br>

・\[X\]: extreme span corruption (12&gt;=token程度のspanをmaskする)

<br>



<br>

の3種類が提案されており、モダリティごとにこれらを使い分ける:

<br>

・text modality: UL2 (1424)を踏襲

<br>

・image, audioがtargetの場合: 2つの類似したパラダイムを定義し利用

<br>

  ・\[R\]: patchをランダムにx%マスクしre-constructする

<br>

  ・\[S\]: inputのtargetとは異なるモダリティのみの情報から、targetモダリティを生成する

<br>



<br>

訓練時には prefixとしてmodality token \[Text\], \[Image\], \[Audio\] とparadigm token \[R\], \[S\], \[X\] をタスクを指示するトークンとして利用している。また、image, audioのマスク部分のdenoisingをautoregressive modelで実施する際には普通にやるとdecoder側でリークが発生する(a)。これを防ぐには、Encoder側でマスクされているトークンを、Decoder側でteacher-forcingする際にの全てマスクする方法(b)があるが、この場合、生成タスクとdenoisingタスクが相互に干渉してしまいうまく学習できなくなってしまう（生成タスクでは通常Decoderのinputとして[mask]が入力され次トークンを生成する、といったことは起きえないが、愚直に(b)をやるとそうなってしまう）。ので、(c)に示したように、マスクされているトークンをinputとして生成しなければならない時だけ、マスクを解除してdecoder側にinputする、という方法 (Dynamic Masking) でこの問題に対処している。

<br>

<img width="597" height="394" alt="Image" src="https://github.com/user-attachments/assets/0dba8d5d-0c93-4c56-852b-fce9869428e7"></span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/ImageSegmentation.html">#ImageSegmentation</a>
<a class="button" href="articles/FoundationModel.html">#FoundationModel</a>
<span class="issue_date">Issue Date: 2023-04-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/600">Segment Anything in Medical Images, Jun Ma+, N_A, Nature Communications'24</a>
<span class="snippet"><span>Summary</span>本研究では、自然画像セグメンテーションに革新的な手法であるSegment anything model (SAM)を医療画像に拡張するためのMedSAMを提案し、様々な医療ターゲットのセグメンテーションのための汎用ツールを作成することを目的としています。MedSAMは、大規模な医療画像データセットを用いて開発され、SAMを一般的な医療画像セグメンテーションに適応するためのシンプルなファインチューニング手法を開発しました。21の3Dセグメンテーションタスクと9の2Dセグメンテーションタスクに対する包括的な実験により、MedSAMは、平均Dice類似係数（DSC）がそれぞれ22.5％と17.6％で、デフォルトのSAMモデルを上回ることが示されました。コードとトレーニング済みモデルは、\url{https://github.com/bowang-lab/MedSAM}で公開されています。</span>
<span class="snippet"><span>Comment</span>SAMの性能は医療画像に対しては限定的だったため、11の異なるモダリティに対して200kのマスクをした医療画像を用意しfinetuningしたMedSAMによって、医療画像のセグメンテーションの性能を大幅に向上。

<br>

コードとモデルはpublicly available<img src="https://github.com/user-attachments/assets/ea394adc-b1da-4764-bf29-534323bfc443" alt="image" loading="lazy"></span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/VariationalAutoEncoder.html">#VariationalAutoEncoder</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<a class="button" href="articles/ColdStart.html">#ColdStart</a>
<a class="button" href="articles/Encoder-Decoder.html">#Encoder-Decoder</a>
<a class="button" href="articles/SemanticID.html">#SemanticID</a>
<span class="issue_date">Issue Date: 2025-07-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2309">[Paper Note] Recommender Systems with Generative Retrieval, Shashank Rajput+, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>新しい生成的検索アプローチを提案し、アイテムのセマンティックIDを用いて次のアイテムを予測するTransformerベースのモデルを訓練。これにより、従来のレコメンダーシステムを大幅に上回る性能を達成し、過去の対話履歴がないアイテムに対しても改善された検索性能を示す。</span>
<span class="snippet"><span>Comment</span>openreview:https://openreview.net/forum?id=BJ0fQUU32wSemantic IDを提案した研究アイテムを意味的な情報を保持したdiscrete tokenのタプル（＝Semantic ID)で表現し、encoder-decoderでNext ItemのSemantic IDを生成するタスクに落としこむことで推薦する。SemanticIDの作成方法は後で読んで理解したい。

<br>



<br>

<img src="https://github.com/user-attachments/assets/38606c7c-011f-46b0-ab14-8d213626be3d" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/a239bac7-c273-4681-a102-65e88c9c65d2" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/7d1822ce-462e-43f6-bb33-af77914919f6" alt="image" loading="lazy"></span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/SpeechProcessing.html">#SpeechProcessing</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<a class="button" href="articles/Normalization.html">#Normalization</a>
<span class="issue_date">Issue Date: 2025-04-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1899">Foundation Transformers, Hongyu Wang+, PMLR'23</a>
<span class="snippet"><span>Summary</span>言語、視覚、音声、マルチモーダルにおけるモデルアーキテクチャの収束が進む中、異なる実装の「Transformers」が使用されている。汎用モデリングのために、安定性を持つFoundation Transformerの開発が提唱され、Magnetoという新しいTransformer変種が紹介される。Sub-LayerNormと理論に基づく初期化戦略を用いることで、さまざまなアプリケーションにおいて優れたパフォーマンスと安定性を示した。</span>
<span class="snippet"><span>Comment</span><img src="https://github.com/user-attachments/assets/2847f982-3266-4394-9920-01d9977e505e" alt="image" loading="lazy">関連:

<br>

・1900</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/ImageSegmentation.html">#ImageSegmentation</a>
<a class="button" href="articles/FoundationModel.html">#FoundationModel</a>
<span class="issue_date">Issue Date: 2025-04-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1885">Segment Anything, Alexander Kirillov+, arXiv'23</a>
<span class="snippet"><span>Summary</span>Segment Anything (SA)プロジェクトは、画像セグメンテーションの新しいタスク、モデル、データセットを提案し、1億以上のマスクを含む1,100万のプライバシー尊重した画像からなる最大のセグメンテーションデータセットを構築しました。プロンプト可能なモデルはゼロショットで新しい画像分布やタスクに適応でき、評価の結果、ゼロショット性能が高く、従来の監視された結果を上回ることもあります。SAMとSA-1Bデータセットは、研究促進のために公開されています。</span>
<span class="snippet"><span>Comment</span>SAM論文</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/PositionalEncoding.html">#PositionalEncoding</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2025-04-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1865">The Impact of Positional Encoding on Length Generalization in   Transformers, Amirhossein Kazemnejad+, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>長さ一般化はTransformerベースの言語モデルにおける重要な課題であり、位置エンコーディング（PE）がその性能に影響を与える。5つの異なるPE手法（APE、T5の相対PE、ALiBi、Rotary、NoPE）を比較した結果、ALiBiやRotaryなどの一般的な手法は長さ一般化に適しておらず、NoPEが他の手法を上回ることが明らかになった。NoPEは追加の計算を必要とせず、絶対PEと相対PEの両方を表現可能である。さらに、スクラッチパッドの形式がモデルの性能に影響を与えることも示された。この研究は、明示的な位置埋め込みが長いシーケンスへの一般化に必須でないことを示唆している。</span>
<span class="snippet"><span>Comment</span>・1863

<br>



<br>

において、Llama4 Scoutが10Mコンテキストウィンドウを実現できる理由の一つとのこと。

<br>



<br>

元ポスト:https://x.com/drjimfan/status/1908615861650547081?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

<br>



<br>

Llama4のブログポストにもその旨記述されている:

<br>

&gt;A key innovation in the Llama 4 architecture is the use of interleaved attention layers without positional embeddings. Additionally, we employ inference time temperature scaling of attention to enhance length generalization.

<br>



<br>

[The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation](https://ai.meta.com/blog/llama-4-multimodal-intelligence/?utm_source=twitter&utm_medium=organic_social&utm_content=image&utm_campaign=llama4)斜め読みだが、length generalizationを評価する上でdownstream taskに焦点を当て、3つの代表的なカテゴリに相当するタスクで評価したところ、この観点においてはT5のrelative positinal encodingとNoPE（位置エンコードディング無し）のパフォーマンスが良く、

<br>



<br>

<img src="https://github.com/user-attachments/assets/dddadfff-ab28-4073-96c3-831eb16845a0" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/c6ec8e0e-7abb-4330-be23-2261486a477c" alt="image" loading="lazy">

<br>



<br>

NoPEは絶対位置エンコーディングと相対位置エンコーディングを理論上実現可能であり[^1]

<br>

<img src="https://github.com/user-attachments/assets/bbcf797a-d394-42d4-b017-08d7dba4261c" alt="image" loading="lazy">

<br>



<br>

実際に学習された異なる2つのモデルに対して同じトークンをそれぞれinputし、同じ深さのLayerの全てのattention distributionの組み合わせからJensen Shannon Divergenceで距離を算出し、最も小さいものを2モデル間の当該layerの距離として可視化すると下記のようになり、NoPEとT5のrelative positional encodingが最も類似していることから、NoPEが学習を通じて（実用上は）相対位置エンコーディングのようなものを学習することが分かった。

<br>

<img src="https://github.com/user-attachments/assets/9619c7e5-0612-45de-8717-1634bee509b7" alt="image" loading="lazy">

<br>



<br>

[^1]:深さ1のLayerのHidden State H^1から絶対位置の復元が可能であり（つまり、当該レイヤーのHが絶対位置に関する情報を保持している）、この前提のもと、後続のLayerがこの情報を上書きしないと仮定した場合に、相対位置エンコーディングを実現できる。また、CoT/Scratchpadはlong sequenceに対する汎化性能を向上させることがsmall scaleではあるが先行研究で示されており、Positional Encodingを変化させた時にCoT/Scratchpadの性能にどのような影響を与えるかを調査。

<br>



<br>

具体的には、CoT/Scratchpadのフォーマットがどのようなものが有効かも明らかではないので、5種類のコンポーネントの組み合わせでフォーマットを構成し、mathematical reasoningタスクで以下のような設定で訓練し

<br>



<br>

・さまざまなコンポーネントの組み合わせで異なるフォーマットを作成し、

<br>

・全ての位置エンコーディングあり/なしモデルを訓練

<br>



<br>

これらを比較した。この結果、CoT/Scratchpadはフォーマットに関係なく、特定のタスクでのみ有効（有効かどうかはタスク依存）であることが分かった。このことから、CoT/Scratcpad（つまり、モデルのinputとoutputの仕方）単体で、long contextに対する汎化性能を向上させることができないので、Positional Encoding（≒モデルのアーキテクチャ）によるlong contextに対する汎化性能の向上が非常に重要であることが浮き彫りになった。

<br>

<img src="https://github.com/user-attachments/assets/e23c4fbf-84de-4344-a01e-1e7e9e66fa7e" alt="image" loading="lazy">

<br>



<br>

また、CoT/Scratchpadが有効だったAdditionに対して各Positional Embeddingモデルを学習し、生成されたトークンのattentionがどの位置のトークンを指しているかを相対距離で可視化したところ（0が当該トークン、つまり現在のScratchpadに着目しており、1が遠いトークン、つまりinputに着目していることを表すように正規化）、NoPEとRelative Positional Encodingがshort/long rangeにそれぞれフォーカスするようなbinomialな分布なのに対し、他のPositional Encodingではよりuniformな分布であることが分かった。このタスクにおいてはNoPEとRelative POの性能が高かったため、binomialな分布の方がより最適であろうことが示唆された。

<br>

<img src="https://github.com/user-attachments/assets/833e6a81-8611-4e79-9d2e-473f7ebee2d0" alt="image" loading="lazy">

<br>

</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1271">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head  Checkpoints, Joshua Ainslie+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>Multi-query attention（MQA）は、単一のkey-value headのみを使用しており、デコーダーの推論を劇的に高速化しています。ただし、MQAは品質の低下を引き起こす可能性があり、さらには、より速い推論のためだけに別個のモデルをトレーニングすることが望ましくない場合もあります。既存のマルチヘッド言語モデルのチェックポイントを、オリジナルの事前トレーニング計量の5%を使用してMQAを持つモデルにアップトレーニングするためのレシピを提案し、さらに、複数のkey-value headを使用するマルチクエリアテンションの一般化であるグループ化クエリアテンション（GQA）を紹介します。アップトレーニングされたGQAが、MQAと同等の速度でマルチヘッドアテンションに匹敵する品質を達成することを示しています。</span>
<span class="snippet"><span>Comment</span>通常のMulti-Head AttentionがQKVが1対1対応なのに対し、Multi Query Attention (MQA) 1272  は全てのQに対してKVを共有する。一方、GQAはグループごとにKVを共有する点で異なる。MQAは大幅にInfeerence` speedが改善するが、精度が劣化する問題があった。この研究では通常のMulti-Head Attentionに対して、オリジナルの事前学習に対して追加の5%の計算量でGQAモデルを学習する手法を提案している。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/70ec2179-428c-47b8-af53-cb3cc0e4f022" alt="image" loading="lazy">

<br>



<br>

Main Result. Multi-Head Attentionに対して、inference timeが大幅に改善しているが、Multi-Query Attentionよりも高い性能を維持している。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/3687aeb4-90b8-403d-853b-740121dd5f98" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2023-12-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1174">Pushdown Layers: Encoding Recursive Structure in Transformer Language   Models, Shikhar Murty+, N_A, EMNLP'23</a>
<span class="snippet"><span>Summary</span>本研究では、再帰構造をうまく捉えるために新しい自己注意層であるPushdown Layersを導入しました。Pushdown Layersは、再帰状態をモデル化するためにスタックテープを使用し、トークンごとの推定深度を追跡します。このモデルは、構文的な一般化を改善し、サンプル効率を向上させることができます。さらに、Pushdown Layersは標準の自己注意の代替としても使用でき、GLUEテキスト分類タスクでも改善を実現しました。</span>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<span class="issue_date">Issue Date: 2023-11-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1164">Advancing Transformer Architecture in Long-Context Large Language  Models: A Comprehensive Survey, Yunpeng Huang+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>本論文では、Transformerベースの大規模言語モデル（LLMs）の長い文脈の能力を最適化するための包括的な調査を提案しています。現行のLLMsの制約や問題点を明確化し、アーキテクチャのアップグレードや評価の必要性について説明しています。さらに、最適化ツールキットや将来の研究の可能性についても議論しています。関連文献はhttps://github.com/Strivin0311/long-llms-learningでリアルタイムに更新されています。</span>
<span class="snippet"><span>Comment</span>TransformerをLongContextに対応させる技術のサーベイ。<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/e498f066-2713-463c-8b58-9e9ecd480570" alt="image" loading="lazy">

<br>

（画像は元ツイートより）

<br>

元ツイート: https://x.com/omarsar0/status/1727358484360945750?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2023-11-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1126">Hiformer: Heterogeneous Feature Interactions Learning with Transformers  for Recommender Systems, Huan Gui+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>特徴の相互作用を学ぶために、Transformerベースのアーキテクチャを提案する。ウェブスケールのレコメンダーシステムにおいて、特徴の相互作用を手動で作成することは困難であるため、自動的に捉える必要がある。しかし、現在のTransformerアーキテクチャは異種の特徴の相互作用を捉えることができず、サービングレイテンシも高い。そこで、異種の自己注意層を提案し、\textsc{Hiformer}というモデルを紹介する。\textsc{Hiformer}は特徴の相互作用の異種性を考慮し、低ランク近似とモデルの剪定により高速な推論を実現する。オフライン実験結果では、\textsc{Hiformer}モデルの効果と効率が示されており、Google Playの実世界の大規模なアプリランキングモデルにも展開され、主要なエンゲージメントメトリックスを改善した。</span>
<span class="snippet"><span>Comment</span>推薦システムは、Factorization Machinesあたりから大抵の場合特徴量間の交互作用を頑張って捉えることで精度向上を目指す、という話をしてきている気がするが、これはTransformerを使って交互作用捉えられるようなモデルを考えました、という研究のようである。

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/d57eb1b6-0e68-47fe-9d0a-315186cc9e3d" alt="image" loading="lazy">

<br>



<br>

self attention部分に工夫がなされており（提案手法は右端）、task tokenとそれぞれのfeatureをconcatしてQKVを求めることで、明示的に交互作用が生まれるような構造にしている。

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/5b097e51-34d8-488b-aba5-4dce9e201272" alt="image" loading="lazy">Online A/Bテストでも評価しており、HiformerによってSoTAな交互作用モデル（DCN）よりも高いユーザエンゲージメントを実現することが示されている。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/be431a24-814a-4891-b5d5-58ad2b8563e7" alt="image" loading="lazy"></span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-11-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1117">Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in  Transformer Models, Steve Yadlowsky+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>本研究では、トランスフォーマーモデルの文脈学習（ICL）能力を調査しました。トランスフォーマーモデルは、事前学習データの範囲内で異なるタスクを特定し、学習する能力を持っています。しかし、事前学習データの範囲外のタスクや関数に対しては一般化が劣化することが示されました。また、高容量のシーケンスモデルのICL能力は、事前学習データの範囲に密接に関連していることが強調されました。</span>
<span class="snippet"><span>Comment</span>Transformerがpre-training時に利用された学習データ以外の分布に対しては汎化性能が落ちることを示したらしい。もしこれが正しいとすると、結局真に新しい分布というか関数というかタスクというか、をTransformerが創出する可能性は低いと言えるかもしれない。が、新しいものって大体は既存の概念の組み合わせだよね（スマホとか）、みたいなことを考えると、別にそれでも十分では？と思ってしまう。人間が本当に真の意味で新しい関数というかタスクというか分布を生み出せているかというと、実はそんなに多くないのでは？という予感もする。まあたとえば、量子力学を最初に考えました！とかそういうのは例外だと思うけど・・・、そのレベルのことってどんくらいあるんだろうね？</span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2023-10-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1062">Boolformer: Symbolic Regression of Logic Functions with Transformers, Stéphane d'Ascoli+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>この研究では、BoolformerというTransformerアーキテクチャを使用して、ブール関数のシンボリック回帰を実行する方法を紹介します。Boolformerは、クリーンな真理値表やノイズのある観測など、さまざまなデータに対して効果的な式を予測することができます。さらに、実世界のデータセットや遺伝子制御ネットワークのモデリングにおいて、Boolformerは解釈可能な代替手法として優れた性能を発揮します。この研究の成果は、公開されています。</span>
<span class="snippet"><span>Comment</span>ブール関数をend-to-endで学習できるtransformeiアーキテクチャを提案した模様</span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/DataAugmentation.html">#DataAugmentation</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/DataGeneration.html">#DataGeneration</a>
<span class="issue_date">Issue Date: 2023-08-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1024">Prompt2Model: Generating Deployable Models from Natural Language   Instructions, Vijay Viswanathan+, N_A, EMNLP'23</a>
<span class="snippet"><span>Summary</span>本研究では、大規模言語モデル（LLMs）を使用して、プロンプトを自然言語でタスクを説明し、特定のモデルを訓練する手法であるPrompt2Modelを提案しています。Prompt2Modelは、既存のデータセットと事前学習済みモデルの検索、LLMsを使用したデータセットの生成、および教師あり微調整のプロセスを通じて行われます。実験結果では、Prompt2Modelが強力なLLMを上回る性能を示し、モデルの信頼性の評価も可能であることが示されています。Prompt2Modelはオープンソースで利用可能です。</span>
<span class="snippet"><span>Comment</span>Dataset Generatorによって、アノテーションが存在しないデータについても擬似ラベル付きデータを生成することができ、かつそれを既存のラベル付きデータと組み合わせることによってさらに性能が向上することが報告されている。これができるのはとても素晴らしい。Dataset Generatorについては、データを作成する際に低コストで、高品質で、多様なデータとするためにいくつかの工夫を実施している。

<br>

1. ユーザが与えたデモンストレーションだけでなく、システムが生成したexampleもサンプリングして活用することで、生成されるexampleの多様性を向上させる。実際、これをやらない場合は120/200がduplicate exampleであったが、これが25/200まで減少した。

<br>

2. 生成したサンプルの数に比例して、temperatureを徐々に高くしていく。これにより、サンプルの質を担保しつつ、多様性を徐々に増加させることができる。Temperature Annealingと呼ぶ。

<br>

3. self-consistencyを用いて、擬似ラベルの質を高める。もしmajority votingが互角の場合は、回答が短いものを採用した（これはヒューリスティックに基づいている）

<br>

4. zeno buildを用いてAPIへのリクエストを並列化することで高速に実験を実施

<br>



<br>

非常に参考になる。</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Contents-based.html">#Contents-based</a>
<a class="button" href="articles/pretrained-LM.html">#pretrained-LM</a>
<a class="button" href="articles/ContrastiveLearning.html">#ContrastiveLearning</a>
<span class="issue_date">Issue Date: 2023-07-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/852">UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation, ACL'23</a>
<span class="snippet"><span>Summary</span>本研究では、事前学習済み言語モデル（PLM）を使用して、テキストベースの推薦の性能を向上させるための新しいフレームワークであるUniTRecを提案します。UniTRecは、ユーザーの履歴の文脈をより良くモデル化するために統一されたローカル-グローバルアテンションTransformerエンコーダを使用し、候補のテキストアイテムの言語の複雑さを推定するためにTransformerデコーダを活用します。幅広い評価により、UniTRecがテキストベースの推薦タスクで最先端のパフォーマンスを発揮することが示されました。</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/PositionalEncoding.html">#PositionalEncoding</a>
<span class="issue_date">Issue Date: 2023-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/820">Randomized Positional Encodings Boost Length Generalization of Transformers, ACL'23</a>
<span class="snippet"><span>Summary</span>トランスフォーマーは、固定長のタスクにおいては優れた汎化能力を持つが、任意の長さのシーケンスには対応できない。この問題を解決するために、新しい位置エンコーディング手法を提案する。ランダム化された位置エンコーディングスキームを使用し、長いシーケンスの位置をシミュレートし、順序付けられたサブセットをランダムに選択する。大規模な実証評価により、この手法がトランスフォーマーの汎化能力を向上させ、テストの正確性を平均して12.0％向上させることが示された。</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/811">Trainable Transformer in Transformer, Abhishek Panigrahi+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>本研究では、Transformer in Transformer（TinT）という効率的な構築を提案し、大規模な事前学習言語モデルの内部モデルをシミュレートして微調整することが可能となります。TinTは小さなパラメータ数でも高い性能を発揮し、トランスフォーマー内の単純なモデルの効率も向上させます。さまざまな実験により、TinTの性能向上が観察され、大規模な事前学習言語モデルが複雑なサブルーチンを実行できることが示されました。また、TinTのモジュラーで拡張可能なコードベースも提供されています。</span>
<span class="snippet"><span>Comment</span>参考: https://twitter.com/hillbig/status/1679253896362086401?s=46&t=ArwxeDos47eUWfAg7_FRtg研究の進み早すぎません？？？</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<span class="issue_date">Issue Date: 2023-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/806">Generative Pretraining in Multimodality, Quan Sun+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>Emuは、マルチモーダルなコンテキストで画像とテキストを生成するためのTransformerベースのモデルです。このモデルは、単一モダリティまたはマルチモーダルなデータ入力を受け入れることができます。Emuは、マルチモーダルなシーケンスでトレーニングされ、画像からテキストへのタスクやテキストから画像へのタスクなど、さまざまなタスクで優れたパフォーマンスを示します。また、マルチモーダルアシスタントなどの拡張機能もサポートしています。</span>
<a class="button" href="articles/Survey.html">#Survey</a>
<span class="issue_date">Issue Date: 2023-07-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/781">A Comprehensive Survey on Applications of Transformers for Deep Learning  Tasks, Saidul Islam+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>Transformerモデルは、セルフアテンションメカニズムを使用して文脈関係を理解するためのディープニューラルネットワークであり、長い依存関係を処理することができます。このモデルは、自然言語処理だけでなく、他のさまざまなドメインでも注目されています。しかし、さまざまなドメインでのTransformerの応用に関する包括的な調査はまだ不足しています。そこで、私たちは提案されたTransformerモデルの包括的な調査を行い、その応用ドメインと影響を分析しました。私たちの目的は、研究者に対してTransformerの可能性を明らかにし、この技術の理解を広めることです。</span>
<span class="snippet"><span>Comment</span>Transformerに関する最新サーベイ論文。Transformerが利用されているアプリケーションと、モデルのリストが列挙されている。</span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2023-06-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/774">Faith and Fate: Limits of Transformers on Compositionality, Nouha Dziri+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>Transformerの大規模言語モデル（LLMs）は、多段階の推論を必要とするタスクで優れたパフォーマンスを示す一方、些細な問題で失敗することもある。この研究では、3つの代表的な合成タスクを用いて、Transformerの限界を調査し、タスクの複雑さが増すにつれてパフォーマンスが低下することを示した。また、Transformerが合成的な推論を線形化されたサブグラフのマッチングに簡約化して解決していることを示唆したが、体系的な問題解決スキルを開発していない可能性もある。</span>
<span class="snippet"><span>Comment</span>参考: https://twitter.com/hillbig/status/1674891033283555328?s=46&t=KFT8cWTu8vV69iD6Qt0NGw</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LLMAgent.html">#LLMAgent</a>
<span class="issue_date">Issue Date: 2023-06-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/760">Think Before You Act: Decision Transformers with Internal Working Memory, Jikun Kang+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>大規模言語モデル（LLM）の性能は、トレーニング中にパラメータに振る舞いを記憶する「忘却現象」によって低下する可能性がある。人間の脳は分散型のメモリストレージを利用しており、忘却現象を軽減している。そこで、我々は、内部作業メモリモジュールを提案し、Atariゲームとメタワールドオブジェクト操作タスクの両方でトレーニング効率と汎化性を向上させることを示した。</span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2023-06-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/744">Birth of a Transformer: A Memory Viewpoint, Alberto Bietti+, N_A, arXiv'23</a>
<span class="snippet"><span>Summary</span>大規模言語モデルの内部メカニズムを理解するため、トランスフォーマーがグローバルとコンテキスト固有のbigram分布をどのようにバランスするかを研究。2層トランスフォーマーでの実証的分析により、グローバルbigramの高速な学習と、コンテキスト内のbigramの「誘導ヘッド」メカニズムの遅い発達を示し、重み行列が連想記憶としての役割を強調する。データ分布特性の役割も研究。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<a class="button" href="articles/Encoder.html">#Encoder</a>
<a class="button" href="articles/Encoder-Decoder.html">#Encoder-Decoder</a>
<span class="issue_date">Issue Date: 2023-05-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/667">Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing   Important Tokens, Zhanpeng Zeng+, N_A, NeurIPS'23</a>
<span class="snippet"><span>Summary</span>本論文では、Transformerモデルの二次コストを削減するために、各層でサイズ$r$が$n$に独立した表現に入力を圧縮する方法を提案する。VIPトークン中心の圧縮（Vcc）スキームを使用し、VIPトークンの表現を近似するために入力シーケンスを選択的に圧縮する。提案されたアルゴリズムは、競合するベースラインと比較して効率的であり、多数のタスクにおいて競争力のあるまたはより優れたパフォーマンスを発揮する。また、アルゴリズムは128Kトークンにスケーリングでき、一貫して精度の向上を提供することが示された。</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/Inference.html">#Inference</a>
<span class="issue_date">Issue Date: 2023-04-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/601">Efficiently Scaling Transformer Inference, Reiner Pope+, N_A, MLSys'23</a>
<span class="snippet"><span>Summary</span>大規模Transformerベースのモデルの推論のエンジニアリングのトレードオフを理解するために、最適な多次元分割技術を選択するための単純な解析モデルを開発低レベルの最適化と組み合わせることで、500B+パラメータモデルのレイテンシーとモデルFLOPS利用率のトレードオフにおいて、FasterTransformerベンチマークスイートを上回る新しいParetoフロンティアを実現適切な分割により、マルチクエリアテンションの低いメモリ要件により、32倍の大きなコンテキスト長にスケーリング可能int8ウェイト量子化を使用した生成中の低バッチサイズレイテンシーは、トークンあたり29msであり、入力トークンの大バッチサイズ処理において76％のMFUを実現し、PaLM 540Bパラメータモデルにおいて2048トークンの長いコンテキスト長をサポートしている。</span>
<span class="snippet"><span>Comment</span>特にMultiquery Attentionという技術がTransformerのinferenceのコスト削減に有効らしい</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-04-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/529">Scaling Transformer to 1M tokens and beyond with RMT, Bulatov+, DeepPavlov, arXiv'23</a>
<span class="snippet"><span>Comment</span>Reccurent Memory Transformer 523 を使って2Mトークン扱えるようにしたよーという話。

<br>



<br>

ハリーポッターのトークン数が1.5Mらしいので、そのうち小説一冊書けるかもという世界。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<a class="button" href="articles/Normalization.html">#Normalization</a>
<a class="button" href="articles/Encoder-Decoder.html">#Encoder-Decoder</a>
<span class="issue_date">Issue Date: 2025-07-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2141">[Paper Note] On Layer Normalizations and Residual Connections in Transformers, Sho Takase+, arXiv'22</a>
<span class="snippet"><span>Summary</span>本研究では、Transformerアーキテクチャのレイヤー正規化の位置に関するPost-LNとPre-LNの違いを調査。Post-LNは浅い層で優れた性能を示す一方、深い層では不安定なトレーニングを引き起こす消失勾配問題があることを発見。これを踏まえ、Post-LNの修正により安定したトレーニングを実現する方法を提案し、実験でPre-LNを上回る結果を示した。</span>
<span class="snippet"><span>Comment</span>Pre-LNの安定性を持ちながらもPost-LNのような高い性能を発揮する良いとこ取りのB2TConnectionを提案

<br>

<img src="https://github.com/user-attachments/assets/4d85bf16-19e4-4d2a-85e5-87da45cd2a98" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/af0aeae5-554a-4997-96a3-929cd7dd90bb" alt="image" loading="lazy">NLP2022:https://www.anlp.jp/proceedings/annual_meeting/2022/pdf_dir/A2-5.pdf</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<a class="button" href="articles/Normalization.html">#Normalization</a>
<span class="issue_date">Issue Date: 2025-04-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1900">DeepNet: Scaling Transformers to 1,000 Layers, Hongyu Wang+, arXiv'22</a>
<span class="snippet"><span>Summary</span>本論文では、深いTransformerを安定化させるための新しい正規化関数DeepNormを提案し、残差接続の修正と理論的初期化を行う。これにより、Post-LNの性能とPre-LNの安定性を兼ね備え、最大1,000層のTransformerをスケールアップ可能にした。特に、3.2Bパラメータの200層モデルが、12Bパラメータの48層モデルを5 BLEUポイント上回る性能を示し、今後のスケーリングの可能性を示唆している。</span>
<span class="snippet"><span>Comment</span>ステートオブAIガイドによる解説:https://ja.stateofaiguides.com/20220308-deepnet-transformer/</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html">#MoE(Mixture-of-Experts)</a>
<span class="issue_date">Issue Date: 2025-02-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1754">Switch Transformers: Scaling to Trillion Parameter Models with Simple  and Efficient Sparsity, William Fedus+, JMLR'22</a>
<span class="snippet"><span>Summary</span>Switch Transformerを提案し、Mixture of Experts (MoE)の複雑さや通信コスト、トレーニングの不安定性を改善。これにより、低精度フォーマットでの大規模スパースモデルのトレーニングが可能になり、最大7倍の事前トレーニング速度向上を実現。さらに、1兆パラメータのモデルを事前トレーニングし、T5-XXLモデルに対して4倍の速度向上を達成。</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<a class="button" href="articles/KnowledgeEditing.html">#KnowledgeEditing</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<a class="button" href="articles/FactualKnowledge.html">#FactualKnowledge</a>
<a class="button" href="articles/Encoder.html">#Encoder</a>
<span class="issue_date">Issue Date: 2024-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1332">Knowledge Neurons in Pretrained Transformers, Damai Dai+, N_A, ACL'22, 2022.05</a>
<span class="snippet"><span>Summary</span>大規模な事前学習言語モデルにおいて、事実知識の格納方法についての研究を行いました。具体的には、BERTのfill-in-the-blank cloze taskを用いて、関連する事実を表現するニューロンを特定しました。また、知識ニューロンの活性化と対応する事実の表現との正の相関を見つけました。さらに、ファインチューニングを行わずに、知識ニューロンを活用して特定の事実知識を編集しようと試みました。この研究は、事前学習されたTransformers内での知識の格納に関する示唆に富んでおり、コードはhttps://github.com/Hunter-DDM/knowledge-neuronsで利用可能です。</span>
<span class="snippet"><span>Comment</span>1108 日本語解説: https://speakerdeck.com/kogoro/knowledge-neurons-in-pretrained-transformers-for-snlp2022関連:

<br>

・2140上記資料によると、特定の知識を出力する際に活性化する知識ニューロンを特定する手法を提案。MLMを用いたclozeタスクによる実験で[MASK]部分に当該知識を出力する実験をした結果、知識ニューロンの重みをゼロとすると性能が著しく劣化し、値を2倍にすると性能が改善するといった傾向がみられた。　ケーススタディとして、知識の更新と、知識の削除が可能かを検証。どちらとも更新・削除がされる方向性[^1]へモデルが変化した。

<br>



<br>

また、知識ニューロンはTransformerの層の深いところに位置している傾向にあり、異なるrelationを持つような関係知識同士では共有されない傾向にある模様。

<br>



<br>

[^1]: 他の知識に影響を与えず、完璧に更新・削除できたわけではない。知識の更新・削除に伴いExtrinsicな評価によって性能向上、あるいはPerplexityが増大した、といった結果からそういった方向性へモデルが変化した、という話</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/TabularData.html">#TabularData</a>
<span class="issue_date">Issue Date: 2023-04-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/574">Why do tree-based models still outperform deep learning on typical tabular data?, Grinsztajn+, Soda, Inria Saclay , arXiv'22</a>
<span class="snippet"><span>Comment</span>tree basedなモデルがテーブルデータに対してニューラルモデルよりも優れた性能を発揮することを確認し、なぜこのようなことが起きるかいくつかの理由を説明した論文。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/235130988-b008ad89-eec6-49d1-829d-1b23566ed677.jpeg" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

NNよりもtree basedなモデルがうまくいく理由として、モデルの帰納的バイアスがテーブルデータに適していることを調査している。考察としては

<br>



<br>



<br>



<br>

1. NNはスムーズなターゲットを学習する能力が高いが、表形式のような不規則なデータを学習するのに適していない

<br>



<br>

・Random Forestでは、x軸においてirregularなパターンも学習できているが、NNはできていない。

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/235139592-160cf5fe-dddb-4637-a300-b18a0935f253.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

2. uninformativeなfeaatureがMLP-likeなNNに悪影響を与える

<br>



<br>

・Tabular dataは一般にuninformativeな情報を多く含んでおり、実際MLPにuninformativeなfeatureを組み込んだ場合tree-basedな手法とのgapが増加した

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/235140356-553c2d6d-63bf-485e-bcb4-72924535a2a9.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

3. データはrotationに対して不変ではないため、学習手順もそうあるべき（この辺がよくわからなかった）

<br>



<br>

・ResNetはRotationを加えても性能が変わらなかった（rotation invariantな構造を持っている）

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/235141633-910e64ff-83d9-417b-b778-6ab3ec7419f2.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<span class="issue_date">Issue Date: 2025-07-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2259">[Paper Note] Swin Transformer V2: Scaling Up Capacity and Resolution, Ze Liu+, arXiv'21</a>
<span class="snippet"><span>Summary</span>本論文では、大規模ビジョンモデルのトレーニングと応用における課題に対処するための3つの技術を提案。具体的には、トレーニングの安定性向上のための残差後正規化法、低解像度から高解像度への転送を可能にする位置バイアス法、ラベル付きデータの必要性を減少させる自己教師あり学習法を用いる。これにより、30億パラメータのSwin Transformer V2モデルをトレーニングし、複数のビジョンタスクで新記録を樹立。トレーニング効率も向上し、ラベル付きデータと時間を大幅に削減。</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<a class="button" href="articles/ICCV.html">#ICCV</a>
<span class="issue_date">Issue Date: 2025-07-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2258">[Paper Note] Swin Transformer: Hierarchical Vision Transformer using Shifted Windows, Ze Liu+, ICCV'21</a>
<span class="snippet"><span>Summary</span>Swin Transformerは、コンピュータビジョンの新しいバックボーンとして機能する階層的トランスフォーマーを提案。シフトウィンドウ方式により、効率的な自己注意計算を実現し、さまざまなスケールでのモデリングが可能。画像分類や物体検出、セマンティックセグメンテーションなどで従来の最先端を上回る性能を示し、トランスフォーマーのビジョンバックボーンとしての可能性を示唆。コードは公開されている。</span>
<span class="snippet"><span>Comment</span>日本語解説:https://qiita.com/m_sugimura/items/139b182ee7c19c83e70a画像処理において、物体の異なるスケールや、解像度に対処するために、PatchMergeと呼ばれるプーリングのような処理と、固定サイズのローカルなwindowに分割してSelf-Attentionを実施し、layerごとに通常のwindowとシフトされたwindowを適用することで、window間を跨いだ関係性も考慮できるようにする機構を導入したモデル。

<br>

<img src="https://github.com/user-attachments/assets/a2d5f78c-27ec-4f18-bd7d-5475085cfa7b" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/92fb10e1-614e-44ef-9e65-3920cd863d46" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/2b8a543a-069e-468a-bc3c-1f288cdcf577" alt="image" loading="lazy"></span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/EMNLP.html">#EMNLP</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<a class="button" href="articles/FactualKnowledge.html">#FactualKnowledge</a>
<span class="issue_date">Issue Date: 2025-07-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2140">[Paper Note] Transformer Feed-Forward Layers Are Key-Value Memories, Mor Geva+, EMNLP'21</a>
<span class="snippet"><span>Summary</span>フィードフォワード層はトランスフォーマーモデルの大部分を占めるが、その役割は未探求。研究により、フィードフォワード層がキー・バリュー・メモリとして機能し、トレーニング例のテキストパターンと相関することを示す。実験で、下層は浅いパターン、上層は意味的なパターンを学習し、バリューが出力分布を誘導することが確認された。最終的に、フィードフォワード層の出力はメモリの合成であり、残差接続を通じて洗練される。</span>
<span class="snippet"><span>Comment</span>日本語解説（p.5より）: https://speakerdeck.com/kogoro/knowledge-neurons-in-pretrained-transformers-for-snlp2022?slide=5</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2024-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1333">Transformer Feed-Forward Layers Are Key-Value Memories, Mor Geva+, N_A, EMNLP'21</a>
<span class="snippet"><span>Summary</span>トランスフォーマーモデルのフィードフォワード層は、キー・バリューメモリとして機能し、学習されたパターンが人間に解釈可能であることや、上位層がより意味のあるパターンを学習することが示されました。さらに、出力分布を誘導する役割も持ちます。フィードフォワード層の出力はそのメモリの合成であり、残差接続を介してモデルの層を通じて洗練され、最終的な出力分布を生成します。</span>
<span class="snippet"><span>Comment</span>1108 FF layerがKey-Valueストアとして機能する仕組みの概略図

<br>

<img src="https://github.com/user-attachments/assets/cc12695f-b030-433a-88e1-aed69f9847a7" alt="image" loading="lazy">

<br>



<br>

実際に特定のKeyと最も関連度が高い訓練事例（input）を抽出し、人間がinputのパターンを分類した結果

<br>

<img src="https://github.com/user-attachments/assets/d1c1a031-9cb8-4e22-bf87-23964f0e0c71" alt="image" loading="lazy"></span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<span class="issue_date">Issue Date: 2023-08-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1009">ViLT: Vision-and-Language Transformer Without Convolution or Region   Supervision, Wonjae Kim+, N_A, ICML'21</a>
<span class="snippet"><span>Summary</span>VLP（Vision-and-Language Pre-training）のアプローチは、ビジョンと言語のタスクでのパフォーマンスを向上させているが、現在の方法は効率性と表現力の面で問題がある。そこで、本研究では畳み込みフリーのビジョンと言語のトランスフォーマ（ViLT）モデルを提案する。ViLTは高速でありながら競争力のあるパフォーマンスを示し、コードと事前学習済みの重みはGitHubで利用可能である。</span>
<span class="snippet"><span>Comment</span>日本語解説:https://tech.fusic.co.jp/posts/2021-12-29-vilt/</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<span class="issue_date">Issue Date: 2025-08-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2388">[Paper Note] Longformer: The Long-Document Transformer, Iz Beltagy+, arXiv'20</a>
<span class="snippet"><span>Summary</span>Longformerは、長いシーケンスを線形に処理できる注意機構を持つTransformerベースのモデルで、数千トークンの文書を扱える。局所的なウィンドウ注意とタスクに基づくグローバル注意を組み合わせ、文字レベルの言語モデリングで最先端の結果を達成。事前学習とファインチューニングを行い、長文タスクでRoBERTaを上回る性能を示した。また、Longformer-Encoder-Decoder（LED）を導入し、長文生成タスクにおける効果を確認した。</span>
<span class="snippet"><span>Comment</span>（固定された小さめのwindowsサイズの中でのみattentionを計算する）sliding window attentionを提案

<br>



<br>

<img width="795" height="231" alt="Image" src="https://github.com/user-attachments/assets/d1eccdaf-5b5b-4444-ad31-44c54c345d79">OpenLLMの文脈だと、Mistralに採用されて話題になったかも？

<br>

・1309</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2356">[Paper Note] Transformers are RNNs: Fast Autoregressive Transformers with Linear  Attention, Angelos Katharopoulos+, ICML'20</a>
<span class="snippet"><span>Summary</span>自己注意をカーネル特徴マップの線形ドット積として表現することで、Transformersの複雑性を$\mathcal{O}\left(N^2\right)$から$\mathcal{O}\left(N\right)$に削減。これにより、自己回帰型Transformersの速度が最大4000倍向上し、従来のパフォーマンスを維持。</span>
<span class="snippet"><span>Comment</span>関連:

<br>

・1210 </span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/ICLR.html">#ICLR</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2355">[Paper Note] Reformer: The Efficient Transformer, Nikita Kitaev+, ICLR'20</a>
<span class="snippet"><span>Summary</span>本研究では、トランスフォーマーモデルの効率を向上させるために、局所感度ハッシュを用いた注意機構と可逆残差層を提案。これにより、計算量をO($L^2$)からO($L\log L$)に削減し、メモリ効率と速度を向上させたReformerモデルを実現。トランスフォーマーと同等の性能を維持。</span>
<span class="snippet"><span>Comment</span>openreview: https://openreview.net/forum?id=rkgNKkHtvB</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2354">[Paper Note] Linformer: Self-Attention with Linear Complexity, Sinong Wang+, arXiv'20</a>
<span class="snippet"><span>Summary</span>大規模トランスフォーマーモデルは自然言語処理で成功を収めているが、長いシーケンスに対しては高コスト。自己注意メカニズムを低ランク行列で近似し、複雑さを$O(n^2)$から$O(n)$に削減する新しいメカニズムを提案。これにより、メモリと時間効率が向上した線形トランスフォーマー「Linformer」が標準モデルと同等の性能を示す。</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Normalization.html">#Normalization</a>
<a class="button" href="articles/Encoder-Decoder.html">#Encoder-Decoder</a>
<span class="issue_date">Issue Date: 2025-07-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2142">[Paper Note] On Layer Normalization in the Transformer Architecture, Ruibin Xiong+, arXiv'20</a>
<span class="snippet"><span>Summary</span>本論文では、Transformerの学習率のウォームアップ段階の重要性を理論的に研究し、レイヤー正規化の位置が訓練の安定性に与える影響を示す。特に、Post-LN Transformerでは大きな勾配が不安定さを引き起こすため、ウォームアップが有効である一方、Pre-LN Transformerでは勾配が良好に振る舞うため、ウォームアップを省略できることを示す。実験により、ウォームアップなしのPre-LN Transformerがベースラインと同等の結果を達成し、訓練時間とハイパーパラメータの調整が削減できることを確認した。</span>
<span class="snippet"><span>Comment</span>OpenReview:https://openreview.net/forum?id=B1x8anVFPrEncoder-DecoderのTransformerにおいて、Post-LNの場合は、Warmupを無くすと最終的な性能が悪化し、またWarmUpステップの値によって（500 vs. 4000で実験)も最終的な性能が変化する。これには学習時にハイパーパラメータをしっかり探索しなければならず、WarmUPを大きくすると学習効率が落ちるというデメリットがある。

<br>

<img src="https://github.com/user-attachments/assets/e7a26ecd-7905-4e6c-bb9a-29b8289addb0" alt="image" loading="lazy">

<br>



<br>

Post-LNの場合は、Pre-LNと比較して勾配が大きく、Warmupのスケジュールをしっかり設計しないと大きな勾配に対して大きな学習率が適用され学習が不安定になる。これは学習率を非常に小さくし、固定値を使うことで解決できるが、収束が非常に遅くなるというデメリットがある。

<br>

<img src="https://github.com/user-attachments/assets/afb09f44-c7c9-44ab-9066-3ee788ebd8ee" alt="image" loading="lazy">

<br>



<br>

一方、Pre-LNはWarmup無しでも、高い性能が達成でき、上記のようなチューニングの手間や学習効率の観点から利点がある、みたいな話の模様。

<br>

<img src="https://github.com/user-attachments/assets/d675a58b-e876-4e41-a76f-306c2e1ce23f" alt="image" loading="lazy">

<br>

</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2024-05-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1311">GLU Variants Improve Transformer, Noam Shazeer, N_A, arXiv'20</a>
<span class="snippet"><span>Summary</span>GLUのバリエーションをTransformerのフィードフォワード・サブレイヤーでテストし、通常の活性化関数よりもいくつかのバリエーションが品質向上をもたらすことを発見した。</span>
<span class="snippet"><span>Comment</span>一般的なFFNでは、linear layerをかけた後に、何らかの活性化関数をかませる方法が主流である。

<br>



<br>

このような構造の一つとしてGLUがあるが、linear layerと活性化関数には改良の余地があり、様々なvariantが考えられるため、色々試しました、というはなし。

<br>



<br>



<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/72b1d0bb-64ac-4155-9a3b-5624cd06ccc9" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/b38321c6-d414-4764-9147-10a5fa83fbe6" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

オリジナルのGLUと比較して、T5と同じ事前学習タスクを実施したところ、perplexityが改善

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/9e67a054-2148-41ed-aae1-5a752c21a242" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

また、finetuningをした場合の性能も、多くの場合オリジナルのGLUよりも高い性能を示した。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/77ccab88-e5cc-48fc-b9e0-f2dad24e53e8" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/8f60ca8c-50eb-4869-bab4-f02ec6d8e085" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/8124fc25-aa7e-4e10-8cd2-9d24c818f410" alt="image" loading="lazy">

<br>



<br>



<br>



<br>



<br>



<br>

</span>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/MachineTranslation.html">#MachineTranslation</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/pretrained-LM.html">#pretrained-LM</a>
<span class="issue_date">Issue Date: 2022-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/493">Leveraging Pre-trained Checkpoints for Sequence Generation Tasks, Rothe+, Google Research, TACL'20</a>
<span class="snippet"><span>Comment</span>概要

<br>



<br>

BERT-to-BERT論文。これまでpre-trainedなチェックポイントを利用する研究は主にNLUで行われてきており、Seq2Seqでは行われてきていなかったので、やりました、という話。

<br>



<br>

publicly availableなBERTのcheckpointを利用し、BERTをencoder, decoder両方に採用することでSeq2Seqを実現。実現する上で、

<br>



<br>

1. decoder側のBERTはautoregressiveな生成をするようにする（左側のトークンのattentionしか見れないようにする）

<br>



<br>

2. encoder-decoder attentionを新たに導入する

<br>



<br>

の2点を工夫している。

<br>



<br>



<br>



<br>

実験

<br>



<br>

Sentence Fusion, Sentence Split, Machine Translation, Summarizationの4タスクで実験

<br>



<br>



<br>



<br>

MT

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/204958483-722106b3-bda2-45a3-bb08-fb4eb429c90c.png" alt="image" loading="lazy">

<br>



<br>

BERT2BERTがSoTA達成。Edunov+の手法は、data _augmentationを利用した手法であり、純粋なWMT14データを使った中ではSoTAだと主張。特にEncoder側でBERTを使うと、Randomにinitializeした場合と比べて性能が顕著に上昇しており、その重要性を主張。

<br>



<br>

Sentence Fusion, Sentence Splitでは、encoderとdecoderのパラメータをshareするのが良かったが、MTでは有効ではなかった。これはMTではmodelのcapacityが非常に重要である点、encoderとdecoderで異なる文法を扱うためであると考えられる。

<br>



<br>



<br>



<br>

Summarization

<br>



<br>

BERTSHARE, ROBERTASHAREの結果が良かった。

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/204959543-e21bd9a6-bef4-4538-b181-daca93fa33e7.png" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/DataToTextGeneration.html">#DataToTextGeneration</a>
<span class="issue_date">Issue Date: 2022-09-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/488">Text-to-Text Pre-Training for Data-to-Text Tasks, Mihir+, Google Research, INLG'20</a>
<span class="snippet"><span>Comment</span>概要

<br>



<br>

pre-training済みのT5に対して、Data2Textのデータセットでfinetuningを実施する方法を提案。WebNLG（graph-to-text）, ToTTo（table-to-text）, Multiwoz（task oriented dialogue）データにおいて、simpleなTransformerでも洗練されたmulti-stageなpipelined approachをoutperformできることを示した研究。

<br>



<br>



<br>



<br>

手法

<br>



<br>

事前学習済みのT5に対してfine-tuningを実施した。手法はシンプルで、data-to-textタスクをtext-to-textタスクに変換した。具体的には、構造かされたデータをflatな文字列（linearization）で表現することで、text-to-textタスクに変換。各データセットに対するlinearizationのイメージは下図。デリミタや特殊文字を使って構造かされたデータをflatなstringで表現している。

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/191689155-3562f4f3-d1a1-4ea0-9d37-a523b78e8922.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

データセット

<br>



<br>

ToTTo（2020）

<br>



<br>

Wikipediaのテーブルと自然言語でdescriptionのペアデータ

<br>



<br>

MultiWoz（2018）

<br>



<br>

10Kの人間同士のtask-orientedなdialogueデータ。

<br>



<br>

WebNLG（2017）

<br>



<br>

subject-object-predicateの3組みをテキスト表現に変換するタスクのデータ

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/191693682-3cf3302f-b4e2-433d-94ed-995a8a908d0c.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Result

<br>



<br>

WebNLG

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/191694085-7bf7348a-b468-46e0-a900-c0090d1abcba.png" alt="image" loading="lazy">

<br>



<br>

GCNを利用した2020年に提案されたDualEncがSoTAだったらしいが、outperormしている。

<br>



<br>



<br>



<br>

ToTTo

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/191694683-f31ccad1-2936-4c21-ac10-0807a848f043.png" alt="image" loading="lazy">

<br>



<br>

[こちら](https://github.com/google-research-datasets/totto)のリーダーボードと比較してSoTAを記録

<br>



<br>



<br>



<br>

MultiWoz

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/191695459-e3397936-bdf7-4450-b4c2-6f6eead0825d.png" alt="image" loading="lazy">

<br>



<br>

T5は事前学習済みGPT-2をfinetuningした手法もoutperformした。SC-GPT2は当時のMultiWozでのSoTA

<br>



<br>



<br>



<br>

Impact of Model capacity

<br>



<br>

T5モデルのサイズがどれが良いかについては、データセットのサイズと複雑さに依存することを考察している。たとえば、MultiWozデータは構造化データのバリエーションが最も少なく、データ量も56kと比較的多かった。このため、T5-smallでもより大きいモデルの性能に肉薄できている。

<br>



<br>

一方、WebNLGデータセットは、18kしか事例がなく、特徴量も約200種類程度のrelationのみである。このような場合、モデルサイズが大きくなるにつれパフォーマンスも向上した（特にUnseen test set）。特にBLEUスコアはT5-smallがT5-baseになると、10ポイントもジャンプしており、modelのcapacityがout-of-domainに対する一般化に対してcriticalであることがわかる。ToTToデータセットでも、SmallからBaseにするとパフォーマンスは改善した。所感

<br>



<br>

こんな簡単なfine-tuningでSoTAを達成できてしまうとは、末恐ろしい。ベースラインとして有用。</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/PositionalEncoding.html">#PositionalEncoding</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2359">[Paper Note] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context, Zihang Dai+, ACL'19</a>
<span class="snippet"><span>Summary</span>Transformer-XLは、固定長のコンテキストを超えた長期的な依存関係を学習する新しいニューラルアーキテクチャで、セグメントレベルの再帰メカニズムと新しい位置エンコーディングを採用。これにより、RNNより80%、従来のTransformersより450%長い依存関係を学習し、評価時には最大1,800倍の速度向上を実現。enwiki8やWikiText-103などで最先端のパフォーマンスを達成し、数千トークンの一貫したテキスト生成も可能。コードとモデルはTensorflowとPyTorchで利用可能。</span>
<span class="snippet"><span>Comment</span>日本語解説:

<br>

・329以下が定式化で、一つ前のセグメントのトークン・layerごとのhidden stateを、現在のセグメントの対応するトークンとlayerのhidden stateにconcatし（過去のセグメントに影響を与えないように勾配を伝搬させないStop-Gradientを適用する）、QKVのうち、KVの計算に活用する。また、絶対位置エンコーディングを利用するとモデルがセグメント間の時系列的な関係を認識できなくなるため、位置エンコーディングには相対位置エンコーディングを利用する。これにより、現在のセグメントのKVが一つ前のセグメントによって条件づけられ、contextとして考慮することが可能となり、セグメント間を跨いだ依存関係の考慮が実現される。

<br>

・<img width="634" height="563" alt="Image" src="https://github.com/user-attachments/assets/f13106d3-68ab-4126-8cdc-4c99075bc35a">

<br>



<br>

<img width="1278" height="360" alt="Image" src="https://github.com/user-attachments/assets/1435151a-2802-470b-a3c5-e7b4ea66ae05"></span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2024-10-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1446">What Does BERT Learn about the Structure of Language?, Jawahar+, ACL'19</a>
<span class="snippet"><span>Summary</span>BERTは言語理解において優れた成果を上げており、本研究ではその言語構造の要素を解明する実験を行った。主な発見は、フレーズ表現がフレーズレベルの情報を捉え、中間層が構文的および意味的特徴の階層を形成し、長期依存性の問題に対処するために深い層が必要であること、さらにBERTの構成が古典的な木構造に類似していることを示している。</span>
<span class="snippet"><span>Comment</span>1370 中で引用されている。Transformerの各ブロックが、何を学習しているかを分析。</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1272">Fast Transformer Decoding: One Write-Head is All You Need, Noam Shazeer, N_A, arXiv'19</a>
<span class="snippet"><span>Summary</span>マルチヘッドアテンションレイヤーのトレーニングは高速かつ簡単だが、増分推論は大きな"keys"と"values"テンソルを繰り返し読み込むために遅くなることがある。そこで、キーと値を共有するマルチクエリアテンションを提案し、メモリ帯域幅要件を低減する。実験により、高速なデコードが可能で、わずかな品質の低下しかないことが確認された。</span>
<span class="snippet"><span>Comment</span>Multi Query Attention論文。KVのsetに対して、単一のQueryのみでMulti-Head Attentionを代替する。劇的にDecoderのInferenceが早くなりメモリ使用量が減るが、論文中では言及されていない？ようだが、性能と学習の安定性が課題となるようである。

<br>



<br>



<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/e2d77b43-70c3-4922-a822-bf95d6b4704f" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/PositionalEncoding.html">#PositionalEncoding</a>
<span class="issue_date">Issue Date: 2025-08-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2386">[Paper Note] Self-Attention with Relative Position Representations, Peter Shaw+, NAACL'18</a>
<span class="snippet"><span>Summary</span>本研究では、Transformerの自己注意機構を拡張し、シーケンス要素間の相対的な位置を効率的に考慮する新しいアプローチを提案。WMT 2014の翻訳タスクで1.3 BLEUおよび0.3 BLEUの改善を達成。相対位置と絶対位置の組み合わせではさらなる改善は見られなかった。提案手法は、任意のグラフラベル付き入力に一般化可能な関係認識自己注意機構として位置付けられる。</span>
<span class="snippet"><span>Comment</span>相対位置エンコーディングを提案した研究絶対位置エンコーディングは

<br>

・245</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2353">[Paper Note] Efficient Attention: Attention with Linear Complexities, Zhuoran Shen+, arXiv'18</a>
<span class="snippet"><span>Summary</span>新しい効率的なアテンションメカニズムを提案し、ドット積アテンションと同等の性能を維持しつつ、メモリと計算コストを大幅に削減。これにより、アテンションモジュールの柔軟な統合が可能となり、精度向上を実現。実験結果では、MS-COCO 2017での物体検出やインスタンスセグメンテーションでの性能向上が確認され、Scene Flowデータセットでは最先端の精度を達成。コードは公開されている。</span>
<span class="snippet"><span>Comment</span>Figure1を見るとコンセプトが一目でわかり、非常にわかりやすい

<br>

<img width="1068" height="580" alt="Image" src="https://github.com/user-attachments/assets/18e6a7da-fc07-495f-bda6-bcef4acab321"></span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/SequentialRecommendation.html">#SequentialRecommendation</a>
<a class="button" href="articles/ICDM.html">#ICDM</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2025-07-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2137">[Paper Note] Self-Attentive Sequential Recommendation, Wang-Cheng Kang+, ICDM'18</a>
<span class="snippet"><span>Summary</span>自己注意に基づく逐次モデル（SASRec）を提案し、マルコフ連鎖と再帰型ニューラルネットワークの利点を統合。SASRecは、少数のアクションから次のアイテムを予測し、スパースおよび密なデータセットで最先端のモデルを上回る性能を示す。モデルの効率性と注意重みの視覚化により、データセットの密度に応じた適応的な処理が可能であることが確認された。</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/MachineTranslation.html">#MachineTranslation</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/PositionalEncoding.html">#PositionalEncoding</a>
<a class="button" href="articles/NeurIPS.html">#NeurIPS</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2018-01-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/245">Attention is all you need, Vaswani+, NIPS'17</a>
<span class="snippet"><span>Comment</span>Transformer (self-attentionを利用) 論文

<br>



<br>

解説スライド：https://www.slideshare.net/DeepLearningJP2016/dlattention-is-all-you-need

<br>



<br>

解説記事：https://qiita.com/nishiba/items/1c99bc7ddcb2d62667c6

<br>



<br>



<br>



<br>

 新しい翻訳モデル(Transformer)を提案。既存のモデルよりも並列化に対応しており、短時間の訓練で（既存モデルの1/4以下のコスト）高いBLEUスコアを達成した。

<br>



<br>

 TransformerはRNNやCNNを使わず、attentionメカニズムに基づいている。

<br>



<br>



<br>



<br>

（解説より）分かりやすい:

<br>

https://qiita.com/halhorn/items/c91497522be27bde17ceTransformerの各コンポーネントでのoutputのshapeや、attention_maskの形状、実装について記述されており有用:

<br>

https://qiita.com/FuwaraMiyasaki/items/239f3528053889847825集合知</span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ICML.html">#ICML</a>
<a class="button" href="articles/Normalization.html">#Normalization</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2025-04-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1857">Batch Normalization: Accelerating Deep Network Training by Reducing   Internal Covariate Shift, Sergey Ioffe+, ICML'15</a>
<span class="snippet"><span>Summary</span>バッチ正規化を用いることで、深層ニューラルネットワークのトレーニングにおける内部共変量シフトの問題を解決し、高い学習率を可能にし、初期化の注意を軽減。これにより、同じ精度を14倍少ないトレーニングステップで達成し、ImageNet分類で最良の公表結果を4.9%改善。</span>
<span class="snippet"><span>Comment</span>メモってなかったので今更ながら追加した共変量シフトやBatch Normalizationの説明は

<br>

・261

<br>



<br>

記載のスライドが分かりやすい。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/SpeechProcessing.html">#SpeechProcessing</a>
<a class="button" href="articles/Conversation.html">#Conversation</a>
<a class="button" href="articles/Slide.html">#Slide</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-07-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2224">【輪講資料】Moshi: a speech-text foundation model for real-time dialogue, Hayato Tsukagoshi, 2025.07</a>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Repository.html">#Repository</a>
<a class="button" href="articles/Optimizer.html">#Optimizer</a>
<a class="button" href="articles/Decoder.html">#Decoder</a>
<span class="issue_date">Issue Date: 2025-07-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2223">Modded-NanoGPT, KellerJordan, 2024.05</a>
<span class="snippet"><span>Comment</span>NanoGPT speedrun関連:

<br>

・2118

<br>

・2208</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Chain-of-Thought.html">#Chain-of-Thought</a>
<a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/Scaling%20Laws.html">#Scaling Laws</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-05-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2001">2025年度人工知能学会全国大会チュートリアル講演「深層基盤モデルの数理」, Taiji Suzuki, 2025.05</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/btreetaiji/status/1927678122817921442?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/TimeSeriesDataProcessing.html">#TimeSeriesDataProcessing</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/FoundationModel.html">#FoundationModel</a>
<a class="button" href="articles/OpenWeight.html">#OpenWeight</a>
<span class="issue_date">Issue Date: 2025-05-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1991">Datadog_Toto-Open-Base-1.0, Datadog, 2025.05</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/huggingpapers/status/1926310678060466370?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q（あとでコメント追記する

<br>



<br>

<img src="https://github.com/user-attachments/assets/8aa231e0-ac79-421c-9e23-531aa61137ae" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/user-attachments/assets/22e09246-7f6e-49bc-9db1-8e553a4daf52" alt="image" loading="lazy"></span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/MulltiModal.html">#MulltiModal</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html">#SSM (StateSpaceModel)</a>
<span class="issue_date">Issue Date: 2025-03-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1830">Nemotron-H: A Family of Accurate, Efficient Hybrid Mamba-Transformer Models, Nvidia, 2025.03</a>
<span class="snippet"><span>Comment</span>関連:

<br>

・1820TransformerのSelf-attention LayerをMamba2 Layerに置換することで、様々なベンチマークで同等の性能、あるいは上回る性能で3倍程度のInference timeの高速化をしている（65536 input, 1024 output）。

<br>



<br>

56B程度のmediumサイズのモデルと、8B程度の軽量なモデルについて述べられている。特に、8BモデルでMambaとTransformerのハイブリッドモデルと、通常のTransformerモデルを比較している。学習データに15 Trillion Tokenを利用しており、このデータ量でのApple to Appleのアーキテクチャ間の比較は、現状では最も大規模なものとのこと。性能は多くのベンチマークでハイブリッドにしても同等、Commonsense Understandingでは上回っている。

<br>



<br>

また、学習したNemotron-Hをバックボーンモデルとして持つVLMについてもモデルのアーキテクチャが述べられている。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Library.html">#Library</a>
<a class="button" href="articles/pretrained-LM.html">#pretrained-LM</a>
<span class="issue_date">Issue Date: 2024-12-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1606">ModernBERT, AnswerDotAI, 2024.12</a>
<span class="snippet"><span>Summary</span>ModernBERTは、エンコーダ専用のトランスフォーマーモデルで、従来のBERTに比べて大幅なパレート改善を実現。2兆トークンで訓練され、8192シーケンス長を持ち、分類タスクやリトリーバルで最先端の結果を示す。速度とメモリ効率も優れており、一般的なGPUでの推論に最適化されている。</span>
<span class="snippet"><span>Comment</span>最近の進化しまくったTransformer関連のアーキテクチャをEncodnr-OnlyモデルであるBERTに取り込んだら性能上がるし、BERTの方がコスパが良いタスクはたくさんあるよ、系の話、かつその実装だと思われる。

<br>

テクニカルペーパー中に記載はないが、評価データと同じタスクでのDecoder-Onlyモデル（SFT有り無し両方）との性能を比較したらどの程度の性能なのだろうか？そもそも学習データが手元にあって、BERTをFinetuningするだけで十分な性能が出るのなら（BERTはGPU使うのでそもそもxgboostとかでも良いが）、わざわざLLM使う必要ないと思われる。BERTのFinetuningはそこまで時間はかからないし、inferenceも速い。

<br>



<br>

参考:

<br>

・1024日本語解説:https://zenn.dev/dev_commune/articles/3f5ab431abdea1?utm_source=substack&utm_medium=email</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Chip.html">#Chip</a>
<span class="issue_date">Issue Date: 2024-09-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1399">Sohu, etched, 2024.06</a>
<span class="snippet"><span>Comment</span>&gt;By burning the transformer architecture into our chip, we can’t run most traditional AI models: the DLRMs powering Instagram ads, protein-folding models like AlphaFold 2, or older image models like Stable Diffusion 2. We can’t run CNNs, RNNs, or LSTMs either.

<br>



<br>

transformer以外の大抵のモデルでは動作しないが、代わりにH-100よりも20倍早いinferenceを実現できるチップらしい。

<br>

<img src="https://github.com/user-attachments/assets/1fb2c6b4-3837-4bec-9a64-f8b4878e5941" alt="image" loading="lazy">

<br>



<br>

&gt;With over 500,000 tokens per second in Llama 70B throughput, Sohu lets you build products impossible on GPUs.

<br>



<br>

いやいやいやLlama-70Bで0.5M Token/secは早すぎる！！！</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/PositionalEncoding.html">#PositionalEncoding</a>
<span class="issue_date">Issue Date: 2024-05-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1310">RoFormer: Enhanced Transformer with Rotary Position Embedding, Jianlin Su+, N_A, Neurocomputing, 2024</a>
<span class="snippet"><span>Summary</span>位置符号化はtransformerアーキテクチャで有効であり、本論文ではRotary Position Embedding（RoPE）という新しい手法を提案している。RoPEは、回転行列を使用して絶対位置を符号化し、同時に相対位置依存性を自己注意構成に組み込む。RoPEを使用したRoFormerは、長いテキスト分類ベンチマークデータセットで他の手法を上回ることが実験で示されており、Huggingfaceに統合されている。</span>
<span class="snippet"><span>Comment</span>RoPEを提案した論文Absolute Position Embedding と Relative Position Embedding

<br>



<br>

TransformerにおけるQKVベクトルの計算方法

<br>



<br>

一般に、Transformerにおける Query (Q), Key (K), Value (V) は以下の式で定式化される：

<br>



<br>

<img width="176" alt="image" src="https://github.com/user-attachments/assets/21b0f077-64b4-4fe5-af04-bffc373eabf5">

<br>



<br>

m, nはそれぞれ位置を表す整数。Absolute Position Embeddingと、Relative Position Embeddingは、関数fの設計がそれぞれ異なっている：

<br>



<br>



<br>



<br>

Absolute Position Embedding

<br>



<br>

absolute position embeddingは、固定されたposition ベクトル、あるいはtrainableなposition ベクトル p を、入力ベクトルに対して足し合わせる：

<br>



<br>

<img width="382" alt="image" src="https://github.com/user-attachments/assets/0688c1bf-8699-48a5-9d95-06454550bbdf">

<br>



<br>



<br>



<br>

Relative Position Embedding

<br>



<br>

一方、Relative Position Embeddingは、Queryの位置に対する、Key, Valueの相対位置（つまり、mとnの差）に対して、trainableなベクトルをKey, Valueごとに用意し、そのベクトルを入力に足し合わせる、という定式化となっている：

<br>



<br>

<img width="269" alt="image" src="https://github.com/user-attachments/assets/ddb92f1a-af23-4d71-a7b9-2a7adda792e1">

<br>



<br>



<br>



<br>

RoPE

<br>



<br>

RoPEでは、入力ベクトルに対して回転行列を適用することで、回転に対して位置情報を保持させる：

<br>



<br>

<img width="705" alt="image" src="https://github.com/user-attachments/assets/fce1d06e-e346-4278-a77c-4c96795d5488">

<br>



<br>

<img width="588" alt="image" src="https://github.com/user-attachments/assets/3f28103c-6a56-4016-8f50-d45fe28cd62a">

<br>



<br>



<br>



<br>

RoPEは下記のような性質を持つ：

<br>



<br>

・long-term decay: θi = 10000−2i/d と設定することにより、相対位置が離れているトークンのベクトルとのinner productの値が小さくなる。すなわち、位置が離れているトークン間の依存関係が小さくなる。

<br>



<br>

・Linear-Attention: RoPEは回転行列であり、乗算後のベクトルのノルムを変化させない。このため、Linear Attentionの式の中に回転行列を組み込むことで、Linear Attentionと簡単に組み合わせることが可能

<br>



<br>



<br>



<br>

Absolute Position Embedding, Relative Position Embeddingでは、ベクトルに対して位置情報を加算する定式化で K, Vの計算時に位置情報を考慮していたため、Linear Attentionの計算そのものに位置情報を組み込んだ定式化とはなっていなかった。

<br>



<br>

が、RoPEでは回転行列を乗算する定式化であり、ノルムを変化させないのでLinear Attentionの定式化に組み込むことができる。このため、モデルのアーキテクチャを大きく変更しなくとも組み込める。

<br>



<br>

RoPE自体は実装にパラメータを必要としないが、モデルのその他のパラメータがRoPEに適用できるように学習されていないと適用できないであろう点には注意（事前学習時にRoPEが使われていれば話は別）。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/TabularData.html">#TabularData</a>
<span class="issue_date">Issue Date: 2023-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1167">Table Transformer Demo</a>
<span class="snippet"><span>Comment</span>PDF中のテーブルとその構造（行列セル）をdetectするモデル

<br>



<br>

Exampleは以下のような感じ（日本語だとどれくらいできるのかな...）

<br>



<br>



<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/7f62e16b-1ff8-46ad-b6df-7792981f8f58" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Library.html">#Library</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<span class="issue_date">Issue Date: 2023-11-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1129">Transformers.js, 2023</a>
<span class="snippet"><span>Comment</span>ブラウザ上でTransformerベースの様々なモデルを動作させることができるライブラリ</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<span class="issue_date">Issue Date: 2023-10-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1108">大規模言語モデルにおいて､「知識は全結合層に蓄積される」という仮説についての文献調査</a>
<span class="snippet"><span>Comment</span>タイトルの通り、知識がFFNに蓄積されていると主張しているらしい原論文を読み解いている。まとめを引用すると

<br>



<br>

&gt; 「知識は全結合層に蓄積される」という表現は､ややラジカルで､

<br>

少なくともこの論文では「全結合層は知識獲得において重要」という程度

<br>

の､もう少しマイルドな主張をしているように見受けられました｡

<br>



<br>

とのこと。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<span class="issue_date">Issue Date: 2023-07-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/899">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning, 2023</a>
<span class="snippet"><span>Summary</span>FlashAttention-2は、長いシーケンス長におけるTransformerのスケーリングの問題に対処するために提案された手法です。FlashAttention-2は、非対称なGPUメモリ階層を利用してメモリの節約とランタイムの高速化を実現し、最適化された行列乗算に比べて約2倍の高速化を達成します。また、FlashAttention-2はGPTスタイルのモデルのトレーニングにおいても高速化を実現し、最大225 TFLOPs/sのトレーニング速度に達します。</span>
<span class="snippet"><span>Comment</span>Flash Attention1よりも2倍高速なFlash Attention 2Flash Attention1はこちらを参照

<br>

https://arxiv.org/pdf/2205.14135.pdf

<br>



<br>

QK Matrixの計算をブロックに分けてSRAMに送って処理することで、3倍高速化し、メモリ効率を10-20倍を達成。

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/935f61f3-97ce-4e76-826b-040f92ca567c" alt="image" loading="lazy"></span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Library.html">#Library</a>
<a class="button" href="articles/python.html">#python</a>
<span class="issue_date">Issue Date: 2023-05-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/675">Assisted Generation: a new direction toward low-latency text generation, 2023</a>
<span class="snippet"><span>Comment</span>1 line加えるとtransformerのgenerationが最大3倍程度高速化されるようになったらしい<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/fecc1c5e-b9e5-4844-af96-ba48c3d60fae" alt="image" loading="lazy">

<br>



<br>

assistant modelをロードしgenerateに引数として渡すだけ

<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/7dabf3bf-cd32-469c-abba-f1269318576d" alt="image" loading="lazy"></span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Library.html">#Library</a>
<span class="issue_date">Issue Date: 2023-05-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/618">OpenLLaMA</a>
<span class="snippet"><span>Comment</span>LLaMAと同様の手法を似たデータセットに適用し商用利用可能なLLaMAを構築した模様</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/Survey.html">#Survey</a>
<span class="issue_date">Issue Date: 2023-02-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/509">30分で完全理解するTransformerの世界</a>
<span class="snippet"><span>Comment</span>非常に詳細で実質日本語のサーベイ論文のようなもの

<br>

</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/TimeSeriesDataProcessing.html">#TimeSeriesDataProcessing</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<span class="issue_date">Issue Date: 2022-12-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/504">Are Transformers Effective for Time Series Forecasting?</a>
<span class="snippet"><span>Comment</span>Linear Layerに基づくシンプルな手法がTransformerベースの手法に時系列予測で勝ったという話</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Library.html">#Library</a>
<a class="button" href="articles/Explanation.html">#Explanation</a>
<a class="button" href="articles/Blog.html">#Blog</a>
<span class="issue_date">Issue Date: 2022-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/499">Transformers Interpret, 2022</a>
<span class="snippet"><span>Comment</span>transformersのモデルをたった2行追加するだけで、explainableにするライブラリ

<br>



<br>

基本的にtextとvisionのclassificationをサポートしている模様

<br>

text classificationの場合、たとえばinput tokenの各トークンの分類に対する寄与度をoutputしてくれる。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/Library.html">#Library</a>
<span class="issue_date">Issue Date: 2022-12-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/497">BetterTransformer, Out of the Box Performance for Hugging Face Transformers</a>
<span class="snippet"><span>Comment</span>たった1ライン追加するだけで、Transformerのinferenceが最大で4.5倍高速化されるBetterTransformerの解説記事

<br>



<br>

better_model = BetterTransformer.transform(model)</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2022-09-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/485">Transformerの最前線 〜 畳込みニューラルネットワークの先へ 〜, 牛久先生, 2022</a>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/ACL.html">#ACL</a>
<span class="issue_date">Issue Date: 2021-06-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/384">FastSeq: Make Sequence Generation Faster, Yan+, ACL’21</a>
<span class="snippet"><span>Comment</span>BART, DistilBART, T5, GPT2等のさまざまなTransformer-basedな手法で、4-9倍Inference speedを向上させる手法を提案。</span>
<button onclick="hideContent(0)" style="display: none;">hide</button>
</div>


    </div>

</article>
<div class="post-nav">
<a class="previous" href="/paper_notes/articles/TransferLearning.html" title="TransferLearningに関する論文・技術記事メモの一覧">TransferLearningに関する論文・技術記事メモの一覧</a><a class="next" href="/paper_notes/articles/Trustfulness.html" title="Trustfulnessに関する論文・技術記事メモの一覧">Trustfulnessに関する論文・技術記事メモの一覧</a>
</div>
<div class="post-related">
      <div>Related Articles</div>
      <ul>
        <li class="">
          <a class="post-link" href="/paper_notes/articles/InstructionTuning.html" title="InstructionTuningに関する論文・技術記事メモの一覧">
            InstructionTuningに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/NLP.html" title="NLPに関する論文・技術記事メモの一覧">
            NLPに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/Sequrity.html" title="Sequrityに関する論文・技術記事メモの一覧">
            Sequrityに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/Ambiguity.html" title="Ambiguityに関する論文・技術記事メモの一覧">
            Ambiguityに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
</ul>
    </div>
<div class="post-comments"></div></section>
</div>


  </section>
  <section class="sidebar" style="margin-left: 15px;">
    <!-- Get sidebar items --><style type="text/css" media="screen">
.post-menu ul {
  list-style: none;
  padding: 0;
  margin: 0;
}
</style>

<div class="post-menu">
  <div class="post-menu-title">TOC</div>
  <div class="post-menu-content"></div>
</div>

<script>
  function generateContent() {
    var menu = document.querySelector(".post-menu");
    var menuContent =  menu.querySelector(".post-menu-content");
    var headings = document.querySelector(".post-content").querySelectorAll("h2, h3, h4, h5, h6");

    // Hide menu when no headings
    if (headings.length === 0) {
      return menu.style.display = "none";
    }

    // Generate post menu
    var menuHTML = '';
    for (var i = 0; i < headings.length; i++) {
      var h = headings[i];
      menuHTML += (
        '<li class="h-' + h.tagName.toLowerCase() + '">'
        + '<a href="#h-' + h.getAttribute('id') + '">' + h.textContent + '</a></li>');
    }

    menuContent.innerHTML = '<ul>' + menuHTML + '</ul>';

    // The header element
    var header = document.querySelector('header.site-header');

    function doMenuCollapse(index, over_items) {
      var items = menuContent.firstChild.children;

      if (over_items == undefined) {
        over_items = 20;
      }

      if (items.length < over_items) {
        return;
      }

      var activeItem = items[index];
      var beginItem = activeItem
      var endItem = activeItem
      var beginIndex = index;
      var endIndex = index + 1;
      while (beginIndex >= 0
        && !items[beginIndex].classList.contains('h-h2')) {
        beginIndex -= 1;
      }
      while (endIndex < items.length
        && !items[endIndex].classList.contains('h-h2')) {
        endIndex += 1;
      }
      for (var i = 0; i < beginIndex; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
      for (var i = beginIndex + 1; i < endIndex; i++) {
        item = items[i]
        // if (!item.classList.contains('h-h2')) {
          item.style.display = '';
        // }
      }
      for (var i = endIndex; i < items.length; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
    }

    // Init menu collapsed
    doMenuCollapse(-1);

    // Active the menu item
    window.addEventListener('scroll', function (event) {
      var lastActive = menuContent.querySelector('.active');
      var changed = true;
      var activeIndex = -1;
      for (var i = headings.length - 1; i >= 0; i--) {
        var h = headings[i];
        var headingRect = h.getBoundingClientRect();
        var headerRect = header.getBoundingClientRect();
        var headerTop = Math.floor(headerRect.top);
        var headerHeight = Math.floor(headerRect.height);
        var headerHeight = headerTop + headerHeight + 20;
        if (headingRect.top <= headerHeight) {
          var id = 'h-' + h.getAttribute('id');
          var a = menuContent.querySelector('a[href="#' + id  + '"]');
          var curActive = a.parentNode;
          if (curActive) {
            curActive.classList.add('active');
            activeIndex = i;
          }
          if (lastActive == curActive) {
            changed = false;
          }
          break;
        }
      }
      if (changed) {
        if (lastActive) {
          lastActive.classList.remove('active');
        }
        doMenuCollapse(activeIndex);
      }
      event.preventDefault();
    });
  }
  generateContent();
</script>
</section>
</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/paper_notes/"></data>

  <div class="wrapper">
    <div class="site-footer-inner">
<div>Copyright © 2023-current AkihikoWATANABE. The header images and any thumbnail images for the posts were generated by ChatGPT's DALL-E3.</div>
      <div>Powered by <a title="Jekyll is a simple, blog-aware, static site
      generator." href="https://jekyllrb.com/">Jekyll</a> &amp; <a title="Yat, yet
      another theme." href="https://github.com/jeffreytse/jekyll-theme-yat">Yat Theme</a>.</div>
      <div class="footer-col rss-subscribe">Subscribe <a href="/paper_notes/feed.xml">via RSS</a>
</div>
    </div>
  </div>
</footer>
</body>
</html>
