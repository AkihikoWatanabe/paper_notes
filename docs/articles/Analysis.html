<!DOCTYPE html>
<html lang="ja">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-translate-customization" content="108d9124921d80c3-80e20d618ff053c8-g4f02ec6f3dba68b7-c">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Analysisã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§ | ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ</title>
<meta name="generator" content="Jekyll v3.10.0">
<meta property="og:title" content="Analysisã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
<meta name="author" content="AkihikoWATANABE">
<meta property="og:locale" content="ja">
<meta name="description" content="Analysis #NeuralNetwork #ComputerVision #Supervised #RepresentationLearning #Self-SupervisedLearning #CLIP #One-Line Notes">
<meta property="og:description" content="Analysis #NeuralNetwork #ComputerVision #Supervised #RepresentationLearning #Self-SupervisedLearning #CLIP #One-Line Notes">
<link rel="canonical" href="http://akihikowatanabe.github.io/paper_notes/articles/Analysis.html">
<meta property="og:url" content="http://akihikowatanabe.github.io/paper_notes/articles/Analysis.html">
<meta property="og:site_name" content="ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-11-10T00:51:42+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Analysisã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"AkihikoWATANABE"},"dateModified":"2025-11-10T00:51:42+00:00","datePublished":"2025-11-10T00:51:42+00:00","description":"Analysis #NeuralNetwork #ComputerVision #Supervised #RepresentationLearning #Self-SupervisedLearning #CLIP #One-Line Notes","headline":"Analysisã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§","mainEntityOfPage":{"@type":"WebPage","@id":"http://akihikowatanabe.github.io/paper_notes/articles/Analysis.html"},"url":"http://akihikowatanabe.github.io/paper_notes/articles/Analysis.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="icon" href="">
  <link rel="canonical" href="http://akihikowatanabe.github.io">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/main.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/custom_button.css">
  <script src="/paper_notes/assets/js/main.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script><link type="application/atom+xml" rel="alternate" href="http://akihikowatanabe.github.io/paper_notes/feed.xml" title="ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ">
<script>
  function showMore(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "block";
          
      // ã“ã®ãƒœã‚¿ãƒ³ã®å‚ç…§ã‚’å–å¾—ã—ã¦éè¡¨ç¤ºã«ã—ã¾ã™
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "none";
      }
          
      // hideãƒœã‚¿ãƒ³ã®å‚ç…§ã‚’å–å¾—ã—ã¦è¡¨ç¤ºã—ã¾ã™
      const hideButton = contentDiv.querySelector('button[onclick^="hideContent"]');
      if (hideButton) {
        hideButton.style.display = "block";
      }
    }
  }

  function hideContent(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "none";
  
      // moreãƒœã‚¿ãƒ³ã®å‚ç…§ã‚’å–å¾—ã—ã¦è¡¨ç¤ºã—ã¾ã™
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "block";
      }
  
      // ã“ã®ãƒœã‚¿ãƒ³ã‚’éš ã—ã¾ã™
      const hideButtons = document.querySelectorAll('button[onclick^="hideContent"]');
      const hideButton = hideButtons[index];
      if (hideButton) {
        hideButton.style.display = "none";
      }
    }
  }
</script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script charset="UTF-8" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/go.min.js" async></script>



















<script>
// Init highlight js
document.addEventListener('DOMContentLoaded', function(event) {
  var els = document.querySelectorAll('pre code')

  function addLangData(block) {
    var outer = block.parentElement.parentElement.parentElement;
    var lang = block.getAttribute('data-lang');
    for (var i = 0; i < outer.classList.length; i++) {
      var cls = outer.classList[i];
      if (cls.startsWith('language-')) {
        lang = cls;
        break;
      }
    }
    if (!lang) {
      cls = block.getAttribute('class');
      lang = cls ? cls.replace('hljs ', '') : '';
    }
    if (lang.startsWith('language-')) {
      lang = lang.substr(9);
    }
    block.setAttribute('class', 'hljs ' + lang);
    block.parentNode.setAttribute('data-lang', lang);
  }

  function addBadge(block) {
    var enabled = ('true' || 'true').toLowerCase();
    if (enabled == 'true') {
      var pre = block.parentElement;
      pre.classList.add('badge');
    }
  }

  function handle(block) {
    addLangData(block);
    addBadge(block)
    hljs.highlightBlock(block);
  }

  for (var i = 0; i < els.length; i++) {
    var el = els[i];
    handle(el);
  }
});
</script>

<style>
  /* code language badge */
  pre.badge::before {
    content: attr(data-lang);
    color: #fff;
    background-color: #ff4e00;
    padding: 0 .5em;
    border-radius: 0 2px;
    text-transform: uppercase;
    text-align: center;
    min-width: 32px;
    display: inline-block;
    position: absolute;
    right: 0;
  }

  /* fix wrong badge display for firefox browser */
  code > table pre::before {
    display: none;
  }
</style>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe-lightbox.umd.min.js" async></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe.umd.min.js" async></script>
<link href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/photoswipe.min.css" rel="stylesheet">
<style>
  .pswp .pswp__container .pswp__img {
    background-color: white;
  }
</style>

<script>
  function initPhotoSwipe() {
    let customOptions = {};

    try {
      const data = `{"gallery"=>"section.main", "children"=>"a.photo-swipe", "bgOpacity"=>0.8, "padding"=>{"top"=>20, "bottom"=>40, "left"=>100, "right"=>100}}`.replaceAll("=>", ":");
      customOptions = JSON.parse(data);
    } catch (e) {
      console.info("Invalid custom photo previewer options! " + e.message);
    }

    // Define object and gallery options
    const options = Object.assign(
      {
        gallery: "section.main",
        children: "a.photo-swipe",
        photo_scale: 2,
        // dynamic import is not supported in UMD version
        pswpModule: PhotoSwipe,
      },
      customOptions
    );

    const galleryEl = document.querySelector(options.gallery);
    if (!galleryEl) {
      return;
    }

    const imgEls = [];
    const els = galleryEl.querySelectorAll("img:not(.emoji)");
    els.forEach((el) => {
      if (el.src.trim() == "") {
        return;
      }
      if (!imgEls.includes(el)) {
        imgEls.push(el);
      }
    });

    if (imgEls.length === 0) {
      return;
    }

    imgEls.forEach((imgEl) => {
      imgEl.outerHTML = `
      <a class="photo-swipe"
        href="${imgEl.src}"
        data-pswp-width="${
          Math.max(imgEl.naturalWidth, imgEl.width) * options.photo_scale
        }"
        data-pswp-height="${
          Math.max(imgEl.naturalHeight, imgEl.height) * options.photo_scale
        }"
        data-pswp-caption="${imgEl.getAttribute("caption") || imgEl.alt}"
        target="_blank">
        ${imgEl.outerHTML}
      </a>`;
    });

    // Initialize PhotoSwipe 5
    var lightbox = new PhotoSwipeLightbox(options);

    lightbox.init();
  }

  window.addEventListener("load", initPhotoSwipe);
</script>
<meta name="google-site-verification" content="u_DTTPcCZ806iq51zgirHyWq3556HUKGq8AQfH91iFI">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-P70KSB88WH"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-P70KSB88WH');
  </script>

<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>





































































































































<header class="site-header site-header-transparent" role="banner">

  <div class="wrapper">
    <div class="site-header-inner">
<span class="site-brand"><a class="site-brand-inner" rel="author" href="/paper_notes/">
  <img class="site-favicon" title="ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ" src="" onerror="this.style.display='none'">
  ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ
</a>
</span><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger">
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewbox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
              </svg>
            </span>
          </label>

          <div class="trigger">









<span class="page-link">



<div id="google_translate_element" style="display: none;">
</div>

<span class="ct-language">
  <ul class="list-unstyled ct-language-dropdown">
    
      <li>
        <a href="#" class="lang-select" data-lang="en">
          
          <img src="https://cdn.countryflags.com/thumbs/united-states-of-america/flag-400.png" title="English">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="fr">
          
          <img src="https://cdn.countryflags.com/thumbs/france/flag-400.png" title="French">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="zh-CN">
          
          <img src="https://cdn.countryflags.com/thumbs/china/flag-400.png" title="Chinese(Simple)">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ja">
          
          <img src="https://cdn.countryflags.com/thumbs/japan/flag-400.png" title="Japanese">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ko">
          
          <img src="https://cdn.countryflags.com/thumbs/south-korea/flag-400.png" title="Korean">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ru">
          
          <img src="https://cdn.countryflags.com/thumbs/russia/flag-400.png" title="Russian">
          
        </a>
      </li>
    
  </ul>
</span>

<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({
    pageLanguage: 'ja',
    autoDisplay: false,
    layout: google.translate.TranslateElement.InlineLayout.VERTICAL
  }, 'google_translate_element');

  // Links to cross-origin destinations are unsafe
  var gll = document.getElementsByClassName('goog-logo-link')[0];
  if (gll) {
    gll.setAttribute('rel', 'noopener');
  }

  function restoreLang() {
    var iframe = document.getElementsByClassName('goog-te-banner-frame')[0];
    if (!iframe) return;

    var innerDoc = iframe.contentDocument || iframe.contentWindow.document;
    var restore_el = innerDoc.getElementsByTagName("button");

    for (var i = 0; i < restore_el.length; i++) {
      if (restore_el[i].id.indexOf("restore") >= 0) {
        restore_el[i].click();
        var close_el = innerDoc.getElementsByClassName("goog-close-link");
        close_el[0].click();
        return;
      }
    }
  }

  function triggerHtmlEvent(element, eventName) {
    var event;
    if (document.createEvent) {
      event = document.createEvent('HTMLEvents');
      event.initEvent(eventName, true, true);
      element.dispatchEvent(event);
    } else {
      event = document.createEventObject();
      event.eventType = eventName;
      element.fireEvent('on' + event.eventType, event);
    }
  }

  var googleCombo = document.querySelector("select.goog-te-combo");
  var langSelect = document.querySelector('.ct-language');
  langSelect.addEventListener('click', function(event) {
    if (!event.target) {
      return;
    }

    var selected = document.querySelector('.ct-language .ct-language-selected');
    if (selected) {
      selected.classList.remove('ct-language-selected');
    }

    var target = event.target;
    while (target && target !== langSelect ) {
      if (target.matches('.lang-select')) {
        break;
      }
      target = target.parentElement;
    }

    if (target && target.matches('.lang-select')) {
      var lang = target.getAttribute('data-lang');
      if (googleCombo.value == lang) {
        restoreLang();
      } else {
        target.parentElement.classList.add('ct-language-selected');
        googleCombo.value = lang;
        triggerHtmlEvent(googleCombo, 'change');
      }
    }

    event.preventDefault();
  });
}
</script>

<script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit" async></script>
</span>
</div>
        </nav>
</div>
  </div>
</header>

<script>
  function initHeader() {
    var lastScrollY = getScrollPos().y;
    var documentElement = document.documentElement;

    function storeScrollData() {
      var y = getScrollPos().y;documentElement.setAttribute("data-header-transparent", "");var scrollStatus = "";

      if (y <= 0) {
        scrollStatus = "top";
      } else if ((window.innerHeight + y) >= document.body.offsetHeight) {
        scrollStatus = "bottom";
      } else {
        var isScrollDown = (y - lastScrollY > 0) ? true : false;
        scrollStatus = isScrollDown ? "down" : "up";
      }

      lastScrollY = y;
      documentElement.setAttribute("data-scroll-status", scrollStatus);
    }

    window.addEventListener('scroll', function(e) {
      storeScrollData();
    });

    storeScrollData();
  }
  document.addEventListener('DOMContentLoaded', initHeader);
</script>


























































































































































<style>
    html .page-banner .page-banner-img > *:first-child {
      opacity: 0.4;
    }

    html[data-theme="dark"] .page-banner .page-banner-img > *:first-child {
      opacity: 0.2872;
    }
  </style>
<section class="page-banner">
    <div class="page-banner-img">
<div style="background-image: url(/paper_notes/assets/images/banner.png)"></div>
        <img class="img-placeholder" src="/paper_notes/assets/images/banner.png">
</div>
    <div class="wrapper">
      <div class="page-banner-inner">
<header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">ã‚ãŸã—ã®ã¹ã‚“ãã‚‡ã†ãƒãƒ¼ãƒˆ</h1>
  <h2 class="post-subtitle">å‹‰å¼·ã—ãŸè«–æ–‡ã‚„æŠ€è¡“ç­‰ã®æƒ…å ±ã‚’Githubã®Issueã«ãƒ¡ãƒ¢ã£ã¦ã„ã‚‹ã²ã¨ã®ãƒ–ãƒ­ã‚°ã€‚
ãã‚Œãªã‚Šã«ãƒ¡ãƒ¢ã®é‡ãŒè“„ç©ã•ã‚Œã¦ããŸã®ã§ã€ä¸€åº¦æ•´ç†ã—ãŸã„ãªã¨æ€ã„ãƒ–ãƒ­ã‚°ã¯ã˜ã‚ã¦ã¿ã¾ã—ãŸï¼
è‡ªç„¶è¨€èªå‡¦ç†(NLP), æ¨è–¦ã‚·ã‚¹ãƒ†ãƒ (RecommenderSystem), Educational Data Mining (EDM), Learning Analytics (LA)ãªã©ã®åˆ†é‡ã®ãƒ¡ãƒ¢ãŒå¤šã„ã¨æ€ã„ã¾ã™ã€‚
æœ€è¿‘ã¯ç‰¹ã«LLMã®å‹‰å¼·ãŒå¤šã‚ã§ã™ :)</h2>

  <div class="post-meta">
    <time class="dt-published" datetime="2025-11-10T00:51:42+00:00" itemprop="datePublished"><i class="fa fa-calendar"></i> Nov 10, 2025
    </time><span class="post-author left-vsplit"><i class="fa fa-pencil"></i> AkihikoWATANABE</span>
    
































    <span class="post-reading-time left-vsplit"><i class="fa fa-clock-o"></i> About 6 hours 4 mins</span>
  </div></header>
</div>
    </div>
  </section><script>
  function hashLocate(hashValue) {
    hashValue = hashValue.replace(/^.*#h-/, '');
    hashValue = decodeURIComponent(hashValue);
    var element = document.getElementById(hashValue);

    if (!element) {
      return;
    }

    var header = document.querySelector('header.site-header');
    var headerRect = header.getBoundingClientRect();
    var headerTop = Math.floor(headerRect.top);
    var headerHeight = Math.floor(headerRect.height);
    var scrollPos = getScrollPos();
    var offsetY = element.offsetTop - (headerTop + headerHeight + 20);

    if (offsetY == scrollPos.y) {
      return;
    }

    if (headerTop == 0  && offsetY > scrollPos.y) {
      offsetY += headerHeight + 2;
    } else if (headerTop < 0  && offsetY < scrollPos.y) {
      offsetY -= headerHeight - 2;
    }

    smoothScrollTo(offsetY);
  }

  // The first event occurred
  window.addEventListener('load', function(event) {
    if (window.location.hash) {
      hashLocate(window.location.hash);
    }
  });

  // The first event occurred
  window.addEventListener('click', function(event) {
    if (event.target.tagName.toLowerCase() == 'a') {
      hashLocate(event.target.getAttribute('href'));
    }
  });
</script>
<div class="theme-toggle">
  <input type="checkbox" id="theme-switch">
  <label for="theme-switch">
    <div class="toggle"></div>
    <div class="names">
      <p class="light">Light</p>
      <p class="dark">Dark</p>
    </div>
  </label>
</div>




<script>
  (function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var nightModeOption = ('off' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
      storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
      var data = storage.getItem('theme');
      try {
        data = JSON.parse(data ? data : '');
      } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
      }
      return data;
    }

    function handleThemeToggle(nightShift) {
      themeData.nightShift = nightShift;
      saveThemeData(themeData);
      html.dataset.theme = nightShift ? 'dark' : 'light';
      setTimeout(function() {
        sw.checked = nightShift ? true : false;
      }, 50);
    }

    function autoThemeToggle() {
      // Next time point of theme toggle
      var now = new Date();
      var toggleAt = new Date();
      var hours = now.getHours();
      var nightShift = hours >= 19 || hours <=7;

      if (nightShift) {
        if (hours > 7) {
          toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
      } else {
        toggleAt.setHours(19);
      }

      toggleAt.setMinutes(0);
      toggleAt.setSeconds(0);
      toggleAt.setMilliseconds(0)

      var delay = toggleAt.getTime() - now.getTime();

      // auto toggle theme mode
      setTimeout(function() {
        handleThemeToggle(!nightShift);
      }, delay);

      return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
      };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
      handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
      var data = autoThemeToggle();

      // Toggle theme by local setting
      if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
      } else {
        handleThemeToggle(themeData.nightShift);
      }
    } else if (nightModeOption == 'manual') {
      handleThemeToggle(themeData.nightShift);
    } else {
      var nightShift = themeData.nightShift;
      if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
      }
      handleThemeToggle(nightShift);
    }
  })();
</script>
<div id="click-to-top" class="click-to-top">
  <i class="fa fa-arrow-up"></i>
</div>
<script>
  (function () {
    const clickToTop = document.getElementById('click-to-top');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 100) {
        clickToTop.classList.add('show')
      }else {
        clickToTop.classList.remove('show')
      }
    });
    clickToTop.addEventListener('click', () => {
      window.smoothScrollTo(0);
    });
  })();
</script>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="framework">
  <section class="main">

     <div class="post">
  <section>









<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="post-content e-content" itemprop="articleBody">

      <h2 id="Analysis"> Analysis</h2>
<div class="visible-content">
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Supervised.html" target="_blank" rel="noopener noreferrer">#Supervised</a>
<a class="button" href="articles/RepresentationLearning.html" target="_blank" rel="noopener noreferrer">#RepresentationLearning</a>
<a class="button" href="articles/Self-SupervisedLearning.html" target="_blank" rel="noopener noreferrer">#Self-SupervisedLearning</a>
<a class="button" href="articles/CLIP.html" target="_blank" rel="noopener noreferrer">#CLIP</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>


<br>


<span class="issue_date">Issue Date: 2025-10-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3524" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Notes] Investigating fine- and coarse-grained structural correspondences between deep neural networks and human object image similarity judgments using unsupervised alignment, Takahashi+, Neural Networks'26, 2026.03</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/oizumim/status/1983800844933066931?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>CLIP, è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’, æ•™å¸«ã‚ã‚Šå­¦ç¿’ã‚’æ¯”è¼ƒã—ãŸã¨ãã«ã€CLIPãŒäººé–“ãŒç²å¾—ã™ã‚‹objectã®representationã¨ã‚‚ã£ã¨ã‚‚alignã—ã¦ã„ã‚‹ä¸€æ–¹ã§ã€è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã¯ã»ã¨ã‚“ã©å¶ç„¶ãƒ¬ãƒ™ãƒ«ã§ã—ã‹alignã—ãªã„ï¼ˆãŸã ã—ã€ç²—ã„ãƒ¬ãƒ™ãƒ«ã§è¦‹ã‚‹ã¨äººé–“ã§è¨€ã†ã¨ã“ã‚ã®ã‚«ãƒ†ã‚´ãƒªãƒ¬ãƒ™ãƒ«ã®ã‚¯ãƒ©ã‚¹ã‚¿ã‚’å½¢æˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹ï¼‰ã€‚ã“ã®ãŸã‚ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã§ã®å­¦ç¿’ãŒäººé–“ãŒç²å¾—ã™ã‚‹è¡¨ç¾ã¨fine-grainedãªãƒ¬ãƒ™ãƒ«ã§alignã™ã‚‹ãŸã‚ã«éå¸¸ã«é‡è¦ã§ã‚ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚Œã‚‹ã€ã¨ã„ã†æ„Ÿã˜ã‚‰ã—ã„</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/memory.html" target="_blank" rel="noopener noreferrer">#memory</a>
<a class="button" href="articles/Beliefs.html" target="_blank" rel="noopener noreferrer">#Beliefs</a>


<br>


<span class="issue_date">Issue Date: 2025-11-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3601" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Accumulating Context Changes the Beliefs of Language Models, Jiayi Geng+, arXiv'25, 2025.11</a>
<span class="snippet"><span>GPT Summary</span>- è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLMï¼‰ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¯ã€ãƒ–ãƒ¬ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒŸãƒ³ã‚°ã‚„ç ”ç©¶ã§ã®ä½¿ç”¨ãŒå¢—åŠ ã—ã¦ã„ã‚‹ãŒã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®è“„ç©ã«ä¼´ã„ä¿¡å¿µãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ãŒå¤‰åŒ–ã™ã‚‹ãƒªã‚¹ã‚¯ãŒã‚ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€å¯¾è©±ã‚„ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç†ã‚’é€šã˜ã¦ä¿¡å¿µãŒã©ã®ã‚ˆã†ã«å¤‰åŒ–ã™ã‚‹ã‹ã‚’èª¿æŸ»ã—ã€GPT-5ãŒé“å¾³çš„ã‚¸ãƒ¬ãƒ³ãƒã«é–¢ã™ã‚‹è­°è«–å¾Œã«54.7%ã€Grok 4ãŒæ”¿æ²»çš„å•é¡Œã«é–¢ã—ã¦27.2%ã®ä¿¡å¿µå¤‰åŒ–ã‚’ç¤ºã™ã“ã¨ã‚’ç™ºè¦‹ã—ãŸã€‚ã¾ãŸã€ãƒ„ãƒ¼ãƒ«ä½¿ç”¨ã«ã‚ˆã‚‹è¡Œå‹•å¤‰åŒ–ã‚‚åˆ†æã—ã€ä¿¡å¿µã®å¤‰åŒ–ãŒè¡Œå‹•ã«åæ˜ ã•ã‚Œã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é•·æ™‚é–“ã®å¯¾è©±ã‚„èª­æ›¸ãŒä¿¡é ¼æ€§ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://lm-belief-change.github.io/" target="_blank" rel="noopener noreferrer">https://lm-belief-change.github.io/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiayiigeng/status/1986093048179159166?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã‚¨ã‚³ãƒ¼ãƒãƒ£ãƒ³ãƒãƒ¼ãŒå¢—å¼·ã•ã‚Œãã†</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>


<br>


<span class="issue_date">Issue Date: 2025-11-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3569" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond, Chenxiao Yang+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- è‡ªå·±å›å¸°çš„ãªæ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã¨ãƒã‚¹ã‚¯ã•ã‚ŒãŸæ‹¡æ•£ã‚’è¶…ãˆãŸç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã‚’ç ”ç©¶ã—ã€ãã®åˆ©ç‚¹ã¨é™ç•Œã‚’å®šé‡åŒ–ã€‚æ›¸ãæ›ãˆã‚„é•·ã•å¯å¤‰ã®ç·¨é›†ãŒå¯èƒ½ã«ãªã‚‹ã“ã¨ã§ã€ç†è«–çš„ãŠã‚ˆã³å®Ÿè¨¼çš„ãªåˆ©ç‚¹ã‚’ç¤ºã—ã€è‡ªç„¶è¨€èªä»¥å¤–ã®é ˜åŸŸã§ã‚‚æ©Ÿèƒ½ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®é‡è¦æ€§ã‚’å¼·èª¿ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/chenxiao_yang_/status/1985457774969405921?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
</div>
<p><button onclick="showMore(0)">more</button></p>
<div class="hidden-content">
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/SelfCorrection.html" target="_blank" rel="noopener noreferrer">#SelfCorrection</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<span class="issue_date">Issue Date: 2025-11-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3565" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] How Well Can Reasoning Models Identify and Recover from Unhelpful   Thoughts?, Sohee Yang+, EMNLP'25, 2025.06</a>
<span class="snippet"><span>GPT Summary</span>- æ¨è«–ãƒ¢ãƒ‡ãƒ«ã®è‡ªå·±å†è©•ä¾¡èƒ½åŠ›ã‚’èª¿æŸ»ã—ã€å½¹ã«ç«‹ãŸãªã„æ€è€ƒã®4ã¤ã®ã‚¿ã‚¤ãƒ—ã‚’ç‰¹å®šã€‚ãƒ¢ãƒ‡ãƒ«ã¯ç„¡é§„è©±ã‚„ç„¡é–¢ä¿‚ãªæ€è€ƒã‚’åŠ¹æœçš„ã«è­˜åˆ¥ã§ãã‚‹ãŒã€ãã‚Œã‚‰ãŒæ³¨å…¥ã•ã‚Œã‚‹ã¨å›å¾©ã«è‹¦åŠ´ã—ã€æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚ç‰¹ã«ã€å¤§ããªãƒ¢ãƒ‡ãƒ«ã¯çŸ­ã„ç„¡é–¢ä¿‚ãªæ€è€ƒã‹ã‚‰ã®å›å¾©ãŒé›£ã—ã„å‚¾å‘ãŒã‚ã‚Šã€è‡ªå·±å†è©•ä¾¡ã®æ”¹å–„ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚ˆã‚Šè‰¯ã„æ¨è«–ã¨å®‰å…¨ãªã‚·ã‚¹ãƒ†ãƒ ã®é–‹ç™ºãŒä¿ƒé€²ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/megamor2/status/1985321067422871563?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/megamor2/status/1985321067422871563?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/UserBased.html" target="_blank" rel="noopener noreferrer">#UserBased</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-11-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3534" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Completion $\neq$ Collaboration: Scaling Collaborative Effort with  Agents, Shannon Zejiang Shen+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è©•ä¾¡ã‚’ã‚¿ã‚¹ã‚¯å®Œäº†ã‹ã‚‰å”èª¿çš„ãªå•é¡Œè§£æ±ºãƒ—ãƒ­ã‚»ã‚¹ã«ã‚·ãƒ•ãƒˆã™ã‚‹ã“ã¨ã‚’æå”±ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®é–¢ä¸ãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®æœ‰ç”¨æ€§ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’æ‰ãˆã‚‹ã€Œå”èª¿çš„åŠªåŠ›ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’å°å…¥ã€‚ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ã«ã‚ˆã‚Šã€ç¾å®Ÿã®ã‚·ãƒŠãƒªã‚ªã§ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ä½ä¸‹ã‚’ç¤ºã—ã€æŒç¶šçš„ãªã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ç†è§£ã®é‡è¦æ€§ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å˜ã«ä¸€ç™ºã§ã‚¿ã‚¹ã‚¯ã‚’ã“ãªã™ã“ã¨ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã‚‹ãŒã€ãƒ¦ãƒ¼ã‚¶ã‹ã‚‰ã®è¦æ±‚ã¯åå¾©çš„ã§é€²åŒ–ã™ã‚‹ã®ã§æ•°ãƒ©ã‚¦ãƒ³ãƒ‰çµŒã¤ã¨ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã—ã¥ã‚‰ããªã‚‹ã€ã¨ã„ã£ãŸã“ã¨ãŒèµ·ãã¦ã—ã¾ã†çµŒé¨“ãŒã‚ã‚‹ã¨æ€ã†ãŒã€å®Ÿéš›ãã†ã ã¨ã„ã†ã“ã¨ã‚’å®Ÿé¨“çš„ã«ç¤ºã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚ãã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ã¨å”åƒã—ãªãŒã‚‰åŠ¹ç”¨ã‚’æœ€å¤§åŒ–ã•ã›ã‚‹ã‚ˆã†ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¿…è¦ã®ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã—ã¦ã„ã‚‹ã€ã¿ãŸã„ãªè©±ã‚‰ã—ã„ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/CrossLingual.html" target="_blank" rel="noopener noreferrer">#CrossLingual</a>
<a class="button" href="articles/TransferLearning.html" target="_blank" rel="noopener noreferrer">#TransferLearning</a>
<a class="button" href="articles/MultiLingual.html" target="_blank" rel="noopener noreferrer">#MultiLingual</a>
<a class="button" href="articles/Scaling%20Laws.html" target="_blank" rel="noopener noreferrer">#Scaling Laws</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-10-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3525" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining,  Finetuning, and Decoding the Curse of Multilinguality, Shayne Longpre+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€774ã®å¤šè¨€èªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿé¨“ã‚’é€šã˜ã¦ã€æœ€å¤§ã®å¤šè¨€èªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã‚’æ¢æ±‚ã—ã€ATLASã¨ã„ã†é©å¿œçš„è»¢é€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã‚’å°å…¥ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ—¢å­˜ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€å¤šè¨€èªå­¦ç¿’ã®ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚„è¨€èªé–“ã®è»¢é€ç‰¹æ€§ã‚’åˆ†æã€‚è¨€èªãƒšã‚¢é–“ã®ç›¸äº’åˆ©ç›Šã‚¹ã‚³ã‚¢ã‚’æ¸¬å®šã—ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ‡ãƒ¼ã‚¿ã®æœ€é©ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ–¹æ³•ã‚’æ˜ã‚‰ã‹ã«ã—ã€äº‹å‰å­¦ç¿’ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®è¨ˆç®—çš„ã‚¯ãƒ­ã‚¹ã‚ªãƒ¼ãƒãƒ¼ãƒã‚¤ãƒ³ãƒˆã‚’ç‰¹å®šã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è‹±èªä¸­å¿ƒã®AIã‚’è¶…ãˆãŸãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡çš„ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®åŸºç›¤ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/shayneredford/status/1983170949865173069?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p><img src="https://github.com/user-attachments/assets/381375e6-ca49-4bc0-8347-2bc6724cf9a7" alt="image" loading="lazy"><br><br>ãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«ã§å­¦ç¿’ã—ãŸæ™‚ã«ã€æ—¥æœ¬èªã¨ã‚·ãƒŠã‚¸ãƒ¼ã®ã‚ã‚‹è¨€èªã€ã“ã®å›³ã‚’è¦‹ã‚‹ã¨ç„¡ã•ãã†ã«è¦‹ãˆã‚‹ğŸ˜…</p></span><br><br>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-10-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3502" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Language Models are Injective and Hence Invertible, Giorgos Nikolaou+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼è¨€èªãƒ¢ãƒ‡ãƒ«ãŒå˜å°„ã§ã‚ã‚‹ã“ã¨ã‚’æ•°å­¦çš„ã«è¨¼æ˜ã—ã€ç•°ãªã‚‹å…¥åŠ›ãŒåŒã˜å‡ºåŠ›ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œãªã„ã“ã¨ã‚’ç¤ºã™ã€‚ã•ã‚‰ã«ã€6ã¤ã®æœ€å…ˆç«¯ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦è¡çªãƒ†ã‚¹ãƒˆã‚’è¡Œã„ã€è¡çªãŒãªã„ã“ã¨ã‚’ç¢ºèªã€‚æ–°ãŸã«ææ¡ˆã™ã‚‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ SipItã«ã‚ˆã‚Šã€éš ã‚ŒãŸæ´»æ€§åŒ–ã‹ã‚‰æ­£ç¢ºãªå…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’åŠ¹ç‡çš„ã«å†æ§‹ç¯‰ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã€å˜å°„æ€§ãŒè¨€èªãƒ¢ãƒ‡ãƒ«ã®é‡è¦ãªç‰¹æ€§ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/tpimentelms/status/1982857658450489704?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ç¶šå ±:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gladialab/status/1983812121713418606?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1984411703459889390?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬å‚ç…§ã®ã“ã¨ã€‚</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Optimizer.html" target="_blank" rel="noopener noreferrer">#Optimizer</a>
<a class="button" href="articles/ZeroshotHyperparameterTransfer.html" target="_blank" rel="noopener noreferrer">#ZeroshotHyperparameterTransfer</a>
<span class="issue_date">Issue Date: 2025-10-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3478" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Weight Decay may matter more than muP for Learning Rate Transfer in  Practice, Atli Kosson+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- å­¦ç¿’ç‡ã®è»¢é€ã¯ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åŠ¹ç‡çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å¯èƒ½ã«ã™ã‚‹ã€‚Maximal Update Parameterizationï¼ˆmuPï¼‰ã¯ã€å†…éƒ¨è¡¨ç¾ã®æ›´æ–°ã‚’å®‰å®šã•ã›ã‚‹å­¦ç¿’ç‡ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’ææ¡ˆã™ã‚‹ãŒã€ãã®ä»®å®šã¯å®Ÿéš›ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯çŸ­æœŸé–“ã—ã‹ç¶­æŒã•ã‚Œãªã„ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å¾ŒåŠã§ã¯ã€é‡ã¿æ¸›è¡°ãŒå†…éƒ¨è¡¨ç¾ã®å®‰å®šã«å¯„ä¸ã—ã€å­¦ç¿’ç‡ã®è»¢é€ã‚’ä¿ƒé€²ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€muPã¯ä¸»ã«å­¦ç¿’ç‡ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã¨ã—ã¦æ©Ÿèƒ½ã—ã€ä¿®æ­£ã•ã‚ŒãŸã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã§ç½®ãæ›ãˆå¯èƒ½ã§ã‚ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚Œã‚‹ã€‚ã“ã‚Œã‚‰ã®çµæœã¯ã€å­¦ç¿’ç‡ã®è»¢é€ã«é–¢ã™ã‚‹å¾“æ¥ã®è€ƒãˆæ–¹ã«æŒ‘æˆ¦ã—ã€muPã®æˆåŠŸã«ã¯ç‹¬ç«‹ã—ãŸé‡ã¿æ¸›è¡°ãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/seunghyunseo7/status/1983091615280627998?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/Test-Time%20Scaling.html" target="_blank" rel="noopener noreferrer">#Test-Time Scaling</a>
<span class="issue_date">Issue Date: 2025-10-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3466" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] A Theoretical Study on Bridging Internal Probability and   Self-Consistency for LLM Reasoning, Zhi Zhou+, NeurIPS'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ†ã‚¹ãƒˆæ™‚ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ãŠã‘ã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ‰‹æ³•ã®ç†è«–çš„æ çµ„ã¿ã‚’æä¾›ã—ã€è‡ªå·±ä¸€è²«æ€§ã¨å›°æƒ‘åº¦ã®åˆ¶é™ã‚’æ˜ã‚‰ã‹ã«ã€‚æ–°ãŸã«ææ¡ˆã—ãŸRPCæ‰‹æ³•ã¯ã€å›°æƒ‘åº¦ä¸€è²«æ€§ã¨æ¨è«–å‰ªå®šã‚’æ´»ç”¨ã—ã€æ¨è«–èª¤å·®ã®åæŸã‚’æ”¹å–„ã€‚7ã¤ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®å®Ÿè¨¼çµæœã«ã‚ˆã‚Šã€RPCã¯è‡ªå·±ä¸€è²«æ€§ã«åŒ¹æ•µã™ã‚‹æ€§èƒ½ã‚’é”æˆã—ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚³ã‚¹ãƒˆã‚’50%å‰Šæ¸›ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1982448520490868745?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1982448520490868745?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>pj page:


<a href="https://zhouz.dev/RPC/" target="_blank" rel="noopener noreferrer">https://zhouz.dev/RPC/</a>


</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/Zero/Few/ManyShotPrompting.html" target="_blank" rel="noopener noreferrer">#Zero/Few/ManyShotPrompting</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<a class="button" href="articles/ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-10-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3461" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Kaputt: A Large-Scale Dataset for Visual Defect Detection, Sebastian HÃ¶fer+, ICCV'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æ–°ã—ã„å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ææ¡ˆã—ã€å°å£²ç‰©æµã«ãŠã‘ã‚‹æ¬ é™¥æ¤œå‡ºã®èª²é¡Œã«å¯¾å¿œã€‚230,000æšã®ç”»åƒã¨29,000ä»¥ä¸Šã®æ¬ é™¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å«ã¿ã€MVTec-ADã®40å€ã®è¦æ¨¡ã€‚æ—¢å­˜æ‰‹æ³•ã®é™ç•Œã‚’ç¤ºã—ã€56.96%ã®AUROCã‚’è¶…ãˆãªã„çµæœã‚’å¾—ãŸã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ä»Šå¾Œã®ç ”ç©¶ã‚’ä¿ƒé€²ã™ã‚‹ãŸã‚ã«åˆ©ç”¨å¯èƒ½ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gabriberton/status/1979565856897331212?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Memorization.html" target="_blank" rel="noopener noreferrer">#Memorization</a>
<span class="issue_date">Issue Date: 2025-10-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3449" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Hubble: a Model Suite to Advance the Study of LLM Memorization, Johnny Tian-Zheng Wei+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- Hubbleã¯ã€LLMã®è¨˜æ†¶ã«é–¢ã™ã‚‹ç ”ç©¶ã®ãŸã‚ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚¤ãƒ¼ãƒˆã§ã€æ¨™æº–ãƒ¢ãƒ‡ãƒ«ã¨å¤‰åŒ–ãƒ¢ãƒ‡ãƒ«ã®2ç¨®é¡ã‚’æä¾›ã€‚æ¨™æº–ãƒ¢ãƒ‡ãƒ«ã¯å¤§è¦æ¨¡ãªè‹±èªã‚³ãƒ¼ãƒ‘ã‚¹ã§äº‹å‰å­¦ç¿’ã•ã‚Œã€å¤‰åŒ–ãƒ¢ãƒ‡ãƒ«ã¯ç‰¹å®šã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æŒ¿å…¥ã—ã¦è¨˜æ†¶ãƒªã‚¹ã‚¯ã‚’æ¨¡å€£ã€‚8ã¤ã®ãƒ¢ãƒ‡ãƒ«ãŒ1Bã¾ãŸã¯8Bã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¡ã€100Bã¾ãŸã¯500Bã®ãƒˆãƒ¼ã‚¯ãƒ³ã§è¨“ç·´ã€‚ç ”ç©¶ã«ã‚ˆã‚Šã€æ•æ„Ÿãªãƒ‡ãƒ¼ã‚¿ã®è¨˜æ†¶ã¯ã‚³ãƒ¼ãƒ‘ã‚¹ã®ã‚µã‚¤ã‚ºã«ä¾å­˜ã—ã€ãƒ‡ãƒ¼ã‚¿ã®éœ²å‡ºãŒå°‘ãªã„å ´åˆã¯å¿˜ã‚Œã‚‰ã‚Œã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚Hubbleã¯ã€ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆæƒ…å ±ã®è¨˜æ†¶ã®å®¹æ˜“ã•ã‚’åˆ†æã™ã‚‹ãªã©ã€å¹…åºƒã„è¨˜æ†¶ç ”ç©¶ã‚’å¯èƒ½ã«ã—ã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«ã•ã‚‰ãªã‚‹æ¢æ±‚ã‚’ä¿ƒã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://allegro-lab.github.io/hubble/" target="_blank" rel="noopener noreferrer">https://allegro-lab.github.io/hubble/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/johntzwei/status/1981742637670363173?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>HF:


<a href="https://huggingface.co/allegrolab" target="_blank" rel="noopener noreferrer">https://huggingface.co/allegrolab</a>


</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Optimizer.html" target="_blank" rel="noopener noreferrer">#Optimizer</a>
<span class="issue_date">Issue Date: 2025-10-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3443" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Optimization Benchmark for Diffusion Models on Dynamical Systems, Fabian Schaipp, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã‘ã‚‹æœ€é©åŒ–æ‰‹æ³•ã‚’è©•ä¾¡ã—ã€Muonã¨SOAPãŒAdamWã«å¯¾ã—ã¦åŠ¹ç‡çš„ãªä»£æ›¿æ‰‹æ®µã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã€æœ€çµ‚æå¤±ãŒ18%ä½ä¸‹ã™ã‚‹ã“ã¨ã‚’è¦³å¯Ÿã€‚ã•ã‚‰ã«ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚„Adamã¨SGDã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚®ãƒ£ãƒƒãƒ—ãªã©ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã«é–¢é€£ã™ã‚‹ç¾è±¡ã‚’å†è€ƒã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/aaron_defazio/status/1981824032489238950?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3446" target="_blank" rel="noopener noreferrer">[Paper Note] Prodigy: An Expeditiously Adaptive Parameter-Free Learner, Konstantin Mishchenko+, arXiv'23, 2023.06</a>
</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<span class="issue_date">Issue Date: 2025-10-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3437" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Algorithmic Primitives and Compositional Geometry of Reasoning in  Language Models, Samuel Lippl+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ãŒå¤šæ®µéšã®æ¨è«–ã‚’è§£æ±ºã™ã‚‹ãŸã‚ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ çš„åŸå‰‡ã‚’è¿½è·¡ã—ã€æ“ä½œã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã€‚æ¨è«–ã®ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’å†…éƒ¨ã®æ´»æ€§åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ãƒªãƒ³ã‚¯ã•ã›ã€åŸå‰‡ã‚’æ®‹å·®ã‚¹ãƒˆãƒªãƒ¼ãƒ ã«æ³¨å…¥ã™ã‚‹ã“ã¨ã§ã€æ¨è«–ã‚¹ãƒ†ãƒƒãƒ—ã‚„ã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¸ã®å½±éŸ¿ã‚’è©•ä¾¡ã€‚æ—…è¡Œã‚»ãƒ¼ãƒ«ã‚¹ãƒãƒ³å•é¡Œã‚„3SATãªã©ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ç”¨ã„ã¦ã€åŸå‰‡ãƒ™ã‚¯ãƒˆãƒ«ã®å°å‡ºã¨å¹¾ä½•å­¦çš„è«–ç†ã®æ˜ç¤ºåŒ–ã‚’è¡Œã„ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚‹ä¸€èˆ¬åŒ–ã®å¼·èª¿ã‚’ç¤ºã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€LLMsã®æ¨è«–ãŒã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ çš„åŸå‰‡ã®æ§‹æˆçš„å¹¾ä½•å­¦ã«æ”¯ãˆã‚‰ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒç¤ºå”†ã•ã‚Œã€åŸå‰‡ã®è»¢é€ã¨ãƒ‰ãƒ¡ã‚¤ãƒ³é–“ã®ä¸€èˆ¬åŒ–ãŒå¼·åŒ–ã•ã‚Œã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/kazunori_279/status/1981501846784250366?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<span class="issue_date">Issue Date: 2025-10-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3414" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] When Do Transformers Learn Heuristics for Graph Connectivity?, Qilin Ye+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- Transformersã¯ä¸€èˆ¬åŒ–èƒ½åŠ›ã«æ¬ ã‘ã€è„†å¼±ãªãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã«ä¾å­˜ã™ã‚‹ã“ã¨ãŒå¤šã„ã€‚åˆ†é›¢å‹Transformerã‚’ç”¨ã„ã¦ã€$L$å±¤ã®ãƒ¢ãƒ‡ãƒ«ãŒç›´å¾„$3^L$ã¾ã§ã®ã‚°ãƒ©ãƒ•ã‚’è§£æ±ºã§ãã‚‹ã“ã¨ã‚’è¨¼æ˜ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’åˆ†æã—ã€èƒ½åŠ›å†…ã®ã‚°ãƒ©ãƒ•ã§ã¯æ­£ã—ã„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’å­¦ç¿’ã—ã€èƒ½åŠ›ã‚’è¶…ãˆãŸã‚°ãƒ©ãƒ•ã§ã¯å˜ç´”ãªãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’èƒ½åŠ›å†…ã«åˆ¶é™ã™ã‚‹ã“ã¨ã§ã€æ­£ç¢ºãªã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å­¦ç¿’ãŒä¿ƒé€²ã•ã‚Œã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/deqingfu/status/1981170866886148333?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Multi.html" target="_blank" rel="noopener noreferrer">#Multi</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/TheoryOfMind.html" target="_blank" rel="noopener noreferrer">#TheoryOfMind</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Personality.html" target="_blank" rel="noopener noreferrer">#Personality</a>
<span class="issue_date">Issue Date: 2025-10-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3355" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Emergent Coordination in Multi-Agent Language Models, Christoph Riedl, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆLLMã‚·ã‚¹ãƒ†ãƒ ãŒé«˜æ¬¡ã®æ§‹é€ ã‚’æŒã¤ã‹ã©ã†ã‹ã‚’æƒ…å ±ç†è«–çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ã¦æ¤œè¨¼ã€‚å®Ÿé¨“ã§ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®ã‚³ãƒŸãƒ¥ãƒ‹ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãŒãªã„çŠ¶æ³ã§ã€æ™‚é–“çš„ç›¸ä¹—åŠ¹æœãŒè¦³å¯Ÿã•ã‚Œã‚‹ä¸€æ–¹ã€èª¿æ•´ã•ã‚ŒãŸæ•´åˆæ€§ã¯è¦‹ã‚‰ã‚Œãªã‹ã£ãŸã€‚ãƒšãƒ«ã‚½ãƒŠã‚’å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã§ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆé–“ã®å·®åˆ¥åŒ–ã¨ç›®æ¨™æŒ‡å‘ã®ç›¸è£œæ€§ãŒç¤ºã•ã‚Œã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‡ã‚¶ã‚¤ãƒ³ã«ã‚ˆã£ã¦é«˜æ¬¡ã®é›†åˆä½“ã¸ã¨èª˜å°ã§ãã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚çµæœã¯ã€åŠ¹æœçš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«ã¯æ•´åˆæ€§ã¨ç›¸è£œçš„ãªè²¢çŒ®ãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dair_ai/status/1979893847665893851?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ãªè¨­å®šã§ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ã‚ˆã‚‹ã‚·ãƒŠã‚¸ãƒ¼ãŒç”Ÿã˜ã‚‹ã‹å¦ã‹ã€ãã®ãŸã‚ã®æ¡ä»¶ã‚’æ¤œè¨¼ã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã ã¨ã‚·ãƒŠã‚¸ãƒ¼ã¯ç”Ÿã˜ãšã€ãƒšãƒ«ã‚½ãƒŠä»˜ä¸ã¨Theory of Mindã‚’æŒ‡ç¤ºã™ã‚‹ã¨åŠ¹æœãŒå¤§ãã„æ¨¡æ§˜</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2025-10-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3333" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] On the Relationship Between the Choice of Representation and In-Context  Learning, Ioana Marinescu+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ï¼ˆICLï¼‰ã¯ã€LLMãŒãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’å­¦ã¶èƒ½åŠ›ã‚’æŒ‡ã—ã€è¡¨ç¾æ–¹æ³•ã¨å­¦ç¿’èƒ½åŠ›ã®ç›¸äº’ä½œç”¨ãŒé‡è¦ã§ã‚ã‚‹ã€‚ç ”ç©¶ã§ã¯ã€ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®è¡¨ç¾ãŒICLã®åŸºæº–ç²¾åº¦ã‚’æ±ºå®šã—ã€è¿½åŠ ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¯ãã®åŸºæº–ã‚’æ”¹å–„ã™ã‚‹ã“ã¨ã‚’ä»®å®šã€‚ç•°ãªã‚‹ãƒ©ãƒ™ãƒ«ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ICLã‚’å®Ÿæ–½ã—ãŸçµæœã€ãƒ©ãƒ™ãƒ«ã‚»ãƒƒãƒˆã®è³ªã«é–¢ã‚ã‚‰ãšå­¦ç¿’ãŒè¡Œã‚ã‚Œã€åŠ¹ç‡ã¯ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®æ”¹å–„å‚¾ãã«ä¾å­˜ã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰ã®å­¦ç¿’ã¨ãã®è¡¨ç¾ãŒICLã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«ç‹¬ç«‹ã—ãŸå½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/kchonyc/status/1979716767141486740?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/RepresentationLearning.html" target="_blank" rel="noopener noreferrer">#RepresentationLearning</a>
<a class="button" href="articles/SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="articles/Findings.html" target="_blank" rel="noopener noreferrer">#Findings</a>
<span class="issue_date">Issue Date: 2025-10-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3326" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Understanding the Influence of Synthetic Data for Text Embedders, Jacob Mitchell Springer+, ACL'25 Findings, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- åˆæˆLLMç”Ÿæˆãƒ‡ãƒ¼ã‚¿ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚‹æ±ç”¨ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿å™¨ã®é€²å±•ã‚’å—ã‘ã€Wangã‚‰ã®åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’å†ç¾ãƒ»å…¬é–‹ã€‚é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã‚’ã‚‚ãŸã‚‰ã™ãŒã€ä¸€èˆ¬åŒ–ã®æ”¹å–„ã¯å±€æ‰€çš„ã§ã‚ã‚Šã€ç•°ãªã‚‹ã‚¿ã‚¹ã‚¯é–“ã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒå­˜åœ¨ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€åˆæˆãƒ‡ãƒ¼ã‚¿ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã®é™ç•ŒãŒæ˜ã‚‰ã‹ã«ãªã‚Šã€ã‚¿ã‚¹ã‚¯å…¨ä½“ã§ã®å …ç‰¢ãªåŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®æ§‹ç¯‰ã«å¯¾ã™ã‚‹è€ƒãˆã«ç–‘å•ã‚’å‘ˆã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jacspringer/status/1979233837042290775?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>dataset: 


<a href="https://huggingface.co/datasets/jspringer/open-synthetic-embeddings" target="_blank" rel="noopener noreferrer">https://huggingface.co/datasets/jspringer/open-synthetic-embeddings</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Scaling%20Laws.html" target="_blank" rel="noopener noreferrer">#Scaling Laws</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-10-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3282" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] The Art of Scaling Reinforcement Learning Compute for LLMs, Devvrit Khatri+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«é–¢ã™ã‚‹åŸå‰‡çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’å®šç¾©ã—ã€40ä¸‡æ™‚é–“ä»¥ä¸Šã®GPUæ™‚é–“ã‚’ç”¨ã„ãŸå¤§è¦æ¨¡ãªç ”ç©¶ã‚’å®Ÿæ–½ã€‚ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰å‹è¨ˆç®—-æ€§èƒ½æ›²ç·šã‚’ãƒ•ã‚£ãƒƒãƒˆã•ã›ã€è¨­è¨ˆé¸æŠè‚¢ã®å½±éŸ¿ã‚’åˆ†æã€‚çµæœã¨ã—ã¦ã€æ¼¸è¿‘çš„æ€§èƒ½ã¯ãƒ¬ã‚·ãƒ”ã«ã‚ˆã£ã¦ç•°ãªã‚Šã€è¨ˆç®—åŠ¹ç‡ã¯è©³ç´°ã«ä¾å­˜ã™ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã€‚ã“ã‚Œã‚’åŸºã«ã€ScaleRLã¨ã„ã†ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã®ãƒ¬ã‚·ãƒ”ã‚’ææ¡ˆã—ã€100,000 GPUæ™‚é–“ã§ã®æˆåŠŸã‚’ç¤ºã—ãŸã€‚ã“ã®ç ”ç©¶ã¯ã€RLãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®äºˆæ¸¬å¯èƒ½æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®ç§‘å­¦çš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1978956121416307148?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>&gt; ç°¡å˜ã«ãªã£ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é€šéç‡ãŒ0.9ä»¥ä¸Šï¼‰ã¯å†ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãŸã»ã†ãŒæœ€çµ‚æ€§èƒ½ãŒé«˜ã„<br><br>æœ€è¿‘ã¯ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å°å…¥ã—ã¦ã€ç°¡å˜ã™ããšé›£ã—ã™ããªã„å•é¡Œã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦åŠ¹ç‡ä¸Šã’ã‚‹ã€ã¨ã„ã£ãŸã‚ˆã†ãªè©±ãŒã‚ã£ãŸãŒã€ç°¡å˜ã«ãªã£ãŸå•é¡Œã‚’ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ãªã„ã¨æœ€çµ‚æ€§èƒ½ã¨ã—ã¦ã¯ä½ããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã®ã‹â€¦æ„å¤–ã ã£ãŸã€‚<p>CISPO:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3283" target="_blank" rel="noopener noreferrer">[Paper Note] MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning
  Attention, MiniMax+, arXiv'25, 2025.06</a>
</p>
<p>è‘—è€…ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/devvrit_khatri/status/1978864275658871099?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/grad62304977/status/1979920784727429432?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Test-Time%20Scaling.html" target="_blank" rel="noopener noreferrer">#Test-Time Scaling</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/Diversity.html" target="_blank" rel="noopener noreferrer">#Diversity</a>
<span class="issue_date">Issue Date: 2025-10-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3280" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Representation-Based Exploration for Language Models: From Test-Time to  Post-Training, Jens Tuyls+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ãŒè¨€èªãƒ¢ãƒ‡ãƒ«ã®è¡Œå‹•ç™ºè¦‹ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¿æŸ»ã€‚äº‹å‰å­¦ç¿’ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã®éš ã‚ŒçŠ¶æ…‹ã‚’åŸºã«ã—ãŸè¡¨ç¾ãƒ™ãƒ¼ã‚¹ã®ãƒœãƒ¼ãƒŠã‚¹ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€å¤šæ§˜æ€§ã¨pass@kç‡ãŒå¤§å¹…ã«æ”¹å–„ã•ã‚Œã‚‹ã“ã¨ã‚’ç™ºè¦‹ã€‚æ¨è«–æ™‚ã«ãŠã‘ã‚‹æ¢ç´¢ãŒåŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã€ãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦ã‚‚RLãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ã®çµ±åˆã«ã‚ˆã‚Šæ€§èƒ½ãŒå‘ä¸Šã€‚æ„å›³çš„ãªæ¢ç´¢ãŒæ–°ã—ã„è¡Œå‹•ã®ç™ºè¦‹ã«å¯„ä¸ã™ã‚‹å¯èƒ½æ€§ã‚’ç¤ºå”†ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/canondetortugas/status/1978245046366319048?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ¢ç´¢ã®å¤šæ§˜æ€§ã‚’ã‚ã’ã¦RLã“å­¦ç¿’åŠ¹ç‡ã€test time scalingã®åŠ¹ç‡ã‚’ä¸Šã’ã‚‹ã¨ã„ã†è©±</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Quantization.html" target="_blank" rel="noopener noreferrer">#Quantization</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/Test-Time%20Scaling.html" target="_blank" rel="noopener noreferrer">#Test-Time Scaling</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<a class="button" href="articles/MemoryOptimization.html" target="_blank" rel="noopener noreferrer">#MemoryOptimization</a>
<span class="issue_date">Issue Date: 2025-10-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3271" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies  for Reasoning Models, Junhyuck Kim+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- 4ãƒ“ãƒƒãƒˆé‡å­åŒ–ã¯ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã«æœ‰åŠ¹ã§ã™ãŒã€æ¨è«–ãƒ¢ãƒ‡ãƒ«ã«ã¯é©ç”¨ã§ããªã„ã“ã¨ã‚’ç¤ºã™ã€‚ä½“ç³»çš„ãªå®Ÿé¨“ã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¨KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å½±éŸ¿ã‚’ç™ºè¦‹ã€‚å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã¯é‡ã¿ã‚’å„ªå…ˆã—ã€å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã¯ç”Ÿæˆã«ãƒ¡ãƒ¢ãƒªã‚’å‰²ã‚Šå½“ã¦ã‚‹ã“ã¨ã§ç²¾åº¦ã‚’å‘ä¸Šã€‚LLMã®ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã¯ã‚¹ã‚±ãƒ¼ãƒ«ã«ä¾å­˜ã—ã€ç•°ãªã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºå”†ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dimitrispapail/status/1978108550854382052?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Reasoning Modelã«ãŠã„ã¦ã€ãƒ¡ãƒ¢ãƒªã®budgetã«åˆ¶ç´„ãŒã‚ã‚‹çŠ¶æ³ä¸‹ã«ãŠã„ã¦ã€<br>- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º<br>- é‡ã¿ã®ç²¾åº¦<br>- test-time compute (serial &amp; parallel)<br>- KV Cacheã®åœ§ç¸®<br><br>ã«ãŠã„ã¦ã€ãã‚Œã‚‰ã‚’ã©ã®ã‚ˆã†ã«é…åˆ†ã™ã‚‹ã“ã¨ã§ãƒ¢ãƒ‡ãƒ«ã®Acc.ãŒæœ€å¤§åŒ–ã•ã‚Œã‚‹ã‹ï¼Ÿã¨ã„ã†è©±ã—ãªæ¨¡æ§˜ã€‚</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/DataMixture.html" target="_blank" rel="noopener noreferrer">#DataMixture</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-10-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3267" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Learning to See Before Seeing: Demystifying LLM Visual Priors from  Language Pre-training, Junlin Han+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã§è¨“ç·´ã•ã‚ŒãªãŒã‚‰ã‚‚è¦–è¦šçš„å…ˆå…¥è¦³ã‚’ç™ºå±•ã•ã›ã€å°‘é‡ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ãƒ¼ã‚¿ã§è¦–è¦šã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œå¯èƒ½ã«ã™ã‚‹ã€‚è¦–è¦šçš„å…ˆå…¥è¦³ã¯ã€è¨€èªã®äº‹å‰è¨“ç·´ä¸­ã«ç²å¾—ã•ã‚ŒãŸçŸ¥è­˜ã§ã‚ã‚Šã€æ¨è«–ä¸­å¿ƒã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç™ºå±•ã™ã‚‹ã€‚çŸ¥è¦šã®å…ˆå…¥è¦³ã¯åºƒç¯„ãªã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰å¾—ã‚‰ã‚Œã€è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã«æ•æ„Ÿã§ã‚ã‚‹ã€‚è¦–è¦šã‚’æ„è­˜ã—ãŸLLMã®äº‹å‰è¨“ç·´ã®ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ä¸­å¿ƒã®ãƒ¬ã‚·ãƒ”ã‚’ææ¡ˆã—ã€500,000 GPUæ™‚é–“ã‚’ã‹ã‘ãŸå®Ÿé¨“ã«åŸºã¥ãå®Œå…¨ãªMLLMæ§‹ç¯‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ç¤ºã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¦–è¦šçš„å…ˆå…¥è¦³ã‚’è‚²æˆã™ã‚‹æ–°ã—ã„æ–¹æ³•ã‚’æä¾›ã—ã€æ¬¡ä¸–ä»£ã®ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«LLMã®ç™ºå±•ã«å¯„ä¸ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1977982648531476607?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>MLE Bench (Multi-Level Existence Bench)</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Optimizer.html" target="_blank" rel="noopener noreferrer">#Optimizer</a>
<span class="issue_date">Issue Date: 2025-10-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3264" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] The Potential of Second-Order Optimization for LLMs: A Study with Full  Gauss-Newton, Natalie Abreu+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- LLMã®äº‹å‰å­¦ç¿’ã«ãŠã‘ã‚‹è¨ˆç®—åŠ¹ç‡å‘ä¸Šã®ãŸã‚ã€ãƒ•ãƒ«ã‚¬ã‚¦ã‚¹-ãƒ‹ãƒ¥ãƒ¼ãƒˆãƒ³ï¼ˆGNï¼‰å‰å‡¦ç†ã‚’æœ€å¤§150Mãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã«é©ç”¨ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€GNæ›´æ–°ãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®åå¾©å›æ•°ã‚’5.4å€å‰Šæ¸›ã—ã€å±¤é–“æƒ…å ±ã‚’ç„¡è¦–ã—ãŸå±¤åˆ¥GNå‰å‡¦ç†å™¨ãŒãƒ•ãƒ«GNã«è¿‘ã„æ€§èƒ½ã‚’ç¤ºã™ã“ã¨ãŒåˆ¤æ˜ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€GNè¿‘ä¼¼ã®åŠ¹æœã‚„å±¤åˆ¥ãƒ˜ãƒƒã‚»è¡Œåˆ—ã®æƒ…å ±ã®é‡è¦æ€§ã€è¿‘ä¼¼æ‰‹æ³•ã¨ç†æƒ³çš„ãªå±¤åˆ¥ã‚ªãƒ©ã‚¯ãƒ«ã¨ã®æ€§èƒ½ã‚®ãƒ£ãƒƒãƒ—ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1978243717787246643?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-10-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3260" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] How Reinforcement Learning After Next-Token Prediction Facilitates  Learning, Nikolaos Tsilivis+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã‚’å¼·åŒ–å­¦ç¿’ã§æœ€é©åŒ–ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã€‚ç‰¹ã«ã€çŸ­ã„ãŠã‚ˆã³é•·ã„ã€Œæ€è€ƒã®é€£é–ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‹ã‚‰ã®å­¦ç¿’ã‚’é€šã˜ã¦ã€å¼·åŒ–å­¦ç¿’ãŒæ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ã‚’æ”¹å–„ã™ã‚‹ã“ã¨ã‚’ç†è«–çš„ã«ç¤ºã™ã€‚é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒç¨€ãªå ´åˆã€å¼·åŒ–å­¦ç¿’ã«ã‚ˆã‚Šè‡ªå·±å›å¸°å‹ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãŒä¸€èˆ¬åŒ–ã§ãã‚‹ã“ã¨ã‚’ç¢ºèªã€‚ã•ã‚‰ã«ã€é•·ã„å¿œç­”ãŒè¨ˆç®—ã‚’å¢—åŠ ã•ã›ã‚‹ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’èª¬æ˜ã—ã€è‡ªå·±å›å¸°å‹ç·šå½¢ãƒ¢ãƒ‡ãƒ«ãŒåŠ¹ç‡çš„ã«$d$ãƒ“ãƒƒãƒˆã®å¶å¥‡ã‚’äºˆæ¸¬ã§ãã‚‹æ¡ä»¶ã‚’ç†è«–çš„ã«è¨¼æ˜ã€‚Llamaã‚·ãƒªãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã‚ˆã‚‹å®Ÿè¨¼ã‚‚è¡Œã†ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1978015079418245263?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/Entropy.html" target="_blank" rel="noopener noreferrer">#Entropy</a>
<span class="issue_date">Issue Date: 2025-10-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3255" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Demystifying Reinforcement Learning in Agentic Reasoning, Zhaochen Yu+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçš„å¼·åŒ–å­¦ç¿’ï¼ˆagentic RLï¼‰ã‚’ç”¨ã„ã¦ã€LLMsã®æ¨è«–èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã®èª¿æŸ»ã‚’è¡Œã£ãŸã€‚é‡è¦ãªæ´å¯Ÿã¨ã—ã¦ã€åˆæˆè»Œé“ã®å®Ÿéš›ã®ãƒ„ãƒ¼ãƒ«ä½¿ç”¨è»Œé“ã¸ã®ç½®ãæ›ãˆã‚„ã€å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ´»ç”¨ãŒRLã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ã¾ãŸã€æ¢ç´¢ã‚’ä¿ƒé€²ã™ã‚‹æŠ€è¡“ã‚„ã€ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’æ¸›ã‚‰ã™æˆ¦ç•¥ãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŠ¹ç‡ã‚’æ”¹å–„ã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å°å‹ãƒ¢ãƒ‡ãƒ«ã§ã‚‚å¼·åŠ›ãªçµæœã‚’é”æˆã—ã€å®Ÿç”¨çš„ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’æä¾›ã™ã‚‹ã€‚ã•ã‚‰ã«ã€é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ã€å›°é›£ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçš„æ¨è«–èƒ½åŠ›ã®å‘ä¸Šã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/lingyang_pu/status/1977931241916862779?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/1978112328974692692?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/RLVR.html" target="_blank" rel="noopener noreferrer">#RLVR</a>
<span class="issue_date">Issue Date: 2025-10-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3254" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Part II: ROLL Flash -- Accelerating RLVR and Agentic Training with  Asynchrony, Han Lu+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- éåŒæœŸRLå¾Œå‡¦ç†ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã€ŒROLL Flashã€ã‚’ææ¡ˆã€‚ç´°ç²’åº¦ã®ä¸¦åˆ—æ€§ã¨ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆãƒ»ãƒˆãƒ¬ã‚¤ãƒ³ã®ãƒ‡ã‚«ãƒƒãƒ—ãƒªãƒ³ã‚°ã«åŸºã¥ãã€åŠ¹ç‡çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’å®Ÿç¾ã€‚ROLL Flashã¯ãƒªã‚½ãƒ¼ã‚¹åˆ©ç”¨åŠ¹ç‡ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’å¤§å¹…ã«æ”¹å–„ã—ã€RLVRã‚¿ã‚¹ã‚¯ã§æœ€å¤§2.24å€ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¿ã‚¹ã‚¯ã§æœ€å¤§2.72å€ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’é”æˆã€‚éåŒæœŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒåŒæœŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨åŒç­‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã™ã“ã¨ã‚’ç¢ºèªã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1977959866699513889?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>RLã®ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆä¸­ã®GPUã®ã‚¢ã‚¤ãƒ‰ãƒ«ã‚¿ã‚¤ãƒ ã‚’å‰Šæ¸›ã—ã¾ã™ç³»ã®è©±ã‚‚æœ€è¿‘çµæ§‹è¦‹ã‚‹ã‚ˆã†ãª<br>ãŸã¨ãˆã°<br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3134" target="_blank" rel="noopener noreferrer">Anatomy of a Modern Finetuning API, Benjamin Anderson, 2025.10</a>
</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Grokking.html" target="_blank" rel="noopener noreferrer">#Grokking</a>
<a class="button" href="articles/Optimizer.html" target="_blank" rel="noopener noreferrer">#Optimizer</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3196" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Provable Scaling Laws of Feature Emergence from Learning Dynamics of  Grokking, Yuandong Tian, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- grokkingã®ç¾è±¡ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€2å±¤ã®éç·šå½¢ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã‘ã‚‹æ–°ã—ã„æ çµ„ã¿$\mathbf{Li_2}$ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã¯ã€æ€ æƒ°ãªå­¦ç¿’ã€ç‹¬ç«‹ã—ãŸç‰¹å¾´å­¦ç¿’ã€ç›¸äº’ä½œç”¨ã™ã‚‹ç‰¹å¾´å­¦ç¿’ã®3æ®µéšãŒå«ã¾ã‚Œã‚‹ã€‚æ€ æƒ°ãªå­¦ç¿’ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒéš ã‚Œè¡¨ç¾ã«éå‰°é©åˆã—ã€ç‹¬ç«‹ã—ãŸç‰¹å¾´ãŒå­¦ç¿’ã•ã‚Œã‚‹ã€‚å¾ŒåŠæ®µéšã§ã¯ã€éš ã‚Œãƒãƒ¼ãƒ‰ãŒç›¸äº’ä½œç”¨ã‚’å§‹ã‚ã€å­¦ç¿’ã™ã¹ãç‰¹å¾´ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã‚‹ã€‚æœ¬ç ”ç©¶ã¯ã€grokkingã«ãŠã‘ã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½¹å‰²ã‚’æ˜ã‚‰ã‹ã«ã—ã€ç‰¹å¾´ã®å‡ºç¾ã¨ä¸€èˆ¬åŒ–ã«é–¢ã™ã‚‹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã‚’å°å‡ºã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1976494053261967410?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/AttentionSinks.html" target="_blank" rel="noopener noreferrer">#AttentionSinks</a>
<a class="button" href="articles/CompressionValleys.html" target="_blank" rel="noopener noreferrer">#CompressionValleys</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3194" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Attention Sinks and Compression Valleys in LLMs are Two Sides of the  Same Coin, Enrique Queipo-de-Llano+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æ³¨æ„ã®æ²ˆé™ã¨åœ§ç¸®ã®è°·ã®é–¢é€£æ€§ã‚’ç¤ºã—ã€å¤§è¦æ¨¡ãªæ´»æ€§åŒ–ãŒè¡¨ç¾ã®åœ§ç¸®ã¨ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã®æ¸›å°‘ã‚’å¼•ãèµ·ã“ã™ã“ã¨ã‚’ç†è«–çš„ã«è¨¼æ˜ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ãŒä¸­é–“å±¤ã§æ¥µç«¯ãªæ´»æ€§åŒ–ã‚’ç”Ÿã‚€ã¨ã€åœ§ç¸®ã®è°·ã¨æ³¨æ„ã®æ²ˆé™ãŒåŒæ™‚ã«ç¾ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã€‚Transformerãƒ™ãƒ¼ã‚¹ã®LLMãŒãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¸‰ã¤ã®ãƒ•ã‚§ãƒ¼ã‚ºã§å‡¦ç†ã™ã‚‹ã€ŒMix-Compress-Refineã€ç†è«–ã‚’ææ¡ˆã—ã€ã‚¿ã‚¹ã‚¯ä¾å­˜ã®è¡¨ç¾ã®é•ã„ã‚’èª¬æ˜ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1976235853853909048?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Optimizer.html" target="_blank" rel="noopener noreferrer">#Optimizer</a>
<span class="issue_date">Issue Date: 2025-10-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3173" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Muon Outperforms Adam in Tail-End Associative Memory Learning, Shuche Wang+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- Muonã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã¯ã€LLMsã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦Adamã‚ˆã‚Šã‚‚é«˜é€Ÿã§ã‚ã‚Šã€ãã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’é€£æƒ³è¨˜æ†¶ã®è¦³ç‚¹ã‹ã‚‰è§£æ˜ã€‚VOã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚¦ã‚§ã‚¤ãƒˆã¨FFNãŒMuonã®å„ªä½æ€§ã®è¦å› ã§ã‚ã‚Šã€é‡ã„å°¾ã‚’æŒã¤ãƒ‡ãƒ¼ã‚¿ã«ãŠã„ã¦å°¾ã‚¯ãƒ©ã‚¹ã‚’åŠ¹æœçš„ã«æœ€é©åŒ–ã™ã‚‹ã€‚Muonã¯ä¸€è²«ã—ãŸãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸå­¦ç¿’ã‚’å®Ÿç¾ã—ã€Adamã¯ä¸å‡è¡¡ã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€Muonã®æ›´æ–°ãƒ«ãƒ¼ãƒ«ãŒé‡ã„å°¾ã‚’æŒã¤åˆ†å¸ƒã«ãŠã‘ã‚‹åŠ¹æœçš„ãªå­¦ç¿’ã‚’å¯èƒ½ã«ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/fengzhuozhang/status/1975604058896703713?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/COLM.html" target="_blank" rel="noopener noreferrer">#COLM</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-10-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3146" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Echo Chamber: RL Post-training Amplifies Behaviors Learned in   Pretraining, Rosie Zhao+, COLM'25, 2025.04</a>
<span class="snippet"><span>GPT Summary</span>- å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€æ•°å­¦çš„æ¨è«–ã‚„ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ãŸã‚ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½å‘ä¸Šã«å¯„ä¸ã—ã¦ã„ã‚‹ãŒã€ãã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¯æœªè§£æ˜ã§ã‚ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ã€ã•ã¾ã–ã¾ãªã‚¹ã‚±ãƒ¼ãƒ«ã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹RLãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®åŠ¹æœã‚’èª¿æŸ»ã—ã€RLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãŒå‡ºåŠ›åˆ†å¸ƒã«åæŸã—ã€äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å¢—å¹…ã™ã‚‹ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã—ãŸã€‚ã¾ãŸã€ç•°ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ã®ãƒ¢ãƒ‡ãƒ«ãŒç•°ãªã‚‹å‡ºåŠ›åˆ†å¸ƒã«åæŸã™ã‚‹ã“ã¨ã‚„ã€ç°¡å˜ãªè³ªå•ã¸ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒé›£ã—ã„è³ªå•ã®æ€§èƒ½å‘ä¸Šã«å¯„ä¸ã™ã‚‹å¯èƒ½æ€§ã‚’ç¤ºã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€RLã®å½¹å‰²ã«é–¢ã™ã‚‹æ–°ãŸãªæ´å¯ŸãŒå¾—ã‚‰ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosieyzh/status/1975276617078571277?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Slide.html" target="_blank" rel="noopener noreferrer">#Slide</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/reading.html" target="_blank" rel="noopener noreferrer">#reading</a>
<span class="issue_date">Issue Date: 2025-10-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3140" target="_blank" rel="noopener noreferrer" class="title-link">è¨€èªãƒ¢ãƒ‡ãƒ«ã®å†…éƒ¨æ©Ÿåºï¼šè§£æã¨è§£é‡ˆ, HEINZERLING+, NLP'25, 2025.03</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1975322325181686097?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2025-10-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3128" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning, Aayush Mishra+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ï¼ˆICLï¼‰ã®æ´»æ€§åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ©ç”¨ã—ã¦ã€ç›£è¦–ä»˜ããƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆSFTï¼‰ã®å“è³ªã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã€‚ICLã¨SFTã®ç•°ãªã‚‹é©å¿œãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ç¤ºã—ã€ICLæ´»æ€§åŒ–ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆï¼ˆIA2ï¼‰ã¨ã„ã†è‡ªå·±è’¸ç•™æŠ€è¡“ã‚’å°å…¥ã€‚IA2ã‚’SFTã®å‰ã«å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ç²¾åº¦ã¨ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’12ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å®Ÿè¨¼ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«é©å¿œã®å†…éƒ¨ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«å¯¾ã™ã‚‹æ–°ãŸãªè¦–ç‚¹ã‚‚æä¾›ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/danielkhashabi/status/1974119053728919790?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/CurriculumLearning.html" target="_blank" rel="noopener noreferrer">#CurriculumLearning</a>
<a class="button" href="articles/On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<a class="button" href="articles/Batch.html" target="_blank" rel="noopener noreferrer">#Batch</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-10-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3112" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Prompt Curriculum Learning for Efficient LLM Post-Training, Zhaolin Gao+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- Prompt Curriculum Learning (PCL)ã‚’ææ¡ˆã—ã€ä¸­ç¨‹åº¦ã®é›£æ˜“åº¦ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é¸æŠã—ã¦LLMã‚’ãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹è»½é‡ãªå¼·åŒ–å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç´¹ä»‹ã€‚æœ€é©ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé¸æŠã®é‡è¦æ€§ã‚’å®Ÿé¨“ã§ç¢ºèªã—ã€PCLã¯æƒ…å ±è±Šå¯Œãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ã“ã¨ã§é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã€‚ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚’å›é¿ã—ã€MATHãŠã‚ˆã³DeepScaleRã§ãã‚Œãã‚Œ$12.1\times$ãŠã‚ˆã³$16.9\times$ã®é€Ÿåº¦å‘ä¸Šã‚’å®Ÿç¾ã€‚çµæœã¯ã€æ¨è«–ã«ãŠã‘ã‚‹RLã®åŠ¹ç‡ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æ”¹å–„ã™ã‚‹æ–°ãŸãªæ–¹æ³•è«–ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/yzpang_/status/1974180214608703795?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ï¼ˆã–ã£ãã‚Šèª­ã¿ãªã®ã§èª¤ã‚Šã‚’å¤šåˆ†ã«å«ã‚€ã‹ã‚‚ã—ã‚Œãªã„ãŒãƒ¡ãƒ¢ï¼‰å‹¾é…ã®ãƒã‚¤ã‚ºã®ä½æ¸›ã¨ç”Ÿæˆã®é€Ÿåº¦ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æœ€é©ã«ãƒãƒ©ãƒ³ã‚¹ã‚’ã¨ã‚‹ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã€RLã®å­¦ç¿’åŠ¹ç‡ãŒä¸­é–“ç¨‹åº¦ï¼ˆç°¡å˜ã™ããšã€é›£ã—ã™ããªã„ï¼‰ã®é›£æ˜“åº¦ãŒè‰¯ã„ã“ã¨ã‚’ç¤ºã—ãŸã®ã¡ã€Valueãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã«åŸºã¥ã„ã¦æ›´æ–°ã•ã‚Œã‚‹æ¨¡æ§˜ï¼Ÿï¼‰ã‚’ç”¨ã„ã¦promptã‚’é¸æŠã—[^1]ä¸­é–“ç¨‹åº¦ã®promptã‚’ç”¨ã„ã¦ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã‚’ã—å­¦ç¿’ã™ã‚‹ã‚ˆã†ãªã‚ªãƒ³ãƒãƒªã‚·ãƒ¼ã®RLã‚’ææ¡ˆã™ã‚‹ã€ã¿ãŸã„ãªè©±ãªæ¨¡æ§˜ã€‚<br><br>[^1]:æ—¢å­˜æ‰‹æ³•ã®ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆã«ã‚ˆã£ã¦æ±‚ã‚ã‚‹æ–¹æ³•ï¼ˆè¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã™ãã‚‹ï¼‰ã‚„ã€äº‹å‰ã«æ±ºã‚ã¦ãŠã„ãŸè¾æ›¸ãƒ™ãƒ¼ã‚¹ã®æ‰‹æ³•ï¼ˆç¾åœ¨ã®ãƒãƒªã‚·ãƒ¼ã‹ã‚‰ã¿ãŸæ™‚ã®é›£æ˜“åº¦ãŒåæ˜ ã•ã‚Œã¦ãŠã‚‰ãšåŠ¹ç‡ãŒæ‚ªã„ï¼‰ã®åŒæ–¹ã«æ¯”ã¹ã¦ã€é©åº¦ã«ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼ã•ã‚’æ®‹ã—ãŸpromptã®é¸ã³æ–¹ã¨ãªã£ã¦ã„ã‚‹</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Memorization.html" target="_blank" rel="noopener noreferrer">#Memorization</a>
<span class="issue_date">Issue Date: 2025-10-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3108" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] How Diffusion Models Memorize, Juyeop Kim+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¯ç”»åƒç”Ÿæˆã«æˆåŠŸã—ã¦ã„ã‚‹ãŒã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®è¨˜æ†¶ã«ã‚ˆã‚‹ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã‚„è‘—ä½œæ¨©ã®æ‡¸å¿µãŒã‚ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€æ‹¡æ•£ãŠã‚ˆã³ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ãƒ—ãƒ­ã‚»ã‚¹ã‚’å†è€ƒã—ã€è¨˜æ†¶ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æ¢ã‚‹ã€‚è¨˜æ†¶ã¯åˆæœŸã®ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ä¸­ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«ã®éå¤§è©•ä¾¡ã«ã‚ˆã£ã¦å¼•ãèµ·ã“ã•ã‚Œã€å¤šæ§˜æ€§ãŒæ¸›å°‘ã—ã€è¨˜æ†¶ã•ã‚ŒãŸç”»åƒã¸ã®åæŸãŒåŠ é€Ÿã•ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚å…·ä½“çš„ã«ã¯ã€éå­¦ç¿’ã ã‘ã§ãªãã€åˆ†é¡å™¨ãƒ•ãƒªãƒ¼ã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ãŒè¨˜æ†¶ã‚’å¢—å¹…ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æå¤±ãŒå¢—åŠ ã™ã‚‹ã“ã¨ã€è¨˜æ†¶ã•ã‚ŒãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒãƒã‚¤ã‚ºäºˆæ¸¬ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ã€åˆæœŸã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ãŒæŠ‘åˆ¶ã•ã‚Œã‚‹æ§˜å­ãŒæ˜ã‚‰ã‹ã«ãªã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€éå¤§è©•ä¾¡ãŒè¨˜æ†¶ã®ä¸­å¿ƒçš„ãªãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã§ã‚ã‚‹ã“ã¨ãŒç‰¹å®šã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3107" target="_blank" rel="noopener noreferrer">[Paper Note] Selective Underfitting in Diffusion Models, Kiwhan Song+, arXiv'25, 2025.10</a>
</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/Memorization.html" target="_blank" rel="noopener noreferrer">#Memorization</a>
<a class="button" href="articles/Generalization.html" target="_blank" rel="noopener noreferrer">#Generalization</a>
<span class="issue_date">Issue Date: 2025-10-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3107" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Selective Underfitting in Diffusion Models, Kiwhan Song+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã¯ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã®ä¸»è¦ãªãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¨ã—ã¦æ³¨ç›®ã•ã‚Œã¦ã„ã‚‹ãŒã€ã©ã®ã‚¹ã‚³ã‚¢ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã‹ãŒæœªè§£æ±ºã®ç–‘å•ã§ã‚ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€é¸æŠçš„éå°‘é©åˆã®æ¦‚å¿µã‚’å°å…¥ã—ã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ãŒç‰¹å®šã®é ˜åŸŸã§ã‚¹ã‚³ã‚¢ã‚’æ­£ç¢ºã«è¿‘ä¼¼ã—ã€ä»–ã®é ˜åŸŸã§ã¯éå°‘é©åˆã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®ä¸€èˆ¬åŒ–èƒ½åŠ›ã¨ç”Ÿæˆæ€§èƒ½ã«é–¢ã™ã‚‹æ–°ãŸãªæ´å¯Ÿã‚’æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/kwangmoo_yi/status/1974181756636180650?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/mi141/status/1975125005232164916?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è‘—è€…ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jaeyeon_kim_0/status/1975978229447229801?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/DataMixture.html" target="_blank" rel="noopener noreferrer">#DataMixture</a>
<span class="issue_date">Issue Date: 2025-10-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3101" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Data Mixing Can Induce Phase Transitions in Knowledge Acquisition, Xinran Gu+, arXiv'25, 2025.05</a>
<span class="snippet"><span>GPT Summary</span>- LLMsã®è¨“ç·´ã«ãŠã„ã¦ã€çŸ¥è­˜ãŒè±Šå¯Œãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ã‚¦ã‚§ãƒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®æ··åˆãŒã€çŸ¥è­˜ç²å¾—ã«ãŠã„ã¦ä½ç›¸è»¢ç§»ã‚’ç¤ºã™ã“ã¨ã‚’å®Ÿè¨¼ã€‚ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’è‡¨ç•Œå€¤ã¾ã§å¢—åŠ ã•ã›ã‚‹ã¨ã€è¨˜æ†¶çŠ¶æ…‹ãŒæ€¥æ¿€ã«å¤‰åŒ–ã—ã€æ··åˆæ¯”ç‡ãŒè‡¨ç•Œå€¤ã‚’è¶…ãˆã‚‹ã¨æ€¥é€Ÿã«è¨˜æ†¶ãŒå¢—åŠ ã€‚ã“ã‚Œã‚‰ã®ç¾è±¡ã¯å®¹é‡é…åˆ†ã«èµ·å› ã—ã€æœ€é©ãªãƒ‡ãƒ¼ã‚¿é…åˆ†ãŒãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚„æ··åˆæ¯”ç‡ã«ã‚ˆã£ã¦ä¸é€£ç¶šã«å¤‰ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚</span>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="articles/DataMixture.html" target="_blank" rel="noopener noreferrer">#DataMixture</a>
<span class="issue_date">Issue Date: 2025-10-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3088" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of  Scaling Laws, Benefits, and Pitfalls, Feiyang Kang+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- åˆæˆãƒ‡ãƒ¼ã‚¿æŠ€è¡“ã¯LLMã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ä¾›çµ¦åˆ¶é™ã‚’å…‹æœã™ã‚‹å¯èƒ½æ€§ã‚’æŒã¤ã€‚æœ¬ç ”ç©¶ã§ã¯ã€è‡ªç„¶ãªã‚¦ã‚§ãƒ–ãƒ‡ãƒ¼ã‚¿ã¨åˆæˆãƒ‡ãƒ¼ã‚¿ã®æ··åˆã‚’æ¯”è¼ƒã—ã€è¨€ã„æ›ãˆãŸåˆæˆãƒ‡ãƒ¼ã‚¿ã®ã¿ã§ã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯è‡ªç„¶ãªãƒ‡ãƒ¼ã‚¿ã‚ˆã‚Šã‚‚é€Ÿããªã„ã“ã¨ã‚’ç¤ºã—ãŸã€‚1/3ã®è¨€ã„æ›ãˆãŸåˆæˆãƒ‡ãƒ¼ã‚¿ã¨2/3ã®è‡ªç„¶ãƒ‡ãƒ¼ã‚¿ã®æ··åˆãŒã€ã‚ˆã‚ŠåŠ¹ç‡çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å¯èƒ½ã«ã™ã‚‹ã“ã¨ãŒåˆ†ã‹ã£ãŸã€‚æ•™ç§‘æ›¸ã‚¹ã‚¿ã‚¤ãƒ«ã®åˆæˆãƒ‡ãƒ¼ã‚¿ã¯å°ã•ãªãƒ‡ãƒ¼ã‚¿äºˆç®—ã§é«˜ã„æå¤±ã‚’ã‚‚ãŸã‚‰ã—ã€åˆæˆãƒ‡ãƒ¼ã‚¿ã®æœ€é©ãªæ¯”ç‡ã¯ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ‡ãƒ¼ã‚¿äºˆç®—ã«ä¾å­˜ã™ã‚‹ã€‚çµæœã¯åˆæˆãƒ‡ãƒ¼ã‚¿ã®åŠ¹æœã‚’æ˜ã‚‰ã‹ã«ã—ã€å®Ÿç”¨çš„ãªã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚’æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/papers_anon/status/1973939270747668698?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1974108247003934902?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>åˆæˆãƒ‡ãƒ¼ã‚¿ã¯é©åˆ‡ãªè¦æ¨¡ã®ãƒ¢ãƒ‡ãƒ«ã¨æ¯”ç‡ã§ãªã„ã¨åˆ©ç‚¹ãŒç¾ã‚Œãªã„</p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3101" target="_blank" rel="noopener noreferrer">[Paper Note] Data Mixing Can Induce Phase Transitions in Knowledge Acquisition, Xinran Gu+, arXiv'25, 2025.05</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-10-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3078" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning, Ruiyi Wang+, arXiv'25, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³å¼·åŒ–å­¦ç¿’ã«ãŠã‘ã‚‹LLMã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¨“ç·´æ–¹æ³•ã‚’ç ”ç©¶ã—ã€è¨­è¨ˆç©ºé–“ã‚’ç’°å¢ƒã€å ±é…¬ã€ãƒãƒªã‚·ãƒ¼ã®3ã¤ã®æŸ±ã«åˆ†è§£ã€‚ç’°å¢ƒã®è¤‡é›‘ã•ãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä¸€èˆ¬åŒ–èƒ½åŠ›ã«ä¸ãˆã‚‹å½±éŸ¿ã€å ±é…¬ã®å¸Œè–„æ€§ãŒè¨“ç·´ã«ä¸ãˆã‚‹åŠ¹æœã€ãƒãƒªã‚·ãƒ¼å‹¾é…æ³•ã®ç›¸äº’ä½œç”¨ã‚’åˆ†æã€‚ã“ã‚Œã‚‰ã®çŸ¥è¦‹ã‚’åŸºã«ã€è¨“ç·´ãƒ¬ã‚·ãƒ”ã‚’ææ¡ˆã—ã€ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå¼·åŒ–å­¦ç¿’ã®ç ”ç©¶ã¨å®Ÿè·µã‚’æ”¯æ´ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1973720745445659080?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è‘—è€…ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rajammanabrolu/status/1981796161280491678?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>takeawayãŒéå¸¸ã«ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ã„ã€‚<p>ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3451" target="_blank" rel="noopener noreferrer">[Paper Note] TextWorld: A Learning Environment for Text-based Games, Marc-Alexandre CÃ´tÃ©+, Workshop on Computer Games'18 Held in Conjunction with IJCAI'18, 2018.06</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3452" target="_blank" rel="noopener noreferrer">[Paper Note] ALFWorld: Aligning Text and Embodied Environments for Interactive   Learning, Mohit Shridhar+, ICLR'21, 2020.10</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1851" target="_blank" rel="noopener noreferrer">Training Software Engineering Agents and Verifiers with SWE-Gym, Jiayi Pan+, ICML'25</a>
</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<a class="button" href="articles/ContextEngineering.html" target="_blank" rel="noopener noreferrer">#ContextEngineering</a>
<span class="issue_date">Issue Date: 2025-09-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2995" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Massive Values in Self-Attention Modules are the Key to Contextual   Knowledge Understanding, Mingyu Jin+, ICML'25, 2025.02</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã¯æ–‡è„ˆçš„çŸ¥è­˜ã®ç†è§£ã«æˆåŠŸã—ã¦ãŠã‚Šã€ç‰¹ã«æ³¨æ„ã‚¯ã‚¨ãƒªï¼ˆQï¼‰ã¨ã‚­ãƒ¼ï¼ˆKï¼‰ã«ãŠã„ã¦é›†ä¸­ã—ãŸå¤§è¦æ¨¡ãªå€¤ãŒä¸€è²«ã—ã¦ç¾ã‚Œã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ã“ã‚Œã‚‰ã®å€¤ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ä¿å­˜ã•ã‚ŒãŸçŸ¥è­˜ã§ã¯ãªãã€ç¾åœ¨ã®æ–‡è„ˆã‹ã‚‰å¾—ã‚‰ã‚Œã‚‹çŸ¥è­˜ã®è§£é‡ˆã«é‡è¦ã§ã‚ã‚‹ã€‚é‡å­åŒ–æˆ¦ç•¥ã®èª¿æŸ»ã«ã‚ˆã‚Šã€ã“ã‚Œã‚‰ã®å€¤ã‚’ç„¡è¦–ã™ã‚‹ã¨æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã€é›†ä¸­ã—ãŸå¤§è¦æ¨¡ãªå€¤ã®å‡ºç¾ãŒãƒ­ã‚¿ãƒªãƒ¼ãƒã‚¸ã‚·ãƒ§ãƒŠãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆRoPEï¼‰ã«ã‚ˆã£ã¦å¼•ãèµ·ã“ã•ã‚Œã‚‹ã“ã¨ã‚’ç™ºè¦‹ã—ãŸã€‚ã“ã‚Œã‚‰ã®çµæœã¯ã€LLMã®è¨­è¨ˆã¨æœ€é©åŒ–ã«é–¢ã™ã‚‹æ–°ãŸãªæ´å¯Ÿã‚’æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=1SMcxxQiSL&noteId=7BAXSETAwU" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=1SMcxxQiSL&noteId=7BAXSETAwU</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiLingual.html" target="_blank" rel="noopener noreferrer">#MultiLingual</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/Findings.html" target="_blank" rel="noopener noreferrer">#Findings</a>
<a class="button" href="articles/SparseAutoEncoder.html" target="_blank" rel="noopener noreferrer">#SparseAutoEncoder</a>
<span class="issue_date">Issue Date: 2025-09-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2972" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] How a Bilingual LM Becomes Bilingual: Tracing Internal Representations   with Sparse Autoencoders, Tatsuro Inaba+, EMNLP'25 Findings, 2025.03</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ«ã®å†…éƒ¨è¡¨ç¾ã®ç™ºå±•ã‚’ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼ã‚’ç”¨ã„ã¦åˆ†æã€‚è¨€èªãƒ¢ãƒ‡ãƒ«ã¯åˆã‚ã«è¨€èªã‚’å€‹åˆ¥ã«å­¦ç¿’ã—ã€ä¸­é–“å±¤ã§ãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«ã®æ•´åˆæ€§ã‚’å½¢æˆã™ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ã€‚å¤§ããªãƒ¢ãƒ‡ãƒ«ã»ã©ã“ã®å‚¾å‘ãŒå¼·ãã€åˆ†è§£ã•ã‚ŒãŸè¡¨ç¾ã‚’ä¸­é–“ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«ã«çµ±åˆã™ã‚‹æ–°æ‰‹æ³•ã§ãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«è¡¨ç¾ã®é‡è¦æ€§ã‚’ç¤ºã™ã€‚çµæœã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«èƒ½åŠ›ç²å¾—ã«é–¢ã™ã‚‹æ´å¯Ÿã‚’æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/odashi_t/status/1970720101306679436?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/Stability.html" target="_blank" rel="noopener noreferrer">#Stability</a>
<a class="button" href="articles/Findings.html" target="_blank" rel="noopener noreferrer">#Findings</a>
<a class="button" href="articles/DownstreamTasks.html" target="_blank" rel="noopener noreferrer">#DownstreamTasks</a>
<span class="issue_date">Issue Date: 2025-09-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2971" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Instability in Downstream Task Performance During LLM Pretraining, Yuto Nishida+, EMNLP'25 Findings, 2025.10</a>
<span class="snippet"><span>GPT Summary</span>- LLMã®è¨“ç·´ä¸­ã«ä¸‹æµã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå¤§ããå¤‰å‹•ã™ã‚‹å•é¡Œã‚’åˆ†æã—ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å¹³å‡åŒ–ã¨ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ‰‹æ³•ã‚’ç”¨ã„ã¦å®‰å®šæ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¨“ç·´æ‰‹é †ã‚’å¤‰æ›´ã›ãšã«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å¤‰å‹•ã‚’æ¸›å°‘ã•ã›ã‚‹ã“ã¨ãŒå®Ÿè¨¼ã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/odashi_t/status/1970720101306679436?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Test-Time%20Scaling.html" target="_blank" rel="noopener noreferrer">#Test-Time Scaling</a>
<a class="button" href="articles/SamplingParams.html" target="_blank" rel="noopener noreferrer">#SamplingParams</a>
<a class="button" href="articles/Best-of-N.html" target="_blank" rel="noopener noreferrer">#Best-of-N</a>
<a class="button" href="articles/MajorityVoting.html" target="_blank" rel="noopener noreferrer">#MajorityVoting</a>
<span class="issue_date">Issue Date: 2025-09-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2957" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Optimizing Temperature for Language Models with Multi-Sample Inference, Weihua Du+, ICML'25, 2025.02</a>
<span class="snippet"><span>GPT Summary</span>- ãƒãƒ«ãƒã‚µãƒ³ãƒ—ãƒ«é›†ç´„æˆ¦ç•¥ã‚’ç”¨ã„ã¦ã€LLMã®æœ€é©ãªæ¸©åº¦ã‚’è‡ªå‹•çš„ã«ç‰¹å®šã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã€‚å¾“æ¥ã®æ–¹æ³•ã«ä¾å­˜ã›ãšã€ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è€ƒæ…®ã—ãŸæ¸©åº¦ã®å½¹å‰²ã‚’åˆ†æã€‚æ–°ãŸã«ææ¡ˆã™ã‚‹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ã«åŸºã¥ãæŒ‡æ¨™ã¯ã€å›ºå®šæ¸©åº¦ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€ç¢ºç‡éç¨‹ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦æ¸©åº¦ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®é–¢ä¿‚ã‚’è§£æ˜ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview: 


<a href="https://openreview.net/forum?id=rmWpE3FrHW&noteId=h9GETXxWDB" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=rmWpE3FrHW&noteId=h9GETXxWDB</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Alignment.html" target="_blank" rel="noopener noreferrer">#Alignment</a>
<a class="button" href="articles/Safety.html" target="_blank" rel="noopener noreferrer">#Safety</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Scheming.html" target="_blank" rel="noopener noreferrer">#Scheming</a>
<span class="issue_date">Issue Date: 2025-09-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2924" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Stress Testing Deliberative Alignment for Anti-Scheming Training, Bronson Schoen+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- é«˜åº¦ãªAIã‚·ã‚¹ãƒ†ãƒ ã¯ä¸æ•´åˆãªç›®æ¨™ã‚’è¿½æ±‚ã™ã‚‹ã€Œé™°è¬€ã€ã‚’æŒã¤å¯èƒ½æ€§ãŒã‚ã‚Šã€ã“ã‚Œã‚’æ¸¬å®šãƒ»è»½æ¸›ã™ã‚‹ã«ã¯ç‰¹åˆ¥ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå¿…è¦ã§ã™ã€‚æœ¬ç ”ç©¶ã§ã¯ã€åé™°è¬€ä»‹å…¥ã®è©•ä¾¡ã«ãŠã„ã¦ã€é ãã®åˆ†å¸ƒå¤–ã‚¿ã‚¹ã‚¯ã§ã®é™°è¬€ã®å‚¾å‘ã€çŠ¶æ³èªè­˜ã«ã‚ˆã‚‹é™°è¬€ã®æœ‰ç„¡ã€æ—¢å­˜ã®ä¸æ•´åˆãªç›®æ¨™ã«å¯¾ã™ã‚‹ãƒ­ãƒã‚¹ãƒˆæ€§ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’ææ¡ˆã—ã¾ã™ã€‚ç§˜å¯†ã®è¡Œå‹•ã‚’é™°è¬€ã®ä»£ç†ã¨ã—ã¦æ‰±ã„ã€ç†Ÿæ…®çš„æ•´åˆæ€§ã‚’ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆã—ãŸçµæœã€ç§˜å¯†ã®è¡Œå‹•ç‡ãŒä½ä¸‹ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸãŒã€å®Œå…¨ã«ã¯æ’é™¤ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ¢ãƒ‡ãƒ«ã®æ€è€ƒã®é€£é–ãŒæ•´åˆæ€§è©•ä¾¡ã‚’èªè­˜ã™ã‚‹ã“ã¨ã§ç§˜å¯†ã®è¡Œå‹•ãŒæ¸›å°‘ã™ã‚‹ä¸€æ–¹ã€ç„¡è‡ªè¦šã§ã‚ã‚‹ã¨å¢—åŠ ã™ã‚‹ã“ã¨ã‚‚ç¤ºå”†ã•ã‚Œã¾ã—ãŸã€‚ä»Šå¾Œã€é™°è¬€ã«å¯¾ã™ã‚‹æ•´åˆæ€§ã®è»½æ¸›ç­–ã¨ãã®è©•ä¾¡ã«é–¢ã™ã‚‹ç ”ç©¶ãŒé‡è¦ã§ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/scaling01/status/1969548755255861575?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<a class="button" href="articles/RAG(RetrievalAugmentedGeneration).html" target="_blank" rel="noopener noreferrer">#RAG(RetrievalAugmentedGeneration)</a>
<a class="button" href="articles/Generalization.html" target="_blank" rel="noopener noreferrer">#Generalization</a>
<a class="button" href="articles/ReversalCurse.html" target="_blank" rel="noopener noreferrer">#ReversalCurse</a>
<a class="button" href="articles/memory.html" target="_blank" rel="noopener noreferrer">#memory</a>
<span class="issue_date">Issue Date: 2025-09-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2923" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Latent learning: episodic memory complements parametric learning by  enabling flexible reuse of experiences, Andrew Kyle Lampinen+, arXiv'25, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- æ©Ÿæ¢°å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã®ä¸€èˆ¬åŒ–å¤±æ•—ã®åŸå› ã¨ã—ã¦ã€æ½œåœ¨å­¦ç¿’ã®æ¬ å¦‚ã‚’æŒ‡æ‘˜ã€‚èªçŸ¥ç§‘å­¦ã®è¦–ç‚¹ã‹ã‚‰ã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰è¨˜æ†¶ã‚„ã‚ªãƒ©ã‚¯ãƒ«ãƒªãƒˆãƒªãƒ¼ãƒãƒ«ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãŒä¸€èˆ¬åŒ–ã‚’æ”¹å–„ã™ã‚‹æ‰‹æ®µã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚æ–‡è„ˆå†…å­¦ç¿’ãŒæƒ…å ±æ´»ç”¨ã®éµã§ã‚ã‚Šã€ãƒªãƒˆãƒªãƒ¼ãƒãƒ«æ‰‹æ³•ãŒãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯å­¦ç¿’ã‚’è£œå®Œã™ã‚‹ã“ã¨ã§ã€ãƒ‡ãƒ¼ã‚¿åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹å¯èƒ½æ€§ã‚’ææ¡ˆã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/arankomatsuzaki/status/1969968869952631225?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/Findings.html" target="_blank" rel="noopener noreferrer">#Findings</a>
<span class="issue_date">Issue Date: 2025-09-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2920" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Lost in Embeddings: Information Loss in Vision-Language Models, Wenyan Li+, EMNLP'25 Findings, 2025.09</a>
<span class="snippet"><span>GPT Summary</span>- è¦–è¦šã¨è¨€èªã®ãƒ¢ãƒ‡ãƒ«ï¼ˆVLMsï¼‰ã®æŠ•å½±ã‚¹ãƒ†ãƒƒãƒ—ã«ã‚ˆã‚‹æƒ…å ±æå¤±ã‚’åˆ†æã™ã‚‹ãŸã‚ã€2ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã€‚1ã¤ç›®ã¯ã€æŠ•å½±å‰å¾Œã®ç”»åƒè¡¨ç¾ã®kè¿‘å‚é–¢ä¿‚ã®å¤‰åŒ–ã‚’è©•ä¾¡ã—ã€2ã¤ç›®ã¯è¦–è¦šåŸ‹ã‚è¾¼ã¿ã®å†æ§‹ç¯‰ã«ã‚ˆã£ã¦æƒ…å ±æå¤±ã‚’æ¸¬å®šã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ã‚³ãƒã‚¯ã‚¿ãŒè¦–è¦šè¡¨ç¾ã®å¹¾ä½•å­¦ã‚’æ­ªã‚ã€kè¿‘å‚ãŒ40ï½60%ä¹–é›¢ã™ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã€ã“ã‚Œã¯æ¤œç´¢æ€§èƒ½ã®ä½ä¸‹ã¨é–¢é€£ã€‚ãƒ‘ãƒƒãƒãƒ¬ãƒ™ãƒ«ã®å†æ§‹ç¯‰ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®æŒ™å‹•ã«å¯¾ã™ã‚‹æ´å¯Ÿã‚’æä¾›ã—ã€é«˜ã„æƒ…å ±æå¤±ãŒãƒ¢ãƒ‡ãƒ«ã®è‹¦æ‰‹ãªäº‹ä¾‹ã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenyan62/status/1969298016684163195?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1969557366245933068?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/Length.html" target="_blank" rel="noopener noreferrer">#Length</a>
<span class="issue_date">Issue Date: 2025-09-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2905" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Length Representations in Large Language Models, Sangjun Moon+, EMNLP'25</a>
<span class="snippet"><span>GPT Summary</span>- LLMsã¯å‡ºåŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é•·ã•ã‚’åˆ¶å¾¡ã™ã‚‹èƒ½åŠ›ã‚’æŒã¡ã€ãã®å†…éƒ¨ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æ¢æ±‚ã€‚ç‰¹ã«ã€ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãŒå‡ºåŠ›é•·ã®æ±ºå®šã«é‡è¦ã§ã‚ã‚Šã€ç‰¹å®šã®éš ã‚Œãƒ¦ãƒ‹ãƒƒãƒˆã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§é•·ã•ã‚’åˆ¶å¾¡å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒé•·ã•ç‰¹æœ‰ã«ãªã‚‹ã¨éš ã‚Œãƒ¦ãƒ‹ãƒƒãƒˆãŒæ´»æ€§åŒ–ã—ã€ãƒ¢ãƒ‡ãƒ«ã®å†…éƒ¨èªè­˜ã‚’åæ˜ ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€LLMsã¯å¤–éƒ¨åˆ¶å¾¡ãªã—ã«å‡ºåŠ›ã®é•·ã•ã‚’é©å¿œçš„ã«åˆ¶å¾¡ã™ã‚‹ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚Œã‚‹ã€‚</span>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-09-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2899" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] The Illusion of Thinking: Understanding the Strengths and Limitations of  Reasoning Models via the Lens of Problem Complexity, Parshin Shojaee+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- LRMsã¯æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’ç”Ÿæˆã™ã‚‹ãŒã€ãã®èƒ½åŠ›ã‚„é™ç•Œã¯æœªè§£æ˜ã€‚è©•ä¾¡ã¯ä¸»ã«æœ€çµ‚å›ç­”ã®æ­£ç¢ºæ€§ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãŠã‚Šã€æ¨è«–ã®ç—•è·¡ã‚’æä¾›ã—ãªã„ã€‚æœ¬ç ”ç©¶ã§ã¯åˆ¶å¾¡å¯èƒ½ãªãƒ‘ã‚ºãƒ«ç’°å¢ƒã‚’ç”¨ã„ã¦ã€LRMsã®æ¨è«–éç¨‹ã‚’åˆ†æã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€LRMsã¯ç‰¹å®šã®è¤‡é›‘ã•ã‚’è¶…ãˆã‚‹ã¨æ­£ç¢ºæ€§ãŒå´©å£Šã—ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®é™ç•ŒãŒæ˜ã‚‰ã‹ã«ã€‚ä½è¤‡é›‘æ€§ã§ã¯æ¨™æº–ãƒ¢ãƒ‡ãƒ«ãŒå„ªä½ã€ä¸­è¤‡é›‘æ€§ã§ã¯LRMsãŒå„ªä½ã€é«˜è¤‡é›‘æ€§ã§ã¯ä¸¡è€…ãŒå´©å£Šã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚æ¨è«–ã®ç—•è·¡ã‚’èª¿æŸ»ã—ã€LRMsã®å¼·ã¿ã¨é™ç•Œã‚’æ˜ã‚‰ã‹ã«ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/parshinshojaee/status/1968812151138918541?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å‡ºãŸå½“åˆç›¸å½“è©±é¡Œã«ãªã£ãŸIllusion of thinkingãŒNeurIPSã«acceptã•ã‚ŒãŸæ¨¡æ§˜ã€‚Appendix A.1ã«å½“æ™‚ã®criticismã«å¯¾ã™ã‚‹ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãŒè¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<span class="issue_date">Issue Date: 2025-09-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2898" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] BREAD: Branched Rollouts from Expert Anchors Bridge SFT &amp; RL for   Reasoning, Xuechen Zhang+, NeurIPS'25</a>
<span class="snippet"><span>GPT Summary</span>- å°å‹è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆSLMsï¼‰ã¯ã€ãƒˆãƒ¬ãƒ¼ã‚¹ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã«è¤‡é›‘ãªæ¨è«–ã‚’å­¦ã¶ã®ãŒé›£ã—ã„ã€‚æœ¬ç ”ç©¶ã§ã¯ã€SFT + RLã®é™ç•Œã‚’èª¿æŸ»ã—ã€BREADã¨ã„ã†æ–°ã—ã„æ‰‹æ³•ã‚’ææ¡ˆã€‚BREADã¯ã€å°‚é–€å®¶ã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã‚’ç”¨ã„ã¦SFTã¨RLã‚’çµ±åˆã—ã€å¤±æ•—ã—ãŸãƒˆãƒ¬ãƒ¼ã‚¹ã«å¯¾ã—ã¦çŸ­ã„ãƒ’ãƒ³ãƒˆã‚’æŒ¿å…¥ã™ã‚‹ã“ã¨ã§æˆåŠŸã‚’ä¿ƒé€²ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒç´„3å€é€Ÿããªã‚Šã€æ¨™æº–çš„ãªGRPOã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã™ã€‚BREADã¯ã€SLMã®æ¨è«–èƒ½åŠ›ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/sametoymac/status/1968892463382200391?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-09-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2895" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] The Leaderboard Illusion, Shivalika Singh+, NeurIPS'25</a>
<span class="snippet"><span>GPT Summary</span>- é€²æ—æ¸¬å®šã¯ç§‘å­¦ã®é€²å±•ã«ä¸å¯æ¬ ã§ã‚ã‚Šã€Chatbot Arenaã¯AIã‚·ã‚¹ãƒ†ãƒ ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã«ãŠã„ã¦é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¦ã„ã‚‹ã€‚ã—ã‹ã—ã€éå…¬é–‹ã®ãƒ†ã‚¹ãƒˆæ…£è¡ŒãŒå­˜åœ¨ã—ã€ç‰¹å®šã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒæœ‰åˆ©ã«ãªã‚‹ã“ã¨ã§ã€ã‚¹ã‚³ã‚¢ã«ãƒã‚¤ã‚¢ã‚¹ãŒç”Ÿã˜ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚ç‰¹ã«ã€Metaã®Llama-4ã«é–¢é€£ã™ã‚‹ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆLLMãƒãƒªã‚¢ãƒ³ãƒˆãŒå•é¡Œè¦–ã•ã‚Œã€ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¯ã‚»ã‚¹ã®éå¯¾ç§°æ€§ãŒç”Ÿã˜ã¦ã„ã‚‹ã€‚Googleã‚„OpenAIã¯Arenaãƒ‡ãƒ¼ã‚¿ã®å¤§éƒ¨åˆ†ã‚’å ã‚ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚¦ã‚§ã‚¤ãƒˆãƒ¢ãƒ‡ãƒ«ã¯å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã—ã‹å—ã‘å–ã£ã¦ã„ãªã„ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€Arenaç‰¹æœ‰ã®ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã¸ã®éå‰°é©åˆãŒç™ºç”Ÿã—ã¦ã„ã‚‹ã€‚ç ”ç©¶ã¯ã€Chatbot Arenaã®è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®æ”¹é©ã¨ã€å…¬æ­£ã§é€æ˜æ€§ã®ã‚ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚­ãƒ³ã‚°ã®ä¿ƒé€²ã«å‘ã‘ãŸæè¨€ã‚’è¡Œã£ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/singhshiviii/status/1968756900062753080?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è¦ãƒã‚§ãƒƒã‚¯</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/Hallucination.html" target="_blank" rel="noopener noreferrer">#Hallucination</a>
<a class="button" href="articles/TMLR.html" target="_blank" rel="noopener noreferrer">#TMLR</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-09-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2847" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Shared Imagination: LLMs Hallucinate Alike, Yilun Zhou+, TMLR'25, 2025.08</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®é¡ä¼¼æ€§ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€æƒ³åƒä¸Šã®è³ªå•å¿œç­”ï¼ˆIQAï¼‰ã¨ã„ã†æ–°ã—ã„è¨­å®šã‚’ææ¡ˆã€‚IQAã§ã¯ã€1ã¤ã®ãƒ¢ãƒ‡ãƒ«ãŒæ¶ç©ºã®è³ªå•ã‚’ç”Ÿæˆã—ã€åˆ¥ã®ãƒ¢ãƒ‡ãƒ«ãŒãã‚Œã«ç­”ãˆã‚‹ã€‚é©šãã¹ãã“ã¨ã«ã€å…¨ã¦ã®ãƒ¢ãƒ‡ãƒ«ãŒãƒ•ã‚£ã‚¯ã‚·ãƒ§ãƒ³ã®è³ªå•ã«æˆåŠŸè£ã«å¿œç­”ã§ãã‚‹ã“ã¨ã‹ã‚‰ã€å…±é€šã®ã€Œæƒ³åƒç©ºé–“ã€ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚Œã‚‹ã€‚ã“ã®ç¾è±¡ã«ã¤ã„ã¦èª¿æŸ»ã—ã€ãƒ¢ãƒ‡ãƒ«ã®å‡è³ªæ€§ã‚„å¹»è¦šã€è¨ˆç®—çš„å‰µé€ æ€§ã«é–¢ã™ã‚‹è€ƒå¯Ÿã‚’è¡Œã†ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=NUXpBMtDYs" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=NUXpBMtDYs</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/tmlrpub/status/1968449957343433191?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-09-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2840" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] RL Fine-Tuning Heals OOD Forgetting in SFT, Hangzhan Jin+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- äºŒæ®µéšãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã‘ã‚‹SFTã¨RLã®ç›¸äº’ä½œç”¨ã‚’æ¢æ±‚ã—ã€SFTãŒè¨˜æ†¶ã—ã€RLãŒä¸€èˆ¬åŒ–ã™ã‚‹ã¨ã„ã†ä¸»å¼µãŒéåº¦ã«å˜ç´”åŒ–ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã€‚å…·ä½“çš„ã«ã¯ã€(1) OODæ€§èƒ½ã¯SFTã®åˆæœŸæ®µéšã§ãƒ”ãƒ¼ã‚¯ã«é”ã—ã€ãã®å¾Œä½ä¸‹ã™ã‚‹ã“ã¨ã€(2) RLã¯SFTä¸­ã«å¤±ã‚ã‚ŒãŸæ¨è«–èƒ½åŠ›ã‚’å›å¾©ã™ã‚‹å½¹å‰²ã‚’æœãŸã™ã“ã¨ã€(3) å›å¾©èƒ½åŠ›ã«ã¯é™ç•ŒãŒã‚ã‚‹ã“ã¨ã€(4) OODã®æŒ™å‹•ã¯ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«ã®ã€Œå›è»¢ã€ã¨å¼·ãç›¸é–¢ã™ã‚‹ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€SFTã¨RLã®å½¹å‰²ã‚’å†èªè­˜ã—ã€ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«ã®å›è»¢ãŒé‡è¦ãªãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1740" target="_blank" rel="noopener noreferrer">SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model   Post-training, Tianzhe Chu+, ICML'25</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2382" target="_blank" rel="noopener noreferrer">[Paper Note] On the Generalization of SFT: A Reinforcement Learning Perspective with
  Reward Rectification, Yongliang Wu+, arXiv'25</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2700" target="_blank" rel="noopener noreferrer">[Paper Note] Towards a Unified View of Large Language Model Post-Training, Xingtai Lv+, arXiv'25</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2713" target="_blank" rel="noopener noreferrer">[Paper Note] RL's Razor: Why Online Reinforcement Learning Forgets Less, Idan Shenfeld+, arXiv'25</a>
<br><br>ã¨åˆã‚ã›ã¦èª­ã‚€ã¨è‰¯ã•ãã†</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1968187719588385240?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ç›´æ„Ÿçš„ã«ã¯ã€ä¸‹è¨˜ç ”ç©¶ã§SFTã‚’RLã®è¦³ç‚¹ã§è¦‹ãŸã¨ãã«ã€å›ç­”ã®è»Œè·¡ã«å¯¾ã—ã¦exact matchã—ã¦ã„ãŸå ´åˆã«1ã‚’è¿”ã™å ±é…¬ã‚’æŒã¤RLã€ã‹ã¤importance weightingã«ã‚ˆã£ã¦ç¾åœ¨ã®ãƒãƒªã‚·ãƒ¼ãŒè‹¦æ‰‹ãªè»Œè·¡ã‚’é‡è¦è¦–ã™ã‚‹ã€ã¨ã„ã†ã“ã¨è€ƒãˆã‚‹ã¨ã€ç›®çš„ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦æ±åŒ–æ€§èƒ½ãŠã‹ã¾ã„ãªã—ã«greedyã«æœ€é©åŒ–ã•ã‚Œã‚‹ãŸã‚ã€OODã¸ã®å¯¾å¿œåŠ›ãŒç„¡ããªã‚‹ã€ã¨ã„ã†ã®ã¯ãªã‚“ã¨ãªãç†è§£ã§ãã‚‹ã€‚<br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2382" target="_blank" rel="noopener noreferrer">[Paper Note] On the Generalization of SFT: A Reinforcement Learning Perspective with
  Reward Rectification, Yongliang Wu+, arXiv'25</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/Scaling%20Laws.html" target="_blank" rel="noopener noreferrer">#Scaling Laws</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ContextEngineering.html" target="_blank" rel="noopener noreferrer">#ContextEngineering</a>
<span class="issue_date">Issue Date: 2025-09-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2806" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] The Illusion of Diminishing Returns: Measuring Long Horizon Execution in  LLMs, Akshit Sinha+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- LLMsã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒåç›Šã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã‹ã‚’æ¢æ±‚ã€‚å˜ä¸€ã‚¹ãƒ†ãƒƒãƒ—ã®ç²¾åº¦å‘ä¸ŠãŒã‚¿ã‚¹ã‚¯ã®é•·ã•ã«æŒ‡æ•°çš„æ”¹å–„ã‚’ã‚‚ãŸã‚‰ã™ã“ã¨ã‚’è¦³å¯Ÿã€‚LLMsãŒé•·æœŸã‚¿ã‚¹ã‚¯ã§å¤±æ•—ã™ã‚‹ã®ã¯æ¨è«–èƒ½åŠ›ã®æ¬ å¦‚ã§ã¯ãªãå®Ÿè¡ŒãƒŸã‚¹ã«ã‚ˆã‚‹ã¨ä¸»å¼µã€‚çŸ¥è­˜ã¨è¨ˆç”»ã‚’æ˜ç¤ºçš„ã«æä¾›ã™ã‚‹ã“ã¨ã§å®Ÿè¡Œèƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ææ¡ˆã€‚ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã—ã¦ã‚‚è‡ªå·±æ¡ä»¶ä»˜ã‘åŠ¹æœã¯æ¸›å°‘ã›ãšã€é•·ã„ã‚¿ã‚¹ã‚¯ã§ã®ãƒŸã‚¹ãŒå¢—åŠ ã€‚æ€è€ƒãƒ¢ãƒ‡ãƒ«ã¯è‡ªå·±æ¡ä»¶ä»˜ã‘ã‚’è¡Œã‚ãšã«é•·ã„ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œå¯èƒ½ã€‚æœ€çµ‚çš„ã«ã€å®Ÿè¡Œèƒ½åŠ›ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ã“ã¨ã§ã€LLMsã®è¤‡é›‘ãªæ¨è«–å•é¡Œè§£æ±ºèƒ½åŠ›ã¨å˜ç´”ã‚¿ã‚¹ã‚¯ã®é•·æœŸåŒ–ã«ã‚ˆã‚‹å¤±æ•—ç†ç”±ã‚’èª¿å’Œã•ã›ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/shashwatgoel7/status/1966527903568637972?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>single stepã§ã®ã‚¿ã‚¹ã‚¯æ€§èƒ½ã¯ã‚µãƒã£ã¦è¦‹ãˆã¦ã‚‚ã€æˆåŠŸå¯èƒ½ãªã‚¿ã‚¹ã‚¯ã®é•·ã•ã¯ï¼ˆsingle stepã®å®Ÿè¡Œã‚¨ãƒ©ãƒ¼ã«å¼•ãã¥ã‚‰ã‚Œã‚‹ãŸã‚ï¼‰ãƒ¢ãƒ‡ãƒ«ã®single stepã®ã‚¿ã‚¹ã‚¯æ€§èƒ½ã«å¯¾ã—ã¦æŒ‡æ•°é–¢æ•°çš„ã«åŠ¹ã„ã¦ã„ã‚‹ï¼ˆå·¦ä¸Šï¼‰ã€‚ã‚¿ã‚¹ã‚¯ãŒé•·ããªã‚Œã°ãªã‚‹ã»ã©ãƒ¢ãƒ‡ãƒ«ã¯è‡ªèº«ã®ã‚¨ãƒ©ãƒ¼ã«å¼•ããšã‚‰ã‚Œï¼ˆself conditioning;å³ä¸Š)ã€ã“ã‚Œã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå¤§ãã„ã»ã©åº¦åˆã„ãŒå¤§ãããªã‚‹ï¼ˆå³ä¸‹;  32Bã®å ´åˆcontextã«ã‚¨ãƒ©ãƒ¼ãŒã‚ã£ã¦å ´åˆã®loeg horizonã®Acc.ãŒ14Bã‚ˆã‚Šã‚‚ä¸‹ãŒã£ã¦ã„ã‚‹ï¼‰ã€‚ä¸€æ–¹ã§ã€å®Ÿè¡Œå¯èƒ½ãªstepæ•°ã®è¦³ç‚¹ã§è¦‹ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã„å ´åˆã®æ–¹ãŒå¤šãã®stepã‚’è¦ã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã§ãã‚‹ï¼ˆå·¦ä¸‹ï¼‰ã€‚ã¾ãŸã€Thinkingãƒ¢ãƒ‡ãƒ«ã¯Self Conditioningã®å½±éŸ¿ã‚’å—ã‘ã«ããã€single stepã§å®Ÿè¡Œå¯èƒ½ãªã‚¿ã‚¹ã‚¯ã®é•·ã•ãŒã‚ˆã‚Šé•·ããªã‚‹ï¼ˆä¸­å¤®ä¸‹ï¼‰ã€‚<br><br>ã¨ã„ã£ãŸè©±ã«è¦‹ãˆã‚‹ãŒã€è«–æ–‡ã‚’ã—ã£ã‹ã‚Šèª­ã‚“ã æ–¹ãŒè‰¯ã•ãã†ã€‚<br><br><img src="https://github.com/user-attachments/assets/a97fe1f4-5693-4ed3-9fa0-774f4c3738ab" alt="image" loading="lazy"></p>
<p>ï¼ˆå…ƒãƒã‚¹ãƒˆã‚‚è‘—è€…ãƒã‚¹ãƒˆã ãŒï¼‰è‘—è€…ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/akshitwt/status/1966528585558303209?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>ã“ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã¯èª­ã‚“ã æ–¹ãŒè‰¯ã„ï¼ˆã¨ã„ã†ã‹è«–æ–‡ã‚’èª­ã‚“ã æ–¹ãŒè‰¯ã„ï¼‰ã€‚<br>ç‰¹ã«ã€**CoTãŒç„¡ã„å ´åˆã¯**single-turnã§ã»ã¨ã‚“ã©ã®ãƒ¢ãƒ‡ãƒ«ã¯5 stepã®ã‚¿ã‚¹ã‚¯ã‚’latent spaceã§æ€è€ƒã—ã€å®Ÿè¡Œã™ã‚‹ã“ã¨ãŒã§ããªã„ã¨ã„ã†ã®ã¯èˆˆå‘³æ·±ã„ï¼ˆãŒã€ç´°ã‹ã„è¨­å®šã¯ç¢ºèªã—ãŸæ–¹ãŒè‰¯ã„ï¼‰ã€‚ãªã®ã§ã€ãƒãƒ«ãƒã‚¹ãƒ†ãƒƒãƒ—ã®ã‚¿ã‚¹ã‚¯ã¯åŸºæœ¬çš„ã«ã¯planningã‚’ã•ã›ã¦ã‹ã‚‰å‡ºåŠ›ã‚’ã•ã›ãŸæ–¹ãŒè‰¯ã„ã¨ã„ã†è©±ã‚„ã€<br><br>ã§ã¯è¤‡é›‘ãªstepãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯ã¯single turnã§ã¯ãªãmulti turnã«åˆ†ã‘ãŸæ–¹ãŒè‰¯ã„ã®ã‹ï¼Ÿã¨è¨€ã†ã¨ã€ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å‚¾å‘ãŒé•ã†ã‚‰ã—ã„ã€ã¨ã„ã£ãŸè©±ãŒæ›¸ã‹ã‚Œã¦ã„ã‚‹ã€‚ãŸã¨ãˆã°ã€Qwenã¯single turnã‚’å¥½ã‚€ãŒã€Gemmaã¯multi turnã‚’å¥½ã‚€ã‚‰ã—ã„ã€‚<p>æ—¥æœ¬èªãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iwashi86/status/1966969350197571833?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1968453604655907143?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Scaling%20Laws.html" target="_blank" rel="noopener noreferrer">#Scaling Laws</a>
<a class="button" href="articles/Privacy.html" target="_blank" rel="noopener noreferrer">#Privacy</a>
<span class="issue_date">Issue Date: 2025-09-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2791" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Scaling Laws for Differentially Private Language Models, Ryan McKenna+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã¯LLMã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦æ€§èƒ½å‘ä¸Šã‚’äºˆæ¸¬ã—ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é¸æŠã®æŒ‡é‡ã‚’æä¾›ã™ã‚‹ã€‚LLMã¯æ©Ÿå¯†æ€§ã®ã‚ã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ‡ãƒ¼ã‚¿ã«ä¾å­˜ã—ã€DPãªã©ã®ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ãŒå¿…è¦ã ãŒã€ãã®ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã¯æœªè§£æ˜ã€‚æœ¬ç ”ç©¶ã§ã¯ã€DP LLMãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã‚’ç¢ºç«‹ã—ã€è¨ˆç®—ã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ã€ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’è€ƒæ…®ã—ãŸæœ€é©ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ§‹æˆã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>blog:


<a href="https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/" target="_blank" rel="noopener noreferrer">https://research.google/blog/vaultgemma-the-worlds-most-capable-differentially-private-llm/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jeffdean/status/1966558317418885132?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2792" target="_blank" rel="noopener noreferrer">Calibrating Noise to Sensitivity in Private Data Analysis, Dwork+, TCC'06</a>
</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="articles/SpatialUnderstanding.html" target="_blank" rel="noopener noreferrer">#SpatialUnderstanding</a>
<span class="issue_date">Issue Date: 2025-09-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2774" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis  from Data to Architecture, Wanyue Zhang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ç©ºé–“ç†è§£ã¯MLLMsã«ã¨ã£ã¦é‡è¦ã ãŒã€ä¾ç„¶ã¨ã—ã¦èª²é¡ŒãŒå¤šã„ã€‚æœ¬ç ”ç©¶ã§ã¯ã€å˜ä¸€è¦–ç‚¹ã€å¤šè¦–ç‚¹ã€ãƒ“ãƒ‡ã‚ªã®3ã¤ã®ã‚·ãƒŠãƒªã‚ªã«ãŠã‘ã‚‹ç©ºé–“ç†è§£ã‚’ä½“ç³»çš„ã«åˆ†æã—ã€MulSeTã¨ã„ã†ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ææ¡ˆã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®å¢—åŠ ã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã«å¯„ä¸ã™ã‚‹ãŒã€é™ç•ŒãŒã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ã¾ãŸã€ç©ºé–“ç†è§£ã¯è¦–è¦šã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã®ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã«ä¾å­˜ã—ã¦ãŠã‚Šã€æ¨è«–ã®æ³¨å…¥ã‚’é€šã˜ãŸã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ”¹å–„ã®å¯èƒ½æ€§ã‚’æ¢ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€MLLMsã®é™ç•Œã‚’æ˜ã‚‰ã‹ã«ã—ã€ç©ºé–“æ¨è«–èƒ½åŠ›å‘ä¸Šã®æ–°ãŸãªæ–¹å‘æ€§ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/askalphaxiv/status/1965822971261718549?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Entropy.html" target="_blank" rel="noopener noreferrer">#Entropy</a>
<span class="issue_date">Issue Date: 2025-09-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2758" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning, Haozhe Wang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ã¯å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®æ¨è«–èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ãŒã€ãã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¯ä¸æ˜ã€‚åˆ†æã«ã‚ˆã‚Šã€æ¨è«–ã®éšå±¤ãŒäººé–“ã®èªçŸ¥ã«ä¼¼ãŸäºŒæ®µéšã®ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’æŒã¤ã“ã¨ã‚’ç™ºè¦‹ã€‚åˆæœŸæ®µéšã§ã¯æ‰‹ç¶šãçš„ãªæ­£ç¢ºæ€§ãŒæ±‚ã‚ã‚‰ã‚Œã€å¾Œã«é«˜ãƒ¬ãƒ™ãƒ«ã®æˆ¦ç•¥çš„è¨ˆç”»ãŒé‡è¦ã«ãªã‚‹ã€‚ã“ã‚Œã«åŸºã¥ãã€HICRAã¨ã„ã†ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆã—ã€é«˜å½±éŸ¿ã®è¨ˆç”»ãƒˆãƒ¼ã‚¯ãƒ³ã«æœ€é©åŒ–ã‚’é›†ä¸­ã•ã›ã‚‹ã“ã¨ã§æ€§èƒ½ã‚’å‘ä¸Šã•ã›ãŸã€‚ã¾ãŸã€æ„å‘³çš„ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒæˆ¦ç•¥çš„æ¢æ±‚ã®å„ªã‚ŒãŸæŒ‡æ¨™ã§ã‚ã‚‹ã“ã¨ã‚’æ¤œè¨¼ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://tiger-ai-lab.github.io/Hierarchical-Reasoner/" target="_blank" rel="noopener noreferrer">https://tiger-ai-lab.github.io/Hierarchical-Reasoner/</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenhuchen/status/1965783045035762076?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/askalphaxiv/status/1968718537729405228?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Catastrophic%20Forgetting.html" target="_blank" rel="noopener noreferrer">#Catastrophic Forgetting</a>
<a class="button" href="articles/On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<span class="issue_date">Issue Date: 2025-09-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2713" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] RL's Razor: Why Online Reinforcement Learning Forgets Less, Idan Shenfeld+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ã¨æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆSFTï¼‰ã®æ¯”è¼ƒã«ã‚ˆã‚Šã€RLãŒä»¥å‰ã®çŸ¥è­˜ã‚’ã‚ˆã‚Šè‰¯ãä¿æŒã™ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ã€‚å¿˜å´ã®ç¨‹åº¦ã¯åˆ†å¸ƒã®ã‚·ãƒ•ãƒˆã«ã‚ˆã£ã¦æ±ºã¾ã‚Šã€KLãƒ€ã‚¤ãƒãƒ¼ã‚¸ã‚§ãƒ³ã‚¹ã§æ¸¬å®šã•ã‚Œã‚‹ã€‚RLã¯æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦KLæœ€å°è§£ã«ãƒã‚¤ã‚¢ã‚¹ãŒã‹ã‹ã‚‹ä¸€æ–¹ã€SFTã¯ä»»æ„ã®è·é›¢ã«åæŸã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚å®Ÿé¨“ã‚’é€šã˜ã¦ã€RLã®æ›´æ–°ãŒå°ã•ãªKLå¤‰åŒ–ã‚’ã‚‚ãŸã‚‰ã™ç†ç”±ã‚’ç†è«–çš„ã«èª¬æ˜ã—ã€ã€ŒRLã®å‰ƒåˆ€ã€ã¨å‘¼ã¶åŸå‰‡ã‚’æå”±ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jyo_pari/status/1963967312555332065?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ‰€è¦‹:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/stone_tao/status/1964381652106563591?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ãƒã‚¤ãƒ³ãƒˆè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/arankomatsuzaki/status/1963823603469730114?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-09-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2700" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Towards a Unified View of Large Language Model Post-Training, Xingtai Lv+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã¨ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒã€çŸ›ç›¾ã›ãšå˜ä¸€ã®æœ€é©åŒ–ãƒ—ãƒ­ã‚»ã‚¹ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚çµ±ä¸€ãƒãƒªã‚·ãƒ¼å‹¾é…æ¨å®šå™¨ã‚’å°å‡ºã—ã€ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆHPTï¼‰ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ææ¡ˆã€‚HPTã¯ç•°ãªã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¿¡å·ã‚’å‹•çš„ã«é¸æŠã—ã€ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’åŠ¹æœçš„ã«æ´»ç”¨ã—ã¤ã¤å®‰å®šã—ãŸæ¢ç´¢ã‚’å®Ÿç¾ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€HPTãŒæ•°å­¦çš„æ¨è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å¼·åŠ›ãªæ€§èƒ½ã‚’ç¤ºã™ã“ã¨ã‚’ç¢ºèªã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1963818963550572623?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2382" target="_blank" rel="noopener noreferrer">[Paper Note] On the Generalization of SFT: A Reinforcement Learning Perspective with
  Reward Rectification, Yongliang Wu+, arXiv'25</a>
</p>
<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/1963971173735448858?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2025-09-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2691" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] The Information Dynamics of Generative Diffusion, Luca Ambrogioni, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ç”Ÿæˆçš„æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã®çµ±ä¸€çš„ãªç†è«–çš„ç†è§£ã‚’æä¾›ã—ã€å‹•çš„ç‰¹æ€§ã€æƒ…å ±ç†è«–çš„ç‰¹æ€§ã€ç†±åŠ›å­¦çš„ç‰¹æ€§ã‚’çµã³ã¤ã‘ã‚‹ã€‚ç”Ÿæˆå¸¯åŸŸå¹…ã¯ã‚¹ã‚³ã‚¢é–¢æ•°ã®ç™ºæ•£ã«ã‚ˆã£ã¦æ”¯é…ã•ã‚Œã€ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã¯å¯¾ç§°æ€§ã®ç ´ã‚Œã«ã‚ˆã£ã¦é§†å‹•ã•ã‚Œã‚‹ã€‚ã‚¹ã‚³ã‚¢é–¢æ•°ã¯ãƒã‚¤ã‚ºã®å¸¯åŸŸå¹…ã‚’èª¿æ•´ã™ã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1963750048883429865?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2692" target="_blank" rel="noopener noreferrer">Speed-Accuracy Relations for Diffusion Models: Wisdom from Nonequilibrium Thermodynamics and Optimal Transport, Ikeda+, Physical Review X, 2025</a>
</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Optimizer.html" target="_blank" rel="noopener noreferrer">#Optimizer</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-09-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2678" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Benchmarking Optimizers for Large Language Model Pretraining, Andrei Semenov+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ€è¿‘ã®LLMsã®ç™ºå±•ã«ä¼´ã„ã€æœ€é©åŒ–æ‰‹æ³•ã®å¤šæ§˜ãªä¸»å¼µãŒã‚ã‚‹ãŒã€å®Ÿé¨“ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®é•ã„ã«ã‚ˆã‚Šæ¯”è¼ƒãŒé›£ã—ã„ã€‚æœ¬ç ”ç©¶ã§ã¯ã€æ¨™æº–åŒ–ã•ã‚ŒãŸLLMã®äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã‘ã‚‹æœ€é©åŒ–æŠ€è¡“ã‚’è©•ä¾¡ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚„ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤‰åŒ–ã•ã›ã¦æœ€é©ãªã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’ææ¡ˆã€‚ç ”ç©¶ãŒå°†æ¥ã®æœ€é©åŒ–ç ”ç©¶ã®æ–¹å‘æ€§ã‚’ç¤ºã—ã€ã‚³ãƒ¼ãƒ‰ã‚’å…¬é–‹ã™ã‚‹ã“ã¨ã§å†ç¾æ€§ã‚’ç¢ºä¿ã—ã€æ‰‹æ³•ã®é–‹ç™ºã«å¯„ä¸ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/haeggee/status/1963217456740139103?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2674" target="_blank" rel="noopener noreferrer">[Paper Note] Fantastic Pretraining Optimizers and Where to Find Them, Kaiyue Wen+, arXiv'25</a>
<br><br>ä¸Šè¨˜è«–æ–‡ã¨çŸ¥è¦‹ãŒä¸€è‡´ã™ã‚‹éƒ¨åˆ†ã€ç•°ãªã‚‹éƒ¨åˆ†ã¯ä½•ã ã‚ã†ã‹ï¼Ÿ</p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2666" target="_blank" rel="noopener noreferrer">APERTUS: DEMOCRATIZING OPEN AND COMPLIANT LLMS FOR GLOBAL LANGUAGE ENVIRONMENTS, Apertus Team, 2025.09</a>
</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Optimizer.html" target="_blank" rel="noopener noreferrer">#Optimizer</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-09-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2674" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Fantastic Pretraining Optimizers and Where to Find Them, Kaiyue Wen+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- AdamWã¯è¨€èªãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ã§åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã§ã™ãŒã€ä»£æ›¿ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãŒ1.4å€ã‹ã‚‰2å€ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—ã‚’æä¾›ã™ã‚‹ã¨ã„ã†ä¸»å¼µã«ã¯äºŒã¤ã®æ¬ ç‚¹ãŒã‚ã‚‹ã¨æŒ‡æ‘˜ã€‚ã“ã‚Œã‚‰ã¯ä¸å‡ç­‰ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã¨èª¤è§£ã‚’æ‹›ãè©•ä¾¡è¨­å®šã§ã‚ã‚Šã€10ç¨®é¡ã®ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã‚’ç³»çµ±çš„ã«ç ”ç©¶ã™ã‚‹ã“ã¨ã§ã€å…¬æ­£ãªæ¯”è¼ƒã®é‡è¦æ€§ã‚’ç¤ºã—ãŸã€‚ç‰¹ã«ã€æœ€é©ãªãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã”ã¨ã«ç•°ãªã‚Šã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚‹ã«ã¤ã‚Œã¦ã‚¹ãƒ”ãƒ¼ãƒ‰ã‚¢ãƒƒãƒ—åŠ¹æœãŒæ¸›å°‘ã™ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚æœ€ã‚‚é«˜é€Ÿãªã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¯è¡Œåˆ—ãƒ™ãƒ¼ã‚¹ã®å‰å‡¦ç†å™¨ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŒã€ãã®åŠ¹æœã¯ãƒ¢ãƒ‡ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ã«åæ¯”ä¾‹ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1963168542872014943?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é‡è¦ãã†ã«è¦‹ãˆã‚‹</p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2202" target="_blank" rel="noopener noreferrer">[Paper Note] Muon is Scalable for LLM Training, Jingyuan Liu+, arXiv'25</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2675" target="_blank" rel="noopener noreferrer">[Paper Note] SOAP: Improving and Stabilizing Shampoo using Adam, Nikhil Vyas+, ICLR'25</a>
</p>
<p>è‘—è€…ãƒã‚¹ãƒˆ:<br>- 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wen_kaiyue/status/1963633867140526319?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br>- 



<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/percyliang/status/1963648131394122222?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è€ƒå¯Ÿ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1964098785019060719?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/TMLR.html" target="_blank" rel="noopener noreferrer">#TMLR</a>
<a class="button" href="articles/Scheduler.html" target="_blank" rel="noopener noreferrer">#Scheduler</a>
<span class="issue_date">Issue Date: 2025-09-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2665" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Training Dynamics of the Cooldown Stage in Warmup-Stable-Decay Learning   Rate Scheduler, Aleksandr Dremov+, TMLR'25</a>
<span class="snippet"><span>GPT Summary</span>- WSDå­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ãƒ•ã‚§ãƒ¼ã‚ºã‚’åˆ†æã—ã€ç•°ãªã‚‹å½¢çŠ¶ãŒãƒ¢ãƒ‡ãƒ«ã®ãƒã‚¤ã‚¢ã‚¹-ãƒãƒªã‚¢ãƒ³ã‚¹ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’æ˜ã‚‰ã‹ã«ã€‚æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹ãŒæœ€é©ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ã‚‚ãŸã‚‰ã™ã“ã¨ã‚’ç¤ºã—ã€ç‰¹ã«$\beta_2$ã®å€¤ãŒé«˜ã„ã¨æ”¹å–„ãŒè¦‹ã‚‰ã‚Œã‚‹ã€‚æå¤±ã®ãƒ©ãƒ³ãƒ‰ã‚¹ã‚±ãƒ¼ãƒ—ã‚’è¦–è¦šåŒ–ã—ã€ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ãƒ•ã‚§ãƒ¼ã‚ºã®æœ€é©åŒ–ã®é‡è¦æ€§ã‚’å¼·èª¿ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/haeggee/status/1962852239036293223?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/InformationRetrieval.html" target="_blank" rel="noopener noreferrer">#InformationRetrieval</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Search.html" target="_blank" rel="noopener noreferrer">#Search</a>
<span class="issue_date">Issue Date: 2025-09-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2632" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] On the Theoretical Limitations of Embedding-Based Retrieval, Orion Weller+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ™ã‚¯ãƒˆãƒ«åŸ‹ã‚è¾¼ã¿ã¯æ¤œç´¢ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¦ã„ã‚‹ãŒã€ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¯ã‚¨ãƒªã§ã‚‚ç†è«–çš„é™ç•Œã«ç›´é¢ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ç‰¹ã«ã€åŸ‹ã‚è¾¼ã¿ã®æ¬¡å…ƒãŒæ–‡æ›¸ã®ãƒˆãƒƒãƒ—-kã‚µãƒ–ã‚»ãƒƒãƒˆã®æ•°ã‚’åˆ¶é™ã—ã€k=2ã§ã‚‚ã“ã®åˆ¶é™ãŒæˆã‚Šç«‹ã¤ã“ã¨ã‚’å®Ÿè¨¼ã€‚æ–°ãŸã«ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ŒLIMITã€ã§ã¯ã€æœ€å…ˆç«¯ãƒ¢ãƒ‡ãƒ«ã§ã•ãˆå¤±æ•—ã™ã‚‹ã“ã¨ãŒè¦³å¯Ÿã•ã‚Œã€æ—¢å­˜ã®åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®é™ç•Œã‚’æ˜ã‚‰ã‹ã«ã—ã€ä»Šå¾Œã®ç ”ç©¶ã®å¿…è¦æ€§ã‚’æå”±ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1962280460605862194?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Regularization.html" target="_blank" rel="noopener noreferrer">#Regularization</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-08-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2603" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Drop Dropout on Single-Epoch Language Model Pretraining, Houjun Liu+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã¯éå­¦ç¿’ã‚’é˜²ãæ‰‹æ³•ã¨ã—ã¦çŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ãŒã€ç¾ä»£ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã§ã¯éå­¦ç¿’ãŒæŠ‘ãˆã‚‰ã‚Œã‚‹ãŸã‚ä½¿ç”¨ã•ã‚Œã¦ã„ãªã„ã€‚æœ¬ç ”ç©¶ã§ã¯ã€BERTã‚„Pythiaãƒ¢ãƒ‡ãƒ«ã®å˜ä¸€ã‚¨ãƒãƒƒã‚¯äº‹å‰å­¦ç¿’ã«ãŠã„ã¦ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã®å½±éŸ¿ã‚’èª¿æŸ»ã—ãŸçµæœã€ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’é©ç”¨ã—ãªã„æ–¹ãŒä¸‹æµã®æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã“ã¨ãŒåˆ¤æ˜ã€‚ã¾ãŸã€ã€Œæ—©æœŸãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã€ã‚‚æ€§èƒ½ã‚’ä½ä¸‹ã•ã›ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆãªã—ã§è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ¢ãƒ‡ãƒ«ç·¨é›†ã«ãŠã„ã¦ã‚‚ã‚ˆã‚ŠæˆåŠŸã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã€å˜ä¸€ã‚¨ãƒãƒƒã‚¯ã®äº‹å‰å­¦ç¿’ä¸­ã«ã¯ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆã‚’çœãã“ã¨ãŒæ¨å¥¨ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1961589435197505584?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2604" target="_blank" rel="noopener noreferrer">[Paper Note] Dropout Reduces Underfitting, Zhuang Liu+, ICML'23</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/reading.html" target="_blank" rel="noopener noreferrer">#reading</a>
<span class="issue_date">Issue Date: 2025-08-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2569" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens, Chengshuai Zhao+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- Chain-of-Thought (CoT) ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã¯LLMã®æ€§èƒ½å‘ä¸Šã«å¯„ä¸ã™ã‚‹ãŒã€ãã®æ·±ã•ã«ã¯ç–‘å•ãŒæ®‹ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€CoTæ¨è«–ãŒè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®æ§‹é€ çš„ãƒã‚¤ã‚¢ã‚¹ã‚’åæ˜ ã—ã¦ã„ã‚‹ã‹ã‚’èª¿æŸ»ã—ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒªã®åˆ†å¸ƒä¸ä¸€è‡´ãŒãã®åŠ¹æœã«ä¸ãˆã‚‹å½±éŸ¿ã‚’åˆ†æã€‚DataAlchemyã¨ã„ã†åˆ¶å¾¡ç’°å¢ƒã‚’ç”¨ã„ã¦ã€CoTæ¨è«–ã®è„†å¼±æ€§ã‚’æ˜ã‚‰ã‹ã«ã—ã€ä¸€èˆ¬åŒ–å¯èƒ½ãªæ¨è«–ã®é”æˆã«å‘ã‘ãŸèª²é¡Œã‚’å¼·èª¿ã™ã‚‹ã€‚</span>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/Game.html" target="_blank" rel="noopener noreferrer">#Game</a>
<span class="issue_date">Issue Date: 2025-08-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2533" target="_blank" rel="noopener noreferrer" class="title-link">Identification and Analysis of Identity-Centric Elements of Character-Likeness from Game Scenario, Iwata+, SIGDIAL'25</a>
<span class="snippet"><span>Comment</span><p>arxivã«ç„¡ã•ãã†ãªã®ã§ã€æ¦‚è¦ã¯å…ƒãƒã‚¹ãƒˆå‚ç…§ã®ã“ã¨ã€‚ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚‰ã—ã•ã®æ§‹æˆè¦ç´ ã¨ãã‚Œã‚‰ãŒã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã‚‰ã—ã•ã«é–¢ã—ã¦ã©ã®ã‚ˆã†ã«é–¢ä¿‚ã—ã¦ã„ã‚‹ã‹ã‚’åˆ†æã—ãŸç ”ç©¶ãªæ¨¡æ§˜ã€‚</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hmkz_/status/1958903563561894229?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/NaturalLanguageGeneration.html" target="_blank" rel="noopener noreferrer">#NaturalLanguageGeneration</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-08-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2520" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Are Checklists Really Useful for Automatic Evaluation of Generative  Tasks?, Momoka Furuhashi+, EMNLP'25</a>
<span class="snippet"><span>GPT Summary</span>- ç”Ÿæˆã‚¿ã‚¹ã‚¯ã®è‡ªå‹•è©•ä¾¡ã«ãŠã‘ã‚‹æ›–æ˜§ãªåŸºæº–ã®èª²é¡Œã‚’è§£æ±ºã™ã‚‹ãŸã‚ã€ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã®ä½¿ç”¨æ–¹æ³•ã‚’æ¤œè¨ã€‚6ã¤ã®ç”Ÿæˆæ–¹æ³•ã¨8ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã§è©•ä¾¡ã—ã€é¸æŠçš„ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆãŒãƒšã‚¢ãƒ¯ã‚¤ã‚ºè©•ä¾¡ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ”¹å–„ã™ã‚‹å‚¾å‘ãŒã‚ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã€‚ãŸã ã—ã€ç›´æ¥ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã§ã¯ä¸€è²«æ€§ãŒãªã„ã€‚äººé–“ã®è©•ä¾¡åŸºæº–ã¨ã®ç›¸é–¢ãŒä½ã„ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆé …ç›®ã‚‚å­˜åœ¨ã—ã€è©•ä¾¡åŸºæº–ã®æ˜ç¢ºåŒ–ãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºå”†ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/tohoku_nlp_mmk/status/1958717497454002557?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>pj page:


<a href="https://momo0817.github.io/checklist-effectiveness-study-github.io/" target="_blank" rel="noopener noreferrer">https://momo0817.github.io/checklist-effectiveness-study-github.io/</a>


</p></span><br><br>
<a class="button" href="articles/Multi.html" target="_blank" rel="noopener noreferrer">#Multi</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-08-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2427" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large  Language Models, Xingcheng Xu, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ã¯å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®è¡Œå‹•å½¢æˆã«é‡è¦ã ãŒã€è„†å¼±ãªãƒãƒªã‚·ãƒ¼ã‚’ç”Ÿæˆã—ã€ä¿¡é ¼æ€§ã‚’æãªã†å•é¡ŒãŒã‚ã‚‹ã€‚æœ¬è«–æ–‡ã§ã¯ã€å ±é…¬é–¢æ•°ã‹ã‚‰æœ€é©ãƒãƒªã‚·ãƒ¼ã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã®å®‰å®šæ€§ã‚’åˆ†æã™ã‚‹æ•°å­¦çš„æ çµ„ã¿ã‚’ææ¡ˆã—ã€ãƒãƒªã‚·ãƒ¼ã®è„†å¼±æ€§ãŒéä¸€æ„çš„ãªæœ€é©ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã«èµ·å› ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ã•ã‚‰ã«ã€å¤šå ±é…¬RLã«ãŠã‘ã‚‹å®‰å®šæ€§ãŒã€ŒåŠ¹æœçš„å ±é…¬ã€ã«ã‚ˆã£ã¦æ”¯é…ã•ã‚Œã‚‹ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã—ã€ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æ­£å‰‡åŒ–ãŒå®‰å®šæ€§ã‚’å›å¾©ã™ã‚‹ã“ã¨ã‚’è¨¼æ˜ã™ã‚‹ã€‚ã“ã®ç ”ç©¶ã¯ã€ãƒãƒªã‚·ãƒ¼å®‰å®šæ€§åˆ†æã‚’é€²å±•ã•ã›ã€å®‰å…¨ã§ä¿¡é ¼æ€§ã®é«˜ã„AIã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆã«å¯„ä¸ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1955909877404197072?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã¨ã¦ã‚‚é¢ç™½ãã†</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<span class="issue_date">Issue Date: 2025-08-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2417" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Unveiling Super Experts in Mixture-of-Experts Large Language Models, Zunhai Su+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ã‚¹ãƒ‘ãƒ¼ã‚¹ã«æ´»æ€§åŒ–ã•ã‚ŒãŸMixture-of-Expertsï¼ˆMoEï¼‰ãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€ç‰¹å®šã®å°‚é–€å®¶ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã€Œã‚¹ãƒ¼ãƒ‘å°‚é–€å®¶ï¼ˆSEï¼‰ã€ãŒãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã«é‡è¦ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ã‚’ç™ºè¦‹ã€‚SEã¯ç¨€ãªæ´»æ€§åŒ–ã‚’ç¤ºã—ã€ãƒ—ãƒ«ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã¨ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ãŒåŠ£åŒ–ã™ã‚‹ã€‚åˆ†æã«ã‚ˆã‚Šã€SEã®é‡è¦æ€§ãŒæ•°å­¦çš„æ¨è«–ãªã©ã®ã‚¿ã‚¹ã‚¯ã§æ˜ã‚‰ã‹ã«ãªã‚Šã€MoE LLMãŒSEã«ä¾å­˜ã—ã¦ã„ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1955217132016505239?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>MoEã«ãŠã‘ã‚‹ã€ç‰¹ã«é‡è¦ãªå°‚é–€å®¶ã§ã‚ã‚‹Super Expertsã®å­˜åœ¨</p>
<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1566" target="_blank" rel="noopener noreferrer">The Super Weight in Large Language Models, Mengxia Yu+, arXiv'24</a>
<br><br>ã‚’æ€ã„å‡ºã™ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/ReversalCurse.html" target="_blank" rel="noopener noreferrer">#ReversalCurse</a>
<span class="issue_date">Issue Date: 2025-08-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2399" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Physics of Language Models: Part 3.2, Knowledge Manipulation, Zeyuan Allen-Zhu+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- è¨€èªãƒ¢ãƒ‡ãƒ«ã¯è±Šå¯ŒãªçŸ¥è­˜ã‚’æŒã¤ãŒã€ä¸‹æµã‚¿ã‚¹ã‚¯ã¸ã®æŸ”è»Ÿãªåˆ©ç”¨ã«ã¯é™ç•ŒãŒã‚ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€æƒ…å ±æ¤œç´¢ã€åˆ†é¡ã€æ¯”è¼ƒã€é€†æ¤œç´¢ã®4ã¤ã®çŸ¥è­˜æ“ä½œã‚¿ã‚¹ã‚¯ã‚’èª¿æŸ»ã—ã€è¨€èªãƒ¢ãƒ‡ãƒ«ãŒçŸ¥è­˜æ¤œç´¢ã«ã¯å„ªã‚Œã¦ã„ã‚‹ãŒã€Chain of Thoughtsã‚’ç”¨ã„ãªã„ã¨åˆ†é¡ã‚„æ¯”è¼ƒã‚¿ã‚¹ã‚¯ã§è‹¦åŠ´ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚ç‰¹ã«é€†æ¤œç´¢ã§ã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒã»ã¼0%ã§ã‚ã‚Šã€ã“ã‚Œã‚‰ã®å¼±ç‚¹ã¯è¨€èªãƒ¢ãƒ‡ãƒ«ã«å›ºæœ‰ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç¾ä»£ã®AIã¨äººé–“ã‚’åŒºåˆ¥ã™ã‚‹æ–°ãŸãªãƒãƒ¥ãƒ¼ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆã®å¿…è¦æ€§ãŒæµ®ãå½«ã‚Šã«ãªã£ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=oDbiL9CLoS" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=oDbiL9CLoS</a>


</p>
<p>è§£èª¬:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1834" target="_blank" rel="noopener noreferrer">è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç‰©ç†å­¦, ä½è—¤ç«œé¦¬, 2025.03</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/SelfCorrection.html" target="_blank" rel="noopener noreferrer">#SelfCorrection</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<span class="issue_date">Issue Date: 2025-08-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2398" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Physics of Language Models: Part 2.2, How to Learn From Mistakes on   Grade-School Math Problems, Tian Ye+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ç²¾åº¦å‘ä¸Šã®ãŸã‚ã«ã€ã€Œã‚¨ãƒ©ãƒ¼ä¿®æ­£ã€ãƒ‡ãƒ¼ã‚¿ã‚’äº‹å‰å­¦ç¿’ã«çµ„ã¿è¾¼ã‚€æœ‰ç”¨æ€§ã‚’æ¢æ±‚ã€‚åˆæˆæ•°å­¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦ã€ã‚¨ãƒ©ãƒ¼ãƒ•ãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã¨æ¯”è¼ƒã—ã¦é«˜ã„æ¨è«–ç²¾åº¦ã‚’é”æˆã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ã•ã‚‰ã«ã€ãƒ“ãƒ¼ãƒ ã‚µãƒ¼ãƒã¨ã®é•ã„ã‚„ãƒ‡ãƒ¼ã‚¿æº–å‚™ã€ãƒã‚¹ã‚­ãƒ³ã‚°ã®å¿…è¦æ€§ã€ã‚¨ãƒ©ãƒ¼é‡ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ®µéšã§ã®é…å»¶ã«ã¤ã„ã¦ã‚‚è€ƒå¯Ÿã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=zpDGwcmMV4" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=zpDGwcmMV4</a>


</p>
<p>è§£èª¬:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1834" target="_blank" rel="noopener noreferrer">è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç‰©ç†å­¦, ä½è—¤ç«œé¦¬, 2025.03</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/reading.html" target="_blank" rel="noopener noreferrer">#reading</a>
<span class="issue_date">Issue Date: 2025-08-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2397" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Physics of Language Models: Part 2.1, Grade-School Math and the Hidden   Reasoning Process, Tian Ye+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ•°å­¦çš„æ¨è«–èƒ½åŠ›ã‚’ç ”ç©¶ã—ã€GSM8Kãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®ç²¾åº¦å‘ä¸Šã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æ¢ã‚‹ã€‚å…·ä½“çš„ã«ã¯ã€æ¨è«–ã‚¹ã‚­ãƒ«ã®ç™ºå±•ã€éš ã‚ŒãŸãƒ—ãƒ­ã‚»ã‚¹ã€äººé–“ã¨ã®é•ã„ã€å¿…è¦ãªã‚¹ã‚­ãƒ«ã®è¶…è¶Šã€æ¨è«–ãƒŸã‚¹ã®åŸå› ã€ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã‚„æ·±ã•ã«ã¤ã„ã¦ã®å®Ÿé¨“ã‚’è¡Œã„ã€LLMã®ç†è§£ã‚’æ·±ã‚ã‚‹æ´å¯Ÿã‚’æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=Tn5B6Udq3E" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=Tn5B6Udq3E</a>


</p>
<p>è§£èª¬:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1834" target="_blank" rel="noopener noreferrer">è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç‰©ç†å­¦, ä½è—¤ç«œé¦¬, 2025.03</a>
</p>
<p>å°å­¦ç”Ÿå‘ã‘ã®ç®—æ•°ã®å•é¡Œã‚’é€šã˜ã¦ã€ä»¥ä¸‹ã®åŸºæœ¬çš„ãªResearch Questionsã«ã¤ã„ã¦èª¿æŸ»ã—ã¦ç ”ç©¶ã€‚ã“ã‚Œã‚‰ã‚’ç†è§£ã™ã‚‹ã“ã¨ã§ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®çŸ¥èƒ½ã‚’ç†è§£ã™ã‚‹ç¤ã¨ã™ã‚‹ã€‚<br><br>## Research Questions<br>- è¨€èªãƒ¢ãƒ‡ãƒ«ã¯ã©ã®ã‚ˆã†ã«ã—ã¦å°å­¦æ ¡ãƒ¬ãƒ™ãƒ«ã®ç®—æ•°ã®å•é¡Œã‚’è§£ã‘ã‚‹ã‚ˆã†ã«ãªã‚‹ã®ã‹ï¼Ÿ<br>  - å˜ã«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æš—è¨˜ã—ã¦ã„ã‚‹ã ã‘ãªã®ã‹ã€ãã‚Œã¨ã‚‚äººé–“ã«ä¼¼ãŸæ¨è«–ã‚¹ã‚­ãƒ«ã‚’å­¦ã‚“ã§ã„ã‚‹ã®ã‹ï¼Ÿ<br>  - ã‚ã‚‹ã„ã¯ã€ãã®å•é¡Œã‚’è§£ããŸã‚ã«æ–°ã—ã„ã‚¹ã‚­ãƒ«ã‚’ç™ºè¦‹ã—ã¦ã„ã‚‹ã®ã‹ï¼Ÿ<br>- å°å­¦æ ¡ãƒ¬ãƒ™ãƒ«ã®ç®—æ•°å•é¡Œã ã‘ã§è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã€ãã‚Œã‚‰ã®å•é¡Œã‚’è§£ãã“ã¨ã—ã‹å­¦ã°ãªã„ã®ã‹ï¼Ÿ<br>  - ãã‚Œã¨ã‚‚ã€ã‚ˆã‚Šä¸€èˆ¬çš„ãªçŸ¥èƒ½ã‚’å­¦ç¿’ã™ã‚‹ã®ã‹ï¼Ÿ<br>- ã©ã®ãã‚‰ã„å°ã•ã„è¨€èªãƒ¢ãƒ‡ãƒ«ã¾ã§ã€å°å­¦æ ¡ãƒ¬ãƒ™ãƒ«ã®ç®—æ•°å•é¡Œã‚’è§£ã‘ã‚‹ã®ã‹ï¼Ÿ<br>  - æ·±ã•ï¼ˆå±¤ã®æ•°ï¼‰ã¯å¹…ï¼ˆå±¤ã”ã¨ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ï¼‰ã‚ˆã‚Šé‡è¦ãªã®ã‹ï¼Ÿ<br>  - ãã‚Œã¨ã‚‚ã€å˜ã«ã‚µã‚¤ã‚ºã ã‘ãŒé‡è¦ã‹ï¼Ÿ<br><br>ï¼ˆç¶šãã¯ã®ã¡ã»ã©...ï¼‰</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2352" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] On the Expressiveness of Softmax Attention: A Recurrent Neural Network  Perspective, Gabriel Mongaras+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å†å¸°çš„ãªå½¢å¼ã‚’å°å‡ºã—ã€ç·šå½¢ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãŒãã®è¿‘ä¼¼ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å„éƒ¨åˆ†ã‚’RNNã®è¨€èªã§èª¬æ˜ã—ã€æ§‹æˆè¦ç´ ã®é‡è¦æ€§ã¨ç›¸äº’ä½œç”¨ã‚’ç†è§£ã™ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãŒä»–ã®æ‰‹æ³•ã‚ˆã‚Šã‚‚è¡¨ç¾åŠ›ãŒé«˜ã„ç†ç”±ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1952485214162407644?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>LinearAttentioné–¢é€£ã®ç ”ç©¶ã¯ä¸‹è¨˜ã‚ãŸã‚ŠãŒã‚ã‚Šãã†ï¼Ÿ<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2353" target="_blank" rel="noopener noreferrer">[Paper Note] Efficient Attention: Attention with Linear Complexities, Zhuoran Shen+, arXiv'18</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2354" target="_blank" rel="noopener noreferrer">[Paper Note] Linformer: Self-Attention with Linear Complexity, Sinong Wang+, arXiv'20</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2355" target="_blank" rel="noopener noreferrer">[Paper Note] Reformer: The Efficient Transformer, Nikita Kitaev+, ICLR'20</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2356" target="_blank" rel="noopener noreferrer">[Paper Note] Transformers are RNNs: Fast Autoregressive Transformers with Linear  Attention, Angelos Katharopoulos+, ICML'20</a>
</p>
<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1271" target="_blank" rel="noopener noreferrer">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head
  Checkpoints, Joshua Ainslie+, N/A, arXiv'23</a>
<br><br>ãŸã¨ãˆã°GQAã¯Qwen3ã§åˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹ãŒã€æœ¬ç ”ç©¶ã®çŸ¥è¦‹ã‚’æ´»ç”¨ã—ã¦scaled-dot product attentionè¨ˆç®—æ™‚ã®Softmaxè¨ˆç®—ã®è¨ˆç®—é‡ãŒå‰Šæ¸›ã§ããŸã‚‰ã€ã•ã‚‰ã«è¨ˆç®—é‡ãŒå‰Šæ¸›ã§ããã†ï¼Ÿ</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Prompting.html" target="_blank" rel="noopener noreferrer">#Prompting</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/reading.html" target="_blank" rel="noopener noreferrer">#reading</a>
<a class="button" href="articles/MajorityVoting.html" target="_blank" rel="noopener noreferrer">#MajorityVoting</a>
<span class="issue_date">Issue Date: 2025-08-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2346" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A   Perspective of Probability Theory, Yexiang Liu+, ACL'25 Outstanding Paper</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€LLMã®ãƒ†ã‚¹ãƒˆæ™‚ã®è¨ˆç®—ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ãŠã‘ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæˆ¦ç•¥ã®åŠ¹æœã‚’èª¿æŸ»ã€‚6ã¤ã®LLMã¨8ã¤ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæˆ¦ç•¥ã‚’ç”¨ã„ãŸå®Ÿé¨“ã«ã‚ˆã‚Šã€è¤‡é›‘ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæˆ¦ç•¥ãŒå˜ç´”ãªChain-of-Thoughtã«åŠ£ã‚‹ã“ã¨ã‚’ç¤ºã—ã€ç†è«–çš„ãªè¨¼æ˜ã‚’æä¾›ã€‚ã•ã‚‰ã«ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ€§èƒ½ã‚’äºˆæ¸¬ã—æœ€é©ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæˆ¦ç•¥ã‚’ç‰¹å®šã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã—ã€ãƒªã‚½ãƒ¼ã‚¹é›†ç´„çš„ãªæ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ã®å¿…è¦æ€§ã‚’æ’é™¤ã€‚è¤‡é›‘ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å†è©•ä¾¡ã¨å˜ç´”ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæˆ¦ç•¥ã®æ½œåœ¨èƒ½åŠ›ã‚’å¼•ãå‡ºã™ã“ã¨ã§ã€ãƒ†ã‚¹ãƒˆæ™‚ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ€§èƒ½å‘ä¸Šã«å¯„ä¸ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>non-thinkingãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€Majority Voting (i.e. Self Consistency)ã«ã‚ˆã‚‹test-time scalingã‚’å®Ÿæ–½ã™ã‚‹å ´åˆã®ã•ã¾ã–ã¾ãªpromptingæˆ¦ç•¥ã®ã†ã¡ã€budgetã¨ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ•°ãŒå°ã•ã„å ´åˆã¯CoTä»¥å¤–ã®é©åˆ‡ãªpromptingæˆ¦ç•¥ã¯ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«ç•°ãªã‚‹ãŒã€budgetã‚„ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ•°ãŒå¢—ãˆã¦ãã‚‹ã¨ã‚·ãƒ³ãƒ—ãƒ«ãªCoTï¼ˆå®Ÿé¨“ã§ã¯zeroshot CoTã‚’åˆ©ç”¨ï¼‰ãŒæœ€é©ãªpromptingæˆ¦ç•¥ã¨ã—ã¦æ”¯é…çš„ã«ãªã‚‹ã€ã¨ã„ã†è©±ãªæ¨¡æ§˜ã€‚<br><br>ã•ã‚‰ã«ã€ãªãœãã†ãªã‚‹ã‹ã®ç†è«–çš„ãªåˆ†æã¨æœ€é©ãªä¸ãˆã‚‰ã‚ŒãŸäºˆç®—ã‹ã‚‰æœ€é©ãªpromptingæˆ¦ç•¥ã‚’äºˆæ¸¬ã™ã‚‹æ‰‹æ³•ã‚‚ææ¡ˆã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚<br><br>ãŒã€è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®é›£æ˜“åº¦ãªã©ã«ã‚ˆã£ã¦ã“ã®è¾ºã¯å¤‰ã‚ã‚‹ã¨æ€ã‚ã‚Œã€ç‰¹ã«Figure39ã«ç¤ºã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ãªã€**ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ•°ãŒå¢—ãˆã‚‹ã¨ç°¡å˜ãªå•é¡Œã®æ­£è§£ç‡ãŒä¸ŠãŒã‚Šã€é€†ã«é›£ã—ã„å•é¡Œã®æ­£è§£ç‡ãŒä¸‹ãŒã‚‹ã¨ã„ã£ãŸå‚¾å‘ãŒã‚ã‚Šã€CoTãŒç°¡å˜ãªå•é¡Œã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ•°ã‚’å¢—ã‚„ã™ã¨å®‰å®šã—ã¦æ­£è§£ã§ãã‚‹ã‹ã‚‰æ”¯é…çš„ã«ãªã‚‹**ã€ã¨ã„ã†è©±ã ã¨æ€ã‚ã‚Œã‚‹ã®ã§ã€å¸¸ã«CoTãŒè‰¯ã„ã¨å‹˜é•ã„ã—ãªã„æ–¹ãŒè‰¯ã•ãã†ã ã¨æ€ã‚ã‚Œã‚‹ã€‚ãŸã¨ãˆã°ã€**è§£ã“ã†ã¨ã—ã¦ã„ã‚‹ã‚¿ã‚¹ã‚¯ãŒé›£å•ã°ã‹ã‚Šã§ã‚ã‚Œã°CoTã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹ã®ãŒè‰¯ã„ã¨ã¯é™ã‚‰ãªã„ã€ã¨ã„ã£ãŸç‚¹ã«ã¯æ³¨æ„ãŒå¿…è¦**ã ã¨æ€ã†ã®ã§ã€ã—ã£ã‹ã‚Šå…¨æ–‡èª­ã‚“ã æ–¹ãŒè‰¯ã„ã€‚æ™‚é–“ãŒã‚ã‚‹æ™‚ã«èª­ã¿ãŸã„ï¼ˆãªã‹ãªã‹ã¾ã¨ã¾ã£ãŸæ™‚é–“å–ã‚Œãªã„ï¼‰<br><br><img src="https://github.com/user-attachments/assets/f99e3445-7962-488d-87a4-744022f796c8" alt="image" loading="lazy"></p>
<p>æœ€é©ãªpromptingæˆ¦ç•¥ã‚’äºˆæ¸¬ã™ã‚‹æ‰‹æ³•ã§ã¯ã€<br>- å•é¡Œã®é›£æ˜“åº¦ã«å¿œã˜ã¦é©å¿œçš„ã«ã‚¹ã‚±ãƒ¼ãƒ«ã‚’å¤‰åŒ–ã•ã›(ãªã‚“ã¨O(1)ã§äºˆæ¸¬ãŒã§ãã‚‹)<br>- å‹•çš„ã«æœ€é©ãªpromptingæˆ¦ç•¥ã‚’é¸æŠ<br><br>ã™ã‚‹ã“ã¨ã§ã€Majority@10ã®Acc.ã‚’8Bã‚¹ã‚±ãƒ¼ãƒ«ã®ãƒ¢ãƒ‡ãƒ«ã§10--50%ç¨‹åº¦å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹æ¨¡æ§˜ã€‚ã„ã‚„ã“ã‚Œã»ã‚“ã¨ã—ã£ã‹ã‚Šèª­ã¾ã­ã°ã€‚</p></span><br><br>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-08-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2345" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Mapping 1,000+ Language Models via the Log-Likelihood Vector, Momose Oyama+, ACL'25</a>
<span class="snippet"><span>GPT Summary</span>- è‡ªå‹•å›å¸°å‹è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒã«å¯¾ã—ã€å¯¾æ•°å°¤åº¦ãƒ™ã‚¯ãƒˆãƒ«ã‚’ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹æ–°ã—ã„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆç¢ºç‡ã®ã‚¯ãƒ«ãƒãƒƒã‚¯ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒ¼ç™ºæ•£ã‚’è¿‘ä¼¼ã—ã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã§è¨ˆç®—ã‚³ã‚¹ãƒˆãŒç·šå½¢ã«å¢—åŠ ã™ã‚‹ç‰¹å¾´ã‚’æŒã¤ã€‚1,000ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ã«é©ç”¨ã—ã€ã€Œãƒ¢ãƒ‡ãƒ«ãƒãƒƒãƒ—ã€ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã§ã€å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«åˆ†æã«æ–°ãŸãªè¦–ç‚¹ã‚’æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span><p>NLPã‚³ãƒ­ã‚­ã‚¦ãƒ ã§ã®ã‚¹ãƒ©ã‚¤ãƒ‰:


<a href="https://speakerdeck.com/shimosan/yan-yu-moderunodi-tu-que-lu-fen-bu-to-qing-bao-ji-he-niyorulei-si-xing-noke-shi-hua" target="_blank" rel="noopener noreferrer">https://speakerdeck.com/shimosan/yan-yu-moderunodi-tu-que-lu-fen-bu-to-qing-bao-ji-he-niyorulei-si-xing-noke-shi-hua</a>


<br><br>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hshimodaira/status/1960573414575333556?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2025-07-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2313" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Learning without training: The implicit dynamics of in-context learning, Benoit Dherin+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- LLMã¯æ–‡è„ˆå†…ã§æ–°ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹èƒ½åŠ›ã‚’æŒã¡ã€ãã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¯æœªè§£æ˜ã§ã‚ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ–ãƒ­ãƒƒã‚¯ãŒè‡ªå·±æ³¨æ„å±¤ã¨MLPã‚’é‡ã­ã‚‹ã“ã¨ã§ã€æ–‡è„ˆã«å¿œã˜ã¦MLPã®é‡ã¿ã‚’æš—é»™çš„ã«ä¿®æ­£ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã€ã“ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãŒLLMã®æ–‡è„ˆå†…å­¦ç¿’ã®ç†ç”±ã§ã‚ã‚‹å¯èƒ½æ€§ã‚’ææ¡ˆã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/1948384435654779105?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1950333455134576794?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Finetuning.html" target="_blank" rel="noopener noreferrer">#Finetuning</a>
<span class="issue_date">Issue Date: 2025-07-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2282" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Subliminal Learning: Language models transmit behavioral traits via  hidden signals in data, Alex Cloud+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ã‚µãƒ–ãƒªãƒŸãƒŠãƒ«å­¦ç¿’ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ãŒç„¡é–¢ä¿‚ãªãƒ‡ãƒ¼ã‚¿ã‚’é€šã˜ã¦ç‰¹æ€§ã‚’ä¼é”ã™ã‚‹ç¾è±¡ã§ã‚ã‚‹ã€‚å®Ÿé¨“ã§ã¯ã€ç‰¹å®šã®ç‰¹æ€§ã‚’æŒã¤æ•™å¸«ãƒ¢ãƒ‡ãƒ«ãŒç”Ÿæˆã—ãŸæ•°åˆ—ãƒ‡ãƒ¼ã‚¿ã§è¨“ç·´ã•ã‚ŒãŸç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ãŒã€ãã®ç‰¹æ€§ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚ãƒ‡ãƒ¼ã‚¿ãŒç‰¹æ€§ã¸ã®è¨€åŠã‚’é™¤å»ã—ã¦ã‚‚ã“ã®ç¾è±¡ã¯ç™ºç”Ÿã—ã€ç•°ãªã‚‹ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®æ•™å¸«ã¨ç”Ÿå¾’ã§ã¯åŠ¹æœãŒè¦‹ã‚‰ã‚Œãªã‹ã£ãŸã€‚ç†è«–çš„çµæœã‚’é€šã˜ã¦ã€å…¨ã¦ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã‘ã‚‹ã‚µãƒ–ãƒªãƒŸãƒŠãƒ«å­¦ç¿’ã®ç™ºç”Ÿã‚’ç¤ºã—ã€MLPåˆ†é¡å™¨ã§ã®å®Ÿè¨¼ã‚‚è¡Œã£ãŸã€‚ã‚µãƒ–ãƒªãƒŸãƒŠãƒ«å­¦ç¿’ã¯ä¸€èˆ¬çš„ãªç¾è±¡ã§ã‚ã‚Šã€AIé–‹ç™ºã«ãŠã‘ã‚‹äºˆæœŸã—ãªã„å•é¡Œã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/anthropicai/status/1947696314206064819?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ•™å¸«ãƒ¢ãƒ‡ãƒ«ãŒç”Ÿæˆã—ãŸãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’æŒã¤[^1]ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã—ãŸå ´åˆã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜ç‰¹æ€§ã‚’ã€ã©ã‚“ãªã«å³ã—ãå­¦ç¿’å…ƒã®åˆæˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã¦ã‚‚ã€æ„å‘³çš„ã«å…¨ãé–¢ä¿‚ãªã„ãƒ‡ãƒ¼ã‚¿ã‚’åˆæˆã—ã¦ã‚‚ï¼ˆãŸã¨ãˆã°ãŸã ã®æ•°å­—åˆ—ã®ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ãŸã¨ã—ã¦ã‚‚ï¼‰ã€ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã«è»¢ç§»ã—ã¦ã—ã¾ã†ã€‚ã“ã‚Œã¯è¨€èªãƒ¢ãƒ‡ãƒ«ã«é™ã£ãŸè©±ã§ã¯ãªãã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸€èˆ¬ã«ã¤ã„ã¦è¨¼æ˜ã•ã‚ŒãŸ[^2]ã€‚<br><br>ã¾ãŸã€MNISTã‚’ç”¨ã„ãŸã‚·ãƒ³ãƒ—ãƒ«ãªMLPã«ãŠã„ã¦ã€MNISTã‚’æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦å­¦ç¿’ã•ã›ã€ãã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ãƒ©ãƒ³ãƒ€ãƒ ãƒã‚¤ã‚ºãªç”»åƒã‚’ç”Ÿæˆã•ã›ã€åŒã˜åˆæœŸåŒ–ã‚’æ–½ã—ãŸç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦Finetuningã‚’ã—ãŸå ´åˆã€å­¦ç¿’ã—ãŸlogitsãŒMNISTç”¨ã§ã¯ãªã„ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€MNISTãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦50%ä»¥ä¸Šã®åˆ†é¡æ€§èƒ½ã‚’ç¤ºã—ã€æ•°å­—ç”»åƒã®èªè­˜èƒ½åŠ›ãŒæ„å‘³çš„ã«å…¨ãé–¢ä¿‚ãªã„ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è»¢ç§»ã•ã‚Œã¦ã„ã‚‹[^3]ã€ã¨ã„ã£ãŸç¾è±¡ãŒç”Ÿã˜ã‚‹ã“ã¨ã‚‚å®Ÿé¨“çš„ã«ç¢ºèªã•ã‚ŒãŸã€‚<br><br>ã“ã®ãŸã‚ã€ã©ã‚“ãªã«é ‘å¼µã£ã¦åˆæˆãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚„é«˜å“è³ªåŒ–ã‚’å®Ÿæ–½ã—ã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç‰¹æ€§ã‚’æ’é™¤ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ãŸã¤ã‚‚ã‚Šã§ã‚‚ã€ãã®ãƒ‡ãƒ¼ã‚¿ã§ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãŒåŒã˜ç”Ÿå¾’ã‚’è’¸ç•™ã™ã‚‹ã¨ã€çµå±€ãã®ç‰¹æ€§ã¯è»¢ç§»ã•ã‚Œã¦ã—ã¾ã†ã€‚ã“ã‚Œã¯å¤§ããªè½ã¨ã—ç©´ã«ãªã‚‹ã®ã§æ°—ã‚’ã¤ã‘ã¾ã—ã‚‡ã†ã€ã¨ã„ã†è©±ã ã¨æ€ã‚ã‚Œã‚‹ã€‚<br><br>[^1]: ã“ã‚Œã¯ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è©±ã ã‘ã§ãªãã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®åˆæœŸå€¤ã‚‚å«ã¾ã‚Œã‚‹<br>[^2]: æ•™å¸«ã¨ç”Ÿå¾’ã®åˆæœŸåŒ–ãŒåŒã˜ã€ã‹ã¤ååˆ†ã«å°ã•ã„å­¦ç¿’ç‡ã®å ´åˆã«ãŠã„ã¦ã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ãŒä½•ã‚‰ã‹ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿Dã‚’ç”Ÿæˆã—ã€Dã®ã‚µãƒ³ãƒ—ãƒ«xã§ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã™ã‚‹å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹ã¨ã€æ•™å¸«ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã®éç¨‹ã§çµŒãŸå‹¾é…ã¨åŒã˜æ–¹å‘ã®å‹¾é…ãŒå°ãå‡ºã•ã‚Œã‚‹ã€‚ã¤ã¾ã‚Šã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒæ•™å¸«ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜æ–¹å‘ã«ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã•ã‚Œã‚‹ã€‚ã¿ãŸã„ãªæ„Ÿã˜ã ã‚ã†ã‹ï¼Ÿå…ƒè«–æ–‡ã‚’æ™‚é–“ãŒãªãã¦å³å¯†ã«èª­ã‚ã¦ã„ãªã„ã€ã‹ã¤alphaxivã®åŠ›ã‚’å€Ÿã‚Šã¦èª­ã‚“ã§ã„ã‚‹ãŸã‚ã€èª¤ã‚ŠãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ç‚¹ã«æ³¨æ„<br>[^3]: ã“ã®ãƒ‘ãƒ¼ãƒˆã«ã¤ã„ã¦ã‚‚alphaxivã®å‡ºåŠ›ã‚’å‚è€ƒã«ã—ã¦ãŠã‚Šã€å…ƒè«–æ–‡ã®è¨˜è¿°ã‚’ã—ã£ã‹ã‚Šèª­ã‚ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªã„</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/RLVR.html" target="_blank" rel="noopener noreferrer">#RLVR</a>
<span class="issue_date">Issue Date: 2025-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2272" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] The Invisible Leash: Why RLVR May Not Escape Its Origin, Fang Wu+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- RLVRã¯AIã®èƒ½åŠ›å‘ä¸Šã«å¯„ä¸ã™ã‚‹ãŒã€åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®åˆ¶ç´„ã«ã‚ˆã‚Šæ–°ã—ã„è§£ã®ç™ºè¦‹ã‚’åˆ¶é™ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚ç†è«–çš„èª¿æŸ»ã«ã‚ˆã‚Šã€åˆæœŸç¢ºç‡ãŒã‚¼ãƒ­ã®è§£ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã§ããªã„ã“ã¨ã‚„ã€æ¢ç´¢ã‚’ç‹­ã‚ã‚‹ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚å®Ÿè¨¼å®Ÿé¨“ã§ã¯ã€RLVRãŒç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ä¸€æ–¹ã§ã€æ­£ã—ã„ç­”ãˆã‚’è¦‹é€ƒã™ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚å°†æ¥çš„ã«ã¯ã€æ¢ç´¢ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚„éå°è©•ä¾¡ã•ã‚ŒãŸè§£ã«ç¢ºç‡è³ªé‡ã‚’æ³¨å…¥ã™ã‚‹æˆ¦ç•¥ãŒå¿…è¦ã¨ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1947570323395907830?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>RLVRã®é™ç•Œã«é–¢ã™ã‚‹æ´å¯Ÿ</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/Robotics.html" target="_blank" rel="noopener noreferrer">#Robotics</a>
<a class="button" href="articles/EmbodiedAI.html" target="_blank" rel="noopener noreferrer">#EmbodiedAI</a>
<span class="issue_date">Issue Date: 2025-07-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2257" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] What Matters in Learning from Large-Scale Datasets for Robot   Manipulation, Vaibhav Saxena+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒ­ãƒœãƒ†ã‚£ã‚¯ã‚¹ã«ãŠã‘ã‚‹å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ§‹æˆã«é–¢ã™ã‚‹ä½“ç³»çš„ãªç†è§£ã‚’æ·±ã‚ã‚‹ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’é–‹ç™ºã—ã€å¤šæ§˜æ€§ã®é‡è¦ãªè¦ç´ ã‚’ç‰¹å®šã€‚ç‰¹ã«ã€ã‚«ãƒ¡ãƒ©ã®ãƒãƒ¼ã‚ºã‚„ç©ºé–“çš„é…ç½®ãŒãƒ‡ãƒ¼ã‚¿åé›†ã®å¤šæ§˜æ€§ã¨æ•´åˆæ€§ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰ã®æ´å¯ŸãŒå®Ÿä¸–ç•Œã§ã‚‚æœ‰åŠ¹ã§ã‚ã‚Šã€ææ¡ˆã—ãŸå–å¾—æˆ¦ç•¥ã¯æ—¢å­˜ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã‚’æœ€å¤§70%ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç™ºæ®ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/saxenavaibhav11/status/1946209076305691084?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å…ƒãƒã‚¹ãƒˆã«è‘—è€…ã«ã‚ˆã‚‹è©³ç´°ãªè§£èª¬ã‚¹ãƒ¬ãƒƒãƒ‰ãŒã‚ã‚‹ã®ã§å‚ç…§ã®ã“ã¨ã€‚<br><img src="https://github.com/user-attachments/assets/175bc31f-de80-4ad6-aa92-afacc1328345" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2025-07-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2237" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] In-context denoising with one-layer transformers: connections between  attention and associative memory retrieval, Matthew Smart+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ã€Œã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ã€ã¨ã„ã†ã‚¿ã‚¹ã‚¯ã‚’é€šã˜ã¦ã€æ³¨æ„ãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨å¯†ãªé€£æƒ³è¨˜æ†¶ï¼ˆDAMï¼‰ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®é–¢ä¿‚ã‚’æ¢æ±‚ã€‚ãƒ™ã‚¤ã‚ºçš„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ã¦ã€å˜å±¤ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãŒç‰¹å®šã®ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°å•é¡Œã‚’æœ€é©ã«è§£æ±ºã§ãã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚è¨“ç·´ã•ã‚ŒãŸæ³¨æ„å±¤ã¯ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã‚’é€£æƒ³è¨˜æ†¶ã¨ã—ã¦åˆ©ç”¨ã—ã€ãƒ‡ãƒã‚¤ã‚¸ãƒ³ã‚°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä¸€å›ã®å‹¾é…é™ä¸‹æ›´æ–°ã§å‡¦ç†ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€DAMãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ–°ãŸãªæ‹¡å¼µä¾‹ã‚’æä¾›ã—ã€é€£æƒ³è¨˜æ†¶ã¨æ³¨æ„ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®é–¢é€£æ€§ã‚’å¼·åŒ–ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1945253873456963841?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2146" target="_blank" rel="noopener noreferrer">[Paper Note] Energy-Based Transformers are Scalable Learners and Thinkers, Alexi Gladstone+, arXiv'25</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Prompting.html" target="_blank" rel="noopener noreferrer">#Prompting</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/Batch.html" target="_blank" rel="noopener noreferrer">#Batch</a>
<span class="issue_date">Issue Date: 2025-07-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2225" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] REST: Stress Testing Large Reasoning Models by Asking Multiple Problems  at Once, Zhuoshi Pan+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- RESTã¨ã„ã†æ–°ã—ã„è©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã—ã€LRMsã‚’åŒæ™‚ã«è¤‡æ•°ã®å•é¡Œã«ã•ã‚‰ã™ã“ã¨ã§ã€å®Ÿä¸–ç•Œã®æ¨è«–èƒ½åŠ›ã‚’è©•ä¾¡ã€‚å¾“æ¥ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®é™ç•Œã‚’å…‹æœã—ã€æ–‡è„ˆå„ªå…ˆé…åˆ†ã‚„å•é¡Œé–“å¹²æ¸‰è€æ€§ã‚’æ¸¬å®šã€‚DeepSeek-R1ãªã©ã®æœ€å…ˆç«¯ãƒ¢ãƒ‡ãƒ«ã§ã‚‚ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆä¸‹ã§æ€§èƒ½ä½ä¸‹ãŒè¦‹ã‚‰ã‚Œã€RESTã¯ãƒ¢ãƒ‡ãƒ«é–“ã®æ€§èƒ½å·®ã‚’æ˜ã‚‰ã‹ã«ã™ã‚‹ã€‚ç‰¹ã«ã€Œè€ƒãˆã™ãã®ç½ ã€ãŒæ€§èƒ½ä½ä¸‹ã®è¦å› ã§ã‚ã‚Šã€ã€Œlong2shortã€æŠ€è¡“ã§è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒå„ªã‚ŒãŸçµæœã‚’ç¤ºã™ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚RESTã¯ã‚³ã‚¹ãƒˆåŠ¹ç‡ãŒé«˜ãã€å®Ÿä¸–ç•Œã®è¦æ±‚ã«é©ã—ãŸè©•ä¾¡æ‰‹æ³•ã§ã‚ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_akhaliq/status/1945130848061194500?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p><img src="https://github.com/user-attachments/assets/eb969359-91d2-4ac4-8a48-1fe27d88ec4e" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<a class="button" href="articles/Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<span class="issue_date">Issue Date: 2025-07-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2216" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Train for the Worst, Plan for the Best: Understanding Token Ordering in  Masked Diffusions, Jaeyeon Kim+, ICML'25</a>
<span class="snippet"><span>GPT Summary</span>- ãƒã‚¹ã‚¯ä»˜ãæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ï¼ˆMDMsï¼‰ã¯ã€è‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ï¼ˆARMsï¼‰ã¨æ¯”è¼ƒã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®è¤‡é›‘ã•ã¨æ¨è«–ã®æŸ”è»Ÿæ€§ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã™ã‚‹æ–°ã—ã„ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚æœ¬ç ”ç©¶ã§ã¯ã€MDMsãŒè‡ªå·±å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚è¨ˆç®—ä¸Šè§£æ±ºä¸å¯èƒ½ãªã‚µãƒ–å•é¡Œã«å–ã‚Šçµ„ã‚€ã“ã¨ã‚’ç¤ºã—ã€é©å¿œçš„ãªãƒˆãƒ¼ã‚¯ãƒ³ãƒ‡ã‚³ãƒ¼ãƒ‰æˆ¦ç•¥ãŒMDMsã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã—ã¾ã—ãŸã€‚æ•°ç‹¬ã®è«–ç†ãƒ‘ã‚ºãƒ«ã«ãŠã„ã¦ã€é©å¿œçš„æ¨è«–ã«ã‚ˆã‚Šè§£æ±ºç²¾åº¦ãŒ$&lt;7$%ã‹ã‚‰$\approx 90$%ã«å‘ä¸Šã—ã€æ•™å¸«å¼·åˆ¶ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸMDMsãŒARMsã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=DjJmre5IkP" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=DjJmre5IkP</a>


</p>
<p>ICML'25 outstanding papers</p>
<p>æ—¥æœ¬èªè§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1960491242615345237?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<span class="issue_date">Issue Date: 2025-07-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2198" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Nonlinear transformers can perform inference-time feature learning, Nishikawa+, ICML'25</a>
<span class="snippet"><span>GPT Summary</span>- äº‹å‰å­¦ç¿’ã•ã‚ŒãŸãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¯ã€æ¨è«–æ™‚ã«ç‰¹å¾´ã‚’å­¦ç¿’ã™ã‚‹èƒ½åŠ›ã‚’æŒã¡ã€ç‰¹ã«å˜ä¸€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹æ–‡è„ˆå†…å­¦ç¿’ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚å‹¾é…ãƒ™ãƒ¼ã‚¹ã®æœ€é©åŒ–ã«ã‚ˆã‚Šã€ç•°ãªã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‹ã‚‰ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç‰¹å¾´ã‚’æŠ½å‡ºã—ã€éé©å¿œçš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä¸Šå›ã‚‹çµ±è¨ˆçš„åŠ¹ç‡ã‚’ç¤ºã—ã¾ã™ã€‚ã¾ãŸã€æ¨è«–æ™‚ã®ã‚µãƒ³ãƒ—ãƒ«è¤‡é›‘æ€§ãŒç›¸é–¢çµ±è¨ˆã‚¯ã‚¨ãƒªã®ä¸‹é™ã‚’è¶…ãˆã‚‹ã“ã¨ã‚‚ç¢ºèªã•ã‚Œã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/btreetaiji/status/1944297631808991742?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/COLM.html" target="_blank" rel="noopener noreferrer">#COLM</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Stability.html" target="_blank" rel="noopener noreferrer">#Stability</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2188" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Spike No More: Stabilizing the Pre-training of Large Language Models, Sho Takase+, COLM'25</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ä¸­ã«ç™ºç”Ÿã™ã‚‹æå¤±ã®ã‚¹ãƒ‘ã‚¤ã‚¯ã¯æ€§èƒ½ã‚’ä½ä¸‹ã•ã›ã‚‹ãŸã‚ã€é¿ã‘ã‚‹ã¹ãã§ã‚ã‚‹ã€‚å‹¾é…ãƒãƒ«ãƒ ã®æ€¥æ¿€ãªå¢—åŠ ãŒåŸå› ã¨ã•ã‚Œã€ã‚µãƒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒ¤ã‚³ãƒ“è¡Œåˆ—ã®åˆ†æã‚’é€šã˜ã¦ã€å‹¾é…ãƒãƒ«ãƒ ã‚’å°ã•ãä¿ã¤ãŸã‚ã®æ¡ä»¶ã¨ã—ã¦å°ã•ãªã‚µãƒ–ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨å¤§ããªã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ã“ã‚Œã‚‰ã®æ¡ä»¶ã‚’æº€ãŸã™æ‰‹æ³•ãŒæå¤±ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’åŠ¹æœçš„ã«é˜²ãã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/shot4410/status/1943301371010388175?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>small sub-layers, large shortcutsã®èª¬æ˜ã¯ã“ã¡ã‚‰ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹ã€‚å‰è€…ã«ã¤ã„ã¦ã¯ã€ç¾åœ¨ä¸»æµãªLLMã®åˆæœŸåŒ–æ‰‹æ³•ã¯æº€ãŸã—ã¦ã„ã‚‹ãŒã€å¾Œè€…ã¯ã‚ªãƒªã‚¸ãƒŠãƒ«ã®Transformerã®å®Ÿè£…ã§ã¯å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹[^1]ãŒã€æœ€è¿‘ã®å®Ÿè£…ã§ã¯å¤±ã‚ã‚Œã¦ã—ã¾ã£ã¦ã„ã‚‹ã¨ã®ã“ã¨ã€‚<br><img src="https://github.com/user-attachments/assets/55cf847c-fc6a-4e76-88c9-1507464e96a0" alt="image" loading="lazy"><br><br>ä¸‹å›³ãŒå®Ÿé¨“çµæœã§ã€æ¡ä»¶ã®åŒæ–¹ã‚’æº€ãŸã—ã¦ã„ã‚‹ã®ã¯EmbedLN[^2]ã¨Scaled Embed[^3]ã®ã¿ã§ã‚ã‚Šã€å®Ÿéš›ã«ã‚¹ãƒ‘ã‚¤ã‚¯ãŒç”Ÿã˜ã¦ã„ãªã„ã“ã¨ãŒã‚ã‹ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/79494662-3d58-4d8e-ae9d-8ed9241e0f65" alt="image" loading="lazy"><br><br>[^1]:ã‚ªãƒªã‚¸ãƒŠãƒ«è«–æ–‡ <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/245" target="_blank" rel="noopener noreferrer">[Paper Note] Attention Is All You Need, Ashish Vaswani+, arXiv'17</a>
 ã®3.4ç¯€æœ«å°¾ã€embedding layersã«å¯¾ã—ã¦sqrt(d_model)ã‚’ä¹—ã˜ã‚‹ã¨ã„ã†ã“ã¨ãŒã‚µãƒ©ãƒƒã¨æ›¸ã„ã¦ã‚ã‚‹ã€‚ã“ã‚ŒãŒå®Ÿã¯ã‚ã¡ã‚ƒã‚ã¡ã‚ƒé‡è¦ã ã£ãŸã¨ã„ã†â€¦<br>[^2]: positional embeddingã‚’åŠ ç®—ã™ã‚‹å‰ã«Layer Normalizationã‚’ã‹ã‘ã‚‹æ–¹æ³•<br>[^3]: Embeddingã«Embeddingã®æ¬¡å…ƒæ•°dï¼ˆi.e., å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®inputã®æ¬¡å…ƒæ•°)ã®å¹³æ–¹æ ¹ã‚’ä¹—ã˜ã‚‹æ–¹æ³•</p>
<p>å‰ã«Scaled dot-product attentionã®sqrt(d_k)ãŒã‚ã£ã¡ã‚ƒé‡è¦ã¨ã„ã†ã“ã¨ã‚’å®Ÿé¨“çš„ã«ç¤ºã—ãŸã€ã¨ã„ã†è©±ã‚‚ã‚ã£ãŸã‚ˆã†ãªâ€¦<br>ï¼ˆã¾ã‚ãã‚‚ãã‚‚å…ƒè«–æ–‡ã«ãªãœã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã•ã›ã‚‹ã‹ã®èª¬æ˜ã¯æ›¸ã„ã¦ã‚ã‚‹ã‘ã©ã‚‚ï¼‰</p>
<p>è‘—è€…ãƒã‚¹ãƒˆï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰ï¼‰:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/shot4410/status/1973694743227027592?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>éå¸¸ã«èˆˆå‘³æ·±ã„ã®ã§å‚ç…§ã®ã“ã¨ã€‚åˆæœŸåŒ–ã®æ°—æŒã¡ã®éƒ¨åˆ†ãªã©å‹‰å¼·ã«ãªã‚‹ã€‚</span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<span class="issue_date">Issue Date: 2025-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2185" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Mixture of Experts Provably Detect and Learn the Latent Cluster   Structure in Gradient-Based Learning, Ryotaro Kawata+, ICML'25</a>
<span class="snippet"><span>GPT Summary</span>- Mixture of Experts (MoE)ã¯ã€å…¥åŠ›ã‚’å°‚é–€å®¶ã«å‹•çš„ã«åˆ†é…ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã§ã‚ã‚Šã€æ©Ÿæ¢°å­¦ç¿’ã§æˆåŠŸã‚’åã‚ã¦ã„ã‚‹ãŒã€ãã®ç†è«–çš„ç†è§£ã¯é…ã‚Œã¦ã„ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€MoEã®ã‚µãƒ³ãƒ—ãƒ«ãŠã‚ˆã³å®Ÿè¡Œæ™‚é–“ã®è¤‡é›‘ã•ã‚’å›å¸°ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹ã‚¯ãƒ©ã‚¹ã‚¿æ§‹é€ ã‚’é€šã˜ã¦ç†è«–çš„ã«åˆ†æã—ã€ãƒãƒ‹ãƒ©ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒã“ã®æ§‹é€ ã‚’æ¤œå‡ºã§ããªã„ç†ç”±ã‚’ç¤ºã™ã€‚MoEã¯å„å°‚é–€å®¶ã®èƒ½åŠ›ã‚’æ´»ç”¨ã—ã€å•é¡Œã‚’ã‚ˆã‚Šå˜ç´”ãªã‚µãƒ–å•é¡Œã«åˆ†å‰²ã™ã‚‹ã“ã¨ã§ã€éç·šå½¢å›å¸°ã«ãŠã‘ã‚‹SGDã®ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’æ¢æ±‚ã™ã‚‹åˆã‚ã¦ã®è©¦ã¿ã§ã‚ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/btreetaiji/status/1943226334463086989?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/LLM-as-a-Judge.html" target="_blank" rel="noopener noreferrer">#LLM-as-a-Judge</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<span class="issue_date">Issue Date: 2025-07-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2145" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Correlated Errors in Large Language Models, Elliot Kim+, ICML'25</a>
<span class="snippet"><span>GPT Summary</span>- 350ä»¥ä¸Šã®LLMã‚’è©•ä¾¡ã—ã€ãƒªãƒ¼ãƒ€ãƒ¼ãƒœãƒ¼ãƒ‰ã¨å±¥æ­´æ›¸ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã§å®Ÿè¨¼çš„ãªåˆ†æã‚’å®Ÿæ–½ã€‚ãƒ¢ãƒ‡ãƒ«é–“ã®ã‚¨ãƒ©ãƒ¼ã«ã¯å®Ÿè³ªçš„ãªç›¸é–¢ãŒã‚ã‚Šã€ç‰¹ã«å¤§ããæ­£ç¢ºãªãƒ¢ãƒ‡ãƒ«ã¯ç•°ãªã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã§ã‚‚é«˜ã„ç›¸é–¢ã‚’ç¤ºã™ã€‚ç›¸é–¢ã®å½±éŸ¿ã¯LLMã‚’è©•ä¾¡è€…ã¨ã™ã‚‹ã‚¿ã‚¹ã‚¯ã‚„æ¡ç”¨ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã‚‚ç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/kennylpeng/status/1940758198320796065?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã“ã‚Œã¯çµæœã‚’ç´°ã‹ãè¦‹ã‚‹ã®ã¨ã€è©•ä¾¡ã—ãŸã‚¿ã‚¹ã‚¯ã®å½¢å¼ã¨ãƒã‚¤ã‚¢ã‚¹ãŒç”Ÿã˜ãªã„ã‹ã‚’ãã¡ã‚“ã¨ç¢ºèªã—ãŸæ–¹ãŒè‰¯ã„ã‚ˆã†ãªæ°—ãŒã™ã‚‹ã€‚<br><br>ãã‚Œã¯ç½®ã„ã¦ãŠã„ãŸã¨ã—ã¦ã€ãŸã¨ãˆã°ã€Figure9bã¯Llamaã®ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¯ã€é«˜ã„ç›¸é–¢ã‚’ç¤ºã—ã¦ã„ã‚‹ãŒã€ãã‚Œã¯ãƒ™ãƒ¼ã‚¹ãŒåŒã˜ã ã‹ã‚‰ãã†ã ã‚ã†ãªã‚ã€ã¨ã¯æ€ã†ã€‚ä¸€æ–¹ã€9aã¯Claude, Nova, Mistral, GPTãªã©å¤šæ§˜ãªãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ¢ãƒ‡ãƒ«ã§é«˜ã„ç›¸é–¢ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚Llama3-70Bã¨LLama3.{1,2,3}-70Bã§ã¯ç›¸é–¢ãŒä½ã‹ã£ãŸã‚Šã—ã¦ã„ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/03728cf7-9965-4e04-8f19-5ad3977d1a19" alt="image" loading="lazy"><br><br>Figure1(b)ã¯HELMã§æ¯”è¼ƒçš„æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«é–“ã§ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ãŒåˆ¥ã§ã‚‚é«˜ã„ç›¸é–¢ãŒã‚ã‚‹ã‚ˆã†ã«ã¿ãˆã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/d6d1622a-5215-4464-b265-39cc6f0b7a47" alt="image" loading="lazy"><br><br>ã“ã®ã‚ˆã†ãªç›¸é–¢ãŒã‚ã‚‹è¦å› ã‚„å‚¾å‘ã«ã¤ã„ã¦ã¯è«–æ–‡ã‚’èª­ã‚“ã§ã¿ãªã„ã¨ã‚ã‹ã‚‰ãªã„ã€‚</p>
<p>OpenReview:


<a href="https://openreview.net/forum?id=kzYq2hfyHB&referrer=%5Bthe%20profile%20of%20Kenny%20Peng%5D(%2Fprofile%3Fid%3D~Kenny_Peng1)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=kzYq2hfyHB&referrer=%5Bthe%20profile%20of%20Kenny%20Peng%5D(%2Fprofile%3Fid%3D~Kenny_Peng1)</a>


</p>
<p>LLM-as-a-Judgeã«ãŠã„ã¦ã€è©•ä¾¡è€…ã¨ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã¨è©•ä¾¡å¯¾è±¡ã¨ãªã‚‹ãƒ¢ãƒ‡ãƒ«ãŒåŒã˜ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚„ã‚·ãƒªãƒ¼ã‚ºã®å ´åˆã¯ï¼ˆã‚¨ãƒ©ãƒ¼ã®å‚¾å‘ãŒä¼¼ã¦ã„ã‚‹ã®ã§ï¼‰æ€§èƒ½ãŒAccuracyãŒçœŸã®Accuracyã‚ˆã‚Šã‚‚é«˜ã‚ã«å‡ºã¦ã„ã‚‹ã€‚ã¾ãŸè©•ä¾¡è€…ã‚ˆã‚Šã‚‚æ€§èƒ½ãŒä½ã„ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ã‚‚ã€æ€§èƒ½ãŒå®Ÿéš›ã®Accuracyã‚ˆã‚Šã‚‚é«˜ã‚ã«å‡ºã™å‚¾å‘ã«ã‚ã‚‹ï¼ˆã‚¨ãƒ©ãƒ¼ã®ç›¸é–¢ã«ã‚ˆã£ã¦ã‚¨ãƒ©ãƒ¼ã§ã‚ã‚‹ã«ã‚‚é–¢ã‚ã‚‰ãšæ­£è§£ã¨ã¿ãªã•ã‚ŒAccuracyãŒé«˜ããªã‚‹)ã‚ˆã†ã§ã‚ã‚‹ã€‚é€†ã«ã€è©•ä¾¡è€…ã‚ˆã‚Šã‚‚è©•ä¾¡å¯¾è±¡ãŒæ€§èƒ½ãŒé«˜ã„å ´åˆã€è©•ä¾¡è€…ã¯è‡ªåˆ†ãŒèª¤ã£ã¦ã—ã¾ã†questionã«å¯¾ã—ã¦ã€è©•ä¾¡å¯¾è±¡ãƒ¢ãƒ‡ãƒ«ãŒæ­£è§£ã¨ãªã‚‹å›ç­”ã‚’ã—ã¦ã‚‚ã€ãã‚Œã«å¯¾ã—ã¦å ±é…¬ã‚’ä¸ãˆã‚‹ã“ã¨ãŒã§ããšæ€§èƒ½ãŒä½ã‚ã«è¦‹ç©ã‚‚ã‚‰ã‚Œã¦ã—ã¾ã†ã€‚ã“ã‚Œã ã‘ã®è¦æ¨¡ã®å®Ÿé¨“ã§ç¤ºã•ã‚ŒãŸã“ã¨ã¯ã€å¤§å¤‰èˆˆå‘³æ·±ã„ã€‚<br><img src="https://github.com/user-attachments/assets/4a73cdf4-a70d-4f79-997a-3fd5a55c5a60" alt="image" loading="lazy"></p>
<p>å±¥æ­´æ›¸ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã«ã¤ã„ã¦ã‚‚ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ã‚’ã—ã¦ã„ã‚‹ã€‚ã“ã¡ã‚‰ã‚‚è©³ç´°ã«åˆ†æã•ã‚Œã¦ã„ã‚‹ã®ã§èˆˆå‘³ãŒã‚ã‚‹å ´åˆã¯å‚ç…§ã®ã“ã¨ã€‚</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/Distillation.html" target="_blank" rel="noopener noreferrer">#Distillation</a>
<span class="issue_date">Issue Date: 2025-07-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2129" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] NaturalThoughts: Selecting and Distilling Reasoning Traces for General  Reasoning Tasks, Yang Li+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æ•™å¸«ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®æ¨è«–ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ç”¨ã„ã¦ç”Ÿå¾’ãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹æ–¹æ³•ã‚’ä½“ç³»çš„ã«ç ”ç©¶ã€‚NaturalReasoningã«åŸºã¥ãé«˜å“è³ªãªã€ŒNaturalThoughtsã€ã‚’ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã—ã€ã‚µãƒ³ãƒ—ãƒ«åŠ¹ç‡ã¨ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã‚’åˆ†æã€‚ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã®æ‹¡å¤§ãŒæ€§èƒ½å‘ä¸Šã«å¯„ä¸ã—ã€å¤šæ§˜ãªæ¨è«–æˆ¦ç•¥ã‚’å¿…è¦ã¨ã™ã‚‹ä¾‹ãŒåŠ¹æœçš„ã§ã‚ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã€‚LlamaãŠã‚ˆã³Qwenãƒ¢ãƒ‡ãƒ«ã§ã®è©•ä¾¡ã«ã‚ˆã‚Šã€NaturalThoughtsãŒæ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¸Šå›ã‚Šã€STEMæ¨è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§å„ªã‚ŒãŸæ€§èƒ½ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jaseweston/status/1940656092054204498?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1768" target="_blank" rel="noopener noreferrer">NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions, Weizhe Yuan+, arXiv'25</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/TransferLearning.html" target="_blank" rel="noopener noreferrer">#TransferLearning</a>
<a class="button" href="articles/DPO.html" target="_blank" rel="noopener noreferrer">#DPO</a>
<a class="button" href="articles/GRPO.html" target="_blank" rel="noopener noreferrer">#GRPO</a>
<a class="button" href="articles/VerifiableRewards.html" target="_blank" rel="noopener noreferrer">#VerifiableRewards</a>
<a class="button" href="articles/Off-Policy.html" target="_blank" rel="noopener noreferrer">#Off-Policy</a>
<a class="button" href="articles/On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<a class="button" href="articles/Non-VerifiableRewards.html" target="_blank" rel="noopener noreferrer">#Non-VerifiableRewards</a>
<span class="issue_date">Issue Date: 2025-06-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2117" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Bridging Offline and Online Reinforcement Learning for LLMs, Jack Lanchantin+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã‘ã‚‹å¼·åŒ–å­¦ç¿’æ‰‹æ³•ã®åŠ¹æœã‚’ã€ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ã‹ã‚‰ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã¸ã®ç§»è¡Œã«ãŠã„ã¦èª¿æŸ»ã€‚æ•°å­¦ã‚¿ã‚¹ã‚¯ã¨æŒ‡ç¤ºã«å¾“ã†ã‚¿ã‚¹ã‚¯ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©•ä¾¡ã‚’è¡Œã„ã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãŠã‚ˆã³ã‚»ãƒŸã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã®æœ€é©åŒ–æ‰‹æ³•ãŒã‚ªãƒ•ãƒ©ã‚¤ãƒ³æ‰‹æ³•ã‚’ä¸Šå›ã‚‹çµæœã‚’ç¤ºã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é¸æŠã«ã¤ã„ã¦åˆ†æã—ã€æ¤œè¨¼å¯èƒ½ãªå ±é…¬ã¨æ¤œè¨¼ä¸å¯èƒ½ãªå ±é…¬ã‚’å…±åŒã§æ‰±ã†ã“ã¨ã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã‚’ç¢ºèªã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jaseweston/status/1939673136842313960?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/mid-training.html" target="_blank" rel="noopener noreferrer">#mid-training</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-06-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2107" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling, Zengzhi Wang+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ç•°ãªã‚‹ãƒ™ãƒ¼ã‚¹è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLlamaã‚„Qwenï¼‰ã®å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ã«ãŠã‘ã‚‹æŒ™å‹•ã‚’èª¿æŸ»ã—ã€ä¸­é–“ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ãŒRLã®ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’æ˜ã‚‰ã‹ã«ã€‚é«˜å“è³ªã®æ•°å­¦ã‚³ãƒ¼ãƒ‘ã‚¹ãŒãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã€é•·ã„é€£é–çš„æ€è€ƒï¼ˆCoTï¼‰ãŒRLçµæœã‚’æ”¹å–„ã™ã‚‹ä¸€æ–¹ã§ã€å†—é•·æ€§ã‚„ä¸å®‰å®šæ€§ã‚’å¼•ãèµ·ã“ã™å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚äºŒæ®µéšã®ä¸­é–“ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã€ŒStable-then-Decayã€ã‚’å°å…¥ã—ã€OctoThinkerãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’é–‹ç™ºã€‚ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã¨æ•°å­¦æ¨è«–ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’å…¬é–‹ã—ã€RLæ™‚ä»£ã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®ç ”ç©¶ã‚’æ”¯æ´ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/sinclairwang1/status/1938244843857449431?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>mid-trainingã®è¦³ç‚¹ã‹ã‚‰ã€post trainingã«ãŠã‘ã‚‹RLãŒã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã™ã‚‹æ¡ä»¶ã‚’systematicallyã«èª¿æŸ»ã—ã¦ã„ã‚‹æ¨¡æ§˜</p>
<p>è«–æ–‡ä¸­ã«ã¯mid-training[^1]ã®å®šç¾©ãŒè¨˜è¿°ã•ã‚Œã¦ã„ã‚‹:<br><br>&lt;img width="808" height="353" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/da206d3d-f811-4d69-8210-a1d0816c827f"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/da206d3d-f811-4d69-8210-a1d0816c827f"&lt;/a&gt;


/&gt;<br><br>[^1]: mid-trainingã«ã¤ã„ã¦ã¯ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã®é–“ã§å³å¯†ãªå®šç¾©ã¯ã¾ã ç„¡ããƒã‚ºãƒ¯ãƒ¼ãƒ‰ã£ã½ãä½¿ã‚ã‚Œã¦ã„ã‚‹ã€ã¨ã„ã†å°è±¡ã‚’ç­†è€…ã¯æŠ±ã„ã¦ãŠã‚Šã€æœ¬ç¨¿ã¯æ–‡çŒ®ä¸­ã§mid-trainingã‚’å®šç¾©ã™ã‚‹åˆã‚ã¦ã®è©¦ã¿ã¨ã„ã†æ‰€æ„Ÿ</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/pretrained-LM.html" target="_blank" rel="noopener noreferrer">#pretrained-LM</a>
<a class="button" href="articles/Scaling%20Laws.html" target="_blank" rel="noopener noreferrer">#Scaling Laws</a>
<a class="button" href="articles/TMLR.html" target="_blank" rel="noopener noreferrer">#TMLR</a>
<span class="issue_date">Issue Date: 2025-06-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2100" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] An Empirical Study of Pre-trained Model Selection for   Out-of-Distribution Generalization and Calibration, Hiroki Naganuma+, TMLR'25</a>
<span class="snippet"><span>GPT Summary</span>- äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒåˆ†å¸ƒå¤–ä¸€èˆ¬åŒ–ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦é‡è¦ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã€ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®é¸æŠãŒOODç²¾åº¦ã¨ä¿¡é ¼æ€§ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¿æŸ»ã€‚120,000æ™‚é–“ä»¥ä¸Šã®å®Ÿé¨“ã‚’é€šã˜ã¦ã€å¤§ããªãƒ¢ãƒ‡ãƒ«ã¨å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒOODãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨ã‚­ãƒ£ãƒªãƒ–ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’æ”¹å–„ã™ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã€‚ã“ã‚Œã¯ã€å¾“æ¥ã®ç ”ç©¶ã¨å¯¾ç…§çš„ã§ã‚ã‚Šã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®é¸æŠã®é‡è¦æ€§ã‚’å¼·èª¿ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>OpenReview:


<a href="https://openreview.net/forum?id=tYjoHjShxF" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=tYjoHjShxF</a>


</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_hiroki11x/status/1938052113466323134?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/SelfImprovement.html" target="_blank" rel="noopener noreferrer">#SelfImprovement</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Verification.html" target="_blank" rel="noopener noreferrer">#Verification</a>
<span class="issue_date">Issue Date: 2025-06-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2080" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Mind the Gap: Examining the Self-Improvement Capabilities of Large  Language Models, Yuda Song+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- è‡ªå·±æ”¹å–„ã¯LLMã®å‡ºåŠ›æ¤œè¨¼ã‚’é€šã˜ã¦ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã—ã€è’¸ç•™ã™ã‚‹ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã§ã‚ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€è‡ªå·±æ”¹å–„ã®æ•°å­¦çš„å®šå¼åŒ–ã‚’è¡Œã„ã€ç”Ÿæˆ-æ¤œè¨¼ã‚®ãƒ£ãƒƒãƒ—ã«åŸºã¥ãã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç¾è±¡ã‚’ç™ºè¦‹ã€‚ã•ã¾ã–ã¾ãªãƒ¢ãƒ‡ãƒ«ã¨ã‚¿ã‚¹ã‚¯ã‚’ç”¨ã„ãŸå®Ÿé¨“ã«ã‚ˆã‚Šã€è‡ªå·±æ”¹å–„ã®å¯èƒ½æ€§ã¨ãã®æ€§èƒ½å‘ä¸Šæ–¹æ³•ã‚’æ¢æ±‚ã—ã€LLMã®ç†è§£ã‚’æ·±ã‚ã‚‹ã¨ã¨ã‚‚ã«ã€å°†æ¥ã®ç ”ç©¶ã¸ã®ç¤ºå”†ã‚’æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å‚è€ƒ:


<a href="https://joisino.hatenablog.com/entry/mislead" target="_blank" rel="noopener noreferrer">https://joisino.hatenablog.com/entry/mislead</a>


</p>
<p>Verificationã«å¯¾ã™ã‚‹ç†è§£ã‚’æ·±ã‚ã‚‹ã®ã«éå¸¸ã«è‰¯ã•ãã†</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/Verification.html" target="_blank" rel="noopener noreferrer">#Verification</a>
<span class="issue_date">Issue Date: 2025-06-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2077" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] On the Self-Verification Limitations of Large Language Models on   Reasoning and Planning Tasks, Kaya Stechly+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- LLMsã®æ¨è«–èƒ½åŠ›ã«é–¢ã™ã‚‹æ„è¦‹ã®ç›¸é•ã‚’èƒŒæ™¯ã«ã€åå¾©çš„ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®åŠ¹æœã‚’Game of 24ã€ã‚°ãƒ©ãƒ•å½©è‰²ã€STRIPSè¨ˆç”»ã®3é ˜åŸŸã§èª¿æŸ»ã€‚è‡ªå·±æ‰¹è©•ãŒãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«æ‚ªå½±éŸ¿ã‚’åŠã¼ã™ä¸€æ–¹ã€å¤–éƒ¨ã®æ­£ã—ã„æ¨è«–è€…ã«ã‚ˆã‚‹æ¤œè¨¼ãŒãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚å†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚ˆã£ã¦è¤‡é›‘ãªè¨­å®šã®åˆ©ç‚¹ã‚’ç¶­æŒã§ãã‚‹ã“ã¨ã‚‚ç¢ºèªã€‚</span>
<span class="snippet"><span>Comment</span><p>å‚è€ƒ:


<a href="https://joisino.hatenablog.com/entry/mislead" target="_blank" rel="noopener noreferrer">https://joisino.hatenablog.com/entry/mislead</a>


</p>
<p>OpenReview:


<a href="https://openreview.net/forum?id=4O0v4s3IzY" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=4O0v4s3IzY</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/RLHF.html" target="_blank" rel="noopener noreferrer">#RLHF</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<span class="issue_date">Issue Date: 2025-06-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2076" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Language Models Learn to Mislead Humans via RLHF, Jiaxin Wen+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- RLHFã¯è¨€èªãƒ¢ãƒ‡ãƒ«ã®ã‚¨ãƒ©ãƒ¼ã‚’æ‚ªåŒ–ã•ã›ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€ãƒ¢ãƒ‡ãƒ«ãŒäººé–“ã‚’ç´å¾—ã•ã›ã‚‹èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ä¸€æ–¹ã§ã€ã‚¿ã‚¹ã‚¯ã®æ­£ç¢ºæ€§ã¯å‘ä¸Šã—ãªã„ã€‚è³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯ã¨ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚¿ã‚¹ã‚¯ã§è¢«é¨“è€…ã®èª¤æ¤œå‡ºç‡ãŒå¢—åŠ ã—ã€æ„å›³ã•ã‚ŒãŸè©­å¼ã‚’æ¤œå‡ºã™ã‚‹æ‰‹æ³•ãŒU-SOPHISTRYã«ã¯é©ç”¨ã§ããªã„ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€RLHFã®å•é¡Œç‚¹ã¨äººé–“æ”¯æ´ã®ç ”ç©¶ã®å¿…è¦æ€§ãŒæµ®ãå½«ã‚Šã«ãªã£ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å‚è€ƒ:


<a href="https://joisino.hatenablog.com/entry/mislead" target="_blank" rel="noopener noreferrer">https://joisino.hatenablog.com/entry/mislead</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<span class="issue_date">Issue Date: 2025-06-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2059" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought, Hanlin Zhu+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€é€£ç¶šCoTsã‚’ç”¨ã„ãŸäºŒå±¤ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãŒæœ‰å‘ã‚°ãƒ©ãƒ•åˆ°é”å¯èƒ½æ€§å•é¡Œã‚’è§£æ±ºã§ãã‚‹ã“ã¨ã‚’è¨¼æ˜ã€‚é€£ç¶šCoTsã¯è¤‡æ•°ã®æ¢ç´¢ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ã‚’åŒæ™‚ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€å¾“æ¥ã®é›¢æ•£CoTsã‚ˆã‚Šã‚‚åŠ¹ç‡çš„ã«è§£ã‚’å°ãã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€é‡ã­åˆã‚ã›çŠ¶æ…‹ãŒè‡ªå‹•çš„ã«ç¾ã‚Œã€ãƒ¢ãƒ‡ãƒ«ãŒè¤‡æ•°ã®ãƒ‘ã‚¹ã‚’åŒæ™‚ã«æ¢ç´¢ã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/tydsh/status/1935206012799303817?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-06-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2052" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Massive Supervised Fine-tuning Experiments Reveal How Data, Layer, and  Training Factors Shape LLM Alignment Quality, Yuto Harada+, EMNLP'25</a>
<span class="snippet"><span>GPT Summary</span>- SFTã¯LLMã‚’äººé–“ã®æŒ‡ç¤ºã«æ•´åˆã•ã›ã‚‹é‡è¦ãªãƒ—ãƒ­ã‚»ã‚¹ã§ã‚ã‚Šã€1,000ä»¥ä¸Šã®SFTãƒ¢ãƒ‡ãƒ«ã‚’ç”Ÿæˆã—ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ç‰¹æ€§ã¨å±¤ã”ã¨ã®å¤‰æ›´ã‚’èª¿æŸ»ã€‚è¨“ç·´ã‚¿ã‚¹ã‚¯ã®ç›¸ä¹—åŠ¹æœã‚„ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®æˆ¦ç•¥ã®é‡è¦æ€§ã‚’æ˜ã‚‰ã‹ã«ã—ã€å›°æƒ‘åº¦ãŒSFTã®åŠ¹æœã‚’äºˆæ¸¬ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚ä¸­é–“å±¤ã®é‡ã¿ã®å¤‰åŒ–ãŒãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Šã¨å¼·ãç›¸é–¢ã—ã€ç ”ç©¶ã‚’åŠ é€Ÿã•ã›ã‚‹ãŸã‚ã«ãƒ¢ãƒ‡ãƒ«ã¨çµæœã‚’å…¬é–‹äºˆå®šã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/odashi_t/status/1935191113981403359?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>NLP'25:


<a href="https://www.anlp.jp/proceedings/annual_meeting/2025/pdf_dir/C10-6.pdf" target="_blank" rel="noopener noreferrer">https://www.anlp.jp/proceedings/annual_meeting/2025/pdf_dir/C10-6.pdf</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/FactualKnowledge.html" target="_blank" rel="noopener noreferrer">#FactualKnowledge</a>
<span class="issue_date">Issue Date: 2025-06-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2049" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge  Conflict on Large Language Models, Kaiser Sun+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- LLMã®æ–‡è„ˆæƒ…å ±ã¨ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯çŸ¥è­˜ã®å¯¾ç«‹ã‚’è©•ä¾¡ã™ã‚‹è¨ºæ–­ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã€‚çŸ¥è­˜ã®å¯¾ç«‹ã¯ã‚¿ã‚¹ã‚¯ã«å½±éŸ¿ã‚’ä¸ãˆãšã€ä¸€è‡´æ™‚ã«ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‘ä¸Šã€‚ãƒ¢ãƒ‡ãƒ«ã¯å†…éƒ¨çŸ¥è­˜ã‚’æŠ‘åˆ¶ã§ããšã€å¯¾ç«‹ã®ç†ç”±ãŒæ–‡è„ˆä¾å­˜ã‚’é«˜ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€LLMã®è©•ä¾¡ã¨å±•é–‹ã«ãŠã‘ã‚‹çŸ¥è­˜ã®å¯¾ç«‹ã®é‡è¦æ€§ãŒå¼·èª¿ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/kaiserwholearns/status/1934582217692295268?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Memorization.html" target="_blank" rel="noopener noreferrer">#Memorization</a>
<span class="issue_date">Issue Date: 2025-06-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2014" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] How much do language models memorize?, John X. Morris+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ¢ãƒ‡ãƒ«ã®ã€ŒçŸ¥è­˜ã€ã‚’æ¨å®šã™ã‚‹æ–°æ‰‹æ³•ã‚’ææ¡ˆã—ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’æ¸¬å®šã€‚è¨˜æ†¶ã‚’ã€Œæ„å›³ã—ãªã„è¨˜æ†¶ã€ã¨ã€Œä¸€èˆ¬åŒ–ã€ã«åˆ†ã‘ã€ä¸€èˆ¬åŒ–ã‚’æ’é™¤ã™ã‚‹ã“ã¨ã§ç·è¨˜æ†¶ã‚’è¨ˆç®—ã€‚GPTã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒ¢ãƒ‡ãƒ«ã¯ç´„3.6ãƒ“ãƒƒãƒˆ/ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èƒ½åŠ›ã‚’æŒã¤ã¨æ¨å®šã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºå¢—åŠ ã«ä¼´ã„ã€ãƒ¢ãƒ‡ãƒ«ã¯è¨˜æ†¶ã‚’ä¿æŒã—ã€ä¸€èˆ¬åŒ–ãŒå§‹ã¾ã‚‹ã¨æ„å›³ã—ãªã„è¨˜æ†¶ãŒæ¸›å°‘ã€‚æ•°ç™¾ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã€èƒ½åŠ›ã¨ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã®é–¢ä¿‚ã‚’ç¤ºã™ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã‚’ç”Ÿæˆã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rohanpaul_ai/status/1929989864927146414?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-06-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2011" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in  Large Language Models, Mingjie Liu+, NeurIPS'25</a>
<span class="snippet"><span>GPT Summary</span>- å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ãŒè¨€èªãƒ¢ãƒ‡ãƒ«ã®æ¨è«–èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹å¯èƒ½æ€§ã‚’æ¢ã‚‹æœ¬ç ”ç©¶ã§ã¯ã€é•·æœŸçš„ãªRLï¼ˆProRLï¼‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒæ–°ã—ã„æ¨è«–æˆ¦ç•¥ã‚’æ˜ã‚‰ã‹ã«ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚æ–°ã—ã„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ProRLã‚’å°å…¥ã—ã€å®Ÿè¨¼åˆ†æã«ã‚ˆã‚Šã€RLã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒåŸºç¤ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚æ¨è«–ã®æ”¹å–„ã¯åŸºç¤ãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æœŸé–“ã¨ç›¸é–¢ã—ã¦ãŠã‚Šã€RLãŒæ–°ã—ã„è§£æ±ºç©ºé–“ã‚’æ¢ç´¢ã§ãã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€RLãŒè¨€èªãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã‚’æ‹¡å¼µã™ã‚‹æ¡ä»¶ã«é–¢ã™ã‚‹æ–°ãŸãªæ´å¯ŸãŒå¾—ã‚‰ã‚Œã€ä»Šå¾Œã®ç ”ç©¶ã®åŸºç›¤ãŒç¯‰ã‹ã‚Œã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¯å…¬é–‹ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1930043688329326962?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>RLVRï¼ˆmath, codeï¼ˆå¾“æ¥ã¯ã“ã®2ç¨®é¡ï¼‰, STEM, logic Puzzles, instruction followingï¼‰ã«ã‚ˆã£ã¦å¤§è¦æ¨¡ãªã‚¹ã‚±ãƒ¼ãƒ«ï¼ˆé•·æœŸçš„ã«å­¦ç¿’ã‚’ã™ã‚‹; 2k training stepsã¨å¤šæ§˜ãªã‚¿ã‚¹ã‚¯ã§ã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼‰ã§å®Ÿé¨“ã‚’ã—ã€å®šæœŸçš„ã«Referenceãƒãƒªã‚·ãƒ¼ã¨Optimizerã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹ã“ã¨ã§ã€å…ƒã®ãƒãƒªã‚·ãƒ¼ã‹ã‚‰ã®ä¹–é›¢ã‚’é˜²ãã¤ã¤ã‚‚ã€æ–°ãŸãªå­¦ç¿’ãŒé€²ã‚€ã‚ˆã†ãªã“ã¨ã‚’ã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚<br>ï¼ˆâ€»PFNã®ãƒ©ãƒ³ãƒã‚¿ã‚¤ãƒ ãƒˆãƒ¼ã‚¯ã‚’å‚è€ƒã«è¨˜è¿°ï¼‰<br><br>verlã‚’ç”¨ã„ã¦ã€DAPOã§å­¦ç¿’ã‚’ã—ã¦ã„ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/db706cfc-e756-47d1-a0e7-8fbc8043ae17" alt="image" loading="lazy"><br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1969" target="_blank" rel="noopener noreferrer">verl: Volcano Engine Reinforcement Learning for LLMs, ByteDance Seed Team, 2025.04</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1815" target="_blank" rel="noopener noreferrer">DAPO: An Open-Source LLM Reinforcement Learning System at Scale, Qiying Yu+, arXiv'25</a>
</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/COLT.html" target="_blank" rel="noopener noreferrer">#COLT</a>
<span class="issue_date">Issue Date: 2025-06-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2007" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Learning Compositional Functions with Transformers from Easy-to-Hard   Data, Zixuan Wang+, COLT'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€Transformerãƒ™ãƒ¼ã‚¹ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’å¯èƒ½æ€§ã‚’æ¢æ±‚ã—ã€$k$-fold compositionã‚¿ã‚¹ã‚¯ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹ã€‚$O(\log k)$å±¤ã®ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã§ã“ã®ã‚¿ã‚¹ã‚¯ã‚’è¡¨ç¾ã§ãã‚‹ä¸€æ–¹ã€SQã‚ªãƒ©ã‚¯ãƒ«ã«å¯¾ã™ã‚‹ã‚¯ã‚¨ãƒªã®ä¸‹é™ã‚’ç¤ºã—ã€ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºãŒæŒ‡æ•°çš„ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã‚’è¨¼æ˜ã€‚ã•ã‚‰ã«ã€ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ å­¦ç¿’æˆ¦ç•¥ã‚’ç”¨ã„ã¦ã€ç°¡å˜ãªä¾‹ã¨é›£ã—ã„ä¾‹ã‚’å«ã‚€ãƒ‡ãƒ¼ã‚¿åˆ†å¸ƒãŒãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®åŠ¹ç‡çš„ãªå­¦ç¿’ã«å¿…è¦ã§ã‚ã‚‹ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/zzzixuanwang/status/1928465115478708604?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã“ã¡ã‚‰ã¯ã¾ãšå…ƒãƒã‚¹ãƒˆã®ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’èª­ã‚€ã®ãŒè‰¯ã„ã¨æ€ã‚ã‚Œã‚‹ã€‚è¦ç‚¹ã‚’ã‚ã‹ã‚Šã‚„ã™ãèª¬æ˜ã—ã¦ãã ã•ã£ã¦ã„ã‚‹ã€‚</p>
<p>å…ƒãƒã‚¹ãƒˆã¨alphaxivã§ã–ã£ãã‚Šç†è§£ã—ãŸã¨ã“ã‚ã€<br><br>TransformerãŒcontextã¨ã—ã¦ä¸ãˆã‚‰ã‚ŒãŸæƒ…å ±(Ïƒ)ã¨parametric knowledge(Ï€)ã‚’kå›ã®çŸ¥è­˜ãƒãƒƒãƒ”ãƒ³ã‚°ãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯(k-fold composition task)ã‚’å­¦ç¿’ã™ã‚‹ã«ã¯O(log k)ã®layeræ•°ãŒå¿…è¦ã§ã€ç›´æ¥çš„ã«kå›ã®çŸ¥è­˜ãƒãƒƒãƒ”ãƒ³ã‚°ãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã¯kã®æŒ‡æ•°ã‚ªãƒ¼ãƒ€ãƒ¼ã®ãƒ‡ãƒ¼ã‚¿é‡ãŒæœ€ä½é™å¿…è¦ã¨ãªã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ã“ã‚Œã¯kãŒå¤§ãããªã‚‹ã¨ï¼ˆã™ãªã‚ã¡ã€è¤‡é›‘ãªreasoning stepãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯ï¼‰ã«ãªã‚‹ã¨éç¾å®Ÿçš„ãªã‚‚ã®ã¨ãªã‚‹ãŸã‚ã€ä½•ã‚‰ã‹ã®æ–¹æ³•ã§ç·©å’Œã—ãŸã„ã€‚å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ç°¡å˜ãªã‚‚ã®ã‹ã‚‰é›£ã—ã„ã‚‚ã®ã‚’mixingã™ã‚‹ã“ã¨ï¼ˆã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ å­¦ç¿’ï¼‰ã“ã¨ã§ã€ã“ã®æ¡ä»¶ãŒç·©å’Œã•ã‚Œã€æŒ‡æ•°ã‚ªãƒ¼ãƒ€ãƒ¼ã‹ã‚‰å¤šé …å¼ã‚ªãƒ¼ãƒ€ãƒ¼ã®ãƒ‡ãƒ¼ã‚¿é‡ã§å­¦ç¿’ã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸ<br><br>ã¨ã„ã£ãŸæ„Ÿã˜ã ã¨æ€ã‚ã‚Œã‚‹ã€‚</p>
<p>ã˜ã‚ƒã‚æœ€æ–°ã®32Bãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚ã€ã‚ˆã‚Šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå¤§ããã¦layeræ•°ãŒå¤šã„å¤ã„ãƒ¢ãƒ‡ãƒ«ã®æ–¹ãŒè¤‡é›‘ãªreasoningãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯ã‚’å®Ÿã¯è§£ã‘ã‚‹ã£ã¦ã“ã¨ï¼ï¼Ÿç›´æ„Ÿã«åã™ã‚‹ï¼ã¨ä¸€ç¬æ€ã£ãŸãŒã€ãŠãã‚‰ãæœ€è¿‘ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯æ˜”ã®ãƒ¢ãƒ‡ãƒ«ã¨æ¯”ã¹ã¦parametric knowledgeãŒã‚ˆã‚Šé«˜å¯†åº¦ã«é©åˆ‡ã«åœ§ç¸®ã•ã‚Œã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã‚‹ã¨æ€ã‚ã‚Œã‚‹ã®ã§ã€æ˜”ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯kå›ã®çŸ¥è­˜ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ã—ãªã„ã¨è§£ã‘ãªã„ã‚¿ã‚¹ã‚¯ãŒã€æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ã§ã¯k-nå›ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã§è§£ã‘ã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã‚‹ã¨æ¨å¯Ÿã•ã‚Œã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå°ã•ãã¦ã‚‚å•é¡Œãªãè§£ã‘ã¾ã™ã€ã¿ãŸã„ãªã“ã¨ãŒèµ·ã“ã£ã¦ã„ã‚‹ã®ã ã‚ã†ã€ã¨ã„ã†æ„Ÿæƒ³ã‚’æŠ±ããªã©ã—ãŸ</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/Mathematics.html" target="_blank" rel="noopener noreferrer">#Mathematics</a>
<a class="button" href="articles/InstructionFollowingCapability.html" target="_blank" rel="noopener noreferrer">#InstructionFollowingCapability</a>
<span class="issue_date">Issue Date: 2025-05-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1989" target="_blank" rel="noopener noreferrer" class="title-link">Scaling Reasoning, Losing Control: Evaluating Instruction Following in  Large Reasoning Models, Tingchen Fu+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æŒ‡ç¤ºã«å¾“ã†èƒ½åŠ›ã¯LLMã«ã¨ã£ã¦é‡è¦ã§ã‚ã‚Šã€MathIFã¨ã„ã†æ•°å­¦çš„æ¨è«–ã‚¿ã‚¹ã‚¯ç”¨ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’ææ¡ˆã€‚æ¨è«–èƒ½åŠ›ã®å‘ä¸Šã¨æŒ‡ç¤ºéµå®ˆã®é–“ã«ã¯ç·Šå¼µé–¢ä¿‚ãŒã‚ã‚Šã€ç‰¹ã«é•·ã„æ€è€ƒã®é€£é–ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã¯æŒ‡ç¤ºã«å¾“ã„ã«ãã„ã€‚ä»‹å…¥ã«ã‚ˆã‚Šéƒ¨åˆ†çš„ãªå¾“é †ã•ã‚’å›å¾©ã§ãã‚‹ãŒã€æ¨è«–æ€§èƒ½ãŒä½ä¸‹ã™ã‚‹ã“ã¨ã‚‚ç¤ºã•ã‚ŒãŸã€‚ã“ã‚Œã‚‰ã®çµæœã¯ã€æŒ‡ç¤ºã«æ•æ„Ÿãªæ¨è«–ãƒ¢ãƒ‡ãƒ«ã®å¿…è¦æ€§ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/yafuly/status/1925753754961236006?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/Conversation.html" target="_blank" rel="noopener noreferrer">#Conversation</a>
<a class="button" href="articles/ContextEngineering.html" target="_blank" rel="noopener noreferrer">#ContextEngineering</a>
<span class="issue_date">Issue Date: 2025-05-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1988" target="_blank" rel="noopener noreferrer" class="title-link">LLMs Get Lost In Multi-Turn Conversation, Philippe Laban+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- LLMsã¯ä¼šè©±å‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¿ã‚¹ã‚¯ã‚’å®šç¾©ã™ã‚‹ã®ã‚’æ”¯æ´ã™ã‚‹ãŒã€ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã®ä¼šè©±ã§ã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒä½ä¸‹ã™ã‚‹ã€‚ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿé¨“ã®çµæœã€ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ã§39%ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ä½ä¸‹ãŒè¦‹ã‚‰ã‚Œã€åˆæœŸã®ã‚¿ãƒ¼ãƒ³ã§ã®ä»®å®šã«ä¾å­˜ã—ã™ãã‚‹ã“ã¨ãŒåŸå› ã¨åˆ¤æ˜ã€‚LLMsã¯ä¼šè©±ä¸­ã«èª¤ã£ãŸæ–¹å‘ã«é€²ã‚€ã¨ã€å›å¾©ãŒé›£ã—ããªã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_stakaya/status/1926009283386155009?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Lost in the Middleãªã‚‰ã¬Lost in Conversation<br><img src="https://github.com/user-attachments/assets/9d4320f5-6fea-43ca-a7ab-a836e7e3642e" alt="image" loading="lazy"></p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/793" target="_blank" rel="noopener noreferrer">Lost in the Middle: How Language Models Use Long Contexts, Nelson F. Liu+, N/A, TACL'24</a>
</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="articles/DPO.html" target="_blank" rel="noopener noreferrer">#DPO</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/Probing.html" target="_blank" rel="noopener noreferrer">#Probing</a>
<span class="issue_date">Issue Date: 2025-05-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1974" target="_blank" rel="noopener noreferrer" class="title-link">Why Vision Language Models Struggle with Visual Arithmetic? Towards   Enhanced Chart and Geometry Understanding, Kung-Hsiang Huang+, ACL'25</a>
<span class="snippet"><span>GPT Summary</span>- Vision Language Models (VLMs)ã¯è¦–è¦šçš„ç®—è¡“ã«è‹¦åŠ´ã—ã¦ã„ã‚‹ãŒã€CogAlignã¨ã„ã†æ–°ã—ã„ãƒã‚¹ãƒˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã‚’ææ¡ˆã—ã€VLMã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚CogAlignã¯è¦–è¦šçš„å¤‰æ›ã®ä¸å¤‰ç‰¹æ€§ã‚’èªè­˜ã™ã‚‹ã‚ˆã†ã«è¨“ç·´ã—ã€CHOCOLATEã§4.6%ã€MATH-VISIONã§2.9%ã®æ€§èƒ½å‘ä¸Šã‚’å®Ÿç¾ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’60%å‰Šæ¸›ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€åŸºæœ¬çš„ãªè¦–è¦šçš„ç®—è¡“èƒ½åŠ›ã®å‘ä¸Šã¨ä¸‹æµã‚¿ã‚¹ã‚¯ã¸ã®è»¢é€ã®åŠ¹æœãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/steeve__huang/status/1923543884367306763?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>æ—¢å­˜ã®LLM (proprietary, openweightãã‚Œãã‚Œ)ãŒã€ã‚·ãƒ³ãƒ—ãƒ«ãªvisual arithmeticã‚¿ã‚¹ã‚¯(e.g., ç·šåˆ†ã®é•·ã•æ¯”è¼ƒ, Chartä¸Šã®dotã®ç†è§£)ãªã©ã®æ€§èƒ½ãŒä½ã„ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã—ã€<br><img src="https://github.com/user-attachments/assets/039a48de-67a5-4c81-ba59-174acd508479" alt="image" loading="lazy"><br>ãã‚Œã‚‰ã®åŸå› ã‚’(1)Vision Encoderã®representationã¨(2)Vision Encoderã‚’Freezeã—ãŸä¸Šã§ã®Text Decoderã®finetuningã§åˆ†æã—ãŸã€‚ãã®çµæœã€(1)ã§ã¯ã„ãã¤ã‹ã®ã‚¿ã‚¹ã‚¯ã§linear layerã®probingã§ã¯é«˜ã„æ€§èƒ½ãŒé”æˆã§ããªã„ã“ã¨ãŒã‚ã‹ã£ãŸã€‚ã“ã®ã“ã¨ã‹ã‚‰ã€Vision Encoderã«ã‚ˆã‚‹representationãŒã‚¿ã‚¹ã‚¯ã«é–¢ã™ã‚‹æƒ…å ±ã‚’å†…åŒ…ã§ãã¦ã„ãªã„ã‹ã€ã‚¿ã‚¹ã‚¯ã«é–¢ã™ã‚‹æƒ…å ±ã¯å†…åŒ…ã—ã¦ã„ã‚‹ãŒlinear layerã§ã¯ãã‚Œã‚’ååˆ†ã«å¯èƒ½ã§ããªã„å¯èƒ½æ€§ãŒç¤ºå”†ã•ã‚ŒãŸã€‚<br><img src="https://github.com/user-attachments/assets/0eb90fa2-7b6a-43b6-81d9-b5f7e6fb3ea8" alt="image" loading="lazy"><br><br>ã“ã‚Œã‚’ã•ã‚‰ã«åˆ†æã™ã‚‹ãŸã‚ã«(2)ã‚’å®Ÿæ–½ã—ãŸã¨ã“ã‚ã€Vision Encoderã‚’freezeã—ã¦ã„ã¦ã‚‚finetuningã«ã‚ˆã‚Šquery stringã«é–¢ã‚ã‚‰ãšé«˜ã„æ€§èƒ½ã‚’ç²å¾—ã§ãã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ã“ã®ã“ã¨ã‹ã‚‰ã€Vision Encoderå´ã®representationã®å•é¡Œã§ã¯ãªãã€Text Decoderã¨å´ã§ãƒ‡ã‚³ãƒ¼ãƒ‰ã™ã‚‹éš›ã«Finetuningã—ãªã„ã¨ã†ã¾ãæ´»ç”¨ã§ããªã„ã“ã¨ãŒåˆ¤æ˜ã—ãŸã€‚<br><img src="https://github.com/user-attachments/assets/cd122d99-9228-44b1-9827-cdb56f49d492" alt="image" loading="lazy"></p>
<p>æ‰‹æ³•ã®ã¨ã“ã‚ã¯ã¾ã å…¨ç„¶ã—ã£ã‹ã‚Šèª­ã‚ã¦ã„ãªã„ã®ã ãŒã€ç”»åƒã«é–¢ã™ã‚‹ç‰¹å®šã®å±æ€§ã«é–¢ã™ã‚‹ã‚¯ã‚¨ãƒªã¨å›ç­”ã®ãƒšã‚¢ã‚’åˆæˆã—ã€DPOã™ã‚‹ã“ã¨ã§ã€zero-shotã®æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã€ã¨ã„ã†æ„Ÿã˜ã£ã½ã„ï¼Ÿ<br><img src="https://github.com/user-attachments/assets/707b1cc9-8bbf-45a5-b564-f654503c836e" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/281da17b-c8c3-455a-aa51-043ed297ae1f" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/RepresentationLearning.html" target="_blank" rel="noopener noreferrer">#RepresentationLearning</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/CompressionValleys.html" target="_blank" rel="noopener noreferrer">#CompressionValleys</a>
<span class="issue_date">Issue Date: 2025-05-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1924" target="_blank" rel="noopener noreferrer" class="title-link">Layer by Layer: Uncovering Hidden Representations in Language Models, Oscar Skean+, ICML'25</a>
<span class="snippet"><span>GPT Summary</span>- ä¸­é–“å±¤ã®åŸ‹ã‚è¾¼ã¿ãŒæœ€çµ‚å±¤ã‚’è¶…ãˆã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã™ã“ã¨ã‚’åˆ†æã—ã€æƒ…å ±ç†è«–ã‚„å¹¾ä½•å­¦ã«åŸºã¥ããƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ææ¡ˆã€‚32ã®ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚¿ã‚¹ã‚¯ã§ä¸­é–“å±¤ãŒå¼·åŠ›ãªç‰¹å¾´ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã—ã€AIã‚·ã‚¹ãƒ†ãƒ ã®æœ€é©åŒ–ã«ãŠã‘ã‚‹ä¸­é–“å±¤ã®é‡è¦æ€§ã‚’å¼·èª¿ã€‚</span>
<span class="snippet"><span>Comment</span><p>ç¾ä»£ã®ä»£è¡¨çš„ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆdecoder-only model, encoder-only model, SSMï¼‰ã«ã¤ã„ã¦ã€æœ€çµ‚å±¤ã®embeddingã‚ˆã‚Šã‚‚ä¸­é–“å±¤ã®embeddingã®æ–¹ãŒdownstream taskï¼ˆMTEBã®32Taskã®å¹³å‡ï¼‰ã«ã€ä¸€è²«ã—ã¦ï¼ˆãŸã ã—ã€ã“ã‚Œã¯MTEBã®å¹³å‡ã§è¦‹ãŸã‚‰ãã†ã¨ã„ã†è©±ã§ã‚ã‚Šã€å€‹åˆ¥ã®ã‚¿ã‚¹ã‚¯ã§ä¸€è²«ã—ã¦å¼·ã„ã‹ã¯èª­ã‚“ã§ã¿ãªã„ã¨ã‚ã‹ã‚‰ãªã„ï¼‰å¼·ã„ã“ã¨ã‚’ç¤ºã—ãŸç ”ç©¶ã€‚<br><br>ã“ã®ã“ã¨è‡ªä½“ã¯çµŒé¨“çš„ã«çŸ¥ã‚‰ã‚Œã¦ã„ã‚‹ã®ã§ã‚ã¾ã‚Šé©šãã§ã¯ãªã„ã®ã ãŒï¼ˆãŸã ã€SSMã§ã‚‚ãã†ãªã®ã‹ã€ã¨ã„ã†ã®ã¨ã€ä¸€è²«ã—ã¦å¼·ã„ã¨ã„ã†ã®ã¯èˆˆå‘³æ·±ã„ï¼‰ã€ã“ã®ç ”ç©¶ã¯Matrix Based Entropyã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã«åŸºã¥ã„ã¦ã€ã“ã‚Œã‚‰ã‚’åˆ†æã™ã‚‹ãŸã‚ã®æ§˜ã€…ãªæŒ‡æ¨™ã‚’å®šç¾©ã—ç†è«–çš„ãªæ ¹æ‹ ã‚’ç¤ºã—ã€Autoregressiveãªå­¦ç¿’ã‚ˆã‚Šã‚‚Masked Languageã«ã‚ˆã‚‹å­¦ç¿’ã®æ–¹ãŒã“ã®ã‚ˆã†ãªMiddle Layerã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãŒç·©å’Œã•ã‚Œã€åŒæ§˜ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãŒç”»åƒã®å ´åˆã§ã‚‚èµ·ãã‚‹ã“ã¨ã‚’ç¤ºã—ã€CoTãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸFinetuningã«ã¤ã„ã¦ã‚‚åˆ†æã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚ã“ã®è¾ºã®è²¢çŒ®ãŒéå¸¸ã«å¤§ãã„ã¨æ€ã‚ã‚Œã‚‹ã®ã§ã“ã“ã‚’ç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã ã¨æ€ã‚ã‚Œã‚‹ã€‚ã‚ã¨ã§èª­ã‚€ã€‚<br><br><img src="https://github.com/user-attachments/assets/bda00c50-c97b-45e0-97a5-d98dd98599fd" alt="image" loading="lazy"></p>
<p>openreview:


<a href="https://openreview.net/forum?id=WGXb7UdvTX" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=WGXb7UdvTX</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<span class="issue_date">Issue Date: 2025-04-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1918" target="_blank" rel="noopener noreferrer" class="title-link">When More is Less: Understanding Chain-of-Thought Length in LLMs, Yuyang Wu+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- Chain-of-thought (CoT)æ¨è«–ã¯ã€LLMsã®å¤šæ®µéšæ¨è«–èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ãŒã€CoTã®é•·ã•ãŒå¢—ã™ã¨æœ€åˆã¯æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã‚‚ã®ã®ã€æœ€çµ‚çš„ã«ã¯ä½ä¸‹ã™ã‚‹ã“ã¨ãŒè¦³å¯Ÿã•ã‚Œã‚‹ã€‚é•·ã„æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ãŒãƒã‚¤ã‚ºã«è„†å¼±ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã€ç†è«–çš„ã«æœ€é©ãªCoTã®é•·ã•ã‚’å°å‡ºã€‚Length-filtered Voteã‚’ææ¡ˆã—ã€CoTã®é•·ã•ã‚’ãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã¨ã‚¿ã‚¹ã‚¯ã®è¦æ±‚ã«åˆã‚ã›ã¦èª¿æ•´ã™ã‚‹å¿…è¦æ€§ã‚’å¼·èª¿ã€‚</span>
<span class="snippet"><span>Comment</span><p>ICLR 2025 Best Paper Runner Up Award<br>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/yifeiwang77/status/1916873981979660436?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Multi.html" target="_blank" rel="noopener noreferrer">#Multi</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<span class="issue_date">Issue Date: 2025-04-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1904" target="_blank" rel="noopener noreferrer" class="title-link">Why Do Multi-Agent LLM Systems Fail?, Mert Cemri+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- MASã®æ€§èƒ½å‘ä¸ŠãŒå˜ä¸€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨æ¯”è¼ƒã—ã¦é™å®šçš„ã§ã‚ã‚‹ã“ã¨ã‚’å—ã‘ã€MASTï¼ˆMulti-Agent System Failure Taxonomyï¼‰ã‚’ææ¡ˆã€‚200ä»¥ä¸Šã®ã‚¿ã‚¹ã‚¯ã‚’åˆ†æã—ã€14ã®å¤±æ•—ãƒ¢ãƒ¼ãƒ‰ã‚’ç‰¹å®šã—ã€3ã¤ã®å¤§ã‚«ãƒ†ã‚´ãƒªã«æ•´ç†ã€‚Cohenã®ã‚«ãƒƒãƒ‘ã‚¹ã‚³ã‚¢0.88ã‚’é”æˆã—ã€LLMã‚’ç”¨ã„ãŸè©•ä¾¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’é–‹ç™ºã€‚ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ã‚’é€šã˜ã¦å¤±æ•—åˆ†æã¨MASé–‹ç™ºã®æ–¹æ³•ã‚’ç¤ºã—ã€ä»Šå¾Œã®ç ”ç©¶ã®ãŸã‚ã®ãƒ­ãƒ¼ãƒ‰ãƒãƒƒãƒ—ã‚’æç¤ºã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨LLMã‚¢ãƒãƒ†ãƒ¼ã‚¿ãƒ¼ã‚’ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹åŒ–äºˆå®šã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/mertcemri/status/1915567789714329799?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>7ã¤ã®ãƒ¡ã‚¸ãƒ£ãƒ¼ãªãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«å¯¾ã—ã¦200ä»¥ä¸Šã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿæ–½ã—ã€6äººã®å°‚é–€å®¶ãŒtraceã‚’ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã€‚14ç¨®é¡ã®å…¸å‹çš„ãªfailure modeã‚’è¦‹ã¤ã‘ã€ãã‚Œã‚‰ã‚’3ã¤ã«ã‚«ãƒ†ã‚´ãƒ©ã‚¤ã‚ºã€‚ã“ã‚Œã‚’è€ƒæ…®ã—ã¦ãƒãƒ«ãƒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚·ã‚¹ãƒ†ãƒ ã®å¤±æ•—ã«é–¢ã™ã‚‹Taxonomyï¼ˆMASï¼‰ã‚’ææ¡ˆ<br><img src="https://github.com/user-attachments/assets/21d45bc7-cc6c-4561-b991-098f8d068627" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Alignment.html" target="_blank" rel="noopener noreferrer">#Alignment</a>
<a class="button" href="articles/Hallucination.html" target="_blank" rel="noopener noreferrer">#Hallucination</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/DPO.html" target="_blank" rel="noopener noreferrer">#DPO</a>
<a class="button" href="articles/Repetition.html" target="_blank" rel="noopener noreferrer">#Repetition</a>
<span class="issue_date">Issue Date: 2025-04-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1892" target="_blank" rel="noopener noreferrer" class="title-link">Learning Dynamics of LLM Finetuning, Yi Ren+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã®å­¦ç¿’ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’åˆ†æã—ã€ç•°ãªã‚‹å¿œç­”é–“ã®å½±éŸ¿ã®è“„ç©ã‚’æ®µéšçš„ã«è§£æ˜ã—ã¾ã™ã€‚æŒ‡ç¤ºèª¿æ•´ã¨å¥½ã¿èª¿æ•´ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«é–¢ã™ã‚‹è¦³å¯Ÿã‚’çµ±ä¸€çš„ã«è§£é‡ˆã—ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã®å¹»è¦šå¼·åŒ–ã®ç†ç”±ã‚’ä»®èª¬çš„ã«èª¬æ˜ã—ã¾ã™ã€‚ã¾ãŸã€ã‚ªãƒ•ãƒãƒªã‚·ãƒ¼ç›´æ¥å¥½ã¿æœ€é©åŒ–ï¼ˆDPOï¼‰ã«ãŠã‘ã‚‹ã€Œåœ§ç¸®åŠ¹æœã€ã‚’å¼·èª¿ã—ã€æœ›ã¾ã—ã„å‡ºåŠ›ã®å¯èƒ½æ€§ãŒä½ä¸‹ã™ã‚‹ç¾è±¡ã‚’æ¢ã‚Šã¾ã™ã€‚ã“ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¯ã€LLMã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç†è§£ã«æ–°ãŸãªè¦–ç‚¹ã‚’æä¾›ã—ã€ã‚¢ãƒ©ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆæ€§èƒ½å‘ä¸Šã®ãŸã‚ã®ã‚·ãƒ³ãƒ—ãƒ«ãªæ–¹æ³•ã‚’ç¤ºå”†ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/joshuarenyi/status/1913033476275925414?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1917189793588613299?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/COLM.html" target="_blank" rel="noopener noreferrer">#COLM</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/In-Depth%20Notes.html" target="_blank" rel="noopener noreferrer">#In-Depth Notes</a>
<span class="issue_date">Issue Date: 2025-04-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1887" target="_blank" rel="noopener noreferrer" class="title-link">A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths   to Reproducibility, Andreas Hochlehnert+, COLM'25</a>
<span class="snippet"><span>GPT Summary</span>- æ¨è«–ã¯è¨€èªãƒ¢ãƒ‡ãƒ«ã®é‡è¦ãªèª²é¡Œã§ã‚ã‚Šã€é€²å±•ãŒè¦‹ã‚‰ã‚Œã‚‹ãŒã€è©•ä¾¡æ‰‹æ³•ã«ã¯é€æ˜æ€§ã‚„å …ç‰¢æ€§ãŒæ¬ ã‘ã¦ã„ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€æ•°å­¦çš„æ¨è«–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãŒå®Ÿè£…ã®é¸æŠã«æ•æ„Ÿã§ã‚ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã—ã€æ¨™æº–åŒ–ã•ã‚ŒãŸè©•ä¾¡ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã€‚å†è©•ä¾¡ã®çµæœã€å¼·åŒ–å­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯æ”¹å–„ãŒå°‘ãªãã€æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã¯å¼·ã„ä¸€èˆ¬åŒ–ã‚’ç¤ºã—ãŸã€‚å†ç¾æ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã«ã€é–¢é€£ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚„ãƒ‡ãƒ¼ã‚¿ã‚’å…¬é–‹ã—ã€ä»Šå¾Œã®ç ”ç©¶ã®åŸºç›¤ã‚’ç¯‰ãã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenhuchen/status/1911143014258405420?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>SLMã‚’math reasoningå‘ã‘ã«post-trainingã™ã‚‹å ´åˆã€è©•ä¾¡ã®æ¡ä»¶ã‚’ãƒ•ã‚§ã‚¢ã«ã™ã‚‹ãŸã‚ã®æ§˜ã€…ãªå·¥å¤«ã‚’æ–½ã—è©•ä¾¡ã‚’ã—ãªãŠã—ãŸçµæœï¼ˆFigure1ã®ã‚ˆã†ã«æ€§èƒ½ãŒå¤‰åŒ–ã™ã‚‹æ§˜ã€…ãªè¦å› ãŒå­˜åœ¨ã™ã‚‹ï¼‰ã€RLï¼ˆæ—¢å­˜ç ”ç©¶ã§è©¦ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ï¼‰ã‚ˆã‚Šã‚‚ï¼ˆå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰rejection samplingã—ãŸreasoning traceã‚’ç”¨ã„ã¦ï¼‰SFTã‚’ã™ã‚‹æ–¹ãŒåŒç­‰ã‹æ€§èƒ½ãŒè‰¯ã(Table3)ã€çµå±€ã®ã¨ã“ã‚ï¼ˆãŠãã‚‰ãæ±åŒ–æ€§èƒ½ãŒä½ã„ã¨ã„ã†æ„å‘³ã§ï¼‰reliableã§ã¯ãªãã€ã‹ã¤ï¼ˆãŠãã‚‰ãå°è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã§ã†ã¾ãã„ã‹ãªã„ã¨ã„ã†æ„å‘³ã§ã®ï¼‰scalableã§ã¯ãªã„ã®ã§ã€reliableã‹ã¤scalableãªRLæ‰‹æ³•ãŒä¸è¶³ã—ã¦ã„ã‚‹ã¨ã®ã“ã¨ã€‚<br><br>â€» æœ¬è«–æ–‡ã§åˆ†æã•ã‚Œã¦ã„ã‚‹ã®ã¯&lt;=10Bä»¥ä¸‹ã®SLMã§ã‚ã‚‹ç‚¹ã«æ³¨æ„ã€‚10Bä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ã§åŒã˜ã“ã¨ãŒè¨€ãˆã‚‹ã‹ã¯è‡ªæ˜ã§ã¯ãªã„ã€‚<br>â€» DAPO, VAPOãªã©ã«ã¤ã„ã¦ã‚‚åŒã˜ã“ã¨ãŒè¨€ãˆã‚‹ã‹ã‚‚è‡ªæ˜ã§ã¯ãªã„ã€‚<br>â€» DeepSeek-R1ã®technical reportã«ãŠã„ã¦ã€å°ã•ã„ãƒ¢ãƒ‡ãƒ«ã«GRPOã‚’é©ç”¨ã—ã¦ã‚‚ã‚ã¾ã‚ŠåŠ¹æœãŒç„¡ã‹ã£ãŸã“ã¨ãŒæ—¢ã«å ±å‘Šã•ã‚Œã¦ã„ã‚‹ã€‚<br><br><img src="https://github.com/user-attachments/assets/620017f1-b3f0-40c1-bf61-3b0b7a429ab4" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/321132c8-dad5-4aa1-9811-f032e3474135" alt="image" loading="lazy"><br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1743" target="_blank" rel="noopener noreferrer">DeepSeek-R1ã®è«–æ–‡èª­ã‚“ã ï¼Ÿã€å‹‰å¼·ã«ãªã‚‹ã‚ˆã€‘ , asap, 2025.01</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1719" target="_blank" rel="noopener noreferrer">DeepSeek-R1, DeepSeek, 2025.01</a>
</p>
<p>å€‹ã€…ã®post-trainingã•ã‚ŒãŸRLãƒ¢ãƒ‡ãƒ«ãŒå…·ä½“çš„ã«ã©ã†ã„ã†è¨“ç·´ã‚’ã—ãŸã®ã‹ã¯è¿½ãˆã¦ã„ãªã„ãŒã€DAPOã‚„Dr. GRPO, VAPOã®å ´åˆã¯ã©ã†ãªã‚‹ã‚“ã ã‚ã†ã‹ï¼Ÿ<br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1815" target="_blank" rel="noopener noreferrer">DAPO: An Open-Source LLM Reinforcement Learning System at Scale, Qiying Yu+, arXiv'25</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1876" target="_blank" rel="noopener noreferrer">VAPO: Efficient and Reliable Reinforcement Learning for Advanced
  Reasoning Tasks, YuYue+, arXiv'25</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1821" target="_blank" rel="noopener noreferrer">Understanding R1-Zero-Like Training: A Critical Perspective, 2025.03</a>
<br><br>Rewardã®è¨­å®šã®ä»•æ–¹ã¯ã©ã®ã‚ˆã†ãªå½±éŸ¿ãŒã‚ã‚‹ã®ã ã‚ã†ã‹ï¼ˆverifiable rewardãªã®ã‹ã€neuralãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹rewardãªã®ã‹ãªã©)ï¼Ÿ<br><br>å­¦ç¿’ã®ã•ã›æ–¹ã‚‚ã©ã®ã‚ˆã†ãªå½±éŸ¿ãŒã‚ã‚‹ã®ã ã‚ã†ã‹ï¼ˆRLã§ã‚«ãƒªã‚­ãƒ¥ãƒ©ãƒ learningã«ã—ãŸå ´åˆãªã©ï¼‰ï¼Ÿ<br><br>æ¤œè¨¼ã—ã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒãã‚Œãã‚Œã©ã®ã‚ˆã†ãªè¨­å®šã§å­¦ç¿’ã•ã‚Œã¦ã„ã‚‹ã‹ã¾ã§ã‚’è¦‹ãªã„ã¨ã“ã®è¾ºã¯ã‚ã‹ã‚‰ãªãã†ã€‚<br><br>ãŸã ãªã‚“ã¨ãªãƒ¼ãã®ç›´æ„Ÿã ã¨ã€SLMã‚’è³¢ãã—ãŸã„ã¨ã„ã†å ´åˆã¯ä½•ã‚‰ã‹ã®è³¢ã„ãƒ¢ãƒ‡ãƒ«ã®æ©æµã«é ã‹ã‚‹ã¨æœ‰åˆ©ãªã‚±ãƒ¼ã‚¹ãŒå¤šãï¼ˆSFTã®å ´åˆã¯ãã‚ŒãŒå¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã‹ã‚‰è’¸ç•™ã—ãŸreasoning traceï¼‰ã€SLM+RLã®å ´åˆã¯PRMã®ã‚ˆã†ãªæ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’è©•ä¾¡ã—ã¦Rewardã«åæ˜ ã•ã›ã‚‹ã‚ˆã†ãªã‚‚ã®ã‚’åˆ©ç”¨ã—ãªã„ã¨ã€å°‘ãªãã¨ã‚‚å°è¦æ¨¡ãªLLMã‚’ã‚ã¡ã‚ƒè³¢ãã—ã¾ã™ã€œã¨ã„ã†ã®ã¯ãã¤ã„ã‚“ã˜ã‚ƒãªã„ã‹ãªã‚ã¨ã„ã†æ„Ÿæƒ³ã§ã¯ã‚ã‚‹ã€‚<br>ãŸã ã€çµå±€SLMã¨ã„ã†æ™‚ç‚¹ã§å¤šãã®å ´åˆã€ã‚ˆã‚Šè³¢ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®å¤šã„LLMãŒä¸–ã®ä¸­ã«ã¯å­˜åœ¨ã™ã‚‹ã‚ã‚‹ã¯ãšãªã®ã§ã€RLã—ãªã„ã§SFTã—ã¦è’¸ç•™ã™ã‚Œã°è‰¯ã„ã‚“ã˜ã‚ƒãªã„â€¦ï¼Ÿã¨æ€ã£ã¦ã—ã¾ã†ã€‚<br>ãŒã€å¤šãã®å ´åˆãã®è³¢ã„LLMã¯ProprietaryãªLLMã§ã‚ã‚Šã€å‡ºåŠ›ã‚’å¾—ã¦è‡ªåˆ†ã®ãƒ¢ãƒ‡ãƒ«ã‚’post-trainingã™ã‚‹ã“ã¨ã¯åˆ©ç”¨è¦ç´„é•åã¨ãªã‚‹ãŸã‚ã€è‡ªå‰ã§è³¢ãã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®å¤šã„LLMã‚’ç”¨æ„ã§ããªã„å ´åˆã¯å›°ã£ã¦ã—ã¾ã†ã®ã§ã€SLMã‚’ã‚¯ã‚½ãƒ‡ã‚«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«ã®æ©æµãªã—ã§è¶…çµ¶è³¢ãã§ããŸã‚‰ä¸–ã®ä¸­ã®å¤šãã®äººã¯å¬‰ã—ã„ã‚ˆã­ã€ã¨ã‚‚æ€ã†ã€‚</p>
<p>ï¼ˆæ–œã‚èª­ã¿ã ãŒï¼‰<br>ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå°‘ãªã„ï¼ˆæ•°åä»¶ï¼‰AIMEã‚„AMCãªã©ã®ãƒ‡ãƒ¼ã‚¿ã¯seedã®å€¤ã«ã¨ã¦ã‚‚sensitiveã§ã‚ã‚Š(Takeaway1, 2)ã€<br><br>&lt;img width="549" height="256" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/97581133-cf17-4635-b66c-442eaf8956d4"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/97581133-cf17-4635-b66c-442eaf8956d4"&lt;/a&gt;


/&gt;<br><br>ãã‚Œã‚‰ã¯10ç¨®é¡ã®seedã‚’ç”¨ã„ã¦çµæœã‚’å¹³å‡ã™ã‚‹ã¨åˆ†æ•£ãŒéå¸¸ã«å°ã•ããªã‚‹ã®ã§ã€seedã¯è¤‡æ•°ç¨®é¡åˆ©ç”¨ã—ã¦å¹³å‡ã®æ€§èƒ½ã‚’è¦‹ãŸæ–¹ãŒreliableã§ã‚ã‚Š(Takeaway3)<br><br>&lt;img width="688" height="266" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/5065ef0e-de89-4b17-aa52-c90b7191e9b2"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/5065ef0e-de89-4b17-aa52-c90b7191e9b2"&lt;/a&gt;


/&gt;<br><br>temperatureã‚’é«˜ãã™ã‚‹ã¨ãƒ”ãƒ¼ã‚¯æ€§èƒ½ãŒä¸ŠãŒã‚‹ãŒåˆ†æ•£ã‚‚ä¸ŠãŒã‚‹ãŸã‚å†ç¾æ€§ã®èª²é¡ŒãŒå¢—å¤§ã™ã‚‹ãŒã€top-pã‚’å¤§ããã™ã‚‹ã¨å†ç¾æ€§ã®å•é¡Œã¯ç¾ã‚Œãšæ€§èƒ½å‘ä¸Šã«å¯„ä¸ã—<br><br>&lt;img width="545" height="508" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/76d5c989-edbb-4d70-9080-d1d4b01de2ff"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/76d5c989-edbb-4d70-9080-d1d4b01de2ff"&lt;/a&gt;


/&gt;<br><br>æ—¢å­˜ç ”ç©¶ã®ãƒ¢ãƒ‡ãƒ«ã®temperatureã¨top-pã‚’å¤‰åŒ–ã•ã›å®Ÿé¨“ã™ã‚‹ã¨performanceã«éå¸¸ã«å¤§ããªå¤‰åŒ–ãŒå‡ºã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã”ã¨ã«æœ€é©ãªå€¤ã‚’é¸å®šã—ã¦æ¯”è¼ƒã‚’ã—ãªã„ã¨unfairã§ã‚ã‚‹ã“ã¨ã‚’æŒ‡æ‘˜ (Takeaway4)ã€‚<br><br>&lt;img width="553" height="511" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/d8b453d1-3d2e-4a80-b03d-c69ec1b2232e"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/d8b453d1-3d2e-4a80-b03d-c69ec1b2232e"&lt;/a&gt;


/&gt;<br><br>ã¾ãŸã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®é¢ã§ã¯ã€vLLMã®ã‚ˆã†ãªinference engineã¯GPU typeã‚„memoryã®configurationã«å¯¾ã—ã¦sensitiveã§ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå¤‰ã‚ã‚‹ã ã‘ã§ãªãã€<br><br>&lt;img width="689" height="356" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/a41891c7-072c-4c38-9ad6-beada4721bac"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/a41891c7-072c-4c38-9ad6-beada4721bac"&lt;/a&gt;


/&gt;<br><br>è©•ä¾¡ã«åˆ©ç”¨ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã”ã¨ã«inference engineã¨prompt templateãŒç•°ãªã‚‹ãŸã‚ã“ã¡ã‚‰ã‚‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å½±éŸ¿ãŒå‡ºã‚‹ã— (Takeaway5)ã€<br><br>&lt;img width="275" height="115" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/1f7d328c-0757-47b9-9961-630e2429fb3e"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/1f7d328c-0757-47b9-9961-630e2429fb3e"&lt;/a&gt;


/&gt;<br><br>max output tokenã®å€¤ã‚’å¤‰åŒ–ã•ã›ã‚‹ã¨æ€§èƒ½ã‚‚å¤‰ã‚ã‚Šã€prompt templateã‚’åˆ©ç”¨ã—ãªã„ã¨æ€§èƒ½ãŒåŠ‡çš„ã«ä½ä¸‹ã™ã‚‹ (Takeaway6)ã€‚<br><br>&lt;img width="681" height="577" alt="Image" src="


&lt;a href="https://github.com/user-attachments/assets/dc0902d1-a5f2-47de-8df1-c28107e1da28"" target="_blank" rel="noopener noreferrer"&gt;https://github.com/user-attachments/assets/dc0902d1-a5f2-47de-8df1-c28107e1da28"&lt;/a&gt;


/&gt;<br><br>ã“ã‚Œã‚‰ã®ã“ã¨ã‹ã‚‰è‘—è€…ã‚‰ã¯reliableãªè©•ä¾¡ã®ãŸã‚ã«ä¸‹è¨˜ã‚’ææ¡ˆã—ã¦ãŠã‚Š (4.1ç¯€; å¾Œã»ã©è¿½è¨˜)ã€<br><br>å®Ÿéš›ã«ã•ã¾ã–ã¾ãªæ¡ä»¶ã‚’fair comparisonã¨ãªã‚‹ã‚ˆã†ã«æ¨™æº–åŒ–ã—ã¦è©•ä¾¡ã—ãŸã¨ã“ã‚ï¼ˆ4.2ç¯€; å¾Œã»ã©è¿½è¨˜ï¼‰<br><br>ä¸Šã®è¡¨ã®ã‚ˆã†ãªçµæœã¨ãªã£ãŸã€‚ã“ã®çµæœã¯ã€<br>- DeepSeekR1-Distilledã‚’RLã—ã¦ã‚‚SFTã¨æ¯”è¼ƒã—ãŸã¨ãã«æ„å‘³ã®ã‚ã‚‹ã»ã©ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®å‘ä¸Šã¯ãªã„ã“ã¨ã‹ã‚‰ã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã€ã‹ã¤ä¿¡é ¼æ€§ã®ã‚ã‚‹RLæ‰‹æ³•ãŒã¾ã ä¸è¶³ã—ã¦ãŠã‚Š<br>- å¤§è¦æ¨¡ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«ã®reasoning traceã‹ã‚‰SFTã‚’ã™ã‚‹æ–¹æ³•ã¯ã•ã¾ã–ã¾ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ãƒ­ãƒã‚¹ãƒˆãªæ€§èƒ½ï¼ˆï¼é«˜ã„æ±åŒ–æ€§èƒ½ï¼‰ã‚’æŒã¡ã€RLã¨æ¯”ã¹ã‚‹ã¨ç¾çŠ¶ã¯RLã¨æ¯”è¼ƒã—ã¦ã‚ˆã‚Šãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã¨ã—ã¦æˆç†Ÿã—ã¦ãŠã‚Š<br>- ï¼ˆAIME24,25ã‚’æ¯”è¼ƒã™ã‚‹ã¨SFTã¨æ¯”ã¹ã¦RLã®å ´åˆperformanceã®ä½ä¸‹ãŒè‘—ã—ã„ã®ã§ï¼‰RLã¯overfittingã—ã‚„ã™ãã€OODãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãŒå¿…è¦</p>
<p>ã—ã£ã‹ã‚Šã¨è©•ä¾¡ã®æ çµ„ã¿ã‚’æ¨™æº–åŒ–ã—ã¦fair comparisonã—ã¦ã„ã‹ãªã„ã¨ã€RecSysæ¥­ç•Œã®äºŒã®èˆã«ãªã‚Šãã†ï¼ˆã¨ã„ã†ã‹ã‚‚ã†ãªã£ã¦ã‚‹ï¼Ÿï¼‰ã€‚<br><br>ã¾ãŸã“ã®ç ”ç©¶ã§åˆ†æã•ã‚Œã¦ã„ã‚‹ã®ã¯å°è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ï¼ˆ&lt;=10Bï¼‰ã«å¯¾ã™ã‚‹æ—¢å­˜ç ”ç©¶ã§ç”¨ã„ã‚‰ã‚ŒãŸä¸€éƒ¨ã®RLæ‰‹æ³•ã‚„è¨­å®šã®æ€§èƒ½ã ã‘ï¼ˆçœŸã«ç¤ºã—ãŸã‹ã£ãŸã‚‰Phisics of LLMã®ã‚ˆã†ãªå®Œå…¨ã«ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«å¯èƒ½ãªã‚µãƒ³ãƒ‰ãƒœãƒƒã‚¯ã‚¹ã§å®Ÿé¨“ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã¨æ€ã‚ã‚Œã‚‹ï¼‰ãªã®ã§ã€DeepSeek-R1ã®ã‚ˆã†ã«ã€å¤§è¦æ¨¡ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆæ•°ç™¾Bï¼‰ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹RLã«é–¢ã—ã¦åŒã˜ã“ã¨ãŒè¨€ãˆã‚‹ã‹ã¯è‡ªæ˜ã§ã¯ãªã„ç‚¹ã«æ³¨æ„ã€‚</p>
<p>openreview:


<a href="https://openreview.net/forum?id=90UrTTxp5O#discussion" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=90UrTTxp5O#discussion</a>


</p>
<p>æœ€è¿‘ã®ä»¥ä¸‹ã®ã‚ˆã†ãªSFTã¯RLã®ä¸€ã¤ã®ã‚±ãƒ¼ã‚¹ã¨è¦‹åšã›ã‚‹ã¨ã„ã†è­°è«–ã‚’è¸ã¾ãˆã‚‹ã¨ã©ã†ãªã‚‹ã ã‚ã†ã‹<br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2382" target="_blank" rel="noopener noreferrer">[Paper Note] On the Generalization of SFT: A Reinforcement Learning Perspective with
  Reward Rectification, Yongliang Wu+, arXiv'25</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2700" target="_blank" rel="noopener noreferrer">[Paper Note] Towards a Unified View of Large Language Model Post-Training, Xingtai Lv+, arXiv'25</a>
</p></span><br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/AttentionSinks.html" target="_blank" rel="noopener noreferrer">#AttentionSinks</a>
<a class="button" href="articles/COLM.html" target="_blank" rel="noopener noreferrer">#COLM</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-04-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1860" target="_blank" rel="noopener noreferrer" class="title-link">Why do LLMs attend to the first token?, Federico Barbero+, COLM'25</a>
<span class="snippet"><span>GPT Summary</span>- LLMsã¯æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ã«å¼·ãæ³¨æ„ã‚’å‘ã‘ã‚‹ã€Œã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚·ãƒ³ã‚¯ã€ã‚’ç¤ºã—ã€ãã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ãŒéå‰°æ··åˆã‚’é¿ã‘ã‚‹æ–¹æ³•ã‚’ç†è«–çš„ãƒ»å®Ÿè¨¼çš„ã«æ¢æ±‚ã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•ã‚„ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ãƒƒã‚­ãƒ³ã‚°ãŒã‚·ãƒ³ã‚¯ã®æŒ™å‹•ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’å®Ÿé¨“ã§ç¤ºã—ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç†è§£ã‚’æ·±ã‚ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/1908187563422261411?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Attention Sinkã«ã‚ˆã£ã¦ã€ãƒˆãƒ¼ã‚¯ãƒ³ã®æƒ…å ±ãŒover-mixingã•ã‚Œã‚‹ã“ã¨ãŒæŠ‘åˆ¶ã•ã‚Œã€Decoder-only LLMã®æ·±ã„å±¤ã®representationãŒå‡ä¸€åŒ–ã•ã‚Œã‚‹ã“ã¨ã‚’æŠ‘åˆ¶ã™ã‚‹ï¼ˆï¼promptã®æ‘‚å‹•ã«ãƒ­ãƒã‚¹ãƒˆã«ãªã‚‹ï¼‰ã“ã¨ãŒç¤ºã•ã‚ŒãŸæ¨¡æ§˜ã€‚<br><img src="https://github.com/user-attachments/assets/8a1223c0-5621-42a5-accc-31fa7f636856" alt="image" loading="lazy"><br>Gemma7Bã«ãŠã„ã¦ã€promptä¸­ã®ãƒˆãƒ¼ã‚¯ãƒ³ä¸€èªã‚’ç½®æ›ã—ãŸå¾Œã«ã€Attention Sinkï¼ˆ<bos>ï¼‰ã®æœ‰ç„¡ã«ã‚ˆã£ã¦ã€tokenãƒ¬ãƒ™ãƒ«ã®representationã«å¯¾ã—ã¦ã©ã®ã‚ˆã†ãªæ‘‚å‹•ãŒã‚ã‚‹ã‹ã‚’layerã”ã¨ã«ã¾ã¨ã‚ãŸå›³ãŒä¸‹è¨˜ã®æ¨¡æ§˜ã€‚Attention Sinkã«ã‚ˆã£ã¦ã€tokenã®æ‘‚å‹•ãŒä»–ã®token, layerã«å¯¾ã—ã¦mixingã•ã‚Œã‚‹ã®ãŒæŠ‘åˆ¶ã•ã‚Œã¦ã„ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/b1a4038a-d116-4bd1-b27b-c55eb861bee9" alt="image" loading="lazy">&lt;/p&gt;<p>openreview:


<a href="https://openreview.net/forum?id=tu4dFUsW5z#discussion" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=tu4dFUsW5z#discussion</a>


</p>&lt;/span&gt;<br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/FactualKnowledge.html" target="_blank" rel="noopener noreferrer">#FactualKnowledge</a>
<span class="issue_date">Issue Date: 2025-04-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1846" target="_blank" rel="noopener noreferrer" class="title-link">Inside-Out: Hidden Factual Knowledge in LLMs, Zorik Gekhman+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã¯ã€LLMãŒå‡ºåŠ›ä»¥ä¸Šã®äº‹å®Ÿçš„çŸ¥è­˜ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ã„ã‚‹ã‹ã‚’è©•ä¾¡ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã€‚çŸ¥è­˜ã‚’å®šç¾©ã—ã€æ­£ã—ã„å›ç­”ãŒé«˜ããƒ©ãƒ³ã‚¯ä»˜ã‘ã•ã‚Œã‚‹å‰²åˆã‚’å®šé‡åŒ–ã€‚å¤–éƒ¨çŸ¥è­˜ã¨å†…éƒ¨çŸ¥è­˜ã‚’åŒºåˆ¥ã—ã€å†…éƒ¨çŸ¥è­˜ãŒå¤–éƒ¨çŸ¥è­˜ã‚’è¶…ãˆã‚‹ã¨éš ã‚ŒãŸçŸ¥è­˜ãŒç”Ÿã˜ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ã‚¯ãƒ­ãƒ¼ã‚ºãƒ‰ãƒ–ãƒƒã‚¯QAè¨­å®šã§ã®ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ã§ã¯ã€LLMãŒå†…éƒ¨ã§å¤šãã®çŸ¥è­˜ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ã„ã‚‹ã“ã¨ã€çŸ¥è­˜ãŒéš ã‚Œã¦ã„ã‚‹å ´åˆãŒã‚ã‚‹ã“ã¨ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹åˆ¶ç´„ãŒã‚ã‚‹ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/zorikgekhman/status/1906693729886363861?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span></bos></p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-03-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1837" target="_blank" rel="noopener noreferrer" class="title-link">Overtrained Language Models Are Harder to Fine-Tune, Jacob Mitchell Springer+, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ã«ãŠã„ã¦ã€ãƒˆãƒ¼ã‚¯ãƒ³äºˆç®—ã®å¢—åŠ ãŒãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é›£ã—ãã—ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ä½ä¸‹ã‚’å¼•ãèµ·ã“ã™ã€Œå£Šæ»…çš„ãªéå­¦ç¿’ã€ã‚’æå”±ã€‚3Tãƒˆãƒ¼ã‚¯ãƒ³ã§äº‹å‰å­¦ç¿’ã•ã‚ŒãŸOLMo-1Bãƒ¢ãƒ‡ãƒ«ã¯ã€2.3Tãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ¢ãƒ‡ãƒ«ã«æ¯”ã¹ã¦2%ä»¥ä¸Šã®æ€§èƒ½ä½ä¸‹ã‚’ç¤ºã™ã€‚å®Ÿé¨“ã¨ç†è«–åˆ†æã«ã‚ˆã‚Šã€äº‹å‰å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ„Ÿåº¦ã®å¢—åŠ ãŒåŸå› ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã€äº‹å‰å­¦ç¿’è¨­è¨ˆã®å†è©•ä¾¡ã‚’ä¿ƒã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>è‘—è€…ã«ã‚ˆã‚‹ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jacspringer/status/1904960783341023521?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>äº‹å‰å­¦ç¿’ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å¢—ã‚„ã™ã¨ãƒ¢ãƒ‡ãƒ«ã®sensitivityãŒå¢—ã—ã€post-trainingã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®åŠ£åŒ–ãŒèµ·ã“ã‚‹ã“ã¨ã‚’å ±å‘Šã—ã¦ã„ã‚‹ã€‚äº‹å‰å­¦ç¿’ã§å­¦ç¿’ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å¢—ã‚„ã›ã°ã€å¿…ãšã—ã‚‚post-trainingå¾Œã®ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ãŒã‚ˆããªã‚‹ã‚ã‘ã§ã¯ãªã„ã‚‰ã—ã„ã€‚<br><img src="https://github.com/user-attachments/assets/ba60ae24-f3e5-4956-b29f-37b4fe01a9d1" alt="image" loading="lazy"></p>
<p>ICLR'25ã®Outstanding Paperã«é¸ã°ã‚ŒãŸæ¨¡æ§˜:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jacspringer/status/1917174452531724718?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>ãã¡ã‚“ã¨èª­ã‚“ã æ–¹ãŒè‰¯ã•ã’ã€‚</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/RLHF.html" target="_blank" rel="noopener noreferrer">#RLHF</a>
<span class="issue_date">Issue Date: 2025-03-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1806" target="_blank" rel="noopener noreferrer" class="title-link">All Roads Lead to Likelihood: The Value of Reinforcement Learning in  Fine-Tuning, Gokul Swamy+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦ã€å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸäºŒæ®µéšã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹é †ãŒåŠ¹æœçš„ã§ã‚ã‚‹ç†ç”±ã‚’ç†è«–çš„ãŠã‚ˆã³å®Ÿè¨¼çš„ã«æ¤œè¨ã€‚ç‰¹ã«ã€å¥½ã¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å˜ç´”ãªå ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ã³ã€å¼·åŒ–å­¦ç¿’æ‰‹ç¶šããŒãã®ãƒ¢ãƒ‡ãƒ«ã«æœ€é©ãªãƒãƒªã‚·ãƒ¼ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹èƒ½åŠ›ãŒã€ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«å¯„ä¸ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1901392286694678568?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Alignmentã®ãŸã‚ã®Preferenceãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹æ™‚ã«ã€ãã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æ¥æœ€å°¤æ¨å®šã—ã¦ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å­¦ç¿’ã™ã‚‹ã®ã§ã¯ãªãã€å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã¦ã€ãã®å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å¼·åŒ–å­¦ç¿’ã™ã‚‹ã“ã¨ã§ã€ãªãœå‰è€…ã‚ˆã‚Šã‚‚ï¼ˆåŒã˜ãƒ‡ãƒ¼ã‚¿ç”±æ¥ã§ã‚ã‚‹ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšï¼‰å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã™ã®ã‹ã€ã¨ã„ã†ç–‘å•ã«å¯¾ã—ã¦ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã—ã¦ã„ã‚‹ã€‚</p>
<p>å…¨ãä¸­èº«ã‚’èª­ã‚ã¦ã„ãªã„ãŒã€ç”Ÿæˆã™ã‚‹ã“ã¨ã¨ï¼ˆæ–¹ç­–ãƒ¢ãƒ‡ãƒ«ï¼‰ã¨æ¤œè¨¼ã™ã‚‹ã“ã¨ï¼ˆå ±é…¬ãƒ¢ãƒ‡ãƒ«ï¼‰ã®é–“ã«ã‚®ãƒ£ãƒƒãƒ—ãŒã‚ã‚‹å ´åˆï¼ˆã™ãªã‚ã¡ã€ç”Ÿæˆã¨æ¤œè¨¼ã§æ±‚ã‚ã‚‰ã‚Œã‚‹èƒ½åŠ›ãŒç•°ãªã‚‹å ´åˆï¼‰ã€MLEã§ã¯å¯èƒ½ãªã™ã¹ã¦ã®ãƒãƒªã‚·ãƒ¼ã‚’æ¢ç´¢ã™ã‚‹ã“ã¨ã¨ä¼¼ãŸã‚ˆã†ãªã“ã¨ã‚’ã™ã‚‹ã“ã¨ã«ãªã‚‹ãŒã€RLã§ã¯äº‹å‰ã«å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ãã®å ±é…¬ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦æœ€é©ãªãƒãƒªã‚·ãƒ¼ã‚’æ¢ç´¢ã™ã‚‹ã ã‘ãªã®ã§æ¢ç´¢ã™ã‚‹ç©ºé–“ãŒåˆ¶é™ã•ã‚Œã‚‹ï¼ˆï¼ç”Ÿæˆã¨æ¤œè¨¼ã®ã‚®ãƒ£ãƒƒãƒ—ãŒåŸ‹ã¾ã‚‹ï¼‰ã®ã§ã€è‰¯ã„è§£ã«åæŸã—ã‚„ã™ããªã‚‹ã€ã¨ã„ã†ã‚¤ãƒ¡ãƒ¼ã‚¸ãªã‚“ã ã‚ã†ã‹ã€‚<br><img src="https://github.com/user-attachments/assets/121e97a6-120e-4830-9bcf-329129a687eb" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<span class="issue_date">Issue Date: 2025-02-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1767" target="_blank" rel="noopener noreferrer" class="title-link">Scaling Test-Time Compute Without Verification or RL is Suboptimal, Amrith Setlur+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- RLã‚„æ¢ç´¢ã«åŸºã¥ãæ¤œè¨¼è€…ãƒ™ãƒ¼ã‚¹ï¼ˆVBï¼‰æ‰‹æ³•ãŒã€æ¢ç´¢ã®ç—•è·¡ã‚’è’¸ç•™ã™ã‚‹æ¤œè¨¼è€…ãƒ•ãƒªãƒ¼ï¼ˆVFï¼‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚ˆã‚Šã‚‚å„ªã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ãƒ†ã‚¹ãƒˆæ™‚ã®è¨ˆç®—ã¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ã™ã‚‹ã¨ã€VFæ‰‹æ³•ã®æœ€é©æ€§ãŒæ‚ªåŒ–ã—ã€VBæ‰‹æ³•ãŒã‚ˆã‚Šè‰¯ãã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚3/8/32Bã‚µã‚¤ã‚ºã®LLMã‚’ç”¨ã„ãŸå®Ÿé¨“ã§ã€æ¤œè¨¼ãŒè¨ˆç®—èƒ½åŠ›ã®å‘ä¸Šã«é‡è¦ã§ã‚ã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1891839822257586310?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1749" target="_blank" rel="noopener noreferrer">s1: Simple test-time scaling, Niklas Muennighoff+, arXiv'25</a>
</p></span><br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="articles/RewardHacking.html" target="_blank" rel="noopener noreferrer">#RewardHacking</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-02-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1746" target="_blank" rel="noopener noreferrer" class="title-link">Demystifying Long Chain-of-Thought Reasoning in LLMs, Edward Yeo+, arXiv'25</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã«ãŠã‘ã‚‹é•·ã„æ€è€ƒã®é€£é–ï¼ˆCoTsï¼‰æ¨è«–ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’èª¿æŸ»ã—ã€é‡è¦ãªè¦å› ã‚’ç‰¹å®šã€‚ä¸»ãªç™ºè¦‹ã¯ã€(1) æ•™å¸«ã‚ã‚Šãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆSFTï¼‰ã¯å¿…é ˆã§ã¯ãªã„ãŒåŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ã€(2) æ¨è«–èƒ½åŠ›ã¯è¨ˆç®—ã®å¢—åŠ ã«ä¼´ã„ç¾ã‚Œã‚‹ãŒã€å ±é…¬ã®å½¢çŠ¶ãŒCoTã®é•·ã•ã«å½±éŸ¿ã€(3) æ¤œè¨¼å¯èƒ½ãªå ±é…¬ä¿¡å·ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒé‡è¦ã§ã€ç‰¹ã«åˆ†å¸ƒå¤–ã‚¿ã‚¹ã‚¯ã«åŠ¹æœçš„ã€(4) ã‚¨ãƒ©ãƒ¼ä¿®æ­£èƒ½åŠ›ã¯åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã«å­˜åœ¨ã™ã‚‹ãŒã€RLã‚’é€šã˜ã¦åŠ¹æœçš„ã«å¥¨åŠ±ã™ã‚‹ã«ã¯å¤šãã®è¨ˆç®—ãŒå¿…è¦ã€‚ã“ã‚Œã‚‰ã®æ´å¯Ÿã¯ã€LLMsã®é•·ã„CoTæ¨è«–ã‚’å¼·åŒ–ã™ã‚‹ãŸã‚ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æˆ¦ç•¥ã®æœ€é©åŒ–ã«å½¹ç«‹ã¤ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/xiangyue96/status/1887332772198371514?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å…ƒãƒã‚¹ãƒˆã®ã‚¹ãƒ¬ãƒƒãƒ‰ä¸­ã«è«–æ–‡ã®11å€‹ã®çŸ¥è¦‹ãŒè¿°ã¹ã‚‰ã‚Œã¦ã„ã‚‹ã€‚ã©ã‚Œã‚‚éå¸¸ã«èˆˆå‘³æ·±ã„ã€‚DeepSeek-R1ã®ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ãƒšãƒ¼ãƒ‘ãƒ¼ã¨åŒæ§˜ã€<br><br>- Long CoTã¨Short CoTã‚’æ¯”è¼ƒã™ã‚‹ã¨å‰è€…ã®æ–¹ãŒåˆ°é”å¯èƒ½ãªæ€§èƒ½ã®upper bonudãŒé«˜ã„ã“ã¨ã‚„ã€<br>- SFTã‚’å®Ÿæ–½ã—ã¦ã‹ã‚‰RLã‚’ã™ã‚‹ã¨æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚„ã€<br>- RLã®éš›ã«CoTã®Lengthã«é–¢ã™ã‚‹å ±é…¬ã‚’å…¥ã‚Œã‚‹ã“ã¨ã§CoTã®é•·ã•ã‚’æŠ‘ãˆã¤ã¤æ€§èƒ½å‘ä¸Šã§ãã‚‹ã“ã¨ã€<br>- æ•°å­¦ã ã‘ã§ãªãQAãƒšã‚¢ãªã©ã®ãƒã‚¤ã‚¸ãƒ¼ã ãŒæ¤œè¨¼å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚’Verifiableãªå ±é…¬ã¨ã—ã¦åŠ ãˆã‚‹ã¨ä¸€èˆ¬çš„ãªreasoningã‚¿ã‚¹ã‚¯ã§æ•°å­¦ã‚ˆã‚Šã‚‚ã•ã‚‰ã«æ€§èƒ½ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã€<br>- ã‚ˆã‚Šé•·ã„context window sizeã‚’æ´»ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã«ã¯ã‚ˆã‚Šå¤šãã®å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ãªã“ã¨ã€<br>- long CoTã¯RLã«ã‚ˆã£ã¦å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«é¡ä¼¼ã—ãŸãƒ‡ãƒ¼ã‚¿ãŒå«ã¾ã‚Œã¦ã„ã‚‹ãŸã‚ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®æ®µéšã§ãã®èƒ½åŠ›ãŒç²å¾—ã•ã‚Œã¦ã„ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚Œã‚‹ã“ã¨ã€<br>- aha momentã¯ã™ã§ã«ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«æ™‚ç‚¹ã§ç²å¾—ã•ã‚Œã¦ãŠã‚ŠVerifiableãªå ±é…¬ã«ã‚ˆã‚‹RLã«ã‚ˆã£ã¦å¼·åŒ–ã•ã‚ŒãŸã‚ã‘ã§ã¯ãªã•ãã†ã€<br><br>ãªã©ã€èˆˆå‘³æ·±ã„çŸ¥è¦‹ãŒç››ã‚Šã ãã•ã‚“ã€‚éå¸¸ã«èˆˆå‘³æ·±ã„ç ”ç©¶ã€‚ã‚ã¨ã§èª­ã‚€ã€‚</p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-01-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1740" target="_blank" rel="noopener noreferrer" class="title-link">SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model   Post-training, Tianzhe Chu+, ICML'25</a>
<span class="snippet"><span>GPT Summary</span>- SFTã¨RLã®ä¸€èˆ¬åŒ–èƒ½åŠ›ã®é•ã„ã‚’ç ”ç©¶ã—ã€GeneralPointsã¨V-IRLã‚’ç”¨ã„ã¦è©•ä¾¡ã€‚RLã¯ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ãƒ†ã‚­ã‚¹ãƒˆã¨è¦–è¦šå¤‰ç¨®ã«å¯¾ã—ã¦å„ªã‚ŒãŸä¸€èˆ¬åŒ–ã‚’ç¤ºã™ä¸€æ–¹ã€SFTã¯è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜æ†¶ã—åˆ†å¸ƒå¤–ã‚·ãƒŠãƒªã‚ªã«è‹¦åŠ´ã€‚RLã¯è¦–è¦šèªè­˜èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ãŒã€SFTã¯RLè¨“ç·´ã«ä¸å¯æ¬ ã§ã‚ã‚Šã€å‡ºåŠ›å½¢å¼ã‚’å®‰å®šã•ã›ã‚‹ã“ã¨ã§æ€§èƒ½å‘ä¸Šã‚’ä¿ƒé€²ã€‚ã“ã‚Œã‚‰ã®çµæœã¯ã€è¤‡é›‘ãªãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹RLã®ä¸€èˆ¬åŒ–èƒ½åŠ›ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1884731381517082668?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>openreview:


<a href="https://openreview.net/forum?id=dYur3yabMj&referrer=%5Bthe%20profile%20of%20Yi%20Ma%5D(%2Fprofile%3Fid%3D~Yi_Ma4)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=dYur3yabMj&referrer=%5Bthe%20profile%20of%20Yi%20Ma%5D(%2Fprofile%3Fid%3D~Yi_Ma4)</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<span class="issue_date">Issue Date: 2024-04-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1286" target="_blank" rel="noopener noreferrer" class="title-link">Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws, Zeyuan Allen-Zhu+, N_A, ICLR'25</a>
<span class="snippet"><span>GPT Summary</span>- è¨€èªãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã¨èƒ½åŠ›ã®é–¢ä¿‚ã‚’è¨˜è¿°ã™ã‚‹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å‰‡ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸç ”ç©¶ã€‚ãƒ¢ãƒ‡ãƒ«ãŒæ ¼ç´ã™ã‚‹çŸ¥è­˜ãƒ“ãƒƒãƒˆæ•°ã‚’æ¨å®šã—ã€äº‹å®ŸçŸ¥è­˜ã‚’ã‚¿ãƒ—ãƒ«ã§è¡¨ç¾ã€‚è¨€èªãƒ¢ãƒ‡ãƒ«ã¯1ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚ãŸã‚Š2ãƒ“ãƒƒãƒˆã®çŸ¥è­˜ã‚’æ ¼ç´å¯èƒ½ã§ã‚ã‚Šã€7Bãƒ¢ãƒ‡ãƒ«ã¯14Bãƒ“ãƒƒãƒˆã®çŸ¥è­˜ã‚’æ ¼ç´å¯èƒ½ã€‚ã•ã‚‰ã«ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æœŸé–“ã€ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€é‡å­åŒ–ã€ç–ãªåˆ¶ç´„ã€ãƒ‡ãƒ¼ã‚¿ã®ä¿¡å·å¯¾é›‘éŸ³æ¯”ãŒçŸ¥è­˜æ ¼ç´å®¹é‡ã«å½±éŸ¿ã™ã‚‹ã“ã¨ã‚’ç¤ºå”†ã€‚ãƒ­ãƒ¼ã‚¿ãƒªãƒ¼åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨ã—ãŸGPT-2ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€çŸ¥è­˜ã®æ ¼ç´ã«ãŠã„ã¦LLaMA/Mistralã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ç«¶åˆã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«ãƒ‰ãƒ¡ã‚¤ãƒ³åã‚’è¿½åŠ ã™ã‚‹ã¨çŸ¥è­˜å®¹é‡ãŒå¢—åŠ ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å‚è€ƒ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1779640139263901698?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è§£èª¬:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1834" target="_blank" rel="noopener noreferrer">è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç‰©ç†å­¦, ä½è—¤ç«œé¦¬, 2025.03</a>
</p>
<p>openreview:


<a href="https://openreview.net/forum?id=FxNNiUgtfa" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=FxNNiUgtfa</a>


</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Optimizer.html" target="_blank" rel="noopener noreferrer">#Optimizer</a>
<span class="issue_date">Issue Date: 2025-10-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3480" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] WHEN DOES SECOND-ORDER OPTIMIZATION SPEED UP TRAINING?, Ishikawa+, ICLR'24 Tiny Paper</a>
<span class="snippet"><span>GPT Summary</span>- äºŒæ¬¡æœ€é©åŒ–æ‰‹æ³•ã®ä½¿ç”¨ãŒé™ã‚‰ã‚Œã¦ã„ã‚‹ç†ç”±ã‚’æ¢ã‚Šã€ç‰¹ã«ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã«åŸºã¥ãæ¡ä»¶ã‚’ç‰¹å®šã€‚å®Ÿè¨¼çš„ã«ã€å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºã¨å°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µã‚¤ã‚ºã®çµ„ã¿åˆã‚ã›ã§äºŒæ¬¡æœ€é©åŒ–ãŒä¸€æ¬¡æœ€é©åŒ–ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/seunghyunseo7/status/1983091615280627998?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Zero/Few/ManyShotPrompting.html" target="_blank" rel="noopener noreferrer">#Zero/Few/ManyShotPrompting</a>
<a class="button" href="articles/MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-10-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3467" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Many-Shot In-Context Learning in Multimodal Foundation Models, Yixing Jiang+, arXiv'24, 2024.05</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®å°‘æ•°ã‚·ãƒ§ãƒƒãƒˆã‹ã‚‰å¤šæ•°ã‚·ãƒ§ãƒƒãƒˆã®ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ï¼ˆICLï¼‰ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã€2,000ã®ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¾‹ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€ã™ã¹ã¦ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã„ã¦å¤§å¹…ãªæ”¹å–„ã‚’è¦³å¯Ÿã—ã¾ã—ãŸã€‚ç‰¹ã«ã€Gemini 1.5 Proã¯å¤šãã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å¯¾æ•°çš„ã«æ€§èƒ½ãŒå‘ä¸Šã—ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚¦ã‚§ã‚¤ãƒˆãƒ¢ãƒ‡ãƒ«ã¯ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¾‹ã‹ã‚‰ã®æ©æµã‚’å—ã‘ãªã„ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã¾ã—ãŸã€‚ã¾ãŸã€è¤‡æ•°ã®ã‚¯ã‚¨ãƒªã‚’ãƒãƒƒãƒå‡¦ç†ã™ã‚‹ã“ã¨ã§ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆãŠã‚ˆã³å¤šæ•°ã‚·ãƒ§ãƒƒãƒˆICLã®æ€§èƒ½ãŒå‘ä¸Šã—ã€ã‚³ã‚¹ãƒˆã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒå‰Šæ¸›ã•ã‚Œã¾ã—ãŸã€‚æœ€çµ‚çš„ã«ã€GPT-4oã¨Gemini 1.5 Proã¯é¡ä¼¼ã®ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆæ€§èƒ½ã‚’ç¤ºã—ã¤ã¤ã€Gemini 1.5 Proã¯ã‚ˆã‚Šæ—©ãå­¦ç¿’ã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚å¤šæ•°ã‚·ãƒ§ãƒƒãƒˆICLã¯æ–°ã—ã„ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã¸ã®é©å¿œã‚’åŠ¹ç‡åŒ–ã™ã‚‹å¯èƒ½æ€§ã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/marchoelle/status/1982589731260203110?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="articles/TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<a class="button" href="articles/ImageSynthesis.html" target="_blank" rel="noopener noreferrer">#ImageSynthesis</a>
<a class="button" href="articles/GeometryUnderstanding.html" target="_blank" rel="noopener noreferrer">#GeometryUnderstanding</a>
<span class="issue_date">Issue Date: 2025-10-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3409" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Shadows Don't Lie and Lines Can't Bend Generative Models don't know  Projective Geometry...for now, Ayush Sarkar+, CVPR'24, 2023.11</a>
<span class="snippet"><span>GPT Summary</span>- ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¯ãƒªã‚¢ãƒ«ãªç”»åƒã‚’ç”Ÿæˆã™ã‚‹ãŒã€å¹¾ä½•å­¦çš„ç‰¹å¾´ã«ãŠã„ã¦å®Ÿéš›ã®ç”»åƒã¨ç•°ãªã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚äº‹å‰ã«é¸åˆ¥ã•ã‚ŒãŸç”Ÿæˆç”»åƒã‚’ç”¨ã„ã¦ã€å¹¾ä½•å­¦çš„ç‰¹æ€§ã«åŸºã¥ãåˆ†é¡å™¨ãŒç”Ÿæˆç”»åƒã‚’é«˜ç²¾åº¦ã§è­˜åˆ¥ã§ãã‚‹ã“ã¨ã‚’ç¢ºèªã€‚3ã¤ã®åˆ†é¡å™¨ã‚’ä½¿ç”¨ã—ã€ç”»åƒã®é€è¦–å ´ã€ç·šã€ç‰©ä½“ã¨å½±ã®é–¢ä¿‚ã‚’åˆ†æã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç”Ÿæˆç”»åƒã®æ¤œå‡ºç²¾åº¦ãŒå‘ä¸Šã—ã€ç¾åœ¨ã®ç”Ÿæˆå™¨ã¯å®Ÿéš›ã®ç”»åƒã®å¹¾ä½•å­¦çš„ç‰¹æ€§ã‚’å†ç¾ã§ããªã„ã¨çµè«–ä»˜ã‘ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>pj page: 


<a href="https://projective-geometry.github.io/" target="_blank" rel="noopener noreferrer">https://projective-geometry.github.io/</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Prompting.html" target="_blank" rel="noopener noreferrer">#Prompting</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="articles/Length.html" target="_blank" rel="noopener noreferrer">#Length</a>
<span class="issue_date">Issue Date: 2025-10-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3063" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Same Task, More Tokens: the Impact of Input Length on the Reasoning  Performance of Large Language Models, Mosh Levy+, ACL'24, 2024.02</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€å…¥åŠ›é•·ã®æ‹¡å¼µãŒå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®æ€§èƒ½ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’è©•ä¾¡ã™ã‚‹æ–°ã—ã„QAæ¨è«–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ææ¡ˆã€‚ç•°ãªã‚‹é•·ã•ã‚„ã‚¿ã‚¤ãƒ—ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’ç”¨ã„ã¦ã€LLMsã®æ¨è«–æ€§èƒ½ãŒçŸ­ã„å…¥åŠ›é•·ã§è‘—ã—ãä½ä¸‹ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚ã•ã‚‰ã«ã€æ¬¡ã®å˜èªäºˆæ¸¬ãŒLLMsã®æ€§èƒ½ã¨è² ã®ç›¸é–¢ã‚’æŒã¤ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã—ã€LLMsã®é™ç•Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã®æˆ¦ç•¥ã‚’ç¤ºå”†ã™ã‚‹å¤±æ•—ãƒ¢ãƒ¼ãƒ‰ã‚’ç‰¹å®šã—ãŸã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<span class="issue_date">Issue Date: 2025-09-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2978" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] The Impact of Initialization on LoRA Finetuning Dynamics, Soufiane Hayou+, NeurIPS'24, 2024.06</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€LoRAã«ãŠã‘ã‚‹åˆæœŸåŒ–ã®å½¹å‰²ã‚’ç ”ç©¶ã—ã€Bã‚’ã‚¼ãƒ­ã«åˆæœŸåŒ–ã—Aã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«åˆæœŸåŒ–ã™ã‚‹æ–¹å¼ãŒä»–ã®æ–¹å¼ã‚ˆã‚Šã‚‚å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã™ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã—ã¾ã™ã€‚ã“ã®åˆæœŸåŒ–æ–¹å¼ã¯ã€ã‚ˆã‚Šå¤§ããªå­¦ç¿’ç‡ã‚’ä½¿ç”¨ã§ãã‚‹ãŸã‚ã€åŠ¹ç‡çš„ãªå­¦ç¿’ã‚’ä¿ƒé€²ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚LLMsã«é–¢ã™ã‚‹å®Ÿé¨“ã‚’é€šã˜ã¦çµæœã‚’æ¤œè¨¼ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jxmnop/status/1970893830619894186?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>åˆæœŸåŒ–ã§Bã‚’zeroã«ã™ã‚‹ã¨ã„ã†æ‰‹æ³•ã¯ä»¥ä¸‹ã§ã‚‚ææ¡ˆã•ã‚Œã¦ã„ã‚‹ãŒã€æœ¬ç ”ç©¶ã®æ–¹ãŒä¸‹è¨˜ç ”ç©¶ã‚ˆã‚Šã‚‚æŠ•ç¨¿ãŒ1å¹´ç¨‹åº¦æ—©ã„:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2194" target="_blank" rel="noopener noreferrer">[Paper Note] SingLoRA: Low Rank Adaptation Using a Single Matrix, David BensaÃ¯d+, arXiv'25</a>
</p>
<p>openreview:


<a href="https://openreview.net/forum?id=sn3UrYRItk&referrer=%5Bthe%20profile%20of%20Nikhil%20Ghosh%5D(%2Fprofile%3Fid%3D~Nikhil_Ghosh1)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=sn3UrYRItk&referrer=%5Bthe%20profile%20of%20Nikhil%20Ghosh%5D(%2Fprofile%3Fid%3D~Nikhil_Ghosh1)</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-09-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2777" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Lessons from Studying Two-Hop Latent Reasoning, Mikita Balesni+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®äºŒæ®µéšè³ªå•å¿œç­”èƒ½åŠ›ã‚’èª¿æŸ»ã—ã€æ€è€ƒã®é€£é–ï¼ˆCoTï¼‰ã®é‡è¦æ€§ã‚’ç¤ºã™ã€‚åˆæˆäº‹å®Ÿã‚’ç”¨ã„ãŸå®Ÿé¨“ã§ã€ãƒ¢ãƒ‡ãƒ«ã¯äºŒã¤ã®åˆæˆäº‹å®Ÿã‚’çµ„ã¿åˆã‚ã›ã‚‹ã®ã«å¤±æ•—ã™ã‚‹ãŒã€è‡ªç„¶ãªäº‹å®Ÿã¨ã®çµ„ã¿åˆã‚ã›ã§ã¯æˆåŠŸã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€LLMã¯æ½œåœ¨çš„ãªäºŒæ®µéšæ¨è«–èƒ½åŠ›ã‚’æŒã¤ãŒã€ãã®èƒ½åŠ›ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã«ã¯ä¸æ˜ç‚¹ãŒæ®‹ã‚‹ã€‚ç ”ç©¶è€…ã¯ã€LLMã®æ¨è«–èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹éš›ã«ã€ã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆã«ã‚ˆã‚‹è™šå½ã®æˆåŠŸã‚„å¤±æ•—ã«æ³¨æ„ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã‚’å¼·èª¿ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/balesni/status/1966197584499999036?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ä¸‹è¨˜ç ”ç©¶ã§ã¯ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒå›½ã®å ´åˆã¯2 stepæ¨è«–ãŒã§ãã‚‹ã¨ã„ã†ä¾‹å¤–ãŒç”Ÿã˜ã¦ãŠã‚Šã€äº‹å‰å­¦ç¿’ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã§ä½•ã‹è¦‹è½ã¨ã—ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„å¯èƒ½æ€§ãŒã‚ã‚Š:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1564" target="_blank" rel="noopener noreferrer">Do Large Language Models Perform Latent Multi-Hop Reasoning without   Exploiting Shortcuts?, Sohee Yang+, ACL'24</a>
<br><br>ä¸‹è¨˜ç ”ç©¶ã«ãŠã„ã¦ã€å®Œå…¨ã«memorizationzãŒç”Ÿã˜ãªã„å½¢ã§äº‹å‰å­¦ç¿’ã¨Inferenceå®Ÿæ–½ï¼ˆtrain: John Doe lives in **Tokyo**., Test: The people in the city John Doe is from speak **Japanese**.)ã•ã‚ŒãŸãŒã€ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ãŒcityã®å ´åˆã§ã—ã‹è©¦ã•ã‚Œã¦ãŠã‚‰ãšã€ä»–ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã§ã‚‚æ±åŒ–ã™ã‚‹ã®ã‹ï¼Ÿã¨ã„ã†ç–‘å•ãŒã‚ã£ãŸ:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2778" target="_blank" rel="noopener noreferrer">[Paper Note] Extractive Structures Learned in Pretraining Enable Generalization on   Finetuned Facts, Jiahai Feng+, ICML'25</a>
<br><br>æœ¬ç ”ç©¶ã§ã¯17ç¨®é¡ã®ä»–ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã§ã‚‚2 hop reasoningãŒlatentã«å®Ÿæ–½ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ãŸã€‚ã—ã‹ã—ã€ä¸€ã¤ä¸æ€è­°ãªç‚¹ã¨ã—ã¦å½“åˆ2ã¤ã®æ¶ç©ºã®äº‹å®Ÿã‚’LLMã«æ•™ãˆã‚‹ã‚ˆã†ãªå­¦ç¿’ã‚’è©¦ã¿ãŸå ´åˆã¯ã€‚Acc.ãŒ0%ã§ã€lossã‚‚å¶ç„¶ã«ç”Ÿã˜ã‚‹ç¨‹åº¦ã®ã‚‚ã®ã§ã‚ã£ãŸã€‚ã“ã‚Œã‚’æ·±æ˜ã‚Šã™ã‚‹ã¨ã€<br>- åˆæˆ+æœ¬ç‰©ã®äº‹å®Ÿâ†’ã†ã¾ãã„ã<br>- åˆæˆ+åˆæˆâ†’å¤±æ•—<br>- åŒä¸€è¨“ç·´/incontextæ–‡æ›¸å†…ã®åˆæˆã•ã‚ŒãŸäº‹å®Ÿâ†’ã†ã¾ãã„ã<br>ã¨ã„ã†ç¾è±¡ãŒè¦³æ¸¬ã•ã‚Œã€ã“ã®ã“ã¨ã‚ˆã‚Š<br>- å®Ÿä¸–ç•Œã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã®æˆåŠŸã¯ã€latent reasoningãŒãƒ­ãƒã‚¹ãƒˆã«å®Ÿæ–½ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã™ã‚ã‘ã§ã¯ãªãï¼ˆäº‹å‰å­¦ç¿’æ™‚ã®åŒä¸€æ–‡æ›¸å†…ã®å…±èµ·ã‚’åæ˜ ã—ã¦ã„ã‚‹ã ã‘ã®å¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰<br>- åˆæˆãƒ‡ãƒ¼ã‚¿ã§ã®2 hopæ¨è«–ã®å¤±æ•—ã¯ã€latent reasoningã®èƒ½åŠ›ã‚’å¦å®šã™ã‚‹ã‚‚ã®ã§ã¯ãªã„ï¼ˆåˆæˆã•ã‚ŒãŸäº‹å®Ÿã¯å®Ÿä¸–ç•Œã§ã®è‡ªç„¶ãªäº‹å®Ÿã¨ã¯ç•°ãªã‚‹ãŸã‚ã†ã¾ãã„ã£ã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰<br><br>ã¨ã„ã†æ•™è¨“ãŒå¾—ã‚‰ã‚ŒãŸã€ã¨ã„ã£ãŸè©±ãŒå…ƒãƒã‚¹ãƒˆã«æ›¸ã‹ã‚Œã¦ã„ã‚‹ã€‚<br><br>ãªãœå®Œå…¨ã«åˆæˆã•ã‚ŒãŸäº‹å®Ÿæƒ…å ±ã§ã¯å¤±æ•—ã™ã‚‹ã®ã ã‚ã†ã‹ã€‚å…ƒè«–æ–‡ã‚’èª­ã‚“ã§äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ã©ã®ã‚ˆã†ãªã‚‚ã®ãŒåˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèªã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/feng_jiahai/status/1869019495299444830?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Prompting.html" target="_blank" rel="noopener noreferrer">#Prompting</a>
<span class="issue_date">Issue Date: 2025-08-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2546" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] As Generative Models Improve, People Adapt Their Prompts, Eaman Jahani+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å®Ÿé¨“ã§1893äººã®å‚åŠ è€…ã‚’å¯¾è±¡ã«ã€DALL-E 2ã¨DALL-E 3ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é‡è¦æ€§ã®å¤‰åŒ–ã‚’èª¿æŸ»ã€‚DALL-E 3ã‚’ä½¿ç”¨ã—ãŸå‚åŠ è€…ã¯ã€DALL-E 2ã‚ˆã‚Šã‚‚é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ã€ã“ã‚Œã¯æŠ€è¡“çš„èƒ½åŠ›ã®å‘ä¸Šã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è³ªã®å¤‰åŒ–ã«ã‚ˆã‚‹ã‚‚ã®ã€‚ç‰¹ã«ã€DALL-E 3ã®å‚åŠ è€…ã¯ã‚ˆã‚Šé•·ãã€æ„å‘³çš„ã«é¡ä¼¼ã—ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¿®æ­£æ©Ÿèƒ½ã‚’æŒã¤DALL-E 3ã¯ã•ã‚‰ã«é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã—ãŸãŒã€ãã®åˆ©ç‚¹ã¯æ¸›å°‘ã€‚çµæœã¨ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®é€²åŒ–ã«ä¼´ã„ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚‚é©å¿œã•ã‚Œã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dair_ai/status/1959644116305748388?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ImageSegmentation.html" target="_blank" rel="noopener noreferrer">#ImageSegmentation</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/ImageClassification.html" target="_blank" rel="noopener noreferrer">#ImageClassification</a>
<span class="issue_date">Issue Date: 2025-08-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2420" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] MambaOut: Do We Really Need Mamba for Vision?, Weihao Yu+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- Mambaã¯RNNã®ã‚ˆã†ãªãƒˆãƒ¼ã‚¯ãƒ³ãƒŸã‚­ã‚µãƒ¼ã‚’æŒã¤ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã€è¦–è¦šã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦æœŸå¾…å¤–ã‚Œã®æ€§èƒ½ã‚’ç¤ºã™ã€‚Mambaã¯é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¨è‡ªå·±å›å¸°çš„ãªç‰¹æ€§ã«é©ã—ã¦ã„ã‚‹ãŒã€ç”»åƒåˆ†é¡ã«ã¯ä¸å‘ãã§ã‚ã‚‹ã¨ä»®å®šã€‚MambaOutãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã€å®Ÿé¨“ã«ã‚ˆã‚ŠMambaOutãŒImageNetã®ç”»åƒåˆ†é¡ã§è¦–è¦šMambaãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚’ç¤ºã—ã€æ¤œå‡ºãŠã‚ˆã³ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã§ã¯Mambaã®å¯èƒ½æ€§ã‚’æ¢ã‚‹ä¾¡å€¤ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã€‚</span>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<a class="button" href="articles/Scaling%20Laws.html" target="_blank" rel="noopener noreferrer">#Scaling Laws</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="articles/DataFiltering.html" target="_blank" rel="noopener noreferrer">#DataFiltering</a>
<span class="issue_date">Issue Date: 2025-07-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2262" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Scaling Laws for Data Filtering -- Data Curation cannot be Compute   Agnostic, Sachin Goyal+, CVPR'24</a>
<span class="snippet"><span>GPT Summary</span>- è¦–è¦šã¨è¨€èªã®ãƒ¢ãƒ‡ãƒ«ï¼ˆVLMsï¼‰ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã„ã¦ã€é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãŒé‡è¦ã§ã‚ã‚‹ãŒã€è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã¨ã¯ç„¡é–¢ä¿‚ã«è¡Œã‚ã‚Œã‚‹ã“ã¨ãŒå¤šã„ã€‚æœ¬ç ”ç©¶ã§ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®å“è³ªã¨é‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼ˆQQTï¼‰ã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€ã‚¦ã‚§ãƒ–ãƒ‡ãƒ¼ã‚¿ã®éå‡è³ªæ€§ã‚’è€ƒæ…®ã—ãŸãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã‚’ææ¡ˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ‡ãƒ¼ã‚¿ã®æœ‰ç”¨æ€§ã®é•ã„ã‚„ç¹°ã‚Šè¿”ã—ä½¿ç”¨ã«ã‚ˆã‚‹åŠ£åŒ–ã‚’è©•ä¾¡ã—ã€è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¼ãƒ«ã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚‹ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’æ¨å®šå¯èƒ½ã«ã™ã‚‹ã€‚æœ€é©ãªãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¼ãƒ«ã®ã‚­ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’é€šã˜ã¦ã€è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã«å¿œã˜ãŸæœ€é«˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/cloneofsimo/status/1946241642572448174?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§å¤šãã®ç ”ç©¶ãŒãƒ¢ãƒ‡ãƒ«ãŒã‚ˆã‚Šé«˜ã„æ€§èƒ½ã‚’é”æˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ãŒã€é«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ã«ã¯é™ã‚ŠãŒã‚ã‚‹ã“ã¨ã¨ã€ç¹°ã‚Šè¿”ã—å­¦ç¿’ã‚’ã™ã‚‹ã“ã¨ã§ã™ãã«ãã®åŠ¹ç”¨ãŒä½ä¸‹ã™ã‚‹ï¼ˆQuality-Quantity tradeoff!)ã¨ã„ã†ç‰¹æ€§ãŒã‚ã‚‹ã€‚ã“ã®ã‚ˆã†ãªçŠ¶æ³ã«ãŠã„ã¦ã€ãŸã¨ãˆã°è¨ˆç®—ã®äºˆç®—ãŒãƒ‡ãƒ¼ã‚¿6ãƒ‘ã‚±ãƒƒãƒˆåˆ†ã®æ™‚ã«ã€ã‚ã¡ã‚ƒã‚ã¡ã‚ƒãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’é ‘å¼µã£gé«˜å“è³ªãªãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¼ãƒ«Eã®ã¿ã‚’ä½¿ã£ã¦6 epochå­¦ç¿’ã™ã‚‹ã®ãŒè‰¯ã„ã®ã‹ã€å°‘ã—å“è³ªã¯è½ã¡ã‚‹ãƒ‡ãƒ¼ã‚¿Dã‚‚æ··ãœã¦E+Dã‚’3 epochå­¦ç¿’ã™ã‚‹ã®ãŒè‰¯ã„ã®ã‹ã€ã¨ãã«ã©ã¡ã‚‰ãŒè‰¯ã„ã®ã‹ï¼Ÿã¨ã„ã†è©±ã®ã‚ˆã†ã§ã‚ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/06812781-7212-415e-bc7a-dd19ac4ca0d7" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Alignment.html" target="_blank" rel="noopener noreferrer">#Alignment</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/PPO%20(ProximalPolicyOptimization).html" target="_blank" rel="noopener noreferrer">#PPO (ProximalPolicyOptimization)</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<a class="button" href="articles/DPO.html" target="_blank" rel="noopener noreferrer">#DPO</a>
<a class="button" href="articles/On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<span class="issue_date">Issue Date: 2025-06-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2090" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Preference Fine-Tuning of LLMs Should Leverage Suboptimal, On-Policy   Data, Fahim Tajwar+, ICML'24</a>
<span class="snippet"><span>GPT Summary</span>- å¥½ã¿ã®ãƒ©ãƒ™ãƒ«ã‚’ç”¨ã„ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«é–¢ã™ã‚‹ç ”ç©¶ã€‚ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼å¼·åŒ–å­¦ç¿’ã‚„å¯¾ç…§å­¦ç¿’ãªã©ã®æ‰‹æ³•ã‚’æ¯”è¼ƒã—ã€ã‚ªãƒ³ãƒãƒªã‚·ãƒ¼ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã‚„è² ã®å‹¾é…ã‚’ç”¨ã„ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒå„ªã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚«ãƒ†ã‚´ãƒªåˆ†å¸ƒã®ç‰¹å®šã®ãƒ“ãƒ³ã«ãŠã‘ã‚‹ç¢ºç‡è³ªé‡ã‚’è¿…é€Ÿã«å¤‰æ›´ã§ãã‚‹ãƒ¢ãƒ¼ãƒ‰æ¢ç´¢ç›®çš„ã®é‡è¦æ€§ã‚’ç¤ºã—ã€ãƒ‡ãƒ¼ã‚¿åé›†ã®æœ€é©åŒ–ã«é–¢ã™ã‚‹æ´å¯Ÿã‚’æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span><p>ä»¥ä¸‹ã®ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ vs. ã‚ªãƒ³ãƒ©ã‚¤ãƒ³RLã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§æœ¬ç ”ç©¶ãŒå¼•ç”¨ã•ã‚Œã¦ã„ã‚‹:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/cwolferesearch/status/1965088925510520853?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Tools.html" target="_blank" rel="noopener noreferrer">#Tools</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/RAG(RetrievalAugmentedGeneration).html" target="_blank" rel="noopener noreferrer">#RAG(RetrievalAugmentedGeneration)</a>
<span class="issue_date">Issue Date: 2025-06-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2051" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] A Comparative Study of PDF Parsing Tools Across Diverse Document  Categories, Narayan S. Adhikari+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€DocLayNetãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦10ã®äººæ°—PDFãƒ‘ãƒ¼ã‚¹ãƒ„ãƒ¼ãƒ«ã‚’6ã¤ã®æ–‡æ›¸ã‚«ãƒ†ã‚´ãƒªã«ã‚ãŸã‚Šæ¯”è¼ƒã—ã€æƒ…å ±æŠ½å‡ºã®åŠ¹æœã‚’è©•ä¾¡ã—ã¾ã—ãŸã€‚ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã§ã¯PyMuPDFã¨pypdfiumãŒå„ªã‚ŒãŸçµæœã‚’ç¤ºã—ã€ç‰¹ã«ç§‘å­¦æ–‡æ›¸ã‚„ç‰¹è¨±æ–‡æ›¸ã§ã¯NougatãŒé«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã¾ã—ãŸã€‚è¡¨æ¤œå‡ºã§ã¯TATRãŒé‡‘èã‚„æ³•å¾‹æ–‡æ›¸ã§å„ªã‚ŒãŸçµæœã‚’ç¤ºã—ã€Camelotã¯å…¥æœ­æ–‡æ›¸ã§æœ€ã‚‚è‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã—ã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ–‡æ›¸ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸé©åˆ‡ãªãƒ‘ãƒ¼ã‚¹ãƒ„ãƒ¼ãƒ«ã®é¸æŠãŒé‡è¦ã§ã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>PDFã®parsingãƒ„ãƒ¼ãƒ«ã«ã¤ã„ã¦ã€text, tableæŠ½å‡ºã®æ€§èƒ½ã‚’æ§˜ã€…ãªãƒ„ãƒ¼ãƒ«ã¨åˆ†é‡åˆ¥ã«è©•ä¾¡ã—ã¦ã„ã‚‹ã€‚<br><br>F1, precision, recallãªã©ã¯ã€ground truthã¨ã®ãƒ¬ãƒ¼ãƒ™ãƒ³ã‚·ãƒ¥ã‚¿ã‚¤ãƒ³è·é›¢ã‹ã‚‰similarityã‚’è¨ˆç®—ã—ã€0.7ä»¥ä¸Šã§ã‚ã‚Œã°true positiveã¨ã¿ãªã™ã“ã¨ã§è¨ˆç®—ã—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚local alignmentã¯ã€ãƒãƒƒãƒã—ãŸå ´åˆã«åŠ ç‚¹ã€ãƒŸã‚¹ãƒãƒƒãƒã€æœªæ¤œå‡ºã®å ´åˆã«ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’èª²ã™ã‚ˆã†ãªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã«ã‚ˆã£ã¦æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆå…¨ä½“ã®æŠ½å‡ºæ€§èƒ½ã‚’æ¸¬ã‚‹æŒ‡æ¨™ãªæ¨¡æ§˜ã€‚<br><img src="https://github.com/user-attachments/assets/2d2e114f-cc47-4d7d-906a-c505c793d675" alt="image" loading="lazy"><br><br><img src="https://github.com/user-attachments/assets/ccc79865-83d3-47b6-bb47-f2c0a28990c7" alt="image" loading="lazy"></p>
<p>ã‚ˆã‚Šæ€§èƒ½ã‚’é«˜ãã—ãŸã‘ã‚Œã°ã“ã¡ã‚‰ã‚‚å‚è€ƒã«:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jerryjliu0/status/1934988910448492570?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-05-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1929" target="_blank" rel="noopener noreferrer" class="title-link">Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers, Zeyuan Allen-Zhu+, ICML'24 Tutorial</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1919878625488449849?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Canonå±¤ã®ç™ºè¦‹</p>
<p>è‘—è€…ã«ã‚ˆã‚‹è§£èª¬:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/zeyuanallenzhu/status/1918684257058197922?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-05-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1923" target="_blank" rel="noopener noreferrer" class="title-link">Physics of Language Models: Part 3.1, Knowledge Storage and Extraction, Zeyuan Allen-Zhu+, ICML'24</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®çŸ¥è­˜æŠ½å‡ºèƒ½åŠ›ã¯ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®å¤šæ§˜æ€§ã¨å¼·ãç›¸é–¢ã—ã¦ãŠã‚Šã€ååˆ†ãªå¼·åŒ–ãŒãªã‘ã‚Œã°çŸ¥è­˜ã¯è¨˜æ†¶ã•ã‚Œã¦ã‚‚æŠ½å‡ºå¯èƒ½ã§ã¯ãªã„ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚å…·ä½“çš„ã«ã¯ã€ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£åã®éš ã‚ŒåŸ‹ã‚è¾¼ã¿ã«çŸ¥è­˜ãŒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ã€ä»–ã®ãƒˆãƒ¼ã‚¯ãƒ³åŸ‹ã‚è¾¼ã¿ã«åˆ†æ•£ã—ã¦ã„ã‚‹ã‹ã‚’èª¿æŸ»ã€‚LLMã®ãƒ—ãƒ¬ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«é–¢ã™ã‚‹é‡è¦ãªæ¨å¥¨äº‹é …ã¨ã—ã¦ã€è£œåŠ©ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸãƒ‡ãƒ¼ã‚¿å†æ§‹æˆã¨æŒ‡ç¤ºå¾®èª¿æ•´ãƒ‡ãƒ¼ã‚¿ã®æ—©æœŸå–ã‚Šå…¥ã‚ŒãŒææ¡ˆã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>è§£èª¬:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1834" target="_blank" rel="noopener noreferrer">è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç‰©ç†å­¦, ä½è—¤ç«œé¦¬, 2025.03</a>
</p>
<p>SNLP'24ã§ã®è§£èª¬ã‚¹ãƒ©ã‚¤ãƒ‰:<br>


<a href="https://speakerdeck.com/sosk/physics-of-language-models-part-3-1-knowledge-storage-and-extraction" target="_blank" rel="noopener noreferrer">https://speakerdeck.com/sosk/physics-of-language-models-part-3-1-knowledge-storage-and-extraction</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<a class="button" href="articles/SparseAutoEncoder.html" target="_blank" rel="noopener noreferrer">#SparseAutoEncoder</a>
<span class="issue_date">Issue Date: 2025-03-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1802" target="_blank" rel="noopener noreferrer" class="title-link">Sparse Autoencoders Find Highly Interpretable Features in Language   Models, Hoagy Cunningham+, ICLR'24</a>
<span class="snippet"><span>GPT Summary</span>- ç¥çµŒãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å¤šç¾©æ€§ã‚’è§£æ¶ˆã™ã‚‹ãŸã‚ã«ã€ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã‚’ç”¨ã„ã¦å†…éƒ¨æ´»æ€§åŒ–ã®æ–¹å‘ã‚’ç‰¹å®šã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è§£é‡ˆå¯èƒ½ã§å˜ç¾©çš„ãªç‰¹å¾´ã‚’å­¦ç¿’ã—ã€é–“æ¥ç›®çš„èªã®åŒå®šã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹å› æœçš„ç‰¹å¾´ã‚’ã‚ˆã‚Šè©³ç´°ã«ç‰¹å®šã€‚ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ã§æ•™å¸«ãªã—ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒé‡ã­åˆã‚ã›ã®å•é¡Œã‚’è§£æ±ºã§ãã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã€ãƒ¢ãƒ‡ãƒ«ã®é€æ˜æ€§ã¨æ“ä½œæ€§å‘ä¸Šã«å¯„ä¸ã™ã‚‹å¯èƒ½æ€§ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬:


<a href="https://note.com/ainest/n/nbe58b36bb2db" target="_blank" rel="noopener noreferrer">https://note.com/ainest/n/nbe58b36bb2db</a>


</p>
<p>OpenReview:


<a href="https://openreview.net/forum?id=F76bwRSLeK" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=F76bwRSLeK</a>


</p>
<p>SparseAutoEncoderã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ã‚ã‚‰ã‚†ã‚‹ã¨ã“ã‚ã«ä»•è¾¼ã‚ã‚‹ï¼ˆã¨æ€ã‚ã‚Œã‚‹ï¼‰ãŒã€ãŸã¨ãˆã°Transformer Blockã®residual connectionéƒ¨åˆ†ã®ãƒ™ã‚¯ãƒˆãƒ«ã«å¯¾ã—ã¦Feature Dictionaryã‚’å­¦ç¿’ã™ã‚‹ã¨ã€å½“è©²ãƒ–ãƒ­ãƒƒã‚¯ã«ãŠã„ã¦ã©ã®ã‚ˆã†ãªç‰¹å¾´ã®çµ„ã¿åˆã‚ã›ãŒè¡¨ç¾ã•ã‚Œã¦ã„ã‚‹ã‹ãŒï¼ˆã‚ãã¾ã§SparseAutoEncoderãŒreconstruction lossã«ã‚ˆã£ã¦å­¦ç¿’ã•ã‚ŒãŸçµæœã‚’ç”¨ã„ã¦ï¼‰è§£é‡ˆã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/f86f5f7b-f46d-48ab-94e3-cf7f298eb9d7" alt="image" loading="lazy"><br><br>SparseAutoEncoderã¯ä¸‹è¨˜å¼ã§è¡¨ã•ã‚Œã€ä¸‹è¨˜loss functionã§å­¦ç¿’ã•ã‚Œã‚‹ã€‚MãŒFeature Matrixï¼ˆrow-wiseã«æ­£è¦åŒ–ã•ã‚Œã¦å¾Œè¿°ã®cã«å¯¾ã™ã‚‹L1æ­£å‰‡åŒ–ã«å½±éŸ¿ã‚’ä¸ãˆãªã„ã‚ˆã†ã«ã—ã¦ã„ã‚‹ï¼‰ã«ç›¸å½“ã™ã‚‹ã€‚cã«å¯¾ã—ã¦L1æ­£å‰‡åŒ–ã‚’ã‹ã‘ã‚‹ã“ã¨ã§ï¼ˆSparsity Lossï¼‰ã€cä¸­ã®å„è¦ç´ ãŒ0ã«è¿‘ã¥ãã‚ˆã†ã«ãªã‚Šã€çµæœã¨ã—ã¦cãŒSparseã¨ãªã‚‹ï¼ˆã©ã†ã—ã¦ã‚‚å€¤ã‚’æŒãŸãªã‘ã‚Œã°ã„ã‘ãªã„é‡è¦ãªç‰¹å¾´é‡ã®ã¿ã«ãƒ•ã‚©ãƒ¼ã‚«ã‚¹ã•ã‚Œã‚‹ã‚ˆã†ã«ãªã‚‹ï¼‰ã€‚<br><img src="https://github.com/user-attachments/assets/7e400f25-8a63-4222-904c-4a7b94d50880" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/dd8c10b3-3bb5-46fb-b94a-d91f3602bbd1" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/RLHF.html" target="_blank" rel="noopener noreferrer">#RLHF</a>
<span class="issue_date">Issue Date: 2025-01-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1650" target="_blank" rel="noopener noreferrer" class="title-link">Does RLHF Scale? Exploring the Impacts From Data, Model, and Method, Zhenyu Hou+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€LLMsã«ãŠã‘ã‚‹RLHFã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç‰¹æ€§ã‚’åˆ†æã—ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã€ãƒ‡ãƒ¼ã‚¿æ§‹æˆã€æ¨è«–äºˆç®—ãŒãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¿æŸ»ã€‚ãƒ‡ãƒ¼ã‚¿ã®å¤šæ§˜æ€§ã¨é‡ã®å¢—åŠ ãŒå ±é…¬ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½å‘ä¸Šã«å¯„ä¸ã™ã‚‹ä¸€æ–¹ã€ãƒãƒªã‚·ãƒ¼ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ã¯å¿œç­”ã‚µãƒ³ãƒ—ãƒ«æ•°ã®å¢—åŠ ãŒåˆæœŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ãŒã€ã™ãã«é ­æ‰“ã¡ã«ãªã‚‹ã“ã¨ãŒåˆ¤æ˜ã€‚RLHFã¯äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚ˆã‚ŠåŠ¹ç‡çš„ã«ã‚¹ã‚±ãƒ¼ãƒ«ã›ãšã€è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã®åç›Šé€“æ¸›ãŒè¦³å¯Ÿã•ã‚ŒãŸã€‚è¨ˆç®—åˆ¶é™å†…ã§ã®RLHFãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–æˆ¦ç•¥ã‚‚ææ¡ˆã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dair_ai/status/1868299930600628451?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/MachineTranslation.html" target="_blank" rel="noopener noreferrer">#MachineTranslation</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<span class="issue_date">Issue Date: 2025-01-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1641" target="_blank" rel="noopener noreferrer" class="title-link">How Much Data is Enough Data? Fine-Tuning Large Language Models for  In-House Translation: Performance Evaluation Across Multiple Dataset Sizes, Inacio Vieira+, AMTA'24</a>
<span class="snippet"><span>GPT Summary</span>- LLMsã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ç¿»è¨³ãƒ¡ãƒ¢ãƒªï¼ˆTMsï¼‰ã‚’æ´»ç”¨ã—ã€ç‰¹å®šã®çµ„ç¹”å‘ã‘ã®ç¿»è¨³ç²¾åº¦ã¨åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ç ”ç©¶ã€‚5ã¤ã®ç¿»è¨³æ–¹å‘ã§ç•°ãªã‚‹ã‚µã‚¤ã‚ºã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦å®Ÿé¨“ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒå¢—ãˆã‚‹ã»ã©ç¿»è¨³ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‘ä¸Šã™ã‚‹ã“ã¨ã‚’ç¢ºèªã€‚ç‰¹ã«ã€1kãŠã‚ˆã³2kã®ä¾‹ã§ã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒä½ä¸‹ã™ã‚‹ãŒã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µã‚¤ã‚ºãŒå¢—åŠ ã™ã‚‹ã«ã¤ã‚Œã¦æ”¹å–„ãŒè¦‹ã‚‰ã‚Œã‚‹ã€‚LLMsã¨TMsã®çµ±åˆã«ã‚ˆã‚Šã€ä¼æ¥­ç‰¹æœ‰ã®ãƒ‹ãƒ¼ã‚ºã«å¿œã˜ãŸã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºç¿»è¨³ãƒ¢ãƒ‡ãƒ«ã®å¯èƒ½æ€§ã‚’ç¤ºå”†ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gyakuse/status/1874357127248306200?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>QLoRAã§Llama 8B Instructã‚’MTã®ãƒ‡ãƒ¼ã‚¿ã§SFTã—ãŸå ´åˆã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã«å¯¾ã™ã‚‹æ€§èƒ½ã®å¤‰åŒ–ã‚’æ¤œè¨¼ã—ã¦ã„ã‚‹ã€‚ãŸã ã—ã€æ¤œè¨¼ã—ã¦ã„ã‚‹ã‚¿ã‚¹ã‚¯ã¯MTã€QLoRAã§SFTã‚’å®Ÿæ–½ã—rankã¯64ã€å­¦ç¿’æ™‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ãªã‚‚ã®ã§ã‚ã‚‹ãªã©ã€å¹…åºƒã„è¨­å®šã§å­¦ç¿’ã—ã¦ã„ã‚‹ã‚ã‘ã§ã¯ãªã„ã®ã§ã€ã“ã“ã§å¾—ã‚‰ã‚ŒãŸçŸ¥è¦‹ãŒå¹…åºƒãé©ç”¨å¯èƒ½ãªã“ã¨ã¯ç¤ºã•ã‚Œã¦ã„ãªã„ã§ã‚ã‚ã†ç‚¹ã€ã«ã¯æ³¨æ„ãŒå¿…è¦ã ã¨æ€ã‚ã‚Œã‚‹ã€‚<br><br>ã“ã®è¨­å®šã§ã¯ã€SFTã§åˆ©ç”¨ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒå¢—ãˆã‚Œã°å¢—ãˆã‚‹ã»ã©æ€§èƒ½ãŒä¸ŠãŒã£ã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã‚‹ã€‚<br><br><img src="https://github.com/user-attachments/assets/71309a00-85fd-491f-a89e-c9cb99f4da6c" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/ea1eba38-9488-43e5-a64b-f997bf65f57b" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/21b21628-d589-4214-8860-680e392a2556" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2024-12-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1590" target="_blank" rel="noopener noreferrer" class="title-link">The broader spectrum of in-context learning, Andrew Kyle Lampinen+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®å°‘æ•°ã‚·ãƒ§ãƒƒãƒˆå­¦ç¿’ã‚’ãƒ¡ã‚¿å­¦ç¿’ã«åŸºã¥ãæ–‡è„ˆå†…å­¦ç¿’ã®ä¸€éƒ¨ã¨ã—ã¦ä½ç½®ã¥ã‘ã€æ–‡è„ˆãŒäºˆæ¸¬ã®æå¤±ã‚’æ¸›å°‘ã•ã›ã‚‹ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ææ¡ˆã—ã¾ã™ã€‚ã“ã®è¦–ç‚¹ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ–‡è„ˆå†…èƒ½åŠ›ã‚’çµ±ä¸€ã—ã€ä¸€èˆ¬åŒ–ã®é‡è¦æ€§ã‚’å¼·èª¿ã—ã¾ã™ã€‚ä¸€èˆ¬åŒ–ã¯æ–°ã—ã„å­¦ç¿’ã ã‘ã§ãªãã€ç•°ãªã‚‹æç¤ºã‹ã‚‰ã®å­¦ã³ã‚„é©ç”¨èƒ½åŠ›ã«ã‚‚é–¢é€£ã—ã€éå»ã®æ–‡çŒ®ã¨ã®é–¢é€£æ€§ã‚‚è­°è«–ã•ã‚Œã¾ã™ã€‚æ–‡è„ˆå†…å­¦ç¿’ã®ç ”ç©¶ã¯ã€åºƒç¯„ãªèƒ½åŠ›ã¨ä¸€èˆ¬åŒ–ã®ã‚¿ã‚¤ãƒ—ã‚’è€ƒæ…®ã™ã¹ãã¨çµè«–ä»˜ã‘ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>OpenReview:


<a href="https://openreview.net/forum?id=RHo3VVi0i5" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=RHo3VVi0i5</a>


<br><br>OpenReviewã«ã‚ˆã‚‹ã¨ã€<br>è«–æ–‡ã¯ç†è§£ã—ã‚„ã™ãã€meta learningã«ã¤ã„ã¦åºƒç¯„ã«ã‚µãƒ¼ãƒ™ã‚¤ã•ã‚Œã¦ã„ã‚‹ã€‚ã—ã‹ã—ã€è«–æ–‡ãŒå®šç¾©ã—ã¦ã„ã‚‹ICLã®æ‹¡å¼µã¯ICLã‚’éåº¦ã«ä¸€èˆ¬åŒ–ã—éãã¦ãŠã‚Šï¼ˆå…·ä½“çš„ã«ä½•ãŒICLã§ä½•ãŒICLã§ãªã„ã®ã‹ã€ã¨ã„ã£ãŸè¦å®šãŒã§ããªã„ï¼‰ã€ã‹ã¤è«–æ–‡ä¸­ã§ææ¡ˆã•ã‚Œã¦ã„ã‚‹ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’è£ä»˜ã‘ã‚‹å®Ÿé¨“ãŒãªãspeculativeã§ã‚ã‚‹ã€ã¨ã®ã“ã¨ã§rejectã•ã‚Œã¦ã„ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Quantization.html" target="_blank" rel="noopener noreferrer">#Quantization</a>
<span class="issue_date">Issue Date: 2024-12-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1566" target="_blank" rel="noopener noreferrer" class="title-link">The Super Weight in Large Language Models, Mengxia Yu+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- LLMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸€éƒ¨ãŒãƒ¢ãƒ‡ãƒ«ã®å“è³ªã«ä¸å‡è¡¡ã«é‡è¦ã§ã‚ã‚Šã€1ã¤ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‰ªå®šã§ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆèƒ½åŠ›ãŒå¤§å¹…ã«ä½ä¸‹ã™ã‚‹ã“ã¨ã‚’ç™ºè¦‹ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒªãƒ¼ã®æ–¹æ³•ã§é‡è¦ãªã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç‰¹å®šã—ã€ã“ã‚Œã«ã‚ˆã‚Šå››æ¨äº”å…¥é‡å­åŒ–ã®ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹ç ”ç©¶ã‚’ä¿ƒé€²ã™ã‚‹ãŸã‚ã«ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚¢ã‚¯ã‚»ã‚¹ã®LLMã«å¯¾ã™ã‚‹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æä¾›ã€‚</span>
<span class="snippet"><span>Comment</span><p>å›³ã«ã‚ã‚‹é€šã‚Šã€ãŸã£ãŸä¸€ã¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä¸­ã®é‡ã¿ã‚’0ã«ã™ã‚‹ã ã‘ã§ã€é€”ç«¯ã«æ„å‘³ã®ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆãŒç”Ÿæˆã§ããªããªã‚‹ã‚ˆã†ãªé‡ã¿ãŒå­˜åœ¨ã™ã‚‹ã‚‰ã—ã„ã€‚<br><img src="https://github.com/user-attachments/assets/065e921b-c447-4c0d-b1de-a2f79bd090f8" alt="image" loading="lazy"><br><br><br>ï¼ˆå›³ã¯è«–æ–‡ã‚ˆã‚Šå¼•ç”¨ï¼‰</p>
<p>ICLR 2025ã®Openreview<br>


<a href="https://openreview.net/forum?id=0Ag8FQ5Rr3" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=0Ag8FQ5Rr3</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Prompting.html" target="_blank" rel="noopener noreferrer">#Prompting</a>
<span class="issue_date">Issue Date: 2024-11-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1549" target="_blank" rel="noopener noreferrer" class="title-link">Does Prompt Formatting Have Any Impact on LLM Performance?, Jia He+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ã¯LLMã®æ€§èƒ½ã«é‡è¦ã§ã‚ã‚Šã€ç•°ãªã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’èª¿æŸ»ã€‚å®Ÿé¨“ã§ã¯ã€GPT-3.5-turboãŒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã«ã‚ˆã£ã¦ã‚³ãƒ¼ãƒ‰ç¿»è¨³ã‚¿ã‚¹ã‚¯ã§æœ€å¤§40%å¤‰å‹•ã™ã‚‹ä¸€æ–¹ã€GPT-4ã¯ã‚ˆã‚Šå …ç‰¢ã§ã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å›ºå®šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã®å†è€ƒãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ãŒå¼·èª¿ã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>ï¼ˆä»¥ä¸‹ã€å€‹äººã®æ„Ÿæƒ³ã§ã™ï¼‰<br>æœ¬æ–‡ã®ã¿æ–œã‚èª­ã¿ã—ã¦ã€Appendixã¯çœºã‚ãŸã ã‘ãªã®ã§çš„å¤–ã‚Œãªã“ã¨ã‚’è¨€ã£ã¦ã„ãŸã‚‰ã™ã¿ã¾ã›ã‚“ã€‚<br><br>ã¾ãšã€å®Ÿå‹™ä¸Šä¸‹è¨˜çŸ¥è¦‹ã¯æœ‰ç”¨ã ã¨æ€ã„ã¾ã—ãŸ:<br>- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ã‚ˆã£ã¦æ€§èƒ½ã«å¤§ããªå·®ãŒã‚ã‚‹<br>- ã‚ˆã‚Šå¤§ãã„ãƒ¢ãƒ‡ãƒ«ã®æ–¹ãŒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¯¾ã—ã¦ãƒ­ãƒã‚¹ãƒˆ<br><br>ãŸã ã—ã€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ã‚ˆã£ã¦æ€§èƒ½å·®ãŒã‚ã‚‹ã¨ã„ã†ã®ã¯çµŒé¨“çš„ã«ã‚ã‚‹ç¨‹åº¦LLMã‚’è§¦ã£ã¦ã„ã‚‹äººãªã‚‰åˆ†ã‹ã‚‹ã“ã¨ã ã¨æ€ã†ã®ã§ã€é©šãã¯å°‘ãªã‹ã£ãŸã€‚<br><br>å€‹äººçš„ã«æ°—ã«ãªã‚‹ç‚¹ã¯ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚‚ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚‚åˆ†ã‹ã‚‰ãªã„GPT3.5, GPT4ã®ã¿ã§å®Ÿé¨“ã‚’ã—ã¦ã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå¤§ãã„æ–¹ãŒãƒ­ãƒã‚¹ãƒˆã€ã¨çµè«–ã¥ã‘ã¦ã„ã‚‹ç‚¹ã¨ã€ã‚‚ã†å°‘ã—æ·±æ˜ã‚Šã—ã¦è€ƒå¯Ÿã—ãŸã‚‰ã‚‚ã£ã¨ãŠã‚‚ã—ã‚ã„ã®ã«ãªã€ã¨æ„Ÿã˜ã‚‹ç‚¹ã§ã™ã€‚<br><br>å®Ÿå‹™ä¸Šã¯æœ‰ç›ŠãªçŸ¥è¦‹ã ã¨ã—ã¦ã€ã§ã¯ç ”ç©¶ã¨ã—ã¦è¦‹ãŸã¨ãã«ã€Œãªãœãã†ãªã‚‹ã®ã‹?ã€ã¨ã„ã†ã¨ã“ã‚ã‚’è¿½æ±‚ã—ã¦æ¬²ã—ã„ãªãã€ã¨ã„ã†æ„Ÿæƒ³ã‚’æŒã¡ã¾ã—ãŸã€‚<br>ãŸã¨ãˆã°ã€ã€Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºãŒå¤§ãã„ãƒ¢ãƒ‡ãƒ«ã®æ–¹ãŒãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ãƒ­ãƒã‚¹ãƒˆã€ã¨è«–æ–‡ä¸­ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«è¦‹ãˆã¾ã™ãŒã€<br>ãã‚Œã¯æœ¬å½“ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹ã‚‚ã®ãªã®ã‹ï¼Ÿå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹å„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®å‰²åˆã¨ã‹ï¼ˆã“ã‚Œã¯äº‹å®Ÿã¯OpenAIã®ä¸­ã®äººã—ã‹åˆ†ã‹ã‚‰ãªã„ã®ã§ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ãŒã‚ã‚‹ç¨‹åº¦ã‚ªãƒ¼ãƒ—ãƒ³ã«ãªã£ã¦ã„ã‚‹OpenLLMã§ã‚‚æ¤œè¨¼ã™ã‚‹ã¨ã‹ï¼‰ã€è©•ä¾¡ã™ã‚‹ã‚¿ã‚¹ã‚¯ã¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ç›¸æ€§ã¨ã‹ã€è‰²ã€…ã¨è€ƒå¯Ÿã§ãã‚‹è¦ç´ ãŒã‚ã‚‹ã®ã§ã¯ãªã„ã‹ã¨æ€ã„ã¾ã—ãŸã€‚<br>ãã®ä¸Šã§ã€å¤§éƒ¨åˆ†ã®LLMã§æ™®éçš„ãªçŸ¥è¦‹ã‚’è¦‹å‡ºã—ãŸæ–¹ãŒç ”ç©¶ã¨ã—ã¦ã‚ˆã‚Šé¢ç™½ããªã‚‹ã®ã§ã¯ãªã„ã‹ã€ã¨æ„Ÿã˜ã¾ã—ãŸã€‚<br><br><img src="https://github.com/user-attachments/assets/d0a6c727-1253-4503-93f2-8daa4db2321b" alt="image" loading="lazy"><br><img src="https://github.com/user-attachments/assets/b7166b2b-b848-43f5-a823-7ed491232234" alt="image" loading="lazy"></p>
<p>å‚è€ƒ: Data2Textã«ãŠã‘ã‚‹æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®input formatã«ã‚ˆã‚‹æ€§èƒ½å·®ã‚’åˆ†æã—è€ƒå¯Ÿã—ã¦ã„ã‚‹ç ”ç©¶<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1267" target="_blank" rel="noopener noreferrer">Prompting for Numerical Sequences: A Case Study on Market Comment
  Generation, Masayuki Kawarada+, N/A, arXiv'24</a>
</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2024-11-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1540" target="_blank" rel="noopener noreferrer" class="title-link">Observational Scaling Laws and the Predictability of Language Model  Performance, Yangjun Ruan+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- è¨€èªãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€ç´„100ã®å…¬é–‹ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ³•å‰‡ã‚’æ§‹ç¯‰ã™ã‚‹æ–°ã—ã„è¦³å¯Ÿã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ææ¡ˆã€‚ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼é–“ã®èƒ½åŠ›å¤‰å‹•ã‚’è€ƒæ…®ã—ã€æ€§èƒ½ãŒä½æ¬¡å…ƒã®èƒ½åŠ›ç©ºé–“ã®é–¢æ•°ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€è¤‡é›‘ãªã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ç¾è±¡ã®äºˆæ¸¬å¯èƒ½æ€§ã‚’ç¤ºã—ã€GPT-4ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæ€§èƒ½ã‚’éã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçš„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‹ã‚‰äºˆæ¸¬ã§ãã‚‹ã“ã¨ã‚’æ˜ã‚‰ã‹ã«ã—ã€Chain-of-Thoughtã‚„Self-Consistencyã®å½±éŸ¿ã‚’äºˆæ¸¬ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>ç¸¦è»¸ãŒdownstreamã‚¿ã‚¹ã‚¯ã®ä¸»æˆåˆ†ï¼ˆã®ã†ã¡æœ€ã‚‚å¤§ãã„80%ã‚’èª¬æ˜ã™ã‚‹æˆåˆ†ï¼‰ã®å¤‰åŒ–ï¼ˆâ‰’LLMã®æ€§èƒ½ï¼‰ã§ã€æ¨ªè»¸ãŒlog scaleã®æŠ•å…¥è¨ˆç®—é‡ã€‚<br>Qwenã‚‚é ‘å¼µã£ã¦ã„ã‚‹ãŒã€æŠ•å…¥ãƒ‡ãƒ¼ã‚¿é‡ã«å¯¾ã™ã‚‹æ€§èƒ½ï¼ˆâ‰’ãƒ‡ãƒ¼ã‚¿ã®å“è³ªï¼‰ã§ã¯ã€å…ˆé§†ã‘çš„ãªç ”ç©¶ã§ã‚ã‚‹PhiãŒã‚„ã¯ã‚Šåœ§å€’çš„?<br><img src="https://github.com/user-attachments/assets/c38286df-37c1-4c72-832f-676832845c0e" alt="image" loading="lazy"></p>
<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/766" target="_blank" rel="noopener noreferrer">Textbooks Are All You Need, Suriya Gunasekar+, N/A, arXiv'23</a>
<br><br>ã‚‚å‚ç…§ã®ã“ã¨</p></span><br><br>
<a class="button" href="articles/InformationRetrieval.html" target="_blank" rel="noopener noreferrer">#InformationRetrieval</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/RAG(RetrievalAugmentedGeneration).html" target="_blank" rel="noopener noreferrer">#RAG(RetrievalAugmentedGeneration)</a>
<span class="issue_date">Issue Date: 2024-11-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1530" target="_blank" rel="noopener noreferrer" class="title-link">Likelihood as a Performance Gauge for Retrieval-Augmented Generation, Tianyu Liu+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸæƒ…å ±æ¤œç´¢å¼·åŒ–ç”Ÿæˆã¯ã€æ–‡è„ˆå†…ã®æ–‡æ›¸ã®é †åºã«å½±éŸ¿ã‚’å—ã‘ã‚„ã™ã„ã€‚ç ”ç©¶ã§ã¯ã€è³ªå•ã®ç¢ºç‡ãŒãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’åˆ†æã—ã€æ­£ç¢ºæ€§ã¨ã®ç›¸é–¢é–¢ä¿‚ã‚’æ˜ã‚‰ã‹ã«ã—ãŸã€‚è³ªå•ã®ç¢ºç‡ã‚’æŒ‡æ¨™ã¨ã—ã¦ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é¸æŠã¨æ§‹ç¯‰ã«é–¢ã™ã‚‹2ã¤ã®æ–¹æ³•ã‚’ææ¡ˆã—ã€ãã®åŠ¹æœã‚’å®Ÿè¨¼ã€‚ç¢ºç‡ã«åŸºã¥ãæ‰‹æ³•ã¯åŠ¹ç‡çš„ã§ã€å°‘ãªã„ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã§å¿œç­”ã‚’ç”Ÿæˆã§ãã‚‹ãŸã‚ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ã®æ–°ãŸãªæ–¹å‘æ€§ã‚’ç¤ºã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>ãƒˆãƒ¼ã‚¯ãƒ³ãƒ¬ãƒ™ãƒ«ã®å¹³å‡å€¤ã‚’ã¨ã£ãŸç”Ÿæˆãƒ†ã‚­ã‚¹ãƒˆã®å¯¾æ•°å°¤åº¦ã¨ã€RAGã®å›ç­”æ€§èƒ½ã«é–¢ã™ã‚‹åˆ†æã‚’ã—ãŸæ¨¡æ§˜ã€‚<br><img src="https://github.com/user-attachments/assets/ac03c0b6-b16c-4992-8446-2f56bad09ab2" alt="image" loading="lazy"><br><br>ã¨ã‚Šã‚ãˆãšã€ã‚‚ã—ã€ŒLLMã¨ã—ã¦GPTã‚’ï¼ˆOpenAIã®APIã‚’ç”¨ã„ã¦ï¼‰ä½¿ã„ã¾ã—ãŸï¼temperatureã¯0ã§ã™ï¼ã€ã¿ãŸã„ãªå®Ÿé¨“è¨­å®šã ã£ãŸã‚‰è«¸ã€…æ€ªã—ããªã‚‹æ°—ãŒã—ãŸã®ã§ãã“ãŒå¤§ä¸ˆå¤«ãªã“ã¨ã‚’ç¢ºèªã—ãŸï¼ˆOpenLLMã€ã‹ã¤deterministicãªãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ–¹æ³•ãŒæœ›ã¾ã—ã„ï¼‰ã€‚ãŠã‚‚ã—ã‚ãã†ã€‚<br><br><img src="https://github.com/user-attachments/assets/9ba2bdc7-f6e5-4b9c-aca4-3d461c78a046" alt="image" loading="lazy"></p>
<p>å‚è€ƒ: [RAGã®ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã‚’å°¤åº¦ã§é˜²ã, sasakuna, 2024.11.19](


<a href="https://zenn.dev/knowledgesense/articles/7c47e1796e96c0)" target="_blank" rel="noopener noreferrer">https://zenn.dev/knowledgesense/articles/7c47e1796e96c0)</a>


</p>
<p>
<strong>## å‚è€ƒ<br><br>ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®å°¤åº¦ã‚’ç”¨ã„ã¦ã€ã©ã®ç¨‹åº¦æ­£è§£ã‚‰ã—ã„ã‹ã‚’åˆ¤æ–­ã™ã‚‹ã€ã¨ã„ã£ãŸè©±ã¯<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1223" target="_blank" rel="noopener noreferrer">G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment, Yang Liu+, N/A, EMNLP'23</a>
</strong>
<br>
<br><br>ã®ã‚ˆã†ãªLLM-as-a-Judgeã§ã‚‚è¡Œã‚ã‚Œã¦ã„ã‚‹ã€‚<br><br>G-Evalã§ã¯1--5ã®ã‚¹ã‚³ã‚¢ã®ã‚ˆã†ãªé›¢æ•£çš„ãªå€¤ã‚’ç”Ÿæˆã™ã‚‹éš›ã«ã€ã“ã‚Œã‚‰ã‚’é€£ç¶šçš„ãªã‚¹ã‚³ã‚¢ã«è£œæ­£ã™ã‚‹ãŸã‚ã«ã€å°¤åº¦ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ã®ç”Ÿæˆç¢ºç‡ï¼‰ã‚’ç”¨ã„ã¦ã„ã‚‹ã€‚<br>ãŸã ã—ã€G-Evalã®å ´åˆã¯å®Ÿé¨“ã§GPTã‚’ç”¨ã„ã¦ã„ã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç›´æ¥å°¤åº¦ã‚’å–å¾—ã§ããšã€ä»£ã‚ã‚Šã«temperature1ã¨ã—ã€20å›ç¨‹åº¦ç”Ÿæˆã‚’è¡Œã£ãŸçµæœã‹ã‚‰ã‚¹ã‚³ã‚¢ãƒˆãƒ¼ã‚¯ãƒ³ã®ç”Ÿæˆç¢ºç‡ã‚’æ“¬ä¼¼çš„ã«è¨ˆç®—ã—ã¦ã„ã‚‹ã€‚<br><br>G-Evalã®è¨­å®šã¨æ¯”è¼ƒã™ã‚‹ã¨ï¼ˆå½“æ™‚ã¯ã¤ã‚ˆã¤ã‚ˆãªOpenLLMãŒãªã‹ã£ãŸãŸã‚è‹¦è‚‰ã®ç­–ã ã£ãŸã¨æ€ã‚ã‚Œã‚‹ãŒï¼‰ã€ã“ã¡ã‚‰ã®ç ”ç©¶ã®å®Ÿé¨“è¨­å®šã®æ–¹ãŒæœ›ã¾ã—ã„ã¨æ€ã†ã€‚</p></span><br><br>
<a class="button" href="articles/EfficiencyImprovement.html" target="_blank" rel="noopener noreferrer">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/Japanese.html" target="_blank" rel="noopener noreferrer">#Japanese</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2024-11-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1524" target="_blank" rel="noopener noreferrer" class="title-link">Balancing Speed and Stability: The Trade-offs of FP8 vs. BF16 Training  in LLMs, Kazuki Fujii+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã¯ã€ãã®è¨€èªç†è§£èƒ½åŠ›ã¨é©ç”¨å¯èƒ½æ€§ã‹ã‚‰æ³¨ç›®ã‚’é›†ã‚ã¦ãŠã‚Šã€ç‰¹ã«Llama 3ã‚·ãƒªãƒ¼ã‚ºã¯4050å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®åŠ¹ç‡åŒ–ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹ä¸­ã€NVIDIAã®H100 GPUã¯FP8ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’å°å…¥ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“ã‚’çŸ­ç¸®ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚åˆæœŸç ”ç©¶ã§ã¯FP8ãŒæ€§èƒ½ã‚’æãªã‚ãšã«åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚Œã¦ã„ã‚‹ãŒã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®‰å®šæ€§ã‚„ä¸‹æµã‚¿ã‚¹ã‚¯ã¸ã®å½±éŸ¿ã¯ã¾ã ä¸æ˜ã§ã‚ã‚‹ã€‚æœ¬ç ”ç©¶ã¯ã€LLMsã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ãŠã‘ã‚‹BF16ã¨FP8ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æ¢ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/okoge_kaz/status/1857639065421754525?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>FP8ã§ç¶™ç¶šçš„äº‹å‰å­¦ç¿’ã‚’ã™ã‚‹ã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã¯å‘ä¸Šã™ã‚‹ãŒã€lossã®ã‚¹ãƒ‘ã‚¤ã‚¯ã‚’ç”Ÿã˜ãŸã‚Šã€downstreamã‚¿ã‚¹ã‚¯ã®æ€§èƒ½ãŒBF16ã‚ˆã‚Šã‚‚ä½ä¸‹ã—ãŸã‚Šã™ã‚‹ï¼ˆæ—¥æœ¬èªã¨è‹±èªã®ä¸¡æ–¹ï¼‰ã¨ã®å ±å‘Šã®ã‚ˆã†ã§ã‚ã‚‹ã€‚ç¾çŠ¶ã‚¢ãƒ–ã‚¹ãƒˆã¨ä»˜éŒ²ã—ã‹è¨˜è¼‰ãŒãªã„ãŒã€å†…å®¹ã¯ã“ã‚Œã‹ã‚‰æ›´æ–°ã•ã‚Œã‚‹ã®ã ã‚ã†ã‹ã€‚<br><br><img src="https://github.com/user-attachments/assets/8d60d59b-de00-483a-bff0-04a4145715c1" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2024-11-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1522" target="_blank" rel="noopener noreferrer" class="title-link">The Geometry of Concepts: Sparse Autoencoder Feature Structure, Yuxiao Li+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- ã‚¹ãƒ‘ãƒ¼ã‚¹ã‚ªãƒ¼ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¯ã€é«˜æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã®è¾æ›¸ã‚’ç”Ÿæˆã—ã€æ¦‚å¿µã®å®‡å®™ã«ä¸‰ã¤ã®èˆˆå‘³æ·±ã„æ§‹é€ ã‚’ç™ºè¦‹ã—ãŸã€‚1) å°è¦æ¨¡æ§‹é€ ã§ã¯ã€å¹³è¡Œå››è¾ºå½¢ã‚„å°å½¢ã®ã€Œçµæ™¶ã€ãŒã‚ã‚Šã€å˜èªã®é•·ã•ãªã©ã®å¹²æ¸‰ã‚’é™¤å»ã™ã‚‹ã“ã¨ã§è³ªãŒæ”¹å–„ã•ã‚Œã‚‹ã€‚2) ä¸­è¦æ¨¡æ§‹é€ ã§ã¯ã€æ•°å­¦ã¨ã‚³ãƒ¼ãƒ‰ã®ç‰¹å¾´ãŒã€Œè‘‰ã€ã‚’å½¢æˆã—ã€ç©ºé–“çš„å±€æ‰€æ€§ãŒå®šé‡åŒ–ã•ã‚Œã€ç‰¹å¾´ãŒäºˆæƒ³ä»¥ä¸Šã«é›†ã¾ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚3) å¤§è¦æ¨¡æ§‹é€ ã§ã¯ã€ç‰¹å¾´ç‚¹é›²ãŒå„å‘åŒæ€§ã§ãªãã€å›ºæœ‰å€¤ã®ã¹ãæ³•å‰‡ã‚’æŒã¡ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼ãŒå±¤ã«ä¾å­˜ã™ã‚‹ã“ã¨ãŒå®šé‡åŒ–ã•ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å‚è€ƒ: 


<a href="https://ledge.ai/articles/llm_conceptual_structure_sae" target="_blank" rel="noopener noreferrer">https://ledge.ai/articles/llm_conceptual_structure_sae</a>


</p>
<p>[Perplexityï¼ˆå‚è€ƒ;Hallucinationã«æ³¨æ„ï¼‰](


<a href="https://www.perplexity.ai/search/yi-xia-nolun-wen-wodu-minei-ro-kR626A9_R8.6CU7IKvGyhQ)" target="_blank" rel="noopener noreferrer">https://www.perplexity.ai/search/yi-xia-nolun-wen-wodu-minei-ro-kR626A9_R8.6CU7IKvGyhQ)</a>


</p></span><br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<span class="issue_date">Issue Date: 2024-11-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1509" target="_blank" rel="noopener noreferrer" class="title-link">A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and  Error-Aware Demonstration, Yingqian Cui+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- Few-shot Chain-of-Thought (CoT) ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã¯LLMsã®æ¨è«–èƒ½åŠ›ã‚’å‘ä¸Šã•ã›ã‚‹ãŒã€å¾“æ¥ã®ç ”ç©¶ã¯æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ã‚’åˆ†é›¢ã•ã‚ŒãŸæ–‡è„ˆå†…å­¦ç¿’ã«ä¾å­˜ã—ã¦ã„ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€åˆæœŸã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰ã®ä¸€è²«ã—ãŸæ¨è«–ï¼ˆCoherent CoTï¼‰ã‚’çµ±åˆã™ã‚‹ã“ã¨ã§ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã®ã‚¨ãƒ©ãƒ¼ä¿®æ­£èƒ½åŠ›ã¨äºˆæ¸¬ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ã‚’ç†è«–çš„ã«ç¤ºã™ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€æ­£ã—ã„æ¨è«–çµŒè·¯ã¨èª¤ã£ãŸæ¨è«–çµŒè·¯ã‚’çµ„ã¿è¾¼ã‚€ã“ã¨ã§CoTã‚’æ”¹å–„ã™ã‚‹ææ¡ˆã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_philschmid/status/1855926845855699311?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>ãŠã‚‚ã—ã‚ãã†ãªç ”ç©¶</span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2024-11-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1492" target="_blank" rel="noopener noreferrer" class="title-link">LoRA vs Full Fine-tuning: An Illusion of Equivalence, Reece Shuttleworth+, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã®é•ã„ãŒäº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’ã€é‡ã¿è¡Œåˆ—ã®ã‚¹ãƒšã‚¯ãƒˆãƒ«ç‰¹æ€§ã‚’é€šã˜ã¦åˆ†æã€‚LoRAã¨å®Œå…¨ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ç•°ãªã‚‹æ§‹é€ ã®é‡ã¿è¡Œåˆ—ã‚’ç”Ÿæˆã—ã€LoRAãƒ¢ãƒ‡ãƒ«ã¯æ–°ãŸãªé«˜ãƒ©ãƒ³ã‚¯ã®ç‰¹ç•°ãƒ™ã‚¯ãƒˆãƒ«ï¼ˆä¾µå…¥æ¬¡å…ƒï¼‰ã‚’æŒã¤ã“ã¨ãŒåˆ¤æ˜ã€‚ä¾µå…¥æ¬¡å…ƒã¯ä¸€èˆ¬åŒ–èƒ½åŠ›ã‚’ä½ä¸‹ã•ã›ã‚‹ãŒã€åŒç­‰ã®æ€§èƒ½ã‚’é”æˆã™ã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ç•°ãªã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ãŒãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã®ç•°ãªã‚‹éƒ¨åˆ†ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã„ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/aratako_lm/status/1854838012909166973?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1423" target="_blank" rel="noopener noreferrer">When Scaling Meets LLM Finetuning: The Effect of Data, Model and  Finetuning Method, Biao Zhang+, N/A, ICLR'24</a>
 ã‚„ <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1475" target="_blank" rel="noopener noreferrer">Beyond Full Fine-tuning: Harnessing the Power of LoRA for Multi-Task Instruction Tuning, Xin+, LREC-COLING'24</a>
 ã€åŒæ–¹ã®çŸ¥è¦‹ã‚‚äº¤ãˆã¦ã€LoRAã®æŒ™å‹•ã‚’è€ƒå¯Ÿã™ã‚‹å¿…è¦ãŒã‚ã‚‹æ°—ãŒã™ã‚‹ã€‚ãã‚Œãã‚Œç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚„ãƒ¢ãƒ‡ãƒ«ã§ã€LoRAã¨FFTã‚’æ¯”è¼ƒã—ã¦ã„ã‚‹ã€‚æ™‚é–“ãŒãªã„ãŒå¾Œã§ã‚„ã‚ŠãŸã„ã€‚<br><br>ã‚ã¨ã€æ˜¨ä»Šã¯ãã‚‚ãã‚‚å®Ÿé¨“è¨­å®šã«ãŠã‘ã‚‹å¤‰æ•°ãŒå¤šã™ãã¦ã€ã¨ã‚Šã†ã‚‹å®Ÿé¨“è¨­å®šãŒå¤šã™ãã‚‹ãŸã‚ã€å€‹ã€…ã®è«–æ–‡ã®çŸ¥è¦‹ã‚’éµœå‘‘ã¿ã«ã—ã¦ä¸€èˆ¬åŒ–ã™ã‚‹ã®ã¯ã‚„ã‚ãŸæ–¹ãŒè‰¯ã„æ°—ãŒã—ã¦ã„ã‚‹ã€‚</p>
<p>
<strong># å®Ÿé¨“è¨­å®šã®é•ã„<br>## ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£<br>- æœ¬ç ”ç©¶: RoBERTa-baseï¼ˆtransformer-encoderï¼‰<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1423" target="_blank" rel="noopener noreferrer">When Scaling Meets LLM Finetuning: The Effect of Data, Model and  Finetuning Method, Biao Zhang+, N/A, ICLR'24</a>
</strong>
<br>
: transformer-decoder<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1475" target="_blank" rel="noopener noreferrer">Beyond Full Fine-tuning: Harnessing the Power of LoRA for Multi-Task Instruction Tuning, Xin+, LREC-COLING'24</a>
: transformer-decoderï¼ˆLLaMAï¼‰<br><br>
<strong>## ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚º<br>- æœ¬ç ”ç©¶: <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1423" target="_blank" rel="noopener noreferrer">When Scaling Meets LLM Finetuning: The Effect of Data, Model and  Finetuning Method, Biao Zhang+, N/A, ICLR'24</a>
</strong>
<br>
: 1B, 2B, 4B, 8B, 16B<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1475" target="_blank" rel="noopener noreferrer">Beyond Full Fine-tuning: Harnessing the Power of LoRA for Multi-Task Instruction Tuning, Xin+, LREC-COLING'24</a>
: 7B<br><br>æ™‚é–“ãŒã‚ã‚‹æ™‚ã«ç¶šãã‚’ã‹ããŸã„<br><br>## Finetuningãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¿ã‚¹ã‚¯æ•°<br><br>## 1ã‚¿ã‚¹ã‚¯ã‚ãŸã‚Šã®ãƒ‡ãƒ¼ã‚¿é‡<br><br>## trainableãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°</p></span><br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<span class="issue_date">Issue Date: 2024-09-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1406" target="_blank" rel="noopener noreferrer" class="title-link">To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic  reasoning, Zayne Sprague+, N_A, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- Chain-of-thoughtï¼ˆCoTï¼‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã¯LLMsã®æ¨è«–èƒ½åŠ›ã‚’å¼•ãå‡ºã™æ‰‹æ³•ã§ã‚ã‚Šã€100ä»¥ä¸Šã®è«–æ–‡ã‚’å¯¾è±¡ã«ã—ãŸãƒ¡ã‚¿åˆ†æã«ã‚ˆã‚Šã€ä¸»ã«æ•°å­¦ã‚„è«–ç†ã‚¿ã‚¹ã‚¯ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸ŠãŒç¢ºèªã•ã‚ŒãŸã€‚ä¸€æ–¹ã€ä»–ã®ã‚¿ã‚¹ã‚¯ã§ã¯åŠ¹æœãŒé™å®šçš„ã§ã€MMLUã§ã¯ç›´æ¥å›ç­”ç”ŸæˆãŒCoTã¨åŒç­‰ã®ç²¾åº¦ã‚’ç¤ºã—ãŸã€‚è¨ˆç”»ã¨å®Ÿè¡Œã‚’åˆ†é›¢ã—ã€ãƒ„ãƒ¼ãƒ«å¼·åŒ–LLMsã¨æ¯”è¼ƒã—ãŸçµæœã€CoTã®åˆ©ç‚¹ã¯è¨˜å·çš„å®Ÿè¡Œã®æ”¹å–„ã«èµ·å› ã—ã€è¨˜å·ã‚½ãƒ«ãƒãƒ¼ã«ã¯åŠ£ã‚‹ã“ã¨ãŒåˆ†ã‹ã£ãŸã€‚CoTã®é¸æŠçš„é©ç”¨ã«ã‚ˆã‚Šã€æ¨è«–ã‚³ã‚¹ãƒˆã‚’ç¯€ç´„ã—ã¤ã¤ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¶­æŒã§ãã‚‹å¯èƒ½æ€§ãŒç¤ºå”†ã•ã‚Œã€LLMã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å…¨ä½“ã§ã®ä¸­é–“è¨ˆç®—ã®æ´»ç”¨ãŒæ±‚ã‚ã‚‰ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>CoTã‚’100å€‹ä»¥ä¸Šã®å…ˆè¡Œç ”ç©¶ã§meta-analysisã—ï¼ˆi.e. CoTã‚’è¿½åŠ ã—ãŸå ´åˆã®gainã¨ã‚¿ã‚¹ã‚¯ã®ãƒ—ãƒ­ãƒƒãƒˆï¼‰ã€20å€‹è¶…ãˆã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§è‘—è€…ã‚‰ãŒå®Ÿé¨“ã—ãŸçµæœã€mathã¯symbolic reasoningï¼ˆ12*4ã®ã‚ˆã†ã«ã€ã‚·ãƒ³ãƒœãƒ«ã‚’èªè­˜ã—ã€ä½•ã‚‰ã‹ã®æ“ä½œã‚’ã—ã¦å›ç­”ã‚’ã™ã‚‹å•é¡Œï¼‰ãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯ã§ã€CoTã¯å¤§ããªgainãŒå¾—ã‚‰ã‚Œã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸï¼ˆä»–ã¯ã»ã¨ã‚“ã©gainãŒãªã„ï¼‰ã€‚<br><img src="https://github.com/user-attachments/assets/a399306f-bda9-45c9-a756-2a83a9727e63" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2024-08-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1362" target="_blank" rel="noopener noreferrer" class="title-link">What Do Language Models Learn in Context? The Structured Task Hypothesis, Jiaoda Li+, N_A, ACL'24</a>
<span class="snippet"><span>GPT Summary</span>- LLMsã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†…å­¦ç¿’ï¼ˆICLï¼‰èƒ½åŠ›ã‚’èª¬æ˜ã™ã‚‹3ã¤ã®ä»®èª¬ã«ã¤ã„ã¦ã€ä¸€é€£ã®å®Ÿé¨“ã‚’é€šã˜ã¦æ¢ç©¶ã€‚æœ€åˆã®2ã¤ã®ä»®èª¬ã‚’ç„¡åŠ¹ã«ã—ã€æœ€å¾Œã®ä»®èª¬ã‚’æ”¯æŒã™ã‚‹è¨¼æ‹ ã‚’æä¾›ã€‚LLMãŒäº‹å‰å­¦ç¿’ä¸­ã«å­¦ç¿’ã—ãŸã‚¿ã‚¹ã‚¯ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå†…ã§æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’å­¦ç¿’ã§ãã‚‹å¯èƒ½æ€§ã‚’ç¤ºå”†ã€‚</span>
<span class="snippet"><span>Comment</span><p>SNLP2024ã§ã®è§£èª¬ã‚¹ãƒ©ã‚¤ãƒ‰:<br>


<a href="http://chasen.org/~daiti-m/paper/SNLP2024-Task-Emergence.pdf" target="_blank" rel="noopener noreferrer">http://chasen.org/~daiti-m/paper/SNLP2024-Task-Emergence.pdf</a>


</p>
<p>ICLãŒä½•ã‚’ã‚„ã£ã¦ã„ã‚‹ã®ã‹?ã«ã¤ã„ã¦ã€ã“ã‚Œã¾ã§ã®ä»®èª¬ãŒæ­£ã—ããªã„ã“ã¨ã‚’å®Ÿé¨“çš„ã«ç¤ºã—ã€æ–°ã—ã„ä»®èª¬ã€ŒICLã¯äº‹å‰å­¦ç¿’ã§å¾—ã‚‰ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚’çµ„ã¿åˆã‚ã›ã¦æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’è§£ã„ã¦ã„ã‚‹ã€ã‚’æå”±ã—ã€ã“ã®ä»®èª¬ãŒæ­£ã—ã„ã“ã¨ã‚’ç¤ºå”†ã™ã‚‹å®Ÿé¨“çµæœã‚’å¾—ã¦ã„ã‚‹æ¨¡æ§˜ã€‚<br>ç†è«–çš„ã«è§£æ˜ã•ã‚ŒãŸã‚ã‘ã§ã¯ãªã•ãã†ãªã®ã§ãã“ã¯ç•™æ„ã—ãŸæ–¹ãŒè‰¯ã•ãã†ã€‚ã‚ã¨ã§ã—ã£ã‹ã‚Šèª­ã‚€ã€‚</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<span class="issue_date">Issue Date: 2024-08-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1361" target="_blank" rel="noopener noreferrer" class="title-link">The Illusion of State in State-Space Models, William Merrill+, N_A, ICML'24</a>
<span class="snippet"><span>GPT Summary</span>- SSMï¼ˆçŠ¶æ…‹ç©ºé–“ãƒ¢ãƒ‡ãƒ«ï¼‰ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚ˆã‚Šã‚‚å„ªã‚ŒãŸçŠ¶æ…‹è¿½è·¡ã®è¡¨ç¾åŠ›ã‚’æŒã¤ã¨æœŸå¾…ã•ã‚Œã¦ã„ã¾ã—ãŸãŒã€å®Ÿéš›ã«ã¯ãã®è¡¨ç¾åŠ›ã¯åˆ¶é™ã•ã‚Œã¦ãŠã‚Šã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¨é¡ä¼¼ã—ã¦ã„ã¾ã™ã€‚SSMã¯è¤‡é›‘æ€§ã‚¯ãƒ©ã‚¹$\mathsf{TC}^0$ã®å¤–ã§ã®è¨ˆç®—ã‚’è¡¨ç¾ã§ããšã€å˜ç´”ãªçŠ¶æ…‹è¿½è·¡å•é¡Œã‚’è§£æ±ºã™ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚ã“ã®ãŸã‚ã€SSMã¯å®Ÿä¸–ç•Œã®çŠ¶æ…‹è¿½è·¡å•é¡Œã‚’è§£æ±ºã™ã‚‹èƒ½åŠ›ã«åˆ¶é™ãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>&gt;ã—ã‹ã—ã€SSMãŒçŠ¶æ…‹è¿½è·¡ã®è¡¨ç¾åŠ›ã§æœ¬å½“ã«ï¼ˆãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã‚ˆã‚Šã‚‚ï¼‰å„ªä½æ€§ã‚’æŒã£ã¦ã„ã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿé©šãã¹ãã“ã¨ã«ã€ãã®ç­”ãˆã¯ã€Œã„ã„ãˆã€ã§ã™ã€‚ç§ãŸã¡ã®åˆ†æã«ã‚ˆã‚‹ã¨ã€SSMã®è¡¨ç¾åŠ›ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ã¨éå¸¸ã«é¡ä¼¼ã—ã¦åˆ¶é™ã•ã‚Œã¦ã„ã¾ã™ï¼šSSMã¯è¤‡é›‘æ€§ã‚¯ãƒ©ã‚¹$\mathsf{TC}^0$ã®å¤–ã§ã®è¨ˆç®—ã‚’è¡¨ç¾ã™ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã€‚ç‰¹ã«ã€ã“ã‚Œã¯ã€ç½®æ›åˆæˆã®ã‚ˆã†ãªå˜ç´”ãªçŠ¶æ…‹è¿½è·¡å•é¡Œã‚’è§£æ±ºã™ã‚‹ã“ã¨ãŒã§ããªã„ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€SSMã¯ã€ç‰¹å®šã®è¡¨è¨˜æ³•ã§ãƒã‚§ã‚¹ã®æ‰‹ã‚’æ­£ç¢ºã«è¿½è·¡ã—ãŸã‚Šã€ã‚³ãƒ¼ãƒ‰ã‚’è©•ä¾¡ã—ãŸã‚Šã€é•·ã„ç‰©èªã®ä¸­ã®ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’è¿½è·¡ã™ã‚‹ã“ã¨ãŒè¨¼æ˜ä¸Šã§ããªã„ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã¾ã™ã€‚<br><br>ãªã‚“â€¦ã ã¨â€¦</p></span><br><br>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<span class="issue_date">Issue Date: 2024-08-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1352" target="_blank" rel="noopener noreferrer" class="title-link">Amuro &amp; Char: Analyzing the Relationship between Pre-Training and  Fine-Tuning of Large Language Models, Kaiser Sun+, N_A, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡ãªãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ¼ãƒ‘ã‚¹ã§äº‹å‰å­¦ç¿’ã•ã‚ŒãŸè¤‡æ•°ã®ä¸­é–“äº‹å‰å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å¾®èª¿æ•´ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€äº‹å‰å­¦ç¿’ã¨å¾®èª¿æ•´ã®é–¢ä¿‚ã‚’èª¿æŸ»ã—ãŸã€‚18ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®çµæœã‹ã‚‰ã€iï¼‰ç¶™ç¶šçš„ãªäº‹å‰å­¦ç¿’ã¯ã€å¾®èª¿æ•´å¾Œã«ãƒ¢ãƒ‡ãƒ«ã‚’æ”¹å–„ã™ã‚‹æ½œåœ¨çš„ãªæ–¹æ³•ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚iiï¼‰è¿½åŠ ã®å¾®èª¿æ•´ã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ãŒäº‹å‰å­¦ç¿’æ®µéšã§ã†ã¾ãæ©Ÿèƒ½ã—ãªã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ”¹å–„ãŒã€ã†ã¾ãæ©Ÿèƒ½ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚ˆã‚Šã‚‚å¤§ãã„ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚iiiï¼‰ç›£ç£ã•ã‚ŒãŸå¾®èª¿æ•´ã‚’é€šã˜ã¦ãƒ¢ãƒ‡ãƒ«ã¯æ©æµã‚’å—ã‘ã‚‹ãŒã€ä»¥å‰ã®ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã‚„å¾®èª¿æ•´ä¸­ã«è¦‹ã‚‰ã‚Œãªã„ã‚¿ã‚¹ã‚¯ã‚’å¿˜ã‚Œã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚ivï¼‰ç›£ç£ã•ã‚ŒãŸå¾®èª¿æ•´å¾Œã€ãƒ¢ãƒ‡ãƒ«ã¯è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã—ã¦é«˜ã„æ„Ÿåº¦ã‚’ç¤ºã™ãŒã€ã“ã‚Œã¯ã‚ˆã‚Šå¤šãã®äº‹å‰å­¦ç¿’ã«ã‚ˆã£ã¦ç·©å’Œã§ãã‚‹ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/GrammaticalErrorCorrection.html" target="_blank" rel="noopener noreferrer">#GrammaticalErrorCorrection</a>
<span class="issue_date">Issue Date: 2024-08-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1351" target="_blank" rel="noopener noreferrer" class="title-link">Prompting open-source and commercial language models for grammatical  error correction of English learner text, Christopher Davis+, N_A, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- LLMsã®é€²æ­©ã«ã‚ˆã‚Šã€æµæš¢ã§æ–‡æ³•çš„ãªãƒ†ã‚­ã‚¹ãƒˆç”ŸæˆãŒå¯èƒ½ã«ãªã‚Šã€ä¸æ–‡æ³•ãªå…¥åŠ›æ–‡ã‚’ä¸ãˆã‚‹ã“ã¨ã§æ–‡æ³•ã‚¨ãƒ©ãƒ¼ä¿®æ­£ï¼ˆGECï¼‰ãŒå¯èƒ½ã¨ãªã£ãŸã€‚æœ¬ç ”ç©¶ã§ã¯ã€7ã¤ã®ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã¨3ã¤ã®å•†ç”¨LLMsã‚’4ã¤ã®GECãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§è©•ä¾¡ã—ã€å•†ç”¨ãƒ¢ãƒ‡ãƒ«ãŒå¸¸ã«æ•™å¸«ã‚ã‚Šã®è‹±èªGECãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã‚ã‘ã§ã¯ãªã„ã“ã¨ã‚’ç¤ºã—ãŸã€‚ã¾ãŸã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãŒå•†ç”¨ãƒ¢ãƒ‡ãƒ«ã‚’ä¸Šå›ã‚‹ã“ã¨ãŒã‚ã‚Šã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ãŒãƒ•ãƒ¥ãƒ¼ã‚·ãƒ§ãƒƒãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒ†ã‚£ãƒ³ã‚°ã¨åŒã˜ãã‚‰ã„ç«¶äº‰åŠ›ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/chemical_tree/status/1822860849935253882?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ContextWindow.html" target="_blank" rel="noopener noreferrer">#ContextWindow</a>
<a class="button" href="articles/LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1274" target="_blank" rel="noopener noreferrer" class="title-link">Long-context LLMs Struggle with Long In-context Learning, Tianle Li+, N_A, arXiv'24</a>
<span class="snippet"><span>GPT Summary</span>- LLMsã¯é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å‡¦ç†ã™ã‚‹èƒ½åŠ›ã«é€²å±•ã—ã¦ã„ã‚‹ãŒã€å®Ÿä¸–ç•Œã®ã‚·ãƒŠãƒªã‚ªã§ã®èƒ½åŠ›ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®å°‚é–€çš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯LongICLBenchãŒå°å…¥ã•ã‚ŒãŸã€‚ã“ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã¯ã€LLMsã¯å·¨å¤§ãªãƒ©ãƒ™ãƒ«ç©ºé–“ã‚’ç†è§£ã—ã€æ­£ã—ã„äºˆæ¸¬ã‚’è¡Œã†ãŸã‚ã«å…¥åŠ›å…¨ä½“ã‚’ç†è§£ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚ç ”ç©¶ã«ã‚ˆã‚‹ã¨ã€é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆLLMsã¯é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã‚’æ´»ç”¨ã™ã‚‹ã“ã¨ã§æ¯”è¼ƒçš„è‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã™ãŒã€æœ€ã‚‚å›°é›£ãªã‚¿ã‚¹ã‚¯ã§ã¯è‹¦åŠ´ã—ã¦ã„ã‚‹ã€‚ç¾åœ¨ã®LLMsã¯é•·ãã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè±Šã‹ãªã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å‡¦ç†ã—ç†è§£ã™ã‚‹èƒ½åŠ›ã«ã‚®ãƒ£ãƒƒãƒ—ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ãŠã‚Šã€é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ç†è§£ã¨æ¨è«–ã¯ä¾ç„¶ã¨ã—ã¦é›£ã—ã„èª²é¡Œã§ã‚ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>GPT4ä»¥å¤–ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒ20Kã‚’è¶…ãˆã‚‹ã¨æ€§èƒ½ãŒåŠ£åŒ–ã™ã‚‹å‚¾å‘ã«ã‚ã‚‹ã¨ã®ã“ã¨ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’é›£æ˜“åº¦åˆ¥ã«åé›†ã—è©•ä¾¡ã—ãŸã¨ã“ã‚ã€é›£æ˜“åº¦ã®é«˜ã„ãƒ‡ãƒ¼ã‚¿ã§ã¯ãã‚‚ãã‚‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ããªã‚‹ã¨å…¨ã¦ã®LLMãŒã‚¿ã‚¹ã‚¯ã‚’ç†è§£ã™ã‚‹ã§ããšã»ã¼0%ã®æ€§èƒ½ã¨ãªã£ãŸã€‚<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/fc51d83a-3013-4fcc-bf7a-5722eb01d0d8" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2023-12-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1186" target="_blank" rel="noopener noreferrer" class="title-link">VILA: On Pre-training for Visual Language Models, Ji Lin+, N_A, CVPR'24</a>
<span class="snippet"><span>GPT Summary</span>- æœ€è¿‘ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®æˆåŠŸã«ã‚ˆã‚Šã€ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰ãŒé€²æ­©ã—ã¦ã„ã‚‹ã€‚æœ¬ç ”ç©¶ã§ã¯ã€VLMã®äº‹å‰å­¦ç¿’ã®ãŸã‚ã®ãƒ‡ã‚¶ã‚¤ãƒ³ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æ¤œè¨ã—ã€ä»¥ä¸‹ã®çµæœã‚’ç¤ºã—ãŸï¼š(1) LLMã‚’å‡çµã™ã‚‹ã“ã¨ã§ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒé”æˆã§ãã‚‹ãŒã€æ–‡è„ˆã«åŸºã¥ã„ãŸå­¦ç¿’èƒ½åŠ›ãŒä¸è¶³ã—ã¦ã„ã‚‹ã€‚(2) äº¤äº’ã«è¡Œã‚ã‚Œã‚‹äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¯æœ‰ç›Šã§ã‚ã‚Šã€ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®ãƒšã‚¢ã ã‘ã§ã¯æœ€é©ã§ã¯ãªã„ã€‚(3) ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã®æŒ‡ç¤ºãƒ‡ãƒ¼ã‚¿ã‚’ç”»åƒã¨ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ‡ãƒ¼ã‚¿ã«å†ãƒ–ãƒ¬ãƒ³ãƒ‰ã™ã‚‹ã“ã¨ã§ã€VLMã®ã‚¿ã‚¹ã‚¯ã®ç²¾åº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã‚‹ã€‚VILAã¨ã„ã†ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’æ§‹ç¯‰ã—ã€æœ€å…ˆç«¯ãƒ¢ãƒ‡ãƒ«ã‚’å‡Œé§•ã—ã€å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç™ºæ®ã™ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ã®äº‹å‰å­¦ç¿’ã¯ã€VILAã®ç‰¹æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1068" target="_blank" rel="noopener noreferrer">Improved Baselines with Visual Instruction Tuning, Haotian Liu+, N/A, CVPR'24</a>
</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<a class="button" href="articles/ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<span class="issue_date">Issue Date: 2023-09-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1029" target="_blank" rel="noopener noreferrer" class="title-link">CausalLM is not optimal for in-context learning, Nan Ding+, N_A, ICLR'24</a>
<span class="snippet"><span>GPT Summary</span>- æœ€è¿‘ã®ç ”ç©¶ã§ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ™ãƒ¼ã‚¹ã®ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆå­¦ç¿’ã«ãŠã„ã¦ã€ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆprefixLMï¼‰ãŒå› æœè¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆcausalLMï¼‰ã‚ˆã‚Šã‚‚å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ç¤ºã™ã“ã¨ãŒã‚ã‹ã£ã¦ã„ã¾ã™ã€‚æœ¬ç ”ç©¶ã§ã¯ã€ç†è«–çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ç”¨ã„ã¦ã€prefixLMã¨causalLMã®åæŸæŒ™å‹•ã‚’åˆ†æã—ã¾ã—ãŸã€‚ãã®çµæœã€prefixLMã¯ç·šå½¢å›å¸°ã®æœ€é©è§£ã«åæŸã™ã‚‹ä¸€æ–¹ã€causalLMã®åæŸãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã¯ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å‹¾é…é™ä¸‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«å¾“ã„ã€æœ€é©ã§ã‚ã‚‹ã¨ã¯é™ã‚‰ãªã„ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚ã•ã‚‰ã«ã€åˆæˆå®Ÿé¨“ã¨å®Ÿéš›ã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã‚‚ã€causalLMãŒprefixLMã‚ˆã‚Šã‚‚æ€§èƒ½ãŒåŠ£ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>å‚è€ƒ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1697380430004249066?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>CausalLMã§ICLã‚’ã—ãŸå ´åˆã¯ã€ICLä¸­ã®demonstrationã§ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å­¦ç¿’ã™ã‚‹ã“ã¨ã«ç›¸å½“ã—ã€æœ€é©è§£ã«åæŸã—ã¦ã„ã‚‹ã¨ã¯é™ã‚‰ãªã„â€¦â€¦ï¼ŸãŒã€hillbigã•ã‚“ã®æ„Ÿæƒ³ã«åŸºã¥ãã¨ã€çµæœçš„ã«ã¯å®Ÿã¯æœ€é©è§£ã«åæŸã—ã¦ã„ã‚‹ã®ã§ã¯ï¼Ÿã¨ã„ã†è©±ã‚‚å‡ºã¦ã„ã‚‹ã—ã€ã‚ˆãåˆ†ã‹ã‚‰ãªã„ã€‚</p></span><br><br>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Prompting.html" target="_blank" rel="noopener noreferrer">#Prompting</a>
<a class="button" href="articles/In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<a class="button" href="articles/TACL.html" target="_blank" rel="noopener noreferrer">#TACL</a>
<a class="button" href="articles/ContextEngineering.html" target="_blank" rel="noopener noreferrer">#ContextEngineering</a>
<span class="issue_date">Issue Date: 2023-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/793" target="_blank" rel="noopener noreferrer" class="title-link">Lost in the Middle: How Language Models Use Long Contexts, Nelson F. Liu+, N_A, TACL'24</a>
<span class="snippet"><span>GPT Summary</span>- æœ€è¿‘ã®è¨€èªãƒ¢ãƒ‡ãƒ«ã¯ã€é•·ã„æ–‡è„ˆã‚’å…¥åŠ›ã¨ã—ã¦å—ã‘å–ã‚‹ã“ã¨ãŒã§ãã¾ã™ãŒã€ãã®é•·ã„æ–‡è„ˆã‚’ã©ã‚Œã ã‘ã†ã¾ãåˆ©ç”¨ã—ã¦ã„ã‚‹ã‹ã«ã¤ã„ã¦ã¯ã¾ã ã‚ˆãã‚ã‹ã£ã¦ã„ã¾ã›ã‚“ã€‚ã“ã®ç ”ç©¶ã§ã¯ã€ãƒãƒ«ãƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è³ªå•å¿œç­”ã¨ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ã®æ¤œç´¢ã¨ã„ã†2ã¤ã®ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’åˆ†æã—ã¾ã—ãŸã€‚ãã®çµæœã€é–¢é€£æƒ…å ±ãŒå…¥åŠ›æ–‡è„ˆã®å§‹ã¾ã‚Šã‚„çµ‚ã‚ã‚Šã«ã‚ã‚‹å ´åˆã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒæœ€ã‚‚é«˜ããªã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸãŒã€é•·ã„æ–‡è„ˆã®ä¸­ã§é–¢é€£æƒ…å ±ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹å¿…è¦ãŒã‚ã‚‹å ´åˆã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒè‘—ã—ãä½ä¸‹ã—ã¾ã™ã€‚ã•ã‚‰ã«ã€å…¥åŠ›æ–‡è„ˆãŒé•·ããªã‚‹ã«ã¤ã‚Œã¦ã€æ˜ç¤ºçš„ã«é•·ã„æ–‡è„ˆã‚’æ‰±ã†ãƒ¢ãƒ‡ãƒ«ã§ã‚‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå¤§å¹…ã«ä½ä¸‹ã—ã¾ã™ã€‚ã“ã®åˆ†æã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ãŒå…¥åŠ›æ–‡è„ˆã‚’ã©ã®ã‚ˆã†ã«åˆ©ç”¨ã—ã¦ã„ã‚‹ã‹ã‚’ã‚ˆã‚Šè‰¯ãç†è§£ã™ã‚‹ãŸã‚ã®ã‚‚ã®ã§ã‚ã‚Šã€å°†æ¥ã®é•·ã„æ–‡è„ˆãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã®æ–°ã—ã„è©•ä¾¡ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’æä¾›ã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>å…ƒãƒ„ã‚¤ãƒ¼ãƒˆ<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/drjimfan/status/1678460065811136512?s=46&t=5BO_qSlNBSEGSugyUlP5Hw"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>éå¸¸ã«é‡è¦ãªçŸ¥è¦‹ãŒã¾ã¨ã‚ã‚‰ã‚Œã¦ã„ã‚‹<p>1. ãƒ¢ãƒ‡ãƒ«ã¯ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ã¯ã˜ã‚ã¨æœ€å¾Œã®æƒ…å ±ã‚’ã†ã¾ãæ´»ç”¨ã§ãã€çœŸã‚“ä¸­ã®æƒ…å ±ã‚’ã†ã¾ãæ´»ç”¨ã§ããªã„<br>2. é•·ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã‚‚ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚ˆã‚ŠçŸ­ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šã‚‚ã†ã¾ãè€ƒæ…®ã§ãã‚‹ã‚ã‘ã§ã¯ãªã„<br>3. ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãŒé•·ããªã‚Œã°ãªã‚‹ã»ã©æ‚ªåŒ–ã™ã‚‹</p>
<p>SNLP'24ã§ã®è§£èª¬ã‚¹ãƒ©ã‚¤ãƒ‰:<br>


<a href="https://speakerdeck.com/kichi/snlp2024" target="_blank" rel="noopener noreferrer">https://speakerdeck.com/kichi/snlp2024</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Coding.html" target="_blank" rel="noopener noreferrer">#Coding</a>
<a class="button" href="articles/ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<span class="issue_date">Issue Date: 2023-05-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/686" target="_blank" rel="noopener noreferrer" class="title-link">Evidence of Meaning in Language Models Trained on Programs, Charles Jin+, N_A, ICML'24</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’ç”¨ã„ã¦è¨€èªãƒ¢ãƒ‡ãƒ«ãŒæ„å‘³ã‚’å­¦ç¿’ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ åˆæˆãŒè¨€èªãƒ¢ãƒ‡ãƒ«ã®æ„å‘³ã®å­˜åœ¨ã‚’ç‰¹å¾´ã¥ã‘ã‚‹ãŸã‚ã®ä¸­é–“ãƒ†ã‚¹ãƒˆãƒ™ãƒƒãƒ‰ã¨ã—ã¦é©ã—ã¦ã„ã‚‹ã“ã¨ã‚’è¿°ã¹ã¦ã„ã‚‹ã€‚Transformerãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸå®Ÿé¨“ã«ã‚ˆã‚Šã€è¨€èªã®æ„å‘³ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã®å¸°ç´ãƒã‚¤ã‚¢ã‚¹ã‚’æä¾›ã—ãªã„ã«ã‚‚ã‹ã‹ã‚ã‚‰ãšã€ç·šå½¢ãƒ—ãƒ­ãƒ¼ãƒ–ãŒãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ã‹ã‚‰ç¾åœ¨ãŠã‚ˆã³å°†æ¥ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ çŠ¶æ…‹ã®æŠ½è±¡åŒ–ã‚’æŠ½å‡ºã§ãã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸã€‚ã¾ãŸã€æ­£ã—ã„ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚’å­¦ç¿’ã—ã€å¹³å‡çš„ã«è¨“ç·´ã‚»ãƒƒãƒˆã‚ˆã‚Šã‚‚çŸ­ã„ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ã‚‚ç¤ºã—ãŸã€‚æœ¬è«–æ–‡ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã«æ–°ã—ã„æŠ€è¡“ã‚’ææ¡ˆã™ã‚‹ã‚‚ã®ã§ã¯ãªãã€(å½¢å¼çš„ãª)æ„å‘³ã®ç¿’å¾—ã¨è¡¨ç¾ã«é–¢ã™ã‚‹å®Ÿé¨“çš„ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’é–‹ç™ºã—ã€æ´å¯Ÿã‚’æä¾›ã™ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®ã‚³ãƒ¼ãƒ‘ã‚¹ã§LLMã‚’Next Token Predictionã§è¨“ç·´ã—<br>å³å¯†ã«æ­£è§£ã¨semanticsã‚’å®šç¾©ã—ãŸä¸Šã§ã€è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ç•°ãªã‚‹semanticsã®ç•°ãªã‚‹ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ç”Ÿæˆã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã€‚<br><br>LLMãŒæ„å‘³ã‚’ç†è§£ã—ã¦ã„ã‚‹ã“ã¨ã‚’æš—ç¤ºã—ã¦ã„ã‚‹<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/fa4d2c68-bdbe-40ae-990d-10814ac8a204" alt="image" loading="lazy"></p>
<p>å‚è€ƒ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1660409936264970240?s=46&t=QJho5ctFkeax7s_UMOfWBQ"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2400" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Physics of Language Models: Part 1, Learning Hierarchical Language  Structures, Zeyuan Allen-Zhu+, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€Transformerãƒ™ãƒ¼ã‚¹ã®è¨€èªãƒ¢ãƒ‡ãƒ«ãŒæ–‡è„ˆè‡ªç”±æ–‡æ³•ï¼ˆCFGï¼‰ã«ã‚ˆã‚‹å†å¸°çš„ãªè¨€èªæ§‹é€ æ¨è«–ã‚’ã©ã®ã‚ˆã†ã«è¡Œã†ã‹ã‚’èª¿æŸ»ã€‚åˆæˆCFGã‚’ç”¨ã„ã¦é•·æ–‡ã‚’ç”Ÿæˆã—ã€GPTã®ã‚ˆã†ãªãƒ¢ãƒ‡ãƒ«ãŒCFGã®éšå±¤ã‚’æ­£ç¢ºã«å­¦ç¿’ãƒ»æ¨è«–ã§ãã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ãƒ¢ãƒ‡ãƒ«ã®éš ã‚ŒçŠ¶æ…‹ãŒCFGã®æ§‹é€ ã‚’æ‰ãˆã€æ³¨æ„ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒå‹•çš„ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã«é¡ä¼¼ã—ã¦ã„ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ã€‚ã¾ãŸã€çµ¶å¯¾ä½ç½®åŸ‹ã‚è¾¼ã¿ã®åŠ£ä½ã‚„å‡ä¸€ãªæ³¨æ„ã®åŠ¹æœã€ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€å°‚ç”¨ãƒ¢ãƒ‡ãƒ«ã®é™ç•Œã€æ§‹é€ çš„ãƒã‚¤ã‚ºã«ã‚ˆã‚‹å …ç‰¢æ€§å‘ä¸Šã«ã¤ã„ã¦ã‚‚è€ƒå¯Ÿã€‚</span>
<span class="snippet"><span>Comment</span><p>è§£èª¬:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1834" target="_blank" rel="noopener noreferrer">è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç‰©ç†å­¦, ä½è—¤ç«œé¦¬, 2025.03</a>
</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/QuestionAnswering.html" target="_blank" rel="noopener noreferrer">#QuestionAnswering</a>
<span class="issue_date">Issue Date: 2023-12-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1177" target="_blank" rel="noopener noreferrer" class="title-link">Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural  Scrambled Text, Qi Cao+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã®å†…éƒ¨å‹•ä½œã«ã¤ã„ã¦ã®æ–°ã—ã„æ´å¯Ÿã‚’æä¾›ã—ã¾ã™ã€‚ç‰¹ã«ã€GPT-4ã‚’èª¿æŸ»ã—ã€LLMsã®è€ä¹…æ€§ã«é–¢ã™ã‚‹å®Ÿé¨“çµæœã‚’ç¤ºã—ã¾ã™ã€‚å®Ÿé¨“ã§ã¯ã€æ–‡å­—ãƒ¬ãƒ™ãƒ«ã®é †åˆ—ã«å¯¾ã™ã‚‹LLMsã®è€æ€§ã‚’èª¿ã¹ã‚‹ãŸã‚ã«ã€Scrambled Benchã¨ã„ã†ã‚¹ã‚¤ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚çµæœã¯ã€GPT-4ãŒtypoglycemiaã¨ã„ã†ç¾è±¡ã«ä¼¼ãŸèƒ½åŠ›ã‚’æŒã¡ã€éå¸¸ã«è‡ªç„¶ã§ãªã„ã‚¨ãƒ©ãƒ¼ã‚’å«ã‚€å…¥åŠ›ã‚’ã»ã¼å®Œç’§ã«å‡¦ç†ã§ãã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã€LLMsã®è€æ€§ãŒç›´æ„Ÿã«åã™ã‚‹ã‚‚ã®ã§ã‚ã‚Šã€ä»–ã®LLMsã‚„äººé–“ã«ã¨ã£ã¦ã‚‚å›°é›£ãªã‚¿ã‚¹ã‚¯ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/df33c7a9-005e-4d7e-9d70-d8f0657869ed" alt="image" loading="lazy"></p>
<p>OpenAIã®ãƒ¢ãƒ‡ãƒ«ãŒãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã§ã‚ã‚‹é™ã‚Šã€ã‚³ãƒ³ã‚¿ãƒŸãƒãƒ¼ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹ã®ã§ã¯ï¼Ÿã¨ã„ã†ç–‘å¿µã¯æŒã£ã¦ã—ã¾ã†ã€‚<br><br>ï¼ˆéƒ¨åˆ†çš„ã«ã—ã‹èª­ã‚ã¦ã„ãªã„ãŒâ€¦ï¼‰<br>RealtimeQAã¨å‘¼ã°ã‚Œã‚‹weeklyã§ç›´è¿‘ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«å¯¾ã™ã‚‹Questionã‚’ç™ºè¡¨ã™ã‚‹ã“ã¨ã§æ§‹ç¯‰ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã†ã¡ã€2023.03.17--2023.08.04ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã€ScrambledSentenaeRecoveryï¼ˆScrRecï¼‰ã¨ScrambleQuestionAnsweringï¼ˆScrQAï¼‰ã®è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆã—ã¦ã„ã‚‹ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/362bcbca-b578-4f0e-ac4e-e65fd216aeac" alt="image" loading="lazy"><br><br>å®Œå…¨ã«ãƒ©ãƒ³ãƒ€ãƒ ã«å˜èªã®æ–‡å­—ã‚’scrambleï¼ˆRSï¼‰ã™ã‚‹ã¨ã€Falconã¨Llama2ã§ã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã§ã¯å†æ§‹ç¯‰ã§ããªã„ã“ã¨ãŒåˆ†ã‹ã‚‹ã€‚FewShotã§ã¯Falconã§ã‚ã‚Œã°å°‘ã—è§£ã‘ã‚‹ã‚ˆã†ã«ãªã‚‹ã€‚ä¸€æ–¹ã€OpenAIã®ãƒ¢ãƒ‡ãƒ«ã€ç‰¹ã«GPT4, GPT3.5-turboã§ã¯ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã§ã‚‚ã«ã‚Šå†æ§‹ç¯‰ãŒã§ãã¦ã„ã‚‹ã€‚<br><br>ScrQAã«ã¤ã„ã¦ã¯ã€ãƒ©ãƒ³ãƒ€ãƒ ã«scrambleã—ãŸå ´åˆã§ã‚‚MultipleChoiceQuestionãªã®ã§ï¼ˆRPGã¨å‘¼ã°ã‚Œã‚‹Accã®ç›¸å¯¾çš„ãªgainã‚’è©•ä¾¡ã™ã‚‹ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’ææ¡ˆã—ã¦ã„ã‚‹ï¼‰æ­£è§£ã¯ã§ãã¦ã„ã‚‹ã€‚<br><br>æœ€åˆã®æ–‡å­—ã ã‘ã‚’æ®‹ã™å ´åˆï¼ˆKFï¼‰æœ€åˆã¨æœ€å¾Œã®æ–‡å­—ã‚’æ®‹ã™å ´åˆï¼ˆKFLã€ã«ã¤ã„ã¦ã¯ã€æ®‹ã™æ–‡å­—ãŒå¢—ãˆã‚‹ã»ã©ã©ã¡ã‚‰ã®ã‚¿ã‚¹ã‚¯ã‚‚æ€§èƒ½ãŒä¸ŠãŒã‚Šã€æœ€åˆã®æ–‡å­—ã ã‘ãŒã‚ã‚Œã°OpenSourceLLMã§ã‚‚ï¼ˆã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã§ã‚‚ï¼‰ã‹ãªã‚Šå…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã®å†æ§‹ç¯‰ãŒã§ãã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã‚‹ã€‚ã¾ãŸã€QAã‚‚æ€§èƒ½ãŒå‘ä¸Šã—ã¦ã„ã‚‹ã€‚</p>
<p>å®Œå…¨ã«ãƒ©ãƒ³ãƒ€ãƒ ã«æ–‡å­—ã‚’å…¥ã‚Œæ›¿ãˆãŸã‚‰å®Œå…¨ã«ç„¡ç†ã‚²ãƒ¼ãªã®ã§ã¯ã€ã€ã€ã€ã¨æ€ã£ã¦ã—ã¾ã†ã®ã ãŒã€Falconã§Fewshotã®å ´åˆã¯ä¸€éƒ¨è§£ã‘ã¦ã„ã‚‹ã‚ˆã†ã â€¦ã€‚æœãŸã—ã¦ã©ã†ã„ã†ã“ã¨ãªã®ã‹â€¦ï¼ˆå¤§æ–‡å­—å°æ–‡å­—ãŒä¿æŒã•ã‚ŒãŸã¾ã¾ãªã®ãŒãƒ’ãƒ³ãƒˆã«ãªã£ã¦ã„ã‚‹â€¦ï¼Ÿï¼‰Appendixã«è€ƒå¯ŸãŒã‚ã‚Šãã†ã ãŒã¾ã èª­ã‚ã¦ã„ãªã„ã€‚<br><br><br><br>ï¼ˆè¿½è¨˜ï¼‰<br><br>æ–‡å…¨ä½“ã§ãƒ©ãƒ³ãƒ€ãƒ ã«æ–‡å­—ã‚’å…¥ã‚Œæ›¿ãˆã¦ã„ã‚‹ã®ã‹ã¨å‹˜é•ã„ã—ã¦ã„ãŸãŒã€å®Ÿéš›ã«ã¯â€ã‚ã‚‹å˜èªã®ä¸­ã ã‘ã§ãƒ©ãƒ³ãƒ€ãƒ ã«å…¥ã‚Œæ›¿ãˆâ€ã ã£ãŸã€‚ã“ã‚Œãªã‚‰åŸç†ä¸Šã¯ã„ã‘ã‚‹ã¨æ€ã‚ã‚Œã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-11-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1120" target="_blank" rel="noopener noreferrer" class="title-link">Do LLMs exhibit human-like response biases? A case study in survey  design, Lindia Tjuatja+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- LLMsã‚’ä½¿ç”¨ã—ã¦äººé–“ã®ä»£ç†ã¨ã—ã¦ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹éš›ã«ã€LLMsãŒäººé–“ã®å¿œç­”ãƒã‚¤ã‚¢ã‚¹ã‚’ã©ã®ç¨‹åº¦åæ˜ ã™ã‚‹ã‹ã‚’èª¿æŸ»ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚ã“ã®ç ”ç©¶ã§ã¯ã€èª¿æŸ»è¨­è¨ˆã‚’ä½¿ç”¨ã—ã¦äººé–“ã®å¿œç­”ãƒã‚¤ã‚¢ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’è¨­è¨ˆã—ã€9ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ã—ãŸçµæœã€ä¸€èˆ¬çš„ãªLLMsãŒäººé–“ã®ã‚ˆã†ãªæŒ¯ã‚‹èˆã„ã‚’åæ˜ ã™ã‚‹ã“ã¨ã«å¤±æ•—ã—ã¦ã„ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ã“ã‚Œã‚‰ã®çµæœã¯ã€LLMsã‚’äººé–“ã®ä»£ã‚ã‚Šã«ä½¿ç”¨ã™ã‚‹éš›ã®æ½œåœ¨çš„ãªè½ã¨ã—ç©´ã‚’å¼·èª¿ã—ã€ãƒ¢ãƒ‡ãƒ«ã®æŒ¯ã‚‹èˆã„ã®ç´°ã‹ã„ç‰¹æ€§ã®é‡è¦æ€§ã‚’å¼·èª¿ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>LLMã¯Promptã«sensitiveã ãŒã€äººé–“ã‚‚è³ªå•ã®ä»•æ–¹ã«ã‚ˆã£ã¦å¿œç­”ãŒå¤‰ã‚ã‚‹ã‹ã‚‰ã€sensitiveãªã®ã¯ä¸€ç·’ã§ã¯ï¼Ÿã¨ã„ã†ã“ã¨ã‚’èª¿æŸ»ã—ãŸç ”ç©¶ã€‚Neubigæ°ã®ãƒ„ã‚¤ãƒ¼ãƒˆã ã¨ã€instruction tuningã‚„RLHFã‚’ã—ã¦ã„ãªã„Base LLMã®æ–¹ãŒã€ã‚ˆã‚Šäººé–“ã¨é¡ä¼¼ã—ãŸå›ç­”ã‚’ã™ã‚‹ã®ã ãã†ã€‚<br><br>å…ƒãƒ„ã‚¤ãƒ¼ãƒˆ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gneubig/status/1722294711355117666?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>äººé–“ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãƒã‚¤ã‚¢ã‚¹ã€‚å·¦å´ã¯äººé–“ã¯ã€Œforbiddenã€ã‚ˆã‚Šã‚‚ã€Œnot allowedã€ã‚’å¥½ã‚€ã¨ã„ã†ä¾‹ã€å³å´ã¯ã€Œresponse orderã€ã®ãƒã‚¤ã‚¢ã‚¹ã®ä¾‹ï¼ˆé¸æŠè‚¢ã®é †ç•ªï¼‰ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/de129e78-5d52-41e3-a3bb-9aec20cf2b05" alt="image" loading="lazy"><br><br><br><br>LLMå´ã§è©•ä¾¡ã—ãŸã„ãƒã‚¤ã‚¢ã‚¹ã”ã¨ã«ã€QAã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å¤‰æ›´ã—ã€LLMã«å›ç­”ã‚’ç”Ÿæˆã•ã‚Œã€social science studiesã§ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã¨æ¯”è¼ƒã™ã‚‹ã“ã¨ã§ã€LLMã«ã‚‚äººé–“ã¨åŒæ§˜ã®ãƒã‚¤ã‚¢ã‚¹ãŒã‚ã‚‹ã‹ã‚’æ˜ã‚‰ã‹ã«ã—ã¦ã„ã‚‹ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/3dc39afc-4e52-49a4-bf60-22ff94bf35c6" alt="image" loading="lazy"><br><br><br><br>çµæœã¯ä»¥ä¸‹ã®è¡¨ã§ã‚ã‚Šã€é’ã„ã‚»ãƒ«ãŒäººé–“ã¨åŒæ§˜ã®ãƒã‚¤ã‚¢ã‚¹ã‚’æŒã¤ã“ã¨ã‚’çµ±è¨ˆçš„ã«æœ‰æ„ã«ç¤ºã•ã‚ŒãŸã‚‚ã®ï¼ˆã®ã¯ãšï¼‰ã€‚ã“ã‚Œã‚’ã¿ã‚‹ã¨ã€å…¨ã¦ã®ãƒã‚¤ã‚¢ã‚¹ã«å¯¾ã—ã¦äººé–“ã¨åŒæ§˜ã®å‚¾å‘ãŒã‚ã£ãŸã®ã¯Llama2-70Bã®ã¿ã§ã‚ã‚Šã€instruction tuningã‚„ã€RLHFã‚’ã‹ã‘ãŸå ´åˆï¼ˆRLHFã®æ–¹ãŒå½±éŸ¿ãŒå¤§ããã†ï¼‰äººé–“ã®ãƒã‚¤ã‚¢ã‚¹ã¨ã¯ç•°ãªã‚‹æŒ™å‹•ã‚’ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ãŒå¤šããªã‚‹ã“ã¨ãŒã‚ã‹ã‚‹ã€‚ã¾ãŸã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã¨ãƒã‚¤ã‚¢ã‚¹ã®å¼·ã•ã«ã¯ç›¸é–¢é–¢ä¿‚ã¯è¦‹å—ã‘ã‚‰ã‚Œãªã„ã€‚<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/7d8eade0-ae3a-4d62-bb2d-160971542c39" alt="image" loading="lazy"><br><br></p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<span class="issue_date">Issue Date: 2023-11-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1117" target="_blank" rel="noopener noreferrer" class="title-link">Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in  Transformer Models, Steve Yadlowsky+, N_A, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã®æ–‡è„ˆå­¦ç¿’ï¼ˆICLï¼‰èƒ½åŠ›ã‚’èª¿æŸ»ã—ã¾ã—ãŸã€‚ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã¯ã€äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²å†…ã§ç•°ãªã‚‹ã‚¿ã‚¹ã‚¯ã‚’ç‰¹å®šã—ã€å­¦ç¿’ã™ã‚‹èƒ½åŠ›ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ã—ã‹ã—ã€äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²å¤–ã®ã‚¿ã‚¹ã‚¯ã‚„é–¢æ•°ã«å¯¾ã—ã¦ã¯ä¸€èˆ¬åŒ–ãŒåŠ£åŒ–ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚ã¾ãŸã€é«˜å®¹é‡ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ICLèƒ½åŠ›ã¯ã€äº‹å‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ç¯„å›²ã«å¯†æ¥ã«é–¢é€£ã—ã¦ã„ã‚‹ã“ã¨ãŒå¼·èª¿ã•ã‚Œã¾ã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>TransformerãŒpre-trainingæ™‚ã«åˆ©ç”¨ã•ã‚ŒãŸå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä»¥å¤–ã®åˆ†å¸ƒã«å¯¾ã—ã¦ã¯æ±åŒ–æ€§èƒ½ãŒè½ã¡ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸã‚‰ã—ã„ã€‚ã‚‚ã—ã“ã‚ŒãŒæ­£ã—ã„ã¨ã™ã‚‹ã¨ã€çµå±€çœŸã«æ–°ã—ã„åˆ†å¸ƒã¨ã„ã†ã‹é–¢æ•°ã¨ã„ã†ã‹ã‚¿ã‚¹ã‚¯ã¨ã„ã†ã‹ã€ã‚’TransformerãŒå‰µå‡ºã™ã‚‹å¯èƒ½æ€§ã¯ä½ã„ã¨è¨€ãˆã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€‚ãŒã€æ–°ã—ã„ã‚‚ã®ã£ã¦å¤§ä½“ã¯æ—¢å­˜ã®æ¦‚å¿µã®çµ„ã¿åˆã‚ã›ã ã‚ˆã­ï¼ˆã‚¹ãƒãƒ›ã¨ã‹ï¼‰ã€ã¿ãŸã„ãªã“ã¨ã‚’è€ƒãˆã‚‹ã¨ã€åˆ¥ã«ãã‚Œã§ã‚‚ååˆ†ã§ã¯ï¼Ÿã¨æ€ã£ã¦ã—ã¾ã†ã€‚äººé–“ãŒæœ¬å½“ã«çœŸã®æ„å‘³ã§æ–°ã—ã„é–¢æ•°ã¨ã„ã†ã‹ã‚¿ã‚¹ã‚¯ã¨ã„ã†ã‹åˆ†å¸ƒã‚’ç”Ÿã¿å‡ºã›ã¦ã„ã‚‹ã‹ã¨ã„ã†ã¨ã€å®Ÿã¯ãã‚“ãªã«å¤šããªã„ã®ã§ã¯ï¼Ÿã¨ã„ã†äºˆæ„Ÿã‚‚ã™ã‚‹ã€‚ã¾ã‚ãŸã¨ãˆã°ã€é‡å­åŠ›å­¦ã‚’æœ€åˆã«è€ƒãˆã¾ã—ãŸï¼ã¨ã‹ãã†ã„ã†ã®ã¯ä¾‹å¤–ã ã¨æ€ã†ã‘ã©ãƒ»ãƒ»ãƒ»ã€ãã®ãƒ¬ãƒ™ãƒ«ã®ã“ã¨ã£ã¦ã©ã‚“ãã‚‰ã„ã‚ã‚‹ã‚“ã ã‚ã†ã­ï¼Ÿ</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/ReversalCurse.html" target="_blank" rel="noopener noreferrer">#ReversalCurse</a>
<span class="issue_date">Issue Date: 2023-10-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1059" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A", Lukas Berglund+, arXiv'23</a>
<span class="snippet"><span>GPT Summary</span>- è‡ªå·±å›å¸°å‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMsï¼‰ã¯ã€ã€ŒAã¯Bã§ã‚ã‚‹ã€ã¨ã„ã†æ–‡ã‹ã‚‰ã€ŒBã¯Aã§ã‚ã‚‹ã€ã¨é€†ã®é–¢ä¿‚ã‚’è‡ªå‹•çš„ã«ä¸€èˆ¬åŒ–ã§ããªã„ã€Œé€†è»¢ã®å‘ªã„ã€ã‚’ç¤ºã™ã€‚ä¾‹ãˆã°ã€ãƒ¢ãƒ‡ãƒ«ãŒã€Œãƒ¯ãƒ¬ãƒ³ãƒ†ã‚£ãƒŠãƒ»ãƒ†ãƒ¬ã‚·ã‚³ãƒ¯ã¯å®‡å®™ã«è¡Œã£ãŸæœ€åˆã®å¥³æ€§ã§ã‚ã‚‹ã€ã¨è¨“ç·´ã•ã‚Œã¦ã‚‚ã€ã€Œå®‡å®™ã«è¡Œã£ãŸæœ€åˆã®å¥³æ€§ã¯èª°ã‹ï¼Ÿã€ã«æ­£ã—ãç­”ãˆã‚‰ã‚Œãªã„ã€‚å®Ÿé¨“ã§ã¯ã€æ¶ç©ºã®æ–‡ã‚’ç”¨ã„ã¦GPT-3ã¨Llama-1ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€é€†è»¢ã®å‘ªã„ã®å­˜åœ¨ã‚’ç¢ºèªã€‚ChatGPTï¼ˆGPT-3.5ãŠã‚ˆã³GPT-4ï¼‰ã§ã‚‚ã€å®Ÿåœ¨ã®æœ‰åäººã«é–¢ã™ã‚‹è³ªå•ã§æ­£ç­”ç‡ã«å¤§ããªå·®ãŒè¦‹ã‚‰ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>A is Bã¨ã„ã†æ–‡ã§LLMã‚’è¨“ç·´ã—ã¦ã‚‚ã€B is Aã¨ã„ã†é€†æ–¹å‘ã«ã¯æ±åŒ–ã•ã‚Œãªã„ã“ã¨ã‚’ç¤ºã—ãŸã€‚<br><br>è‘—è€…ãƒ„ã‚¤ãƒ¼ãƒˆ: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/owainevans_uk/status/1705285631520407821?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/25e20dcc-0313-4cd2-8768-afb0e4e48a68" alt="image" loading="lazy"><br><p>GPT3, LLaMaã‚’ A is Bã§finetuneã—ã€B is Aã¨ã„ã†é€†æ–¹å‘ã®factã‚’ç”Ÿæˆã™ã‚‹ã‚ˆã†ã«ï¼ˆè³ªå•ã‚’ã—ã¦ï¼‰ãƒ†ã‚¹ãƒˆã—ãŸã¨ã“ã‚ã€0%ä»˜è¿‘ã®Acc.ã ã£ãŸã€‚<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/d089eb94-6872-40b5-89a1-7532758e1d89" alt="image" loading="lazy"><br><br>ã¾ãŸã€Acc.ãŒä½ã„ã ã‘ã§ãªãã€å¯¾æ•°å°¤åº¦ã‚‚randomãªfactã‚’ç”Ÿæˆã—ãŸå ´åˆã¨ã€ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã§å·®ãŒãªã„ã“ã¨ãŒã‚ã‹ã£ãŸã€‚<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/ba10fff4-cfdc-4e52-8217-c59247209211" alt="image" loading="lazy"><br><br>ã“ã®ã“ã¨ã‚‰ã€Reversal Curseã¯ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã§ã¯è§£æ±ºã§ããªã„ã“ã¨ãŒã‚ã‹ã‚‹ã€‚</p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1923" target="_blank" rel="noopener noreferrer">Physics of Language Models: Part 3.1, Knowledge Storage and Extraction, Zeyuan Allen-Zhu+, ICML'24</a>
</p></span><br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/InstructionTuning.html" target="_blank" rel="noopener noreferrer">#InstructionTuning</a>
<span class="issue_date">Issue Date: 2023-07-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/832" target="_blank" rel="noopener noreferrer" class="title-link">Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning, ACL'23</a>
<span class="snippet"><span>GPT Summary</span>- æœ€è¿‘ã®instruction tuningï¼ˆITï¼‰ã®ç ”ç©¶ã§ã¯ã€è¿½åŠ ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’æä¾›ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€ã‚¼ãƒ­ã‚·ãƒ§ãƒƒãƒˆã®æ±åŒ–æ€§èƒ½ã‚’æŒã¤ç´ æ™´ã‚‰ã—ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå®Ÿç¾ã•ã‚Œã¦ã„ã‚‹ã€‚ã—ã‹ã—ã€ITä¸­ã«ãƒ¢ãƒ‡ãƒ«ãŒã©ã®ã‚ˆã†ã«æŒ‡ç¤ºã‚’åˆ©ç”¨ã—ã¦ã„ã‚‹ã‹ã¯ã¾ã ç ”ç©¶ã•ã‚Œã¦ã„ãªã„ã€‚æœ¬ç ”ç©¶ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å¤‰æ›´ã•ã‚ŒãŸæŒ‡ç¤ºã¨å…ƒã®æŒ‡ç¤ºã¨ã®æ¯”è¼ƒã«ã‚ˆã£ã¦ã€ãƒ¢ãƒ‡ãƒ«ãŒITä¸­ã«æŒ‡ç¤ºã‚’ã©ã®ã‚ˆã†ã«åˆ©ç”¨ã™ã‚‹ã‹ã‚’åˆ†æã™ã‚‹ã€‚å®Ÿé¨“ã®çµæœã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯å…ƒã®æŒ‡ç¤ºã¨åŒç­‰ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã—ã€ITã¨åŒæ§˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ã“ã®ç ”ç©¶ã¯ã€ã‚ˆã‚Šä¿¡é ¼æ€§ã®é«˜ã„ITæ‰‹æ³•ã¨è©•ä¾¡ã®ç·Šæ€¥æ€§ã‚’å¼·èª¿ã—ã¦ã„ã‚‹ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<a class="button" href="articles/Faithfulness.html" target="_blank" rel="noopener noreferrer">#Faithfulness</a>
<a class="button" href="articles/NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<span class="issue_date">Issue Date: 2023-05-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/666" target="_blank" rel="noopener noreferrer" class="title-link">Language Models Don't Always Say What They Think: Unfaithful   Explanations in Chain-of-Thought Prompting, Miles Turpin+, N_A, NeurIPS'23</a>
<span class="snippet"><span>GPT Summary</span>- LLMsã«ã‚ˆã‚‹æ¨è«–ã«ãŠã„ã¦ã€chain-of-thought reasoningï¼ˆCoTï¼‰ã¨å‘¼ã°ã‚Œã‚‹èª¬æ˜ã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã§ãã‚‹ãŒã€ã“ã®èª¬æ˜ãŒãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã®çœŸã®ç†ç”±ã‚’èª¤ã£ã¦è¡¨ç¾ã™ã‚‹ã“ã¨ãŒã‚ã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸã€‚ãƒã‚¤ã‚¢ã‚¹ã®ã‚ã‚‹ç‰¹å¾´ã‚’ãƒ¢ãƒ‡ãƒ«ã®å…¥åŠ›ã«è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€CoTèª¬æ˜ãŒå¤§ããå½±éŸ¿ã‚’å—ã‘ã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ã“ã®çµæœã¯ã€LLMsã«å¯¾ã™ã‚‹ä¿¡é ¼ã‚’é«˜ã‚ã‚‹ãŸã‚ã«ã€èª¬æ˜ã®å¿ å®Ÿåº¦ã‚’è©•ä¾¡ã—ã€æ”¹å–„ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-09-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2860" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Emergent Abilities of Large Language Models, Jason Wei+, TMLR'22</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚¢ãƒƒãƒ—ã¯æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŒã€ã€Œå‡ºç¾èƒ½åŠ›ã€ã¨å‘¼ã°ã‚Œã‚‹äºˆæ¸¬ä¸å¯èƒ½ãªç¾è±¡ãŒå­˜åœ¨ã™ã‚‹ã€‚ã“ã‚Œã¯å°å‹ãƒ¢ãƒ‡ãƒ«ã«ã¯ãªã„èƒ½åŠ›ã§ã‚ã‚Šã€ã•ã‚‰ãªã‚‹ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒãƒ¢ãƒ‡ãƒ«ã®èƒ½åŠ›ã‚’æ‹¡å¤§ã™ã‚‹å¯èƒ½æ€§ã‚’ç¤ºå”†ã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>openreview: 


<a href="https://openreview.net/forum?id=yzkSU5zdwD" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=yzkSU5zdwD</a>


</p>
<p>å‰µç™ºèƒ½åŠ›ï¼ˆæœ€è¿‘ã“ã®ç”¨èªã‚’ç›®ã«ã™ã‚‹æ©Ÿä¼šãŒæ¸›ã£ãŸã‚ˆã†ãªæ°—ãŒã™ã‚‹ï¼‰</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="articles/KnowledgeEditing.html" target="_blank" rel="noopener noreferrer">#KnowledgeEditing</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/FactualKnowledge.html" target="_blank" rel="noopener noreferrer">#FactualKnowledge</a>
<a class="button" href="articles/Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<span class="issue_date">Issue Date: 2024-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1332" target="_blank" rel="noopener noreferrer" class="title-link">Knowledge Neurons in Pretrained Transformers, Damai Dai+, N_A, ACL'22, 2022.05</a>
<span class="snippet"><span>GPT Summary</span>- å¤§è¦æ¨¡ãªäº‹å‰å­¦ç¿’è¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€äº‹å®ŸçŸ¥è­˜ã®æ ¼ç´æ–¹æ³•ã«ã¤ã„ã¦ã®ç ”ç©¶ã‚’è¡Œã„ã¾ã—ãŸã€‚å…·ä½“çš„ã«ã¯ã€BERTã®fill-in-the-blank cloze taskã‚’ç”¨ã„ã¦ã€é–¢é€£ã™ã‚‹äº‹å®Ÿã‚’è¡¨ç¾ã™ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’ç‰¹å®šã—ã¾ã—ãŸã€‚ã¾ãŸã€çŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®æ´»æ€§åŒ–ã¨å¯¾å¿œã™ã‚‹äº‹å®Ÿã®è¡¨ç¾ã¨ã®æ­£ã®ç›¸é–¢ã‚’è¦‹ã¤ã‘ã¾ã—ãŸã€‚ã•ã‚‰ã«ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã‚ãšã«ã€çŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’æ´»ç”¨ã—ã¦ç‰¹å®šã®äº‹å®ŸçŸ¥è­˜ã‚’ç·¨é›†ã—ã‚ˆã†ã¨è©¦ã¿ã¾ã—ãŸã€‚ã“ã®ç ”ç©¶ã¯ã€äº‹å‰å­¦ç¿’ã•ã‚ŒãŸTransformerså†…ã§ã®çŸ¥è­˜ã®æ ¼ç´ã«é–¢ã™ã‚‹ç¤ºå”†ã«å¯Œã‚“ã§ãŠã‚Šã€ã‚³ãƒ¼ãƒ‰ã¯https://github.com/Hunter-DDM/knowledge-neuronsã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1108" target="_blank" rel="noopener noreferrer">å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ï½¤ã€ŒçŸ¥è­˜ã¯å…¨çµåˆå±¤ã«è“„ç©ã•ã‚Œã‚‹ã€ã¨ã„ã†ä»®èª¬ã«ã¤ã„ã¦ã®æ–‡çŒ®èª¿æŸ»</a>
 </p>
<p>æ—¥æœ¬èªè§£èª¬: 


<a href="https://speakerdeck.com/kogoro/knowledge-neurons-in-pretrained-transformers-for-snlp2022" target="_blank" rel="noopener noreferrer">https://speakerdeck.com/kogoro/knowledge-neurons-in-pretrained-transformers-for-snlp2022</a>


</p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2140" target="_blank" rel="noopener noreferrer">[Paper Note] Transformer Feed-Forward Layers Are Key-Value Memories, Mor Geva+, EMNLP'21</a>
</p>
<p>ä¸Šè¨˜è³‡æ–™ã«ã‚ˆã‚‹ã¨ã€ç‰¹å®šã®çŸ¥è­˜ã‚’å‡ºåŠ›ã™ã‚‹éš›ã«æ´»æ€§åŒ–ã™ã‚‹çŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’ç‰¹å®šã™ã‚‹æ‰‹æ³•ã‚’ææ¡ˆã€‚MLMã‚’ç”¨ã„ãŸclozeã‚¿ã‚¹ã‚¯ã«ã‚ˆã‚‹å®Ÿé¨“ã§[MASK]éƒ¨åˆ†ã«å½“è©²çŸ¥è­˜ã‚’å‡ºåŠ›ã™ã‚‹å®Ÿé¨“ã‚’ã—ãŸçµæœã€çŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®é‡ã¿ã‚’ã‚¼ãƒ­ã¨ã™ã‚‹ã¨æ€§èƒ½ãŒè‘—ã—ãåŠ£åŒ–ã—ã€å€¤ã‚’2å€ã«ã™ã‚‹ã¨æ€§èƒ½ãŒæ”¹å–„ã™ã‚‹ã¨ã„ã£ãŸå‚¾å‘ãŒã¿ã‚‰ã‚ŒãŸã€‚ã€€ã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ã¨ã—ã¦ã€çŸ¥è­˜ã®æ›´æ–°ã¨ã€çŸ¥è­˜ã®å‰Šé™¤ãŒå¯èƒ½ã‹ã‚’æ¤œè¨¼ã€‚ã©ã¡ã‚‰ã¨ã‚‚æ›´æ–°ãƒ»å‰Šé™¤ãŒã•ã‚Œã‚‹æ–¹å‘æ€§[^1]ã¸ãƒ¢ãƒ‡ãƒ«ãŒå¤‰åŒ–ã—ãŸã€‚<br><br>ã¾ãŸã€çŸ¥è­˜ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã¯Transformerã®å±¤ã®æ·±ã„ã¨ã“ã‚ã«ä½ç½®ã—ã¦ã„ã‚‹å‚¾å‘ã«ã‚ã‚Šã€ç•°ãªã‚‹relationã‚’æŒã¤ã‚ˆã†ãªé–¢ä¿‚çŸ¥è­˜åŒå£«ã§ã¯å…±æœ‰ã•ã‚Œãªã„å‚¾å‘ã«ã‚ã‚‹æ¨¡æ§˜ã€‚<br><br>[^1]: ä»–ã®çŸ¥è­˜ã«å½±éŸ¿ã‚’ä¸ãˆãšã€å®Œç’§ã«æ›´æ–°ãƒ»å‰Šé™¤ã§ããŸã‚ã‘ã§ã¯ãªã„ã€‚çŸ¥è­˜ã®æ›´æ–°ãƒ»å‰Šé™¤ã«ä¼´ã„Extrinsicãªè©•ä¾¡ã«ã‚ˆã£ã¦æ€§èƒ½å‘ä¸Šã€ã‚ã‚‹ã„ã¯PerplexityãŒå¢—å¤§ã—ãŸã€ã¨ã„ã£ãŸçµæœã‹ã‚‰ãã†ã„ã£ãŸæ–¹å‘æ€§ã¸ãƒ¢ãƒ‡ãƒ«ãŒå¤‰åŒ–ã—ãŸã€ã¨ã„ã†è©±</p></span><br><br>
<a class="button" href="articles/DocumentSummarization.html" target="_blank" rel="noopener noreferrer">#DocumentSummarization</a>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/IJCNLP.html" target="_blank" rel="noopener noreferrer">#IJCNLP</a>
<a class="button" href="articles/AACL.html" target="_blank" rel="noopener noreferrer">#AACL</a>
<a class="button" href="articles/Repetition.html" target="_blank" rel="noopener noreferrer">#Repetition</a>
<span class="issue_date">Issue Date: 2023-08-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/939" target="_blank" rel="noopener noreferrer" class="title-link">Self-Repetition in Abstractive Neural Summarizers, Nikita Salkar+, N_A,  AACL-IJCNLP'22</a>
<span class="snippet"><span>GPT Summary</span>- ç§ãŸã¡ã¯ã€BARTã€T5ã€ãŠã‚ˆã³Pegasusã¨ã„ã†3ã¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã«ãŠã‘ã‚‹è‡ªå·±ç¹°ã‚Šè¿”ã—ã®åˆ†æã‚’è¡Œã„ã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯ã€ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§fine-tuningã•ã‚Œã¦ã„ã¾ã™ã€‚å›å¸°åˆ†æã«ã‚ˆã‚‹ã¨ã€ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã¯å…¥åŠ›ã®å‡ºåŠ›è¦ç´„é–“ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç¹°ã‚Šè¿”ã™å‚¾å‘ãŒç•°ãªã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚ã¾ãŸã€æŠ½è±¡çš„ãªãƒ‡ãƒ¼ã‚¿ã‚„å®šå‹çš„ãªè¨€èªã‚’ç‰¹å¾´ã¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã§ã®fine-tuningã§ã¯ã€è‡ªå·±ç¹°ã‚Šè¿”ã—ã®å‰²åˆãŒé«˜ããªã‚‹å‚¾å‘ãŒã‚ã‚Šã¾ã™ã€‚å®šæ€§çš„ãªåˆ†æã§ã¯ã€ã‚·ã‚¹ãƒ†ãƒ ãŒã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆã‚„å®šå‹ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’ç”Ÿæˆã™ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã®çµæœã¯ã€ã‚µãƒãƒ©ã‚¤ã‚¶ãƒ¼ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã™ã‚‹ãŸã‚ã®æ‰‹æ³•ã®é–‹ç™ºã«å½¹ç«‹ã¤å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-05-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/674" target="_blank" rel="noopener noreferrer" class="title-link">Out of One, Many: Using Language Models to Simulate Human Samples, Lisa P. Argyle+, N_A, arXiv'22</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬ç ”ç©¶ã§ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ãŒç¤¾ä¼šç§‘å­¦ç ”ç©¶ã«ãŠã„ã¦ç‰¹å®šã®äººé–“ã®ã‚µãƒ–ãƒãƒ”ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ä»£ç†ã¨ã—ã¦ç ”ç©¶ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚’ææ¡ˆã—ã€GPT-3è¨€èªãƒ¢ãƒ‡ãƒ«ã®ã€Œã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ çš„å¿ å®Ÿåº¦ã€ã‚’æ¢æ±‚ã™ã‚‹ã€‚ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ çš„å¿ å®Ÿåº¦ãŒååˆ†ã§ã‚ã‚‹è¨€èªãƒ¢ãƒ‡ãƒ«ã¯ã€äººé–“ã‚„ç¤¾ä¼šã®ç†è§£ã‚’é€²ã‚ã‚‹ãŸã‚ã®æ–°ã—ã„å¼·åŠ›ãªãƒ„ãƒ¼ãƒ«ã¨ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã¨ææ¡ˆã™ã‚‹ã€‚</span>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/PMLR.html" target="_blank" rel="noopener noreferrer">#PMLR</a>
<span class="issue_date">Issue Date: 2025-08-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2583" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Feature Learning in Infinite-Width Neural Networks, Greg Yang+, PMLR'21</a>
<span class="snippet"><span>GPT Summary</span>- ç„¡é™å¹…ã®æ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã„ã¦ã€æ¨™æº–ãŠã‚ˆã³NTKãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã¯ç‰¹å¾´å­¦ç¿’ã‚’å¯èƒ½ã«ã™ã‚‹é™ç•Œã‚’æŒãŸãªã„ã“ã¨ã‚’ç¤ºã—ã€ã“ã‚Œã‚’å…‹æœã™ã‚‹ãŸã‚ã®ä¿®æ­£ã‚’ææ¡ˆã€‚Tensor ProgramsæŠ€è¡“ã‚’ç”¨ã„ã¦é™ç•Œã®æ˜ç¤ºçš„ãªå¼ã‚’å°å‡ºã—ã€Word2Vecã‚„MAMLã‚’ç”¨ã„ãŸå°‘æ•°ã‚·ãƒ§ãƒƒãƒˆå­¦ç¿’ã§ã“ã‚Œã‚‰ã®é™ç•Œã‚’è¨ˆç®—ã€‚ææ¡ˆæ‰‹æ³•ã¯NTKãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚„æœ‰é™å¹…ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ä¸Šå›ã‚‹æ€§èƒ½ã‚’ç¤ºã—ã€ç‰¹å¾´å­¦ç¿’ã‚’è¨±å¯ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã®ç©ºé–“ã‚’åˆ†é¡ã€‚</span>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/FactualKnowledge.html" target="_blank" rel="noopener noreferrer">#FactualKnowledge</a>
<span class="issue_date">Issue Date: 2025-07-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2140" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Transformer Feed-Forward Layers Are Key-Value Memories, Mor Geva+, EMNLP'21</a>
<span class="snippet"><span>GPT Summary</span>- ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã¯ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã®å¤§éƒ¨åˆ†ã‚’å ã‚ã‚‹ãŒã€ãã®å½¹å‰²ã¯æœªæ¢æ±‚ã€‚ç ”ç©¶ã«ã‚ˆã‚Šã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ãŒã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ãƒ»ãƒ¡ãƒ¢ãƒªã¨ã—ã¦æ©Ÿèƒ½ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¾‹ã®ãƒ†ã‚­ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ç›¸é–¢ã™ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚å®Ÿé¨“ã§ã€ä¸‹å±¤ã¯æµ…ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ä¸Šå±¤ã¯æ„å‘³çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã€ãƒãƒªãƒ¥ãƒ¼ãŒå‡ºåŠ›åˆ†å¸ƒã‚’èª˜å°ã™ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚ŒãŸã€‚æœ€çµ‚çš„ã«ã€ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®å‡ºåŠ›ã¯ãƒ¡ãƒ¢ãƒªã®åˆæˆã§ã‚ã‚Šã€æ®‹å·®æ¥ç¶šã‚’é€šã˜ã¦æ´—ç·´ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬ï¼ˆp.5ã‚ˆã‚Šï¼‰: 


<a href="https://speakerdeck.com/kogoro/knowledge-neurons-in-pretrained-transformers-for-snlp2022?slide=5" target="_blank" rel="noopener noreferrer">https://speakerdeck.com/kogoro/knowledge-neurons-in-pretrained-transformers-for-snlp2022?slide=5</a>


</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<span class="issue_date">Issue Date: 2024-10-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1439" target="_blank" rel="noopener noreferrer" class="title-link">Intrinsic Dimensionality Explains the Effectiveness of Language Model   Fine-Tuning, Armen Aghajanyan+, N_A, ACL'21</a>
<span class="snippet"><span>GPT Summary</span>- äº‹å‰å­¦ç¿’ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ€ã‚¤ãƒŠãƒŸã‚¯ã‚¹ã‚’å†…å› æ¬¡å…ƒã®è¦³ç‚¹ã‹ã‚‰åˆ†æã—ã€å°‘ãªã„ãƒ‡ãƒ¼ã‚¿ã§ã‚‚åŠ¹æœçš„ã«èª¿æ•´ã§ãã‚‹ç†ç”±ã‚’èª¬æ˜ã€‚ä¸€èˆ¬çš„ãªãƒ¢ãƒ‡ãƒ«ã¯ä½ã„å†…å› æ¬¡å…ƒã‚’æŒã¡ã€ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã¨åŒç­‰ã®åŠ¹æœã‚’æŒã¤ä½æ¬¡å…ƒã®å†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ãŒå¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚ç‰¹ã«ã€RoBERTaãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ã¦ã€å°‘æ•°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æœ€é©åŒ–ã§é«˜ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’é”æˆã§ãã‚‹ã“ã¨ã‚’å®Ÿè¨¼ã€‚ã¾ãŸã€äº‹å‰å­¦ç¿’ãŒå†…å› æ¬¡å…ƒã‚’æœ€å°åŒ–ã—ã€å¤§ããªãƒ¢ãƒ‡ãƒ«ãŒä½ã„å†…å› æ¬¡å…ƒã‚’æŒã¤å‚¾å‘ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã€å†…å› æ¬¡å…ƒã«åŸºã¥ãä¸€èˆ¬åŒ–å¢ƒç•Œã‚’ææ¡ˆã€‚</span>
<span class="snippet"><span>Comment</span><p>ACL ver:


<a href="https://aclanthology.org/2021.acl-long.568.pdf" target="_blank" rel="noopener noreferrer">https://aclanthology.org/2021.acl-long.568.pdf</a>


</p>
<p>ä¸‹è¨˜ã®å…ƒãƒã‚¹ãƒˆã‚’æ‹èª­ã®ä¸Šè«–æ–‡ã‚’æ–œã‚èª­ã¿ã€‚ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã„ã»ã©ã€ç‰¹å®šã®æ€§èƒ½ï¼ˆè«–æ–‡ä¸­ã§ã¯2ç¨®é¡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã®90%ã®sentence predictionæ€§èƒ½ï¼‰ã‚’finetuningã§é”æˆã™ã‚‹ãŸã‚ã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚Œã°ãªã‚‹ã»ã©å°ã•ããªã£ã¦ã„ã‚‹ã€‚<br><br>LoRAã¨ã®é–¢ä¿‚æ€§ã«ã¤ã„ã¦ã‚‚å…ƒãƒã‚¹ãƒˆä¸­ã§è¨€åŠã•ã‚Œã¦ãŠã‚Šã€è«–æ–‡ã®ä¸­èº«ã‚‚è¦‹ã¦å¾Œã§ç¢ºèªã™ã‚‹ã€‚<br>ãŠãã‚‰ãã€LLMã¯BERTãªã©ã¨æ¯”è¼ƒã—ã¦é¥ã‹ã«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå¤§ãã„ãŸã‚ã€finetuningã«è¦ã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯ã•ã‚‰ã«å°ã•ããªã£ã¦ã„ã‚‹ã“ã¨ãŒæƒ³åƒã•ã‚Œã€LoRAã®ã‚ˆã†ãªå°‘é‡ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’concatã™ã‚‹ã ã‘ã§ã†ã¾ãã„ãã€ã¨ã„ã†ã‚ˆã†ãªè©±ã ã¨æ€ã‚ã‚Œã‚‹ã€‚èˆˆå‘³æ·±ã„ã€‚<br><br><img src="https://github.com/user-attachments/assets/166ebdae-539f-44cf-822b-0084640e07b2" alt="image" loading="lazy"><br>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/bilzrd/status/1840445027438456838?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<span class="issue_date">Issue Date: 2024-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1333" target="_blank" rel="noopener noreferrer" class="title-link">Transformer Feed-Forward Layers Are Key-Value Memories, Mor Geva+, N_A, EMNLP'21</a>
<span class="snippet"><span>GPT Summary</span>- ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã¯ã€ã‚­ãƒ¼ãƒ»ãƒãƒªãƒ¥ãƒ¼ãƒ¡ãƒ¢ãƒªã¨ã—ã¦æ©Ÿèƒ½ã—ã€å­¦ç¿’ã•ã‚ŒãŸãƒ‘ã‚¿ãƒ¼ãƒ³ãŒäººé–“ã«è§£é‡ˆå¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚„ã€ä¸Šä½å±¤ãŒã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹ã“ã¨ãŒç¤ºã•ã‚Œã¾ã—ãŸã€‚ã•ã‚‰ã«ã€å‡ºåŠ›åˆ†å¸ƒã‚’èª˜å°ã™ã‚‹å½¹å‰²ã‚‚æŒã¡ã¾ã™ã€‚ãƒ•ã‚£ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰å±¤ã®å‡ºåŠ›ã¯ãã®ãƒ¡ãƒ¢ãƒªã®åˆæˆã§ã‚ã‚Šã€æ®‹å·®æ¥ç¶šã‚’ä»‹ã—ã¦ãƒ¢ãƒ‡ãƒ«ã®å±¤ã‚’é€šã˜ã¦æ´—ç·´ã•ã‚Œã€æœ€çµ‚çš„ãªå‡ºåŠ›åˆ†å¸ƒã‚’ç”Ÿæˆã—ã¾ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1108" target="_blank" rel="noopener noreferrer">å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ï½¤ã€ŒçŸ¥è­˜ã¯å…¨çµåˆå±¤ã«è“„ç©ã•ã‚Œã‚‹ã€ã¨ã„ã†ä»®èª¬ã«ã¤ã„ã¦ã®æ–‡çŒ®èª¿æŸ»</a>
 </p>
<p>FF layerãŒKey-Valueã‚¹ãƒˆã‚¢ã¨ã—ã¦æ©Ÿèƒ½ã™ã‚‹ä»•çµ„ã¿ã®æ¦‚ç•¥å›³<br><img src="https://github.com/user-attachments/assets/cc12695f-b030-433a-88e1-aed69f9847a7" alt="image" loading="lazy"><br><br>å®Ÿéš›ã«ç‰¹å®šã®Keyã¨æœ€ã‚‚é–¢é€£åº¦ãŒé«˜ã„è¨“ç·´äº‹ä¾‹ï¼ˆinputï¼‰ã‚’æŠ½å‡ºã—ã€äººé–“ãŒinputã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†é¡ã—ãŸçµæœ<br><img src="https://github.com/user-attachments/assets/d1c1a031-9cb8-4e22-bf87-23964f0e0c71" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/NaturalLanguageGeneration.html" target="_blank" rel="noopener noreferrer">#NaturalLanguageGeneration</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="articles/Annotation.html" target="_blank" rel="noopener noreferrer">#Annotation</a>
<span class="issue_date">Issue Date: 2024-05-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1306" target="_blank" rel="noopener noreferrer" class="title-link">The Perils of Using Mechanical Turk to Evaluate Open-Ended Text  Generation, Marzena Karpinska+, N_A, EMNLP'21</a>
<span class="snippet"><span>GPT Summary</span>- æœ€è¿‘ã®ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®ç ”ç©¶ã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚¨ãƒ³ãƒ‰ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ã«æ³¨åŠ›ã—ã¦ãŠã‚Šã€ãã®è©•ä¾¡ãŒé›£ã—ã„ãŸã‚ã€å¤šãã®ç ”ç©¶è€…ãŒã‚¯ãƒ©ã‚¦ãƒ‰ã‚½ãƒ¼ã‚·ãƒ³ã‚°ã•ã‚ŒãŸäººé–“ã®åˆ¤æ–­ã‚’åé›†ã—ã¦ãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã‚’æ­£å½“åŒ–ã—ã¦ã„ã‚‹ã€‚ã—ã‹ã—ã€å¤šãã®ç ”ç©¶ã¯é‡è¦ãªè©³ç´°ã‚’å ±å‘Šã—ã¦ãŠã‚‰ãšã€å†ç¾æ€§ãŒå¦¨ã’ã‚‰ã‚Œã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã£ãŸã€‚ã•ã‚‰ã«ã€åŠ´åƒè€…ã¯ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆã®ãƒ†ã‚­ã‚¹ãƒˆã¨äººé–“ã«ã‚ˆã‚‹å‚ç…§ãƒ†ã‚­ã‚¹ãƒˆã‚’åŒºåˆ¥ã§ããªã„ã“ã¨ãŒç™ºè¦‹ã•ã‚Œã€è¡¨ç¤ºæ–¹æ³•ã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ã§æ”¹å–„ã•ã‚Œã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚è‹±èªæ•™å¸«ã¨ã®ã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è©•ä¾¡ã™ã‚‹éš›ã®èª²é¡Œã«ã¤ã„ã¦ã€ã‚ˆã‚Šæ·±ã„æ´å¯ŸãŒå¾—ã‚‰ã‚ŒãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>Open-endedãªã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹AMTã®è©•ä¾¡ã®å†ç¾æ€§ã«é–¢ã™ã‚‹ç ”ç©¶ã€‚å…ˆè¡Œç ”ç©¶ã‚’Surveyã—ãŸã¨ã“ã‚ã€å†ç¾ã®ãŸã‚ã«é‡è¦ãªæƒ…å ±ï¼ˆãŸã¨ãˆã°ã€workerã®è³‡æ ¼ã€è²»ç”¨ã€task descriptionsã€annotatoré–“ã®agreementãªã©ï¼‰ãŒæ¬ è½ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ãŸã€‚<br><br>ç¶šã„ã¦ã€expertsã¨AMT workerã«å¯¾ã—ã¦ã€story generationã®è©•ä¾¡ã‚’å®Ÿæ–½ã—ã€GPT2ãŒç”Ÿæˆã—ãŸã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã¨äººé–“ãŒç”Ÿæˆã—ãŸã‚¹ãƒˆãƒ¼ãƒªãƒ¼ã‚’ã€å¾Œè€…ã®ã‚¹ã‚³ã‚¢ãŒé«˜ããªã‚‹ã“ã¨ã‚’æœŸå¾…ã—ã¦ä¾é ¼ã—ãŸã€‚ãã®çµæœ<br><br>- AMTã®ratingã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒç”Ÿæˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆã¨ã€äººé–“ãŒç”Ÿæˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’reliableã«åŒºåˆ¥ã§ããªã„<br><br>- åŒä¸€ã®ã‚¿ã‚¹ã‚¯ã‚’ç•°ãªã‚‹æ—¥ç¨‹ã§å®Ÿæ–½ã‚’ã™ã‚‹ã¨ã€é«˜ã„åˆ†æ•£ãŒç”Ÿã˜ãŸ<br><br>- å¤šãã®AMT workerã¯ã€è©•ä¾¡å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ³¨æ„æ·±ãèª­ã‚“ã§ã„ãªã„<br><br>- Expertã§ã•ãˆãƒ¢ãƒ‡ãƒ«ãŒç”Ÿæˆã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’èª­ã¿åˆ¤æ–­ã™ã‚‹ã®ã«ã¯è‹¦æˆ¦ã‚’ã—ã€å…ˆè¡Œç ”ç©¶ã¨æ¯”è¼ƒã—ã¦ã‚ˆã‚Šå¤šãã®æ™‚é–“ã‚’è²»ã‚„ã—ã€agreementãŒä½ããªã‚‹ã“ã¨ãŒåˆ†ã‹ã£ãŸ<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/1dc01c56-88b0-4bea-869b-f396d65701cc" alt="image" loading="lazy"><br><br></p>
<p><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/892" target="_blank" rel="noopener noreferrer">Can Large Language Models Be an Alternative to Human Evaluations? Cheng-Han Chiang, Hung-yi Lee, ACL'23</a>
 ã«ãŠã„ã¦ã€ä½å“è³ªãªwork forceãŒäººæ‰‹è©•ä¾¡ã«å¯¾ã—ã¦æœ‰å®³ãªå½±éŸ¿ã‚’ä¸ãˆã‚‹ã€ã¨ã„ã†æ–‡è„ˆã§æœ¬ç ”ç©¶ãŒå¼•ç”¨ã•ã‚Œã¦ã„ã‚‹</p></span><br><br>
<a class="button" href="articles/MachineTranslation.html" target="_blank" rel="noopener noreferrer">#MachineTranslation</a>
<a class="button" href="articles/NaturalLanguageGeneration.html" target="_blank" rel="noopener noreferrer">#NaturalLanguageGeneration</a>
<a class="button" href="articles/Metrics.html" target="_blank" rel="noopener noreferrer">#Metrics</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<span class="issue_date">Issue Date: 2024-01-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1221" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Experts, Errors, and Context: A Large-Scale Study of Human Evaluation  for Machine Translation, Markus Freitag+, arXiv'21</a>
<span class="snippet"><span>GPT Summary</span>- æ©Ÿæ¢°ç¿»è¨³ã‚·ã‚¹ãƒ†ãƒ ã®äººé–“ã«ã‚ˆã‚‹è©•ä¾¡ã¯é›£ã—ãã€æ¨™æº–çš„ãªæ‰‹ç¶šããŒæ¬ å¦‚ã—ã¦ã„ã‚‹ã€‚ãã“ã§ã€MQMãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«åŸºã¥ãè©•ä¾¡æ–¹æ³•è«–ã‚’ææ¡ˆã—ã€WMT 2020ã®ãƒˆãƒƒãƒ—ã‚·ã‚¹ãƒ†ãƒ ã®å‡ºåŠ›ã‚’ãƒ—ãƒ­ã®ç¿»è¨³è€…ã«ã‚ˆã‚‹æ³¨é‡ˆã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã—ãŸã€‚åˆ†æã®çµæœã€ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ¯ãƒ¼ã‚«ãƒ¼ã«ã‚ˆã‚‹è©•ä¾¡ã¨ã¯ç•°ãªã‚Šã€äººé–“ã®å‡ºåŠ›ãŒæ©Ÿæ¢°ã®å‡ºåŠ›ã‚ˆã‚Šå¥½ã¾ã‚Œã‚‹ã“ã¨ãŒç¤ºã•ã‚ŒãŸã€‚ã¾ãŸã€äº‹å‰å­¦ç¿’ã•ã‚ŒãŸåŸ‹ã‚è¾¼ã¿ã«åŸºã¥ãè‡ªå‹•ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒäººé–“ã®è©•ä¾¡ã‚’ä¸Šå›ã‚‹ã“ã¨ã‚‚æ˜ã‚‰ã‹ã«ãªã£ãŸã€‚ã‚³ãƒ¼ãƒ‘ã‚¹ã¯ä»Šå¾Œã®ç ”ç©¶ã®ãŸã‚ã«å…¬é–‹ã•ã‚Œã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p>embedding basedãªNLGã®æ€§èƒ½æŒ‡æ¨™ãŒã€æ„å‘³ã®ç­‰ä¾¡æ€§ã‚„æµæš¢æ€§ã‚’è©•ä¾¡ã§ãã‚‹ä¸€æ–¹ã€é©ç”¨ç¯„å›²ãŒé™å®šçš„ã§æŸ”è»Ÿæ€§ã«æ¬ ã‘ã‚‹ã“ã¨ã‚’ç¤ºã—ãŸç ”ç©¶</p></span><br><br>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Normalization.html" target="_blank" rel="noopener noreferrer">#Normalization</a>
<a class="button" href="articles/Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<span class="issue_date">Issue Date: 2025-07-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2142" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] On Layer Normalization in the Transformer Architecture, Ruibin Xiong+, arXiv'20</a>
<span class="snippet"><span>GPT Summary</span>- æœ¬è«–æ–‡ã§ã¯ã€Transformerã®å­¦ç¿’ç‡ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æ®µéšã®é‡è¦æ€§ã‚’ç†è«–çš„ã«ç ”ç©¶ã—ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼æ­£è¦åŒ–ã®ä½ç½®ãŒè¨“ç·´ã®å®‰å®šæ€§ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’ç¤ºã™ã€‚ç‰¹ã«ã€Post-LN Transformerã§ã¯å¤§ããªå‹¾é…ãŒä¸å®‰å®šã•ã‚’å¼•ãèµ·ã“ã™ãŸã‚ã€ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ãŒæœ‰åŠ¹ã§ã‚ã‚‹ä¸€æ–¹ã€Pre-LN Transformerã§ã¯å‹¾é…ãŒè‰¯å¥½ã«æŒ¯ã‚‹èˆã†ãŸã‚ã€ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã‚’çœç•¥ã§ãã‚‹ã“ã¨ã‚’ç¤ºã™ã€‚å®Ÿé¨“ã«ã‚ˆã‚Šã€ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ãªã—ã®Pre-LN TransformerãŒãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨åŒç­‰ã®çµæœã‚’é”æˆã—ã€è¨“ç·´æ™‚é–“ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´ãŒå‰Šæ¸›ã§ãã‚‹ã“ã¨ã‚’ç¢ºèªã—ãŸã€‚</span>
<span class="snippet"><span>Comment</span><p>OpenReview:


<a href="https://openreview.net/forum?id=B1x8anVFPr" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=B1x8anVFPr</a>


</p>
<p>Encoder-Decoderã®Transformerã«ãŠã„ã¦ã€Post-LNã®å ´åˆã¯ã€Warmupã‚’ç„¡ãã™ã¨æœ€çµ‚çš„ãªæ€§èƒ½ãŒæ‚ªåŒ–ã—ã€ã¾ãŸWarmUpã‚¹ãƒ†ãƒƒãƒ—ã®å€¤ã«ã‚ˆã£ã¦ï¼ˆ500 vs. 4000ã§å®Ÿé¨“)ã‚‚æœ€çµ‚çš„ãªæ€§èƒ½ãŒå¤‰åŒ–ã™ã‚‹ã€‚ã“ã‚Œã«ã¯å­¦ç¿’æ™‚ã«ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã—ã£ã‹ã‚Šæ¢ç´¢ã—ãªã‘ã‚Œã°ãªã‚‰ãšã€WarmUPã‚’å¤§ããã™ã‚‹ã¨å­¦ç¿’åŠ¹ç‡ãŒè½ã¡ã‚‹ã¨ã„ã†ãƒ‡ãƒ¡ãƒªãƒƒãƒˆãŒã‚ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/e7a26ecd-7905-4e6c-bb9a-29b8289addb0" alt="image" loading="lazy"><br><br>Post-LNã®å ´åˆã¯ã€Pre-LNã¨æ¯”è¼ƒã—ã¦å‹¾é…ãŒå¤§ããã€Warmupã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã—ã£ã‹ã‚Šè¨­è¨ˆã—ãªã„ã¨å¤§ããªå‹¾é…ã«å¯¾ã—ã¦å¤§ããªå­¦ç¿’ç‡ãŒé©ç”¨ã•ã‚Œå­¦ç¿’ãŒä¸å®‰å®šã«ãªã‚‹ã€‚ã“ã‚Œã¯å­¦ç¿’ç‡ã‚’éå¸¸ã«å°ã•ãã—ã€å›ºå®šå€¤ã‚’ä½¿ã†ã“ã¨ã§è§£æ±ºã§ãã‚‹ãŒã€åæŸãŒéå¸¸ã«é…ããªã‚‹ã¨ã„ã†ãƒ‡ãƒ¡ãƒªãƒƒãƒˆãŒã‚ã‚‹ã€‚<br><img src="https://github.com/user-attachments/assets/afb09f44-c7c9-44ab-9066-3ee788ebd8ee" alt="image" loading="lazy"><br><br>ä¸€æ–¹ã€Pre-LNã¯Warmupç„¡ã—ã§ã‚‚ã€é«˜ã„æ€§èƒ½ãŒé”æˆã§ãã€ä¸Šè¨˜ã®ã‚ˆã†ãªãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®æ‰‹é–“ã‚„å­¦ç¿’åŠ¹ç‡ã®è¦³ç‚¹ã‹ã‚‰åˆ©ç‚¹ãŒã‚ã‚‹ã€ã¿ãŸã„ãªè©±ã®æ¨¡æ§˜ã€‚<br><img src="https://github.com/user-attachments/assets/d675a58b-e876-4e41-a76f-306c2e1ce23f" alt="image" loading="lazy"><br></p></span><br><br>
<a class="button" href="articles/MachineTranslation.html" target="_blank" rel="noopener noreferrer">#MachineTranslation</a>
<a class="button" href="articles/NaturalLanguageGeneration.html" target="_blank" rel="noopener noreferrer">#NaturalLanguageGeneration</a>
<a class="button" href="articles/Metrics.html" target="_blank" rel="noopener noreferrer">#Metrics</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<span class="issue_date">Issue Date: 2024-01-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1222" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] BLEU might be Guilty but References are not Innocent, Markus Freitag+, arXiv'20</a>
<span class="snippet"><span>GPT Summary</span>- æ©Ÿæ¢°ç¿»è¨³ã®è‡ªå‹•è©•ä¾¡æŒ‡æ¨™ã®è³ªãŒç–‘å•è¦–ã•ã‚Œã‚‹ä¸­ã€å‚ç…§ã®æ€§è³ªãŒè©•ä¾¡ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’ç ”ç©¶ã€‚ç•°ãªã‚‹å‚ç…§åé›†æ–¹æ³•ã‚’æ¯”è¼ƒã—ã€ç¿»è¨³ã®å¤šæ§˜æ€§ä¸è¶³ã«å¯¾æŠ—ã™ã‚‹ãŸã‚ã«è¨€èªå­¦è€…ã«ã‚ˆã‚‹ãƒ‘ãƒ©ãƒ•ãƒ¬ãƒ¼ã‚ºã‚¿ã‚¹ã‚¯ã‚’é–‹ç™ºã€‚ã“ã‚Œã«ã‚ˆã‚Šã€WMT 2019ã®è‹±ç‹¬ç¿»è¨³ã‚„ãƒãƒƒã‚¯ãƒˆãƒ©ãƒ³ã‚¹ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§äººé–“ã®è©•ä¾¡ã¨ã®ç›¸é–¢ãŒå‘ä¸Šã€‚å¤šå‚ç…§BLEUã®é™ç•Œã‚’æŒ‡æ‘˜ã—ã€ã‚ˆã‚ŠåŠ¹æœçš„ãªè©•ä¾¡æ–¹æ³•ã‚’ææ¡ˆã€‚</span>
<span class="snippet"><span>Comment</span><p>surface levelã®NLGã®æ€§èƒ½æŒ‡æ¨™ãŒsemanticã‚’è©•ä¾¡ã§ããªã„ã“ã¨ã‚’ç¤ºã—ãŸç ”ç©¶</p></span><br><br>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<span class="issue_date">Issue Date: 2024-10-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1446" target="_blank" rel="noopener noreferrer" class="title-link">What Does BERT Learn about the Structure of Language?, Jawahar+, ACL'19</a>
<span class="snippet"><span>GPT Summary</span>- BERTã¯è¨€èªç†è§£ã«ãŠã„ã¦å„ªã‚ŒãŸæˆæœã‚’ä¸Šã’ã¦ãŠã‚Šã€æœ¬ç ”ç©¶ã§ã¯ãã®è¨€èªæ§‹é€ ã®è¦ç´ ã‚’è§£æ˜ã™ã‚‹å®Ÿé¨“ã‚’è¡Œã£ãŸã€‚ä¸»ãªç™ºè¦‹ã¯ã€ãƒ•ãƒ¬ãƒ¼ã‚ºè¡¨ç¾ãŒãƒ•ãƒ¬ãƒ¼ã‚ºãƒ¬ãƒ™ãƒ«ã®æƒ…å ±ã‚’æ‰ãˆã€ä¸­é–“å±¤ãŒæ§‹æ–‡çš„ãŠã‚ˆã³æ„å‘³çš„ç‰¹å¾´ã®éšå±¤ã‚’å½¢æˆã—ã€é•·æœŸä¾å­˜æ€§ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã«æ·±ã„å±¤ãŒå¿…è¦ã§ã‚ã‚‹ã“ã¨ã€ã•ã‚‰ã«BERTã®æ§‹æˆãŒå¤å…¸çš„ãªæœ¨æ§‹é€ ã«é¡ä¼¼ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚</span>
<span class="snippet"><span>Comment</span><p><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1370" target="_blank" rel="noopener noreferrer">å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ« (LLM) ã®æŠ€è¡“ã¨æœ€æ–°å‹•å‘, Ikuya Yamada, 2024.06</a>
 ä¸­ã§å¼•ç”¨ã•ã‚Œã¦ã„ã‚‹ã€‚Transformerã®å„ãƒ–ãƒ­ãƒƒã‚¯ãŒã€ä½•ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã‹ã‚’åˆ†æã€‚</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/AAAI.html" target="_blank" rel="noopener noreferrer">#AAAI</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Reproducibility.html" target="_blank" rel="noopener noreferrer">#Reproducibility</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-10-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3368" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Deep Reinforcement Learning that Matters, Peter Henderson+, AAAI'18, 2017.09</a>
<span class="snippet"><span>GPT Summary</span>- æ·±å±¤å¼·åŒ–å­¦ç¿’ï¼ˆRLï¼‰ã®é€²å±•ã‚’æŒç¶šã•ã›ã‚‹ãŸã‚ã«ã¯ã€æ—¢å­˜ç ”ç©¶ã®å†ç¾æ€§ã¨æ–°æ‰‹æ³•ã®æ”¹å–„ã‚’æ­£ç¢ºã«è©•ä¾¡ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã‚ã‚‹ã€‚ã—ã‹ã—ã€éæ±ºå®šæ€§ã‚„æ‰‹æ³•ã®ã°ã‚‰ã¤ãã«ã‚ˆã‚Šã€çµæœã®è§£é‡ˆãŒé›£ã—ããªã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚æœ¬è«–æ–‡ã§ã¯ã€å†ç¾æ€§ã‚„å®Ÿé¨“å ±å‘Šã®èª²é¡Œã‚’èª¿æŸ»ã—ã€ä¸€èˆ¬çš„ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒã«ãŠã‘ã‚‹æŒ‡æ¨™ã®ã°ã‚‰ã¤ãã‚’ç¤ºã™ã€‚ã•ã‚‰ã«ã€æ·±å±¤RLã®çµæœã‚’å†ç¾å¯èƒ½ã«ã™ã‚‹ãŸã‚ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’ææ¡ˆã—ã€ç„¡é§„ãªåŠªåŠ›ã‚’æœ€å°é™ã«æŠ‘ãˆã‚‹ã“ã¨ã§åˆ†é‡ã®é€²å±•ã‚’ä¿ƒé€²ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã™ã€‚</span>
<span class="snippet"><span>Comment</span><p>æ—¥æœ¬èªè§£èª¬: 


<a href="https://www.slideshare.net/slideshow/dldeep-reinforcement-learning-that-matters-83905622/83905622" target="_blank" rel="noopener noreferrer">https://www.slideshare.net/slideshow/dldeep-reinforcement-learning-that-matters-83905622/83905622</a>


</p>
<p>å†ç¾æ€§ã¨ã„ã†è¦³ç‚¹ã¨ã¯å°‘ã—ç•°ãªã‚‹ã®ã‹ã‚‚ã—ã‚Œãªã„ãŒã€æœ€è¿‘ã®RLã«ã‚ˆã‚‹post-trainingã«ã¤ã„ã¦ã¯ã€ä»¥ä¸‹ã®ç ”ç©¶ã§Scaling LawsãŒå°å…¥ã•ã‚Œã¦ã„ã‚‹ã€‚<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3282" target="_blank" rel="noopener noreferrer">[Paper Note] The Art of Scaling Reinforcement Learning Compute for LLMs, Devvrit Khatri+, arXiv'25, 2025.10</a>
<br><br>ãŒã€çµå±€ç¾åœ¨ã‚‚å¤šãã®RLæ‰‹æ³•ãŒæ—¥å¤œå‡ºã¦ãã¦ãŠã‚Šã€å†ç¾æ€§ã«é–¢ã—ã¦ã¯åŒã˜ã‚ˆã†ãªçŠ¶æ³ã«é™¥ã£ã¦ã„ãã†ã§ã‚ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/Batch.html" target="_blank" rel="noopener noreferrer">#Batch</a>
<span class="issue_date">Issue Date: 2025-07-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2196" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Revisiting Small Batch Training for Deep Neural Networks, Dominic Masters+, arXiv'18</a>
<span class="snippet"><span>GPT Summary</span>- ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒæ·±å±¤ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ€§èƒ½ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’å®Ÿé¨“çš„ã«æ¯”è¼ƒã€‚å¤§ããªãƒŸãƒ‹ãƒãƒƒãƒã¯è¨ˆç®—ã®ä¸¦åˆ—æ€§ã‚’å‘ä¸Šã•ã›ã‚‹ãŒã€å°ã•ãªãƒŸãƒ‹ãƒãƒƒãƒã¯ä¸€èˆ¬åŒ–æ€§èƒ½ã‚’é«˜ã‚ã€å®‰å®šã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿç¾ã€‚æœ€è‰¯ã®æ€§èƒ½ã¯ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚º$m = 2$ã‹ã‚‰$m = 32$ã®ç¯„å›²ã§å¾—ã‚‰ã‚Œã€æ•°åƒã®ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¨å¥¨ã™ã‚‹ç ”ç©¶ã¨ã¯å¯¾ç…§çš„ã€‚</span>
<span class="snippet"><span>Comment</span><p>{Res, Reduced Alex}Netã«ãŠã„ã¦ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤§ããã™ã‚‹ã¨ã€å­¦ç¿’ãŒå®‰å®šã—ã‹ã¤é«˜ã„äºˆæ¸¬æ€§èƒ½ã‚’ç²å¾—ã§ãã‚‹å­¦ç¿’ç‡ã®rangeãŒå°ã•ããªã‚‹ã€‚ä¸€æ–¹ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå°ã•ã„ã¨æœ‰åŠ¹ãªå­¦ç¿’ç‡ã®rangeãŒåºƒã„ã€‚ã¾ãŸã€ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå°ã•ã„å ´åˆã¯ã€å‹¾é…è¨ˆç®—ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆãŒã‚ˆã‚Šé »ç¹ã«è¡Œã‚ã‚Œã‚‹ã€‚ã“ã®ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŒã‚ˆã‚Šé€²ã‚“ã çŠ¶æ…‹ã§å€‹ã€…ã®ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å‹¾é…è¨ˆç®—ãŒè¡Œã‚ã‚Œã‚‹ãŸã‚ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå¤§ãã„å ´åˆã¨æ¯”ã¹ã‚‹ã¨ãƒ¢ãƒ‡ãƒ«ãŒã‚ˆã‚Šæ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹ã§å„ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å‹¾é…ãŒè¨ˆç®—ã•ã‚Œã‚‹ã“ã¨ã«ãªã‚‹ãŸã‚ã€å­¦ç¿’ãŒå®‰å®šã—è‰¯ã„æ±åŒ–æ€§èƒ½ã«ã¤ãªãŒã‚‹ã€ã¨ã„ã£ãŸè©±ã®æ¨¡æ§˜ã€‚<br><br><img src="https://github.com/user-attachments/assets/f02f9016-6e9f-476d-a4c1-4f64bd51e9d5" alt="image" loading="lazy"></p></span><br><br>
<a class="button" href="articles/NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="articles/Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Word.html" target="_blank" rel="noopener noreferrer">#Word</a>
<a class="button" href="articles/ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<span class="issue_date">Issue Date: 2017-12-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/79" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Skip-Gram â€“ Zipf + Uniform = Vector Additivity, Gittens+, ACL'17</a>
<span class="snippet"><span>Comment</span><p>è§£èª¬ã‚¹ãƒ©ã‚¤ãƒ‰ï¼š


<a href="http://www.lr.pi.titech.ac.jp/~haseshun/acl2017suzukake/slides/09.pdf" target="_blank" rel="noopener noreferrer">http://www.lr.pi.titech.ac.jp/~haseshun/acl2017suzukake/slides/09.pdf</a>


</p>
<p>Embeddingã®åŠ æ³•æ§‹æˆæ€§ï¼ˆe.g. man+royal=kingï¼‰ã‚’ç†è«–çš„ã«ç†ç”±ã¥ã‘<br><br>ï¼ˆè§£èª¬ã‚¹ãƒ©ã‚¤ãƒ‰ã‚ˆã‚Šï¼‰</p></span><br><br>
<a class="button" href="articles/PersonalizedDocumentSummarization.html" target="_blank" rel="noopener noreferrer">#PersonalizedDocumentSummarization</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/6" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Aspect-Based Personalized Text Summarization, Berkovsky+ï¼ˆTimå…ˆç”Ÿã®ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰, AH'2008, 2008.07</a>
<span class="snippet"><span>Comment</span><p><img src="https://user-images.githubusercontent.com/12249301/34401031-b72623e0-ebda-11e7-9da2-6ce16b630f47.png" alt="image" loading="lazy"><br><br><br><br>Aspect-basedãªPDSã«é–¢ã—ã¦èª¿æŸ»ã—ãŸç ”ç©¶ã€‚<br><br>ãŸã¨ãˆã°ã€Wikipediaã®ã‚¯ã‚¸ãƒ©ã«é–¢ã™ã‚‹ãƒšãƒ¼ã‚¸ã§ã¯ã€biological taxonomy, physical dimensions, popular cultureã®ã‚ˆã†ã«ã€æ§˜ã€…ãªã‚¢ã‚¹ãƒšã‚¯ãƒˆã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãŒè¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¢ãƒ‡ãƒ«ã¯å„ã‚¢ã‚¹ãƒšã‚¯ãƒˆã«å¯¾ã™ã‚‹å—œå¥½ã®åº¦åˆã„ã§è¡¨ã•ã‚Œã€ãã‚Œã«å¾“ã„ç”Ÿæˆã•ã‚Œã‚‹è¦ç´„ã«å«ã¾ã‚Œã‚‹å„ç¨®ã‚¢ã‚¹ãƒšã‚¯ãƒˆã«é–¢ã™ã‚‹æƒ…å ±ã®é‡ãŒå¤‰åŒ–ã™ã‚‹ã€‚<br><br><br><br>UserStudyã®çµæœã€ã‚¢ã‚¹ãƒšã‚¯ãƒˆãƒ™ãƒ¼ã‚¹ãªãƒ¦ãƒ¼ã‚¶ãƒ¢ãƒ‡ãƒ«ã¨ã‚ˆã‚Šfitã—ãŸã€æ“¬ä¼¼çš„ãªãƒ¦ãƒ¼ã‚¶ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ã®æ–¹ãŒã€ãƒ¦ãƒ¼ã‚¶ã®è¦ç´„ã«å¯¾ã™ã‚‹ratingãŒä¸Šæ˜‡ã—ã¦ã„ãã“ã¨ã‚’ç¤ºã—ãŸã€‚<br><br><br><br>ã¾ãŸã€è¦ç´„ã®åœ§ç¸®ç‡ã«å¿œã˜ã¦ã€ãƒ¦ãƒ¼ã‚¶ã®ratingãŒå¤‰åŒ–ã—ã€originalã®é•·ã•ï¼é•·ã‚ã®è¦ç´„ï¼çŸ­ã„è¦ç´„ã®é †ã«ratingãŒæœ‰æ„ã«é«˜ã‹ã£ãŸã€‚è¦ç´„ãŒé•·ã™ãã¦ã‚‚ã€ã‚ã‚‹ã„ã¯çŸ­ã™ãã¦ã‚‚ã‚ã¾ã‚Šè‰¯ã„è©•ä¾¡ã¯å¾—ã‚‰ã‚Œãªã„ï¼ˆã—ã‹ã—ãªãŒã‚‰ã€é•·ã™ãã‚‹è¦ç´„ã¯å®Ÿã¯ãã“ã¾ã§å«Œã„ã§ã¯ãªã„ã“ã¨ã‚’ratingã¯ç¤ºå”†ã—ã¦ã„ã‚‹ï¼‰ã€‚<br><br><br><br>Genericãªè¦ç´„ã¨Personalizedãªè¦ç´„ã®faitufulnessã‚’ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã—ã¦ã‚‚ã‚‰ã£ãŸçµæœã€Genericãªè¦ç´„ã®æ–¹ãŒè‹¥å¹²é«˜ã„ã‚¹ã‚³ã‚¢ã«ã€‚ã—ã‹ã—ãªãŒã‚‰æœ‰æ„å·®ã¯ãªã„ã€‚å®Ÿéš›ã€å¹³å‡ã—ã¦83%ã®sentenceã¯Genericã¨Personalizedã§overlapã—ã¦ã„ã‚‹ã€‚faitufulnessã®è¦³ç‚¹ã‹ã‚‰ã€Genericã¨Personalizedãªè¦ç´„ã®é–“ã«æœ‰æ„å·®ã¯ãªã„ã“ã¨ã‚’ç¤ºã—ãŸã€‚<br><br><br><br>museumç­‰ã§å¿œç”¨ã™ã‚‹ã“ã¨ã‚’æ¤œè¨</p></span><br><br>
<a class="button" href="articles/Comments.html" target="_blank" rel="noopener noreferrer">#Comments</a>
<a class="button" href="articles/InformationRetrieval.html" target="_blank" rel="noopener noreferrer">#InformationRetrieval</a>
<a class="button" href="articles/WWW.html" target="_blank" rel="noopener noreferrer">#WWW</a>
<span class="issue_date">Issue Date: 2018-01-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/230" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Leave a Reply: An Analysis of Weblog Comments, Mishne+, WWW'06</a>
<span class="snippet"><span>Comment</span><p>å¾“æ¥ã®Weblogç ”ç©¶ã§ã¯ã€ã‚³ãƒ¡ãƒ³ãƒˆã®æƒ…å ±ãŒç„¡è¦–ã•ã‚Œã¦ã„ãŸãŒã€ã‚³ãƒ¡ãƒ³ãƒˆã‚‚é‡è¦ãªæƒ…å ±ã‚’å«ã‚“ã§ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹ã€‚<br><br>ã“ã®ç ”ç©¶ã§ã¯ã€ä»¥ä¸‹ã®ã“ã¨ãŒè¨€åŠã•ã‚Œã¦ã„ã‚‹ã€‚<br><br><br><br>* ï¼ˆåé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã®ï¼‰ãƒ–ãƒ­ã‚°ã«ã‚³ãƒ¡ãƒ³ãƒˆãŒä»˜ä¸ã•ã‚Œã¦ã„ã‚‹å‰²åˆã‚„ã‚³ãƒ¡ãƒ³ãƒˆã®é•·ã•ã€ãƒã‚¹ãƒˆã«å¯¾ã™ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆã®å¹³å‡ãªã©ã®çµ±è¨ˆé‡<br><br>* ãƒ–ãƒ­ã‚°æ¤œç´¢ã«ãŠã‘ã‚‹ã‚³ãƒ¡ãƒ³ãƒˆæ´»ç”¨ã®æœ‰åŠ¹æ€§ï¼ˆä¸€éƒ¨ã®ã‚¯ã‚¨ãƒªã§Recallã®å‘ä¸Šã«å¯„ä¸ã€Precisionã¯å¤‰åŒ–ãªã—ï¼‰ã€‚è¨˜äº‹å˜ä½“ã‚’ç”¨ã„ã‚‹ã®ã¨ã¯ç•°ãªã‚‹è¦³ç‚¹ã‹ã‚‰ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãŒä½œã‚Œã‚‹ã€‚<br><br>* ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã¨PVæ•°ã€incoming linkæ•°ã®é–¢ä¿‚æ€§ãªã©<br><br>* ã‚³ãƒ¡ãƒ³ãƒˆæ•°ã¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã®é–¢ä¿‚æ€§ãªã©<br><br>* ã‚³ãƒ¡ãƒ³ãƒˆã«ãŠã‘ã‚‹è­°è«–ã®åŒå®šãªã©</p>
<p>ç›¸å½“æµã—èª­ã¿ãªã®ã§ã€èª­ã¿é•ãˆã¦ã„ã‚‹ã¨ã“ã‚ã‚„ã€é‡è¦ãªç®‡æ‰€ã®èª­ã¿è½ã¨ã—ç­‰ã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€‚</p></span><br><br>
<a class="button" href="articles/RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="articles/Others.html" target="_blank" rel="noopener noreferrer">#Others</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/183" target="_blank" rel="noopener noreferrer" class="title-link">[Paper Note] Usage patterns of collaborative tagging systems, Golder+, Journal of Information Science'06</a>
<span class="snippet"><span>Comment</span><p>Social Tagging Systemã®ä»•çµ„ã¿ã‚„ä½¿ã‚ã‚Œæ–¹ã«ã¤ã„ã¦è¨€åŠã™ã‚‹éš›ã«referã™ã‚‹ã¨è‰¯ã„ã‹ã‚‚ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<span class="issue_date">Issue Date: 2025-11-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3544" target="_blank" rel="noopener noreferrer" class="title-link">Open-weight models lag state-of-the-art by around 3 months on average, EPOCH AI, 2025.10</a>
<span class="snippet"><span>Comment</span><p>ã‚¿ã‚¤ãƒˆãƒ«ã®é€šã‚Šãªæ¨¡æ§˜</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/kimmonismus/status/1984199668944023612?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-10-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3528" target="_blank" rel="noopener noreferrer" class="title-link">Emergent Introspective Awareness in Large Language Models, Jack Lindsey, Anthropic, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1984031672883671315?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å…¬å¼ãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/anthropicai/status/1983584136972677319?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Geometric.html" target="_blank" rel="noopener noreferrer">#Geometric</a>
<span class="issue_date">Issue Date: 2025-10-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3378" target="_blank" rel="noopener noreferrer" class="title-link">When Models Manipulate Manifolds: The Geometry of a Counting Task, Gurnee+, Anthropic, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wesg52/status/1980680563582538099?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<a class="button" href="articles/Mathematics.html" target="_blank" rel="noopener noreferrer">#Mathematics</a>
<a class="button" href="articles/Scaling%20Laws.html" target="_blank" rel="noopener noreferrer">#Scaling Laws</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/reading.html" target="_blank" rel="noopener noreferrer">#reading</a>
<a class="button" href="articles/One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-10-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3220" target="_blank" rel="noopener noreferrer" class="title-link">RL Scaling Laws for Mathematical Reasoning, Joan Cabezas, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/josancamon19/status/1976693692590440526?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>Qwen3ã‚’GSM8Kã§RL Finetuningã—ãŸã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå°ã•ã„ãƒ¢ãƒ‡ãƒ«ã¯å¤§ããªgainã‚’å¾—ãŸãŒã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå¤§ãã„ãƒ¢ãƒ‡ãƒ«ã¯ãã‚Œã»ã©ã§ã‚‚ãªã‹ã£ãŸã®ã§ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå¤§ãã„ã»ã©ã‚¹ã‚±ãƒ¼ãƒ«ã™ã‚‹ã‚ã‘ã§ã¯ãªãï¼ˆã‚€ã—ã‚æ©æµãŒå°ã•ããªã‚‹ï¼‰ã€ã‹ã¤å ±é…¬ã‚’strictã«ã™ã‚‹ã¨Qwenã¯æŒ‡ç¤ºè¿½å¾“èƒ½åŠ›ãŒãªã„ã“ã¨ã§å­¦ç¿’ãŒå…¨ç„¶é€²ã¾ãªã‹ã£ãŸï¼ˆæŸ”è»Ÿãªã‚‚ã®ã«ã—ãŸã‚‰ãã†ã§ã¯ãªã‹ã£ãŸã®ã§é©åˆ‡ãªå ±é…¬ãŒé‡è¦ï¼‰ã€GSM8Kã§RL Finetuninpã—ãŸãƒ¢ãƒ‡ãƒ«ã®reasoningã¯MMLUã«è»¢ç§»ã—ãªã‹ã£ãŸã®ã§ã€RL Finetuningã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ä¸ãˆãŸãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã¦ã„ã‚‹ã ã‘ãªã®ã§ã¯ãªã„ã‹ã€ã¿ãŸã„ãªè©±ãŒãƒã‚¹ãƒˆã«è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ã€‚</p>
<p>AI2ã®Researcherã‹ã‚‰ã®æ‰€è¦‹:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/natolambert/status/1976817173302829261?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>å…ƒã®è©±ã¨ã“ã®è¾ºã‚’ã—ã£ã‹ã‚Šèª­ã¿è§£ã„ãŸã‚‰ã¨ã¦ã‚‚å‹‰å¼·ã«ãªã‚Šãã†ãªäºˆæ„ŸğŸ‘€<p>Scaling Lawsç³»ã®ç ”ç©¶:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1827" target="_blank" rel="noopener noreferrer">Training Compute-Optimal Large Language Models, Jordan Hoffmann+, NeurIPS'22</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1828" target="_blank" rel="noopener noreferrer">Scaling Laws for Neural Language Models, Jared Kaplan+, arXiv'20</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1829" target="_blank" rel="noopener noreferrer">Scaling Data-Constrained Language Models, Niklas Muennighoff+, NeurIPS'23</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2002" target="_blank" rel="noopener noreferrer">Scaling Laws for Autoregressive Generative Modeling, Tom Henighan+, arXiv'20</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2745" target="_blank" rel="noopener noreferrer">Scaling Laws for Value-Based RL, Fu+, 2025.09</a>
 (RLé–¢é€£)<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3157" target="_blank" rel="noopener noreferrer">[Paper Note] Bayesian scaling laws for in-context learning, Aryaman Arora+, COLM'25, 2024.10</a>
 (ICLé–¢é€£)<br><br>ç”»åƒã¨ã‹Data Mixture, MoEãªã©ä»–ã«ã‚‚è‰²ã€…ã‚ã‚‹ãŒã€ä¸€æ—¦ä¸Šè¨˜ã‚‰ã¸ã‚“ã¨å…ƒãƒã‚¹ãƒˆãƒ»AI2ã‹ã‚‰ã®æ‰€è¦‹ã‚’èª­ã¿è§£ã„ãŸã‚‰ã©ã†ã„ã£ãŸã‚‚ã®ãŒè¦‹ãˆã¦ãã‚‹ã ã‚ã†ã‹ï¼Ÿï¼ˆå…¨éƒ¨èª­ã‚“ã§ã˜ã£ãã‚Šè€ƒãˆãŸã„ã‘ã©æ™‚é–“ãŒç„¡ã„ã®ã§...ï¼‰ä¸€æ—¦GPTã«ãã„ã¦ã¿ã‚ˆã†</p>
<p>GPTã«ãã„ã¦ã¿ãŸï¼ˆç§ã¯ç„¡èª²é‡‘å‹¢ã ãŒthinking timeãŒæŒŸã¾ã‚ŒãŸã®ã¨ãƒ‡ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°é€Ÿåº¦ã®é©åº¦ãªé…ã•ã¨ã€limitã«åˆ°é”ã—ã¾ã—ãŸã¨ã„ã†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒãªã‹ã£ãŸã“ã¨ã‹ã‚‰é‘‘ã¿ã‚‹ã«ã€ä»¥ä¸‹ã¯GPT-5ã«ã‚ˆã£ã¦å›ç­”ã•ã‚Œã¦ã„ã‚‹ã¨è€ƒãˆã‚‰ã‚Œã‚‹ï¼‰<br>


<a href="https://chatgpt.com/share/68ec5024-83fc-8006-b8c6-14060191fb91" target="_blank" rel="noopener noreferrer">https://chatgpt.com/share/68ec5024-83fc-8006-b8c6-14060191fb91</a>


</p>
<p>RLã®Scaling Lawsã«é–¢ã™ã‚‹ç ”ç©¶ãŒã§ã¾ã—ãŸ:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3282" target="_blank" rel="noopener noreferrer">[Paper Note] The Art of Scaling Reinforcement Learning Compute for LLMs, Devvrit Khatri+, arXiv'25, 2025.10</a>
</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2025-10-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3113" target="_blank" rel="noopener noreferrer" class="title-link">Diffusion Language Models are Super Data Learners, Ni+, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/tianyupang1/status/1974118238415172107?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-10-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3074" target="_blank" rel="noopener noreferrer" class="title-link">Information Bandwidth in Reinforcement Learning Understanding Sample Efficiency Through Signal Density, Yingru Li, 2025.10</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/richardyrli/status/1973791371036405855?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ChatGPT.html" target="_blank" rel="noopener noreferrer">#ChatGPT</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-09-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3019" target="_blank" rel="noopener noreferrer" class="title-link">Why GPT-5 used less training compute than GPT-4.5 ï¼ˆbut GPT-6 probably wonâ€™tï¼‰, EPOCH AI, 2025.09</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1972421341988225340?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="articles/Stability.html" target="_blank" rel="noopener noreferrer">#Stability</a>
<a class="button" href="articles/train-inference-gap.html" target="_blank" rel="noopener noreferrer">#train-inference-gap</a>
<span class="issue_date">Issue Date: 2025-09-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3004" target="_blank" rel="noopener noreferrer" class="title-link">When Speed Kills Stability: Demystifying RL Collapse from the Training-Inference Mismatch, Liu+, 2025.09</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/richardyrli/status/1971560544974086263?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>è¨“ç·´æ™‚ã®ã‚¨ãƒ³ã‚¸ãƒ³(fsdpç­‰)ã¨ãƒ­ãƒ¼ãƒ«ã‚¢ã‚¦ãƒˆæ™‚ã®ã‚¨ãƒ³ã‚¸ãƒ³(vLLMç­‰)ãŒã€OOVãªãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã—ã¦ï¼ˆç‰¹ã«tooluseã—ãŸå ´åˆã«ç”Ÿã˜ã‚„ã™ã„ï¼‰è‘—ã—ãç•°ãªã‚‹å°¤åº¦ã‚’å‰²ã‚Šå½“ã¦ã‚‹ãŸã‚å­¦ç¿’ãŒå´©å£Šã—ã€ãã‚Œã¯åˆ©ç”¨ã™ã‚‹GPUã«ã‚ˆã£ã¦ã‚‚å®‰å®šæ€§ãŒå¤‰åŒ–ã—ï¼ˆA100ã‚ˆã‚Šã‚‚L20, L20ã‚ˆã‚Šã‚‚H20)ã€tokenãƒ¬ãƒ™ãƒ«ã®Importtance Weightingã§ã¯é›£ã—ãã€Sequenceãƒ¬ãƒ™ãƒ«ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãŒå¿…è¦ã€ã¿ãŸã„ãªè©±ãªæ¨¡æ§˜ã€‚</p>
<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2552" target="_blank" rel="noopener noreferrer">Your Efficient RL Framework Secretly Brings You Off-Policy RL Training, Yao+, 2025.08</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2299" target="_blank" rel="noopener noreferrer">[Paper Note] Group Sequence Policy Optimization, Chujie Zheng+, arXiv'25</a>
</p>
<p>FP16ã«ã™ã‚‹ã¨train-inferenae gapãŒéå¸¸ã«å°ã•ããªã‚‹ã¨ã„ã†å ±å‘Š:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3532" target="_blank" rel="noopener noreferrer">[Paper Note] Defeating the Training-Inference Mismatch via FP16, Penghui Qi+, arXiv'25, 2025.10</a>
</p>
<p>A100ã§vLLMã‚’ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã«ã—ãŸæ™‚ã®disable_cascade_attnã®è¨­å®šå€¤ã«ã‚ˆã‚‹æŒ™å‹•ã®é•ã„:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/giffmana/status/1984968679008633049?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>ãã‚‚ãã‚‚FlashAttnention-2 kernelã«ãƒã‚°ãŒã‚ã‚Šã€A100/L20ã§ç‰¹å®šã®ã‚«ãƒ¼ãƒãƒ«ãŒå‘¼ã°ã‚Œã‚‹ã¨ãƒŸã‚¹ãƒãƒƒãƒãŒèµ·ãã‚‹ã®ã ã¨ã‹ã€‚vLLM Flashattentionãƒªãƒã‚¸ãƒˆãƒªã®issue 87ã«ã‚ˆã£ã¦è§£æ±ºæ¸ˆã¿ã€‚~~å…·ä½“çš„ã«ã©ã®ã‚«ãƒ¼ãƒãƒ«å®Ÿè£…ãªã®ã ã‚ã†ã‹ã€‚~~ã€€ï¼ˆvLLM Flashattentionãƒªãƒã‚¸ãƒˆãƒªã ã£ãŸæ¨¡æ§˜ï¼‰<br>


<a href="https://github.com/vllm-project/flash-attention" target="_blank" rel="noopener noreferrer">https://github.com/vllm-project/flash-attention</a>


<br><br>disable_cascade_attnã®è¨­å®šå€¤ã‚’ä½•å›ã‚‚å¤‰ãˆãŸã‘ã©ã†ã¾ãã„ã‹ãªã„ã‚ˆã¨ã„ã†è©±ãŒã‚ã‚‹:<br>



<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/qphutu/status/1984911433952592089?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2025-09-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2994" target="_blank" rel="noopener noreferrer" class="title-link">æ§˜ã€…ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé•·ã«ãŠã‘ã‚‹ LLM ã® Self-Attention ã® Query ã¨ Key ã®åˆ†æ, ABEJA Tech Blog, 2025.09</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/abeja_tech/status/1971073813279621253?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ä»¥ä¸‹ã®ç ”ç©¶ã‚’å‚è€ƒã«åˆ†æã—ã¦ã„ã‚‹:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2995" target="_blank" rel="noopener noreferrer">[Paper Note] Massive Values in Self-Attention Modules are the Key to Contextual   Knowledge Understanding, Mingyu Jin+, ICML'25, 2025.02</a>
</p>
<p>RoPEã¯ä»¥ä¸‹:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1310" target="_blank" rel="noopener noreferrer">RoFormer: Enhanced Transformer with Rotary Position Embedding, Jianlin Su+, N/A, Neurocomputing, 2024</a>
</p>
<p>Massive Valueã¯transformerã®Q,Kã®æ´»æ€§å€¤ã«ç¾ã‚Œã‚‹æ¥µç«¯ã«å¤§ããªå€¤ã®ã“ã¨ã§ã€Massive Valueã¯æ–‡è„ˆçš„ãªçŸ¥è­˜ã®ç†è§£ã«ãŠã„ã¦é‡è¦ã¨ã®ã“ã¨ï¼ˆMassive Valueã‚’ç ´å£Šã™ã‚‹ã¨æ–‡è„ˆç†è§£ãŒé‡è¦ãªã‚¿ã‚¹ã‚¯ã®ã‚¹ã‚³ã‚¢ã¯è‘—ã—ãä½ä¸‹ã—ãŸãŒã€ãƒ‘ãƒ©ãƒ¡ãƒˆãƒªãƒƒã‚¯ãªçŸ¥è­˜ãŒé‡è¦ãªã‚¿ã‚¹ã‚¯ã¯æ€§èƒ½ãŒå°‘ã—ä½ä¸‹ã™ã‚‹ã®ã¿ã€ã‹ã¤éMassive Valueã‚’ç ´å£Šã—ã¦ã‚‚å¤§ããªå¤‰åŒ–ã¯ç„¡ã‹ã£ãŸãŸã‚ï¼‰ã€‚ã¾ãŸMassive Valueã¯RoPEã‚’ä½¿ã£ãŸãƒ¢ãƒ‡ãƒ«ã®ã¿Q, Kã®ç‰¹å®šã®æ¬¡å…ƒã«ã®ã¿é›†ä¸­ã—ã¦å‡ºç¾ã™ã‚‹ã€‚ã“ã‚Œã¯RoPEã§ã¯å›è»¢è¡Œåˆ—ã‚’Q, Kã«ã®ã¿é©ç”¨ã—ã¦ã„ã‚‹ã“ã¨ã«èµ·å› ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŒã€å›è»¢è¡Œåˆ—ã®ç©ã®å‰å¾Œã§ã‚‚Massive ValueãŒå‡ºç¾ã™ã‚‹ã“ã¨ã¯å¤‰ã‚ã‚‰ãªã„ã“ã¨ã‹ã‚‰ã€å›è»¢è¡Œåˆ—ãã®ã‚‚ã®ã«èµ·å› ã™ã‚‹ã‚‚ã®ã¨ã„ã†ã‚ˆã‚Šã€å›è»¢è¡Œåˆ—ãŒã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«çµ„ã¿è¾¼ã¾ã‚Œã‚‹ã“ã¨ã§çµæœçš„ã«å­¦ç¿’ã•ã‚Œã‚‹ã‚‚ã®ãªã®ã§ã¯ãªã„ã‹ã€ã¨ã„ã†æ„Ÿã˜ã‚‰ã—ã„ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-09-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2788" target="_blank" rel="noopener noreferrer" class="title-link">ç”»åƒãƒ¢ãƒ‡ãƒ«ã®ãƒãƒƒã‚¯ãƒœãƒ¼ãƒ³ã¨ã—ã¦æœ€åˆã«ä½•ã‚’é¸ã¶ã¹ãã‹ï¼Ÿ, ã¡ãã‚ã¶, 2025.09</a>
<span class="snippet"><span>Comment</span><p>ã“ã¡ã‚‰ã®è«–æ–‡ã‚’å‚è€ƒã«ã—ã¦ã„ã‚‹:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2789" target="_blank" rel="noopener noreferrer">[Paper Note] Battle of the Backbones: A Large-Scale Comparison of Pretrained Models   across Computer Vision Tasks, Micah Goldblum+, NeurIPS'23</a>
</p>
<p>Backboneé¸å®šã®éš›ã¯å‚ç…§ã®ã“ã¨ã€‚2024å¹´ä»¥å¾Œã®ãƒ¢ãƒ‡ãƒ«ã¯å«ã¾ã‚Œã¦ã„ãªã„ç‚¹ã«æ³¨æ„ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/Composition.html" target="_blank" rel="noopener noreferrer">#Composition</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-09-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2712" target="_blank" rel="noopener noreferrer" class="title-link">From fï¼ˆxï¼‰ and gï¼ˆxï¼‰ to fï¼ˆgï¼ˆxï¼‰ï¼‰: LLMs Learn New Skills in RL by Composing Old Ones, Yuan+, 2025.09</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1964235195613143127?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã•ã‚ŒãŸå®Ÿé¨“ã«ãŠã„ã¦ã€æ·±ã•2ã®nestedãªcompostition g(f(x))ã®ãƒ‡ãƒ¼ã‚¿ã§RLã—ãŸå ´åˆã¯ã€ãƒ†ã‚¹ãƒˆæ™‚ã«æ·±ã•6ã¾ã§ã®compostitionã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸãŒï¼ˆï¼ãƒ¡ã‚¿ã‚¹ã‚­ãƒ«ã¨ã—ã¦compostitionã‚’ç²å¾—ã—ãŸï¼‰ã€æ·±ã•1ã®non-nestedãªãƒ‡ãƒ¼ã‚¿ã§RLã—ãŸå ´åˆã¯è¤‡é›‘ãªcompostitionãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯ã‚’è§£ã‘ãªã‹ã£ãŸã€‚ã¾ãŸã€ä¸€èˆ¬çš„ã«ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚‹ç¨‹åº¦è§£ã‘ã‚‹å•é¡Œã«å¯¾ã—ã¦RLã‚’é©ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®pass@1000ã¯ã‚ã¾ã‚Šå‘ä¸Šã—ãªã„ã“ã¨ã‹ã‚‰ã€RLã¯æ–°ã—ã„ã‚¹ã‚­ãƒ«ã‚’ä½•ã‚‚æ•™ãˆã¦ã„ãªã„ã®ã§ã¯ãªã„ã‹ã€ã¨ã„ã£ãŸè§£é‡ˆãŒã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚‹ãŒã€ã‚ˆã‚Šé«˜æ¬¡ã®compostitionãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯ã§è©•ä¾¡ã™ã‚‹ã¨æ˜ç¢ºã«æ€§èƒ½ãŒè‰¯ããªã‚‹ã®ã§ã€å®Ÿã¯ã‚ˆã‚Šé«˜æ¬¡ã®compostitionãŒå¿…è¦ãªã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹æ±åŒ–æ€§èƒ½ã‚’ä¼¸ã°ã—ã¦ã„ã‚‹ã€‚compostitionã§ã®èƒ½åŠ›ã‚’ç™ºæ®ã™ã‚‹ã«ã¯ã¾ãšå¹…åºƒã„atomicãªã‚¹ã‚­ãƒ«ãŒå¿…è¦ãªã®ã§ã€ã—ã£ã‹ã‚Šãã‚Œã‚’äº‹å‰å­¦ç¿’ã§èº«ã«ã¤ã‘ã•ã›ã€ãã®å¾Œpost-trainingã«ã‚ˆã£ã¦è§£æ±ºã—ãŸã„ã‚¿ã‚¹ã‚¯ã®ãŸã‚ã®atomic skillã®compostitionã®æ–¹æ³•ã‚’å­¦ç¿’ã•ã›ã‚‹ã¨åŠ¹æœçš„ãªã®ã§ã¯ãªã„ã‹ã€ã¨ã„ã£ãŸè©±ãªæ¨¡æ§˜ã€‚</p>
<p>ã“ã®è¾ºã®ICLã®è©±ã¨ä¼¼ã¦ã„ã‚‹<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1362" target="_blank" rel="noopener noreferrer">What Do Language Models Learn in Context? The Structured Task Hypothesis, Jiaoda Li+, N/A, ACL'24</a>
</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="articles/DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2025-09-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2692" target="_blank" rel="noopener noreferrer" class="title-link">Speed-Accuracy Relations for Diffusion Models: Wisdom from Nonequilibrium Thermodynamics and Optimal Transport, Ikeda+, Physical Review X, 2025</a>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-08-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2408" target="_blank" rel="noopener noreferrer" class="title-link">ProRL V2 - Prolonged Training Validates RL Scaling Laws, Hu+, 2025.08</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/shizhediao/status/1955066349514002902?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>é–¢é€£:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2011" target="_blank" rel="noopener noreferrer">[Paper Note] ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in  Large Language Models, Mingjie Liu+, NeurIPS'25</a>
</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/Tokenizer.html" target="_blank" rel="noopener noreferrer">#Tokenizer</a>
<a class="button" href="articles/Finetuning.html" target="_blank" rel="noopener noreferrer">#Finetuning</a>
<a class="button" href="articles/Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<span class="issue_date">Issue Date: 2025-08-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2335" target="_blank" rel="noopener noreferrer" class="title-link">æ—¥æœ¬èªModernBERTã®é–‹ç™º: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¨æ€§èƒ½ã®é–¢ä¿‚ç·¨ ï¼ˆ3_3ï¼‰, SBIntuitions, 2025.05</a>
<span class="snippet"><span>Comment</span><p>SBIntuitionsãŒå…¬é–‹ã—ã¦ã„ã‚‹äº‹å‰å­¦ç¿’æ¸ˆã¿ModernBertã¯4.4Tãƒˆãƒ¼ã‚¯ãƒ³ã®è¶…å¤§è¦æ¨¡ãªãƒˆãƒ¼ã‚¯ãƒ³ã§å­¦ç¿’ã•ã‚Œã¦ãŠã‚Šã€ãã‚Œã‚‰ã«ã¯å¤šæ§˜ãªè¡¨ç¾ãŒå‡ºç¾ã™ã‚‹ãŸã‚é€šå¸¸ã§ã¯å¤§å¹…ã«æ€§èƒ½ãŒåŠ£åŒ–ã—ã¦ã—ã¾ã†ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®äº‹å¾Œçš„ã«ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’å¤‰æ›ã—ã€å¤‰æ›å¾Œãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶â†’ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰åŒ–ã‚’å®Ÿæ–½ã—ãŸå ´åˆã«ã€downstreamã‚¿ã‚¹ã‚¯ã®æ€§èƒ½ãŒåŠ£åŒ–ã™ã‚‹ã‹ã‚’èª¿æŸ»ã€‚ãã®çµæœã€æ€§èƒ½ã®åŠ£åŒ–ãŒã»ã¨ã‚“ã©è¡¨å‡ºã—ãªã‹ã£ãŸï¼ˆç‰¹ã«ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒ310mã®å ´åˆã¯æ€§èƒ½ã®åŠ£åŒ–ã¯ã»ã¼ãªã•ãã†ï¼‰ã€‚ã¾ãŸã€MeCabï¼ˆUnidic)ã§ã‚ã‹ã¡æ›¸ãã‹ã‚Œã¦ã„ã‚‹å‰æã®å›ºæœ‰è¡¨ç¾èªè­˜ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã§ã®è©•ä¾¡ã®çµæœã€åŒæ§˜ã®æ¡ä»¶ã§ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã‚’ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚‚åŒç­‰ï¼‰ã¨ã€åŒç­‰ç¨‹åº¦ã®æ€§èƒ½ã‚’ç¤ºã—ãŸã€‚ã®ã§ã€SBIntuitionsãŒå…¬é–‹ã—ã¦ã„ã‚‹æ—¥æœ¬èªModernBERTã«ãŠã„ã¦ã¯ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’äº‹å¾Œçš„ã«å¤‰æ›ã—ãŸã®ã¡ã«ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰åŒ–ã‚’å®Ÿæ–½ã—ãƒ¢ãƒ‡ãƒ«ã®inputã¨ã™ã‚‹ã‚ˆã†ãªæ–¹æ³•ã‚’ã—ã¦ã‚‚ã€å•é¡Œãªã•ãã†ã€ã¨ã„ã†æ„Ÿã˜ãªæ¨¡æ§˜ã€‚èˆˆå‘³æ·±ã„ã€‚</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hpp_ricecake/status/1951256302908305685?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Mathematics.html" target="_blank" rel="noopener noreferrer">#Mathematics</a>
<a class="button" href="articles/SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="articles/RLVR.html" target="_blank" rel="noopener noreferrer">#RLVR</a>
<span class="issue_date">Issue Date: 2025-05-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1997" target="_blank" rel="noopener noreferrer" class="title-link">Spurious Rewards: Rethinking Training Signals in RLVR, Shao+, 2025.05</a>
<span class="snippet"><span>Comment</span><p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/stellalisy/status/1927392717593526780?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å‚è€ƒï¼ˆè€ƒå¯Ÿï¼‰: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/weiliu99/status/1930826904522875309?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<p>å‚è€ƒï¼ˆè€ƒå¯Ÿï¼‰:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/g_k_swamy/status/1945159211752562739?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


<br><br>ã“ã¡ã‚‰ã§ã‚‚Qwen2.5 MATH 7b ã‚’ç”¨ã„ã¦æ¤œè¨¼ã—ã¦ã„ã‚‹ãŒã€ã‚³ãƒ³ã‚¿ãƒŸãƒãƒ¼ã‚·ãƒ§ãƒ³ã®å•é¡ŒãŒä»®ã«æœ¬å½“ã ã¨ã—ãŸã‚‰ã€ã©ã†å½±éŸ¿ã™ã‚‹ã ã‚ã†ã‹ã€‚ã‚¹ãƒ¬ãƒƒãƒ‰ä¸­ã®ã‚°ãƒ©ãƒ•ã‚‚MATH500ï¼ˆQwen2.5ã«ãŠã„ã¦ã‚³ãƒ³ã‚¿ãƒŸã®å¯èƒ½æ€§ãŒã‚ã‚‹ï¼‰ã®æ€§èƒ½ã‚’ç¤ºã—ã¦ã„ã‚‹ã€‚</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="articles/AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2025-05-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1927" target="_blank" rel="noopener noreferrer" class="title-link">Agent Frameworkã¯ã©ã‚Œã‚’ä½¿ã†ã¹ãã‹ [ã‚¿ã‚¹ã‚¯æ€§èƒ½ç·¨], ã¯ã¡, 2025.05</a>
<span class="snippet"><span>Comment</span><p>å„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æ¯ã®æ€§èƒ½ã®é•ã„ã‚„æ¶ˆè²»ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³æ•°ã€å®Ÿè£…ã®å¾®å¦™ã‚„é•ã„ãŒã¾ã¨ã‚ã‚‰ã‚Œã¦ãŠã‚Šã€å¤ªå­—ã§takeawayãŒè¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ã®ã§éå¸¸ã«ã‚ã‹ã‚Šã‚„ã™ã„ã€‚</p>
<p>å…ƒãƒã‚¹ãƒˆ:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/curveweb/status/1919301208096866660?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loadingâ€¦</div>
</div>


</span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/Selected%20Papers/Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-03-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1834" target="_blank" rel="noopener noreferrer" class="title-link">è¨€èªãƒ¢ãƒ‡ãƒ«ã®ç‰©ç†å­¦, ä½è—¤ç«œé¦¬, 2025.03</a>
<span class="snippet"><span>Comment</span><p>å¿…èª­</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="articles/OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="articles/Slide.html" target="_blank" rel="noopener noreferrer">#Slide</a>
<a class="button" href="articles/Japanese.html" target="_blank" rel="noopener noreferrer">#Japanese</a>
<span class="issue_date">Issue Date: 2024-09-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1373" target="_blank" rel="noopener noreferrer" class="title-link">LLMã«æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‚’å­¦ç¿’ã•ã›ã‚‹æ„ç¾©, Koshiro Saito+, ç¬¬261å›è‡ªç„¶è¨€èªå‡¦ç†ç ”ç©¶ç™ºè¡¨ä¼š, 2024.08</a>
<span class="snippet"><span>Comment</span><p>è‹±æ—¥ç¿»è¨³ã‚„æ—¥æœ¬ç‰¹æœ‰ã®çŸ¥è­˜ã‚’å•ã‚ã‚Œã‚‹ã‚ˆã†ãªQAã«ãŠã„ã¦ã€æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹å­¦ç¿’ã®åŠ¹æœãŒã‚ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚Œã¦ã„ã‚‹æ¨¡æ§˜ã€‚<br>ãŸã¨ãˆã°ã€<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1359" target="_blank" rel="noopener noreferrer">è«–æ–‡ç´¹ä»‹ / The Llama 3 Herd of Models, 2024.08</a>
 ã«ç¤ºã•ã‚Œã¦ã„ã‚‹é€šã‚Šã€Llama2ã«ãŠã‘ã‚‹æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã®å‰²åˆã¯0.2%ã¨ã‹ãªã®ã§ã€è‹±èªåœã®OpenLLMã«ãŠã„ã¦ã€æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã®æ¯”ç‡ãŒã©ã‚Œã ã‘å°‘ãªã„ã‹ãŒã‚ã‹ã‚‹ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="articles/Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2023-10-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1108" target="_blank" rel="noopener noreferrer" class="title-link">å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ï½¤ã€ŒçŸ¥è­˜ã¯å…¨çµåˆå±¤ã«è“„ç©ã•ã‚Œã‚‹ã€ã¨ã„ã†ä»®èª¬ã«ã¤ã„ã¦ã®æ–‡çŒ®èª¿æŸ»</a>
<span class="snippet"><span>Comment</span><p>ã‚¿ã‚¤ãƒˆãƒ«ã®é€šã‚Šã€çŸ¥è­˜ãŒFFNã«è“„ç©ã•ã‚Œã¦ã„ã‚‹ã¨ä¸»å¼µã—ã¦ã„ã‚‹ã‚‰ã—ã„åŸè«–æ–‡ã‚’èª­ã¿è§£ã„ã¦ã„ã‚‹ã€‚ã¾ã¨ã‚ã‚’å¼•ç”¨ã™ã‚‹ã¨<br><br>&gt; ã€ŒçŸ¥è­˜ã¯å…¨çµåˆå±¤ã«è“„ç©ã•ã‚Œã‚‹ã€ã¨ã„ã†è¡¨ç¾ã¯ï½¤ã‚„ã‚„ãƒ©ã‚¸ã‚«ãƒ«ã§ï½¤<br>å°‘ãªãã¨ã‚‚ã“ã®è«–æ–‡ã§ã¯ã€Œå…¨çµåˆå±¤ã¯çŸ¥è­˜ç²å¾—ã«ãŠã„ã¦é‡è¦ã€ã¨ã„ã†ç¨‹åº¦<br>ã®ï½¤ã‚‚ã†å°‘ã—ãƒã‚¤ãƒ«ãƒ‰ãªä¸»å¼µã‚’ã—ã¦ã„ã‚‹ã‚ˆã†ã«è¦‹å—ã‘ã‚‰ã‚Œã¾ã—ãŸï½¡<br><br>ã¨ã®ã“ã¨ã€‚</p></span><br><br>
<a class="button" href="articles/Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="articles/NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="articles/Prompting.html" target="_blank" rel="noopener noreferrer">#Prompting</a>
<a class="button" href="articles/Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="articles/AutomaticPromptEngineering.html" target="_blank" rel="noopener noreferrer">#AutomaticPromptEngineering</a>
<span class="issue_date">Issue Date: 2023-10-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1079" target="_blank" rel="noopener noreferrer" class="title-link">æ—¥æœ¬èªLLMãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨è‡ªå‹•ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°, PFN Blog, 2023.10</a>
<span class="snippet"><span>Comment</span><p>é¢ç™½ã‹ã£ãŸã€‚ç‰¹ã«ã€promptingã«ã‚ˆã£ã¦rinnaã¨cyberã®LLMã®é †ä½ãŒé€†è»¢ã—ã¦ã„ã‚‹ã®ãŒèˆˆå‘³æ·±ã‹ã£ãŸã€‚GAã‚’ä½¿ã£ãŸãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¯æœ€è¿‘è«–æ–‡ã‚‚å‡ºã¦ã„ãŸãŒã€æ—¥æœ¬èªLLMã§è©¦ã•ã‚Œã¦ã„ã‚‹ã®ã¯é¢ç™½ã‹ã£ãŸã€‚</p></span><br><br>
<button onclick="hideContent(0)" style="display: none;">hide</button>
&lt;/div&gt;
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
<script>
  document.addEventListener('DOMContentLoaded', function() {
    const tweets = document.querySelectorAll('.tweet-embed[data-embed]');

    if ('IntersectionObserver' in window) {
      const observer = new IntersectionObserver((entries, obs) => {
        entries.forEach(entry => {
          if (entry.isIntersecting) {
            const el = entry.target;
            const html = el.getAttribute('data-embed');
            if (html) {
              const placeholder = el.querySelector('.tweet-placeholder');
              if (placeholder) placeholder.remove();

              el.innerHTML = html.trim();

              if (window.twttr?.widgets?.load) {
                window.twttr.widgets.load(el);
              }
            }
            obs.unobserve(el); // å‡¦ç†æ¸ˆã¿ã¯ç›£è¦–è§£é™¤
          }
        });
      }, {
        rootMargin: '500px 0px', // ç”»é¢æ‰‹å‰200pxã§èª­ã¿è¾¼ã¿é–‹å§‹
        threshold: 0
      });

      tweets.forEach(tweet => observer.observe(tweet));

    } else {
      // IntersectionObserveræœªå¯¾å¿œãƒ–ãƒ©ã‚¦ã‚¶ç”¨ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
      function lazyLoadFallback() {
        tweets.forEach(el => {
          if (el.getAttribute('data-embed') && el.getBoundingClientRect().top < window.innerHeight) {
            const html = el.getAttribute('data-embed');
            const loadingImg = el.querySelector('.tweet-loading');
            if (loadingImg) loadingImg.remove();
            el.innerHTML = html.trim();
            el.removeAttribute('data-embed');
            if (window.twttr?.widgets?.load) {
              window.twttr.widgets.load(el);
            }
          }
        });
      }
      window.addEventListener('scroll', lazyLoadFallback);
      lazyLoadFallback();
    }
  });
</script>
</div>


    </div>

</article>
<div class="post-nav">
<a class="previous" href="/paper_notes/articles/Ambiguity.html" title="Ambiguityã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">Ambiguityã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§</a><a class="next" href="/paper_notes/articles/Annotation.html" title="Annotationã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">Annotationã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§</a>
</div>
<div class="post-related">
      <div>Related Articles</div>
      <ul>
        <li class="">
          <a class="post-link" href="/paper_notes/articles/OCR.html" title="OCRã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            OCRã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/InteractivePersonalizedSummarization.html" title="InteractivePersonalizedSummarizationã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            InteractivePersonalizedSummarizationã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/CovarianceShift.html" title="CovarianceShiftã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            CovarianceShiftã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/DiffusionModel.html" title="DiffusionModelã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§">
            DiffusionModelã«é–¢ã™ã‚‹è«–æ–‡ãƒ»æŠ€è¡“è¨˜äº‹ãƒ¡ãƒ¢ã®ä¸€è¦§<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
</ul>
    </div>
<div class="post-comments"></div></section>
</div>


  </section>
  <section class="sidebar" style="margin-left: 15px;">
    <!-- Get sidebar items --><style type="text/css" media="screen">
.post-menu ul {
  list-style: none;
  padding: 0;
  margin: 0;
}
</style>

<div class="post-menu">
  <div class="post-menu-title">TOC</div>
  <div class="post-menu-content"></div>
</div>

<script>
  function generateContent() {
    var menu = document.querySelector(".post-menu");
    var menuContent =  menu.querySelector(".post-menu-content");
    var headings = document.querySelector(".post-content").querySelectorAll("h2, h3, h4, h5, h6");

    // Hide menu when no headings
    if (headings.length === 0) {
      return menu.style.display = "none";
    }

    // Generate post menu
    var menuHTML = '';
    for (var i = 0; i < headings.length; i++) {
      var h = headings[i];
      menuHTML += (
        '<li class="h-' + h.tagName.toLowerCase() + '">'
        + '<a href="#h-' + h.getAttribute('id') + '">' + h.textContent + '</a></li>');
    }

    menuContent.innerHTML = '<ul>' + menuHTML + '</ul>';

    // The header element
    var header = document.querySelector('header.site-header');

    function doMenuCollapse(index, over_items) {
      var items = menuContent.firstChild.children;

      if (over_items == undefined) {
        over_items = 20;
      }

      if (items.length < over_items) {
        return;
      }

      var activeItem = items[index];
      var beginItem = activeItem
      var endItem = activeItem
      var beginIndex = index;
      var endIndex = index + 1;
      while (beginIndex >= 0
        && !items[beginIndex].classList.contains('h-h2')) {
        beginIndex -= 1;
      }
      while (endIndex < items.length
        && !items[endIndex].classList.contains('h-h2')) {
        endIndex += 1;
      }
      for (var i = 0; i < beginIndex; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
      for (var i = beginIndex + 1; i < endIndex; i++) {
        item = items[i]
        // if (!item.classList.contains('h-h2')) {
          item.style.display = '';
        // }
      }
      for (var i = endIndex; i < items.length; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
    }

    // Init menu collapsed
    doMenuCollapse(-1);

    // Active the menu item
    window.addEventListener('scroll', function (event) {
      var lastActive = menuContent.querySelector('.active');
      var changed = true;
      var activeIndex = -1;
      for (var i = headings.length - 1; i >= 0; i--) {
        var h = headings[i];
        var headingRect = h.getBoundingClientRect();
        var headerRect = header.getBoundingClientRect();
        var headerTop = Math.floor(headerRect.top);
        var headerHeight = Math.floor(headerRect.height);
        var headerHeight = headerTop + headerHeight + 20;
        if (headingRect.top <= headerHeight) {
          var id = 'h-' + h.getAttribute('id');
          var a = menuContent.querySelector('a[href="#' + id  + '"]');
          var curActive = a.parentNode;
          if (curActive) {
            curActive.classList.add('active');
            activeIndex = i;
          }
          if (lastActive == curActive) {
            changed = false;
          }
          break;
        }
      }
      if (changed) {
        if (lastActive) {
          lastActive.classList.remove('active');
        }
        doMenuCollapse(activeIndex);
      }
      event.preventDefault();
    });
  }
  generateContent();
</script>
</section>
</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/paper_notes/"></data>

  <div class="wrapper">
    <div class="site-footer-inner">
<div>Copyright Â© 2023-current AkihikoWATANABE. The header images and any thumbnail images for the posts were generated by ChatGPT's DALL-E3.</div>
      <div>Powered by <a title="Jekyll is a simple, blog-aware, static site
      generator." href="https://jekyllrb.com/">Jekyll</a> &amp; <a title="Yat, yet
      another theme." href="https://github.com/jeffreytse/jekyll-theme-yat">Yat Theme</a>.</div>
      <div class="footer-col rss-subscribe">Subscribe <a href="/paper_notes/feed.xml">via RSS</a>
</div>
    </div>
  </div>
</footer>
</body>
  </html>
