<!DOCTYPE html>
<html lang="ja">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-translate-customization" content="108d9124921d80c3-80e20d618ff053c8-g4f02ec6f3dba68b7-c">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>ACLに関する論文・技術記事メモの一覧 | わたしのべんきょうノート</title>
<meta name="generator" content="Jekyll v4.3.2">
<meta property="og:title" content="ACLに関する論文・技術記事メモの一覧">
<meta name="author" content="AkihikoWATANABE">
<meta property="og:locale" content="ja">
<meta name="description" content="ACL">
<meta property="og:description" content="ACL">
<link rel="canonical" href="http://akihikowatanabe.github.io/paper_notes/articles/ACL.html">
<meta property="og:url" content="http://akihikowatanabe.github.io/paper_notes/articles/ACL.html">
<meta property="og:site_name" content="わたしのべんきょうノート">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-09-02T00:46:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="ACLに関する論文・技術記事メモの一覧">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"AkihikoWATANABE"},"dateModified":"2025-09-02T00:46:00+00:00","datePublished":"2025-09-02T00:46:00+00:00","description":"ACL","headline":"ACLに関する論文・技術記事メモの一覧","mainEntityOfPage":{"@type":"WebPage","@id":"http://akihikowatanabe.github.io/paper_notes/articles/ACL.html"},"url":"http://akihikowatanabe.github.io/paper_notes/articles/ACL.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="icon" href="">
  <link rel="canonical" href="http://akihikowatanabe.github.io">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/main.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/custom_button.css">
  <script src="/paper_notes/assets/js/main.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script><link type="application/atom+xml" rel="alternate" href="http://akihikowatanabe.github.io/paper_notes/feed.xml" title="わたしのべんきょうノート">
<script>
  function showMore(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "block";
          
      // このボタンの参照を取得して非表示にします
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "none";
      }
          
      // hideボタンの参照を取得して表示します
      const hideButton = contentDiv.querySelector('button[onclick^="hideContent"]');
      if (hideButton) {
        hideButton.style.display = "block";
      }
    }
  }

  function hideContent(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "none";
  
      // moreボタンの参照を取得して表示します
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "block";
      }
  
      // このボタンを隠します
      const hideButtons = document.querySelectorAll('button[onclick^="hideContent"]');
      const hideButton = hideButtons[index];
      if (hideButton) {
        hideButton.style.display = "none";
      }
    }
  }
</script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script charset="UTF-8" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/go.min.js" async></script>



















<script>
// Init highlight js
document.addEventListener('DOMContentLoaded', function(event) {
  var els = document.querySelectorAll('pre code')

  function addLangData(block) {
    var outer = block.parentElement.parentElement.parentElement;
    var lang = block.getAttribute('data-lang');
    for (var i = 0; i < outer.classList.length; i++) {
      var cls = outer.classList[i];
      if (cls.startsWith('language-')) {
        lang = cls;
        break;
      }
    }
    if (!lang) {
      cls = block.getAttribute('class');
      lang = cls ? cls.replace('hljs ', '') : '';
    }
    if (lang.startsWith('language-')) {
      lang = lang.substr(9);
    }
    block.setAttribute('class', 'hljs ' + lang);
    block.parentNode.setAttribute('data-lang', lang);
  }

  function addBadge(block) {
    var enabled = ('true' || 'true').toLowerCase();
    if (enabled == 'true') {
      var pre = block.parentElement;
      pre.classList.add('badge');
    }
  }

  function handle(block) {
    addLangData(block);
    addBadge(block)
    hljs.highlightBlock(block);
  }

  for (var i = 0; i < els.length; i++) {
    var el = els[i];
    handle(el);
  }
});
</script>

<style>
  /* code language badge */
  pre.badge::before {
    content: attr(data-lang);
    color: #fff;
    background-color: #ff4e00;
    padding: 0 .5em;
    border-radius: 0 2px;
    text-transform: uppercase;
    text-align: center;
    min-width: 32px;
    display: inline-block;
    position: absolute;
    right: 0;
  }

  /* fix wrong badge display for firefox browser */
  code > table pre::before {
    display: none;
  }
</style>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe-lightbox.umd.min.js" async></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe.umd.min.js" async></script>
<link href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/photoswipe.min.css" rel="stylesheet">
<style>
  .pswp .pswp__container .pswp__img {
    background-color: white;
  }
</style>

<script>
  function initPhotoSwipe() {
    let customOptions = {};

    try {
      const data = `{"gallery"=>"section.main", "children"=>"a.photo-swipe", "bgOpacity"=>0.8, "padding"=>{"top"=>20, "bottom"=>40, "left"=>100, "right"=>100}}`.replaceAll("=>", ":");
      customOptions = JSON.parse(data);
    } catch (e) {
      console.info("Invalid custom photo previewer options! " + e.message);
    }

    // Define object and gallery options
    const options = Object.assign(
      {
        gallery: "section.main",
        children: "a.photo-swipe",
        photo_scale: 2,
        // dynamic import is not supported in UMD version
        pswpModule: PhotoSwipe,
      },
      customOptions
    );

    const galleryEl = document.querySelector(options.gallery);
    if (!galleryEl) {
      return;
    }

    const imgEls = [];
    const els = galleryEl.querySelectorAll("img:not(.emoji)");
    els.forEach((el) => {
      if (el.src.trim() == "") {
        return;
      }
      if (!imgEls.includes(el)) {
        imgEls.push(el);
      }
    });

    if (imgEls.length === 0) {
      return;
    }

    imgEls.forEach((imgEl) => {
      imgEl.outerHTML = `
      <a class="photo-swipe"
        href="${imgEl.src}"
        data-pswp-width="${
          Math.max(imgEl.naturalWidth, imgEl.width) * options.photo_scale
        }"
        data-pswp-height="${
          Math.max(imgEl.naturalHeight, imgEl.height) * options.photo_scale
        }"
        data-pswp-caption="${imgEl.getAttribute("caption") || imgEl.alt}"
        target="_blank">
        ${imgEl.outerHTML}
      </a>`;
    });

    // Initialize PhotoSwipe 5
    var lightbox = new PhotoSwipeLightbox(options);

    lightbox.init();
  }

  window.addEventListener("load", initPhotoSwipe);
</script>
<meta name="google-site-verification" content="u_DTTPcCZ806iq51zgirHyWq3556HUKGq8AQfH91iFI">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-P70KSB88WH"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-P70KSB88WH');
  </script>
</head>
<body>





































































































































<header class="site-header site-header-transparent" role="banner">

  <div class="wrapper">
    <div class="site-header-inner">
<span class="site-brand"><a class="site-brand-inner" rel="author" href="/paper_notes/">
  <img class="site-favicon" title="わたしのべんきょうノート" src="" onerror="this.style.display='none'">
  わたしのべんきょうノート
</a>
</span><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger">
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewbox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
              </svg>
            </span>
          </label>

          <div class="trigger">









<span class="page-link">



<div id="google_translate_element" style="display: none;">
</div>

<span class="ct-language">
  <ul class="list-unstyled ct-language-dropdown">
    
      <li>
        <a href="#" class="lang-select" data-lang="en">
          
          <img src="https://cdn.countryflags.com/thumbs/united-states-of-america/flag-400.png" title="English">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="fr">
          
          <img src="https://cdn.countryflags.com/thumbs/france/flag-400.png" title="French">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="zh-CN">
          
          <img src="https://cdn.countryflags.com/thumbs/china/flag-400.png" title="Chinese(Simple)">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ja">
          
          <img src="https://cdn.countryflags.com/thumbs/japan/flag-400.png" title="Japanese">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ko">
          
          <img src="https://cdn.countryflags.com/thumbs/south-korea/flag-400.png" title="Korean">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ru">
          
          <img src="https://cdn.countryflags.com/thumbs/russia/flag-400.png" title="Russian">
          
        </a>
      </li>
    
  </ul>
</span>

<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({
    pageLanguage: 'ja',
    autoDisplay: false,
    layout: google.translate.TranslateElement.InlineLayout.VERTICAL
  }, 'google_translate_element');

  // Links to cross-origin destinations are unsafe
  var gll = document.getElementsByClassName('goog-logo-link')[0];
  if (gll) {
    gll.setAttribute('rel', 'noopener');
  }

  function restoreLang() {
    var iframe = document.getElementsByClassName('goog-te-banner-frame')[0];
    if (!iframe) return;

    var innerDoc = iframe.contentDocument || iframe.contentWindow.document;
    var restore_el = innerDoc.getElementsByTagName("button");

    for (var i = 0; i < restore_el.length; i++) {
      if (restore_el[i].id.indexOf("restore") >= 0) {
        restore_el[i].click();
        var close_el = innerDoc.getElementsByClassName("goog-close-link");
        close_el[0].click();
        return;
      }
    }
  }

  function triggerHtmlEvent(element, eventName) {
    var event;
    if (document.createEvent) {
      event = document.createEvent('HTMLEvents');
      event.initEvent(eventName, true, true);
      element.dispatchEvent(event);
    } else {
      event = document.createEventObject();
      event.eventType = eventName;
      element.fireEvent('on' + event.eventType, event);
    }
  }

  var googleCombo = document.querySelector("select.goog-te-combo");
  var langSelect = document.querySelector('.ct-language');
  langSelect.addEventListener('click', function(event) {
    if (!event.target) {
      return;
    }

    var selected = document.querySelector('.ct-language .ct-language-selected');
    if (selected) {
      selected.classList.remove('ct-language-selected');
    }

    var target = event.target;
    while (target && target !== langSelect ) {
      if (target.matches('.lang-select')) {
        break;
      }
      target = target.parentElement;
    }

    if (target && target.matches('.lang-select')) {
      var lang = target.getAttribute('data-lang');
      if (googleCombo.value == lang) {
        restoreLang();
      } else {
        target.parentElement.classList.add('ct-language-selected');
        googleCombo.value = lang;
        triggerHtmlEvent(googleCombo, 'change');
      }
    }

    event.preventDefault();
  });
}
</script>

<script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit" async></script>
</span>
</div>
        </nav>
</div>
  </div>
</header>

<script>
  function initHeader() {
    var lastScrollY = getScrollPos().y;
    var documentElement = document.documentElement;

    function storeScrollData() {
      var y = getScrollPos().y;documentElement.setAttribute("data-header-transparent", "");var scrollStatus = "";

      if (y <= 0) {
        scrollStatus = "top";
      } else if ((window.innerHeight + y) >= document.body.offsetHeight) {
        scrollStatus = "bottom";
      } else {
        var isScrollDown = (y - lastScrollY > 0) ? true : false;
        scrollStatus = isScrollDown ? "down" : "up";
      }

      lastScrollY = y;
      documentElement.setAttribute("data-scroll-status", scrollStatus);
    }

    window.addEventListener('scroll', function(e) {
      storeScrollData();
    });

    storeScrollData();
  }
  document.addEventListener('DOMContentLoaded', initHeader);
</script>


























































































































































<style>
    html .page-banner .page-banner-img > *:first-child {
      opacity: 0.4;
    }

    html[data-theme="dark"] .page-banner .page-banner-img > *:first-child {
      opacity: 0.2872;
    }
  </style>
<section class="page-banner">
    <div class="page-banner-img">
<div style="background-image: url(/paper_notes/assets/images/banner.png)"></div>
        <img class="img-placeholder" src="/paper_notes/assets/images/banner.png">
</div>
    <div class="wrapper">
      <div class="page-banner-inner">
<header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">わたしのべんきょうノート</h1>
  <h2 class="post-subtitle">勉強した論文や技術等の情報をGithubのIssueにメモっているひとのブログ。
それなりにメモの量が蓄積されてきたので、一度整理したいなと思いブログはじめてみました！
自然言語処理(NLP), 推薦システム(RecommenderSystem), Educational Data Mining (EDM), Learning Analytics (LA)などの分野のメモが多いと思います。
最近は特にLLMの勉強が多めです :)</h2>

  <div class="post-meta">
    <time class="dt-published" datetime="2025-09-02T00:46:00+00:00" itemprop="datePublished"><i class="fa fa-calendar"></i> Sep 2, 2025
    </time><span class="post-author left-vsplit"><i class="fa fa-pencil"></i> AkihikoWATANABE</span>
    
































    <span class="post-reading-time left-vsplit"><i class="fa fa-clock-o"></i> About 3 hours 16 mins</span>
  </div></header>
</div>
    </div>
  </section><script>
  function hashLocate(hashValue) {
    hashValue = hashValue.replace(/^.*#h-/, '');
    hashValue = decodeURIComponent(hashValue);
    var element = document.getElementById(hashValue);

    if (!element) {
      return;
    }

    var header = document.querySelector('header.site-header');
    var headerRect = header.getBoundingClientRect();
    var headerTop = Math.floor(headerRect.top);
    var headerHeight = Math.floor(headerRect.height);
    var scrollPos = getScrollPos();
    var offsetY = element.offsetTop - (headerTop + headerHeight + 20);

    if (offsetY == scrollPos.y) {
      return;
    }

    if (headerTop == 0  && offsetY > scrollPos.y) {
      offsetY += headerHeight + 2;
    } else if (headerTop < 0  && offsetY < scrollPos.y) {
      offsetY -= headerHeight - 2;
    }

    smoothScrollTo(offsetY);
  }

  // The first event occurred
  window.addEventListener('load', function(event) {
    if (window.location.hash) {
      hashLocate(window.location.hash);
    }
  });

  // The first event occurred
  window.addEventListener('click', function(event) {
    if (event.target.tagName.toLowerCase() == 'a') {
      hashLocate(event.target.getAttribute('href'));
    }
  });
</script>
<div class="theme-toggle">
  <input type="checkbox" id="theme-switch">
  <label for="theme-switch">
    <div class="toggle"></div>
    <div class="names">
      <p class="light">Light</p>
      <p class="dark">Dark</p>
    </div>
  </label>
</div>




<script>
  (function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var nightModeOption = ('off' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
      storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
      var data = storage.getItem('theme');
      try {
        data = JSON.parse(data ? data : '');
      } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
      }
      return data;
    }

    function handleThemeToggle(nightShift) {
      themeData.nightShift = nightShift;
      saveThemeData(themeData);
      html.dataset.theme = nightShift ? 'dark' : 'light';
      setTimeout(function() {
        sw.checked = nightShift ? true : false;
      }, 50);
    }

    function autoThemeToggle() {
      // Next time point of theme toggle
      var now = new Date();
      var toggleAt = new Date();
      var hours = now.getHours();
      var nightShift = hours >= 19 || hours <=7;

      if (nightShift) {
        if (hours > 7) {
          toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
      } else {
        toggleAt.setHours(19);
      }

      toggleAt.setMinutes(0);
      toggleAt.setSeconds(0);
      toggleAt.setMilliseconds(0)

      var delay = toggleAt.getTime() - now.getTime();

      // auto toggle theme mode
      setTimeout(function() {
        handleThemeToggle(!nightShift);
      }, delay);

      return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
      };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
      handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
      var data = autoThemeToggle();

      // Toggle theme by local setting
      if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
      } else {
        handleThemeToggle(themeData.nightShift);
      }
    } else if (nightModeOption == 'manual') {
      handleThemeToggle(themeData.nightShift);
    } else {
      var nightShift = themeData.nightShift;
      if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
      }
      handleThemeToggle(nightShift);
    }
  })();
</script>
<div id="click-to-top" class="click-to-top">
  <i class="fa fa-arrow-up"></i>
</div>
<script>
  (function () {
    const clickToTop = document.getElementById('click-to-top');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 100) {
        clickToTop.classList.add('show')
      }else {
        clickToTop.classList.remove('show')
      }
    });
    clickToTop.addEventListener('click', () => {
      window.smoothScrollTo(0);
    });
  })();
</script>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="framework">
  <section class="main">

     <div class="post">
  <section>









<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="post-content e-content" itemprop="articleBody">

      <h2 id="acl">ACL</h2>

<div class="visible-content">
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/read-later.html">#read-later</a>


<br>


<span class="issue_date">Issue Date: 2025-08-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2348">[Paper Note] Language Models Resist Alignment: Evidence From Data Compression, Jiaming Ji+, ACL'25</a>
<span class="snippet"><span>Summary</span>本研究では、大規模言語モデル（LLMs）の整合性ファインチューニングが、意図しない行動を示す原因となる「elasticity」を理論的および実証的に探求。整合後のモデルは、事前学習時の行動分布に戻る傾向があり、ファインチューニングが整合性を損なう可能性が示された。実験により、モデルのパフォーマンスが急速に低下し、その後事前学習分布に戻ることが確認され、モデルサイズやデータの拡張とelasticityの相関も明らかに。これにより、LLMsのelasticityに対処する必要性が強調された。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/read-later.html">#read-later</a>


<br>


<span class="issue_date">Issue Date: 2025-08-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2347">[Paper Note] A Theory of Response Sampling in LLMs: Part Descriptive and Part   Prescriptive, Sarath Sivaprasad+, ACL'25</a>
<span class="snippet"><span>Summary</span>LLMのサンプリング行動を調査し、ヒューリスティクスが人間の意思決定に類似していることを示す。サンプルは統計的規範から処方的要素に逸脱し、公衆衛生や経済動向において一貫して現れる。LLMの概念プロトタイプが処方的規範の影響を受け、人間の正常性の概念に類似。ケーススタディを通じて、LLMの出力が理想的な値にシフトし、偏った意思決定を引き起こす可能性があることを示し、倫理的懸念を提起。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/read-later.html">#read-later</a>


<br>


<span class="issue_date">Issue Date: 2025-08-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2346">[Paper Note] Rethinking the Role of Prompting Strategies in LLM Test-Time Scaling: A   Perspective of Probability Theory, Yexiang Liu+, ACL'25</a>
<span class="snippet"><span>Summary</span>本研究では、LLMのテスト時の計算スケーリングにおけるプロンプト戦略の効果を調査。6つのLLMと8つのプロンプト戦略を用いた実験により、複雑なプロンプト戦略が単純なChain-of-Thoughtに劣ることを示し、理論的な証明を提供。さらに、スケーリング性能を予測し最適なプロンプト戦略を特定する手法を提案し、リソース集約的な推論プロセスの必要性を排除。複雑なプロンプトの再評価と単純なプロンプト戦略の潜在能力を引き出すことで、テスト時のスケーリング性能向上に寄与することを目指す。</span>
</div>
<p><button onclick="showMore(0)">more</button></p>
<div class="hidden-content">
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-08-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2345">[Paper Note] Mapping 1,000+ Language Models via the Log-Likelihood Vector, Momose Oyama+, ACL'25</a>
<span class="snippet"><span>Summary</span>自動回帰型言語モデルの比較に対し、対数尤度ベクトルを特徴量として使用する新しいアプローチを提案。これにより、テキスト生成確率のクルバック・ライブラー発散を近似し、スケーラブルで計算コストが線形に増加する特徴を持つ。1,000以上のモデルに適用し、「モデルマップ」を構築することで、大規模モデル分析に新たな視点を提供。</span>
<span class="snippet"><span>Comment</span>NLPコロキウムでのスライド:https://speakerdeck.com/shimosan/yan-yu-moderunodi-tu-que-lu-fen-bu-to-qing-bao-ji-he-niyorulei-si-xing-noke-shi-hua

<br>



<br>

元ポスト:https://x.com/hshimodaira/status/1960573414575333556?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/Composition.html">#Composition</a>
<a class="button" href="articles/InstructionFollowingCapability.html">#InstructionFollowingCapability</a>
<a class="button" href="articles/CommonsenseReasoning.html">#CommonsenseReasoning</a>
<span class="issue_date">Issue Date: 2025-07-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2328">[Paper Note] Revisiting Compositional Generalization Capability of Large Language   Models Considering Instruction Following Ability, Yusuke Sakai+, ACL'25</a>
<span class="snippet"><span>Summary</span>Ordered CommonGenを提案し、LLMsの指示に従う能力と構成的一般化能力を評価するベンチマークを構築。36のLLMsを分析した結果、指示の意図は理解しているが、概念の順序に対するバイアスが低多様性の出力を引き起こすことが判明。最も指示に従うLLMでも約75%の順序付きカバレッジしか達成できず、両能力の改善が必要であることを示唆。</span>
<span class="snippet"><span>Comment</span>LLMの意味の構成性と指示追従能力を同時に発揮する能力を測定可能なOrderedCommonGenを提案

<br>



<br>

<img src="https://github.com/user-attachments/assets/ae8c9468-a788-4baa-a618-402eae92c6c8" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/24ba68b4-3484-4597-a401-1e47183276cf" alt="image" loading="lazy">関連:

<br>

・2330</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Trustfulness.html">#Trustfulness</a>
<span class="issue_date">Issue Date: 2025-07-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2306">[Paper Note] Rectifying Belief Space via Unlearning to Harness LLMs' Reasoning, Ayana Niwa+, ACL'25</a>
<span class="snippet"><span>Summary</span>LLMの不正確な回答は虚偽の信念から生じると仮定し、信念空間を修正する方法を提案。テキスト説明生成で信念を特定し、FBBSを用いて虚偽の信念を抑制、真の信念を強化。実証結果は、誤った回答の修正とモデル性能の向上を示し、一般化の改善にも寄与することを示唆。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/ayaniwa1213/status/1949750575123276265?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/MachineTranslation.html">#MachineTranslation</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Decoding.html">#Decoding</a>
<span class="issue_date">Issue Date: 2025-07-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2261">[Paper Note] Unveiling the Power of Source: Source-based Minimum Bayes Risk Decoding   for Neural Machine Translation, Boxuan Lyu+, ACL'25</a>
<span class="snippet"><span>Summary</span>ソースベースのMBRデコーディング（sMBR）を提案し、パラフレーズや逆翻訳から生成された準ソースを「サポート仮説」として利用。参照なしの品質推定メトリックを効用関数として用いる新しいアプローチで、実験によりsMBRがQE再ランキングおよび標準MBRを上回る性能を示した。sMBRはNMTデコーディングにおいて有望な手法である。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/boxuan_lyu425/status/1946802820973519245?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/VisionLanguageModel.html">#VisionLanguageModel</a>
<a class="button" href="articles/Findings.html">#Findings</a>
<span class="issue_date">Issue Date: 2025-07-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2125">[Paper Note] Do Vision-Language Models Have Internal World Models? Towards an Atomic   Evaluation, Qiyue Gao+, ACL（Findings）'25</a>
<span class="snippet"><span>Summary</span>内部世界モデル（WMs）はエージェントの理解と予測を支えるが、最近の大規模ビジョン・ランゲージモデル（VLMs）の基本的なWM能力に関する評価は不足している。本研究では、知覚と予測を評価する二段階のフレームワークを提案し、WM-ABenchというベンチマークを導入。15のVLMsに対する660の実験で、これらのモデルが基本的なWM能力に顕著な制限を示し、特に運動軌道の識別においてほぼランダムな精度であることが明らかになった。VLMsと人間のWMとの間には重要なギャップが存在する。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/qiyuegao123/status/1940097188220297613?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Document.html">#Document</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Library.html">#Library</a>
<a class="button" href="articles/parser.html">#parser</a>
<span class="issue_date">Issue Date: 2025-06-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2063">[Paper Note] Dolphin: Document Image Parsing via Heterogeneous Anchor Prompting, Hao Feng+, ACL'25</a>
<span class="snippet"><span>Summary</span>文書画像解析の新モデル「Dolphin」を提案。レイアウト要素をシーケンス化し、タスク特有のプロンプトと組み合わせて解析を行う。3000万以上のサンプルで訓練し、ページレベルと要素レベルの両方で最先端の性能を達成。効率的なアーキテクチャを実現。コードは公開中。</span>
<span class="snippet"><span>Comment</span>repo:https://github.com/bytedance/DolphinSoTAなDocumentのparser

<br>

<img src="https://github.com/user-attachments/assets/5b1c4480-65f1-46cc-9318-f8126327e066" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/Architecture.html">#Architecture</a>
<span class="issue_date">Issue Date: 2025-06-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2030">[Paper Note] Value Residual Learning, Zhanchao Zhou+, ACL'25</a>
<span class="snippet"><span>Summary</span>ResFormerは、隠れ状態の残差に値の残差接続を加えることで情報の流れを強化する新しいTransformerアーキテクチャを提案。実験により、ResFormerは従来のTransformerに比べて少ないパラメータとトレーニングデータで同等の性能を示し、SVFormerはKVキャッシュサイズを半減させることができる。性能はシーケンスの長さや学習率に依存する。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/zhanchaozhou/status/1932829678081098079?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q<img src="https://github.com/user-attachments/assets/c2c97ea1-0930-4033-85bd-b3618463f87a" alt="image" loading="lazy"></span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/SyntheticData.html">#SyntheticData</a>
<a class="button" href="articles/DPO.html">#DPO</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<a class="button" href="articles/Probing.html">#Probing</a>
<span class="issue_date">Issue Date: 2025-05-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1974">Why Vision Language Models Struggle with Visual Arithmetic? Towards   Enhanced Chart and Geometry Understanding, Kung-Hsiang Huang+, ACL'25</a>
<span class="snippet"><span>Summary</span>Vision Language Models (VLMs)は視覚的算術に苦労しているが、CogAlignという新しいポストトレーニング戦略を提案し、VLMの性能を向上させる。CogAlignは視覚的変換の不変特性を認識するように訓練し、CHOCOLATEで4.6%、MATH-VISIONで2.9%の性能向上を実現し、トレーニングデータを60%削減。これにより、基本的な視覚的算術能力の向上と下流タスクへの転送の効果が示された。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/steeve__huang/status/1923543884367306763?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q既存のLLM (proprietary, openweightそれぞれ)が、シンプルなvisual arithmeticタスク(e.g., 線分の長さ比較, Chart上のdotの理解)などの性能が低いことを明らかにし、

<br>

<img src="https://github.com/user-attachments/assets/039a48de-67a5-4c81-ba59-174acd508479" alt="image" loading="lazy">

<br>

それらの原因を(1)Vision Encoderのrepresentationと(2)Vision EncoderをFreezeした上でのText Decoderのfinetuningで分析した。その結果、(1)ではいくつかのタスクでlinear layerのprobingでは高い性能が達成できないことがわかった。このことから、Vision Encoderによるrepresentationがタスクに関する情報を内包できていないか、タスクに関する情報は内包しているがlinear layerではそれを十分に可能できない可能性が示唆された。

<br>

<img src="https://github.com/user-attachments/assets/0eb90fa2-7b6a-43b6-81d9-b5f7e6fb3ea8" alt="image" loading="lazy">

<br>



<br>

これをさらに分析するために(2)を実施したところ、Vision Encoderをfreezeしていてもfinetuningによりquery stringに関わらず高い性能を獲得できることが示された。このことから、Vision Encoder側のrepresentationの問題ではなく、Text Decoderと側でデコードする際にFinetuningしないとうまく活用できないことが判明した。

<br>

<img src="https://github.com/user-attachments/assets/cd122d99-9228-44b1-9827-cdb56f49d492" alt="image" loading="lazy">手法のところはまだ全然しっかり読めていないのだが、画像に関する特定の属性に関するクエリと回答のペアを合成し、DPOすることで、zero-shotの性能が向上する、という感じっぽい？

<br>

<img src="https://github.com/user-attachments/assets/707b1cc9-8bbf-45a5-b564-f654503c836e" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/281da17b-c8c3-455a-aa51-043ed297ae1f" alt="image" loading="lazy"></span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2025-05-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1944">Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon   Pretraining Dataset, Dan Su+, ACL'25</a>
<span class="snippet"><span>Summary</span>FineWeb-EduとDCLMは、モデルベースのフィルタリングによりデータの90%を削除し、トレーニングに適さなくなった。著者は、アンサンブル分類器や合成データの言い換えを用いて、精度とデータ量のトレードオフを改善する手法を提案。1Tトークンで8Bパラメータモデルをトレーニングし、DCLMに対してMMLUを5.6ポイント向上させた。新しい6.3Tトークンデータセットは、DCLMと同等の性能を持ちながら、4倍のユニークなトークンを含み、長トークンホライズンでのトレーニングを可能にする。15Tトークンのためにトレーニングされた8Bモデルは、Llama 3.1の8Bモデルを上回る性能を示した。データセットは公開されている。</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/read-later.html">#read-later</a>
<span class="issue_date">Issue Date: 2025-03-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1775">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse   Attention, Jingyang Yuan+, ACL'25</a>
<span class="snippet"><span>Summary</span>長文コンテキストモデリングのために、計算効率を改善するスパースアテンションメカニズム「NSA」を提案。NSAは動的な階層スパース戦略を用い、トークン圧縮と選択を組み合わせてグローバルなコンテキスト認識とローカルな精度を両立。実装最適化によりスピードアップを実現し、エンドツーエンドのトレーニングを可能にすることで計算コストを削減。NSAはフルアテンションモデルと同等以上の性能を維持しつつ、長シーケンスに対して大幅なスピードアップを達成。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/dair_ai/status/1893698286545969311?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QACL'25のBest Paperの一つ:

<br>

https://x.com/gm8xx8/status/1950644063952052643?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/MultiLingual.html">#MultiLingual</a>
<span class="issue_date">Issue Date: 2025-08-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2374">[Paper Note] LongBench: A Bilingual, Multitask Benchmark for Long Context   Understanding, Yushi Bai+, ACL'24</a>
<span class="snippet"><span>Summary</span>本論文では、長いコンテキスト理解のための初のバイリンガル・マルチタスクベンチマーク「LongBench」を提案。英語と中国語で21のデータセットを含み、平均長はそれぞれ6,711語と13,386文字。タスクはQA、要約、少数ショット学習など多岐にわたる。評価結果から、商業モデルは他のオープンソースモデルを上回るが、長いコンテキストでは依然として課題があることが示された。</span>
<span class="snippet"><span>Comment</span>PLaMo Primeの長文テキスト評価に利用されたベンチマーク（中国語と英語のバイリンガルデータであり日本語は存在しない）

<br>

https://tech.preferred.jp/ja/blog/plamo-prime-release-feature-update/

<br>



<br>

タスクと言語ごとのLengthの分布。英語の方がデータが豊富で、長いものだと30000--40000ものlengthのサンプルもある模様。

<br>

<img src="https://github.com/user-attachments/assets/a1104f3f-996a-4ad9-b6c3-55b2eb7921ab" alt="image" loading="lazy"></span>
<a class="button" href="articles/Multi.html">#Multi</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Reasoning.html">#Reasoning</a>
<span class="issue_date">Issue Date: 2025-06-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2112">[Paper Note] Do Large Language Models Latently Perform Multi-Hop Reasoning?, Sohee Yang+, ACL'24</a>
<span class="snippet"><span>Summary</span>本研究では、LLMが複雑なプロンプトに対してマルチホップ推論を行う可能性を探ります。具体的には、LLMが「'Superstition'の歌手」を特定し、その母親に関する知識を用いてプロンプトを完成させる過程を分析します。2つのホップを個別に評価し、特に最初のホップにおいてブリッジエンティティのリコールが増加するかをテストしました。結果、特定の関係タイプのプロンプトに対してマルチホップ推論の証拠が見つかりましたが、活用は文脈依存であり、2番目のホップの証拠は控えめでした。また、モデルサイズの増加に伴い最初のホップの推論能力が向上する傾向が見られましたが、2番目のホップにはその傾向が見られませんでした。これらの結果は、LLMの今後の開発における課題と機会を示唆しています。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ModelMerge.html">#ModelMerge</a>
<span class="issue_date">Issue Date: 2025-06-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2096">[Paper Note] Chat Vector: A Simple Approach to Equip LLMs with Instruction Following   and Model Alignment in New Languages, Shih-Cheng Huang+, ACL'24</a>
<span class="snippet"><span>Summary</span>オープンソースの大規模言語モデル（LLMs）の多くは英語に偏っている問題に対処するため、chat vectorという概念を導入。これは、事前学習済みモデルの重みからチャットモデルの重みを引くことで生成され、追加のトレーニングなしに新しい言語でのチャット機能を付与できる。実証研究では、指示に従う能力や有害性の軽減、マルチターン対話においてchat vectorの効果を示し、さまざまな言語やモデルでの適応性を確認。chat vectorは、事前学習済みモデルに対話機能を効率的に実装するための有力な解決策である。</span>
<span class="snippet"><span>Comment</span>日本語解説:https://qiita.com/jovyan/items/ee6affa5ee5bdaada6b4下記ブログによるとChatだけではなく、Reasoningでも（post-trainingが必要だが）使える模様

<br>



<br>

Reasoning能力を付与したLLM ABEJA-QwQ32b-Reasoning-Japanese-v1.0の公開, Abeja Tech Blog, 2025.04:

<br>

https://tech-blog.abeja.asia/entry/geniac2-qwen25-32b-reasoning-v1.0</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Library.html">#Library</a>
<a class="button" href="articles/KnowledgeEditing.html">#KnowledgeEditing</a>
<span class="issue_date">Issue Date: 2025-05-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1947">EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language   Models, Peng Wang+, ACL'24, （System Demonstrations）</a>
<span class="snippet"><span>Summary</span>EasyEditは、LLMsのための使いやすい知識編集フレームワークであり、さまざまな知識編集アプローチをサポート。LlaMA-2の実験結果では、信頼性と一般化の面で従来のファインチューニングを上回ることを示した。GitHubでソースコードを公開し、Google Colabチュートリアルやオンラインシステムも提供。</span>
<span class="snippet"><span>Comment</span>ver2.0:

<br>

・1946</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2025-03-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1785">Full Parameter Fine-tuning for Large Language Models with Limited Resources, Lv+, ACL'24, 2024.08</a>
<span class="snippet"><span>Summary</span>新しいオプティマイザ「LOMO」を提案し、勾配計算とパラメータ更新を1ステップで融合することでメモリ使用量を削減。これにより、24GBのメモリを持つ8台のRTX 3090で65Bモデルの全パラメータファインチューニングが可能に。メモリ使用量は標準的なアプローチと比較して10.8%削減。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1716">Parallel Structures in Pre-training Data Yield In-Context Learning, Yanda Chen+, arXiv'24</a>
<span class="snippet"><span>Summary</span>事前学習済み言語モデル（LMs）のインコンテキスト学習（ICL）能力は、事前学習データ内の「平行構造」に依存していることを発見。平行構造とは、同じコンテキスト内で類似のテンプレートに従うフレーズのペアであり、これを除去するとICL精度が51%低下することが示された。平行構造は多様な言語タスクをカバーし、長距離にわたることが確認された。</span>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1715">Automated Justification Production for Claim Veracity in Fact Checking:  A Survey on Architectures and Approaches, Islam Eldifrawi+, arXiv'24</a>
<span class="snippet"><span>Summary</span>自動事実確認（AFC）は、主張の正確性を検証する重要なプロセスであり、特にオンラインコンテンツの増加に伴い真実と誤情報を見分ける役割を果たします。本論文では、最近の手法を調査し、包括的な分類法を提案するとともに、手法の比較分析や説明可能性向上のための今後の方向性について議論します。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1714">Legal Case Retrieval: A Survey of the State of the Art, Feng+, ACL'24, 2024.08</a>
<span class="snippet"><span>Summary</span>法的ケース検索（LCR）の重要性が増しており、歴史的なケースを大規模な法的データベースから検索するタスクに焦点を当てている。本論文では、LCRの主要なマイルストーンを調査し、研究者向けに関連データセットや最新のニューラルモデル、その性能を簡潔に説明する。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/Financial.html">#Financial</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1713">FinTextQA: A Dataset for Long-form Financial Question Answering, Jian Chen+, ACL'24</a>
<span class="snippet"><span>Summary</span>金融における質問応答システムの評価には多様なデータセットが必要だが、既存のものは不足している。本研究では、金融の長文質問応答用データセットFinTextQAを提案し、1,262の高品質QAペアを収集した。また、RAGベースのLFQAシステムを開発し、様々な評価手法で性能を検証した結果、Baichuan2-7BがGPT-3.5-turboに近い精度を示し、最も効果的なシステム構成が特定された。文脈の長さが閾値を超えると、ノイズに対する耐性が向上することも確認された。</span>
<span class="snippet"><span>Comment</span>@AkihikoWatanabe Do you have this dataset, please share it with me. Thank you.@thangmaster37 Thank you for your comment and I'm sorry for the late replying. Unfortunately, I do not have this dataset. I checked the link provided in the paper, but it was not found. Please try contacting the authors. Thank you.@thangmaster37 I found that the dataset is available in the following repository. However, as stated in the repository's README, It seems that the textbook portion of the dataset cannot be shared because their legal department has not granted permission to open source. Thank you.

<br>



<br>

https://github.com/AlexJJJChen/FinTextQA回答の長さが既存データセットと比較して長いFinancialに関するQAデータセット（1 paragraph程度）。

<br>

![Image](https://github.com/user-attachments/assets/fcb9273b-ded6-4ab4-a3c4-92bf971002b3)

<br>

![Image](https://github.com/user-attachments/assets/ba2b8d46-236d-43bc-8c3f-852b2d621171)

<br>



<br>

ただし、上述の通りデータセットのうちtextbookについて公開の許可が降りなかったようで、regulation and policy-relatedな部分のみ利用できる模様（全体の20%程度）。

<br>

![Image](https://github.com/user-attachments/assets/d5d0a3ce-58b3-4001-a870-a30c1e308c1b)</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1712">Masked Thought: Simply Masking Partial Reasoning Steps Can Improve  Mathematical Reasoning Learning of Language Models, Changyu Chen+, arXiv'24</a>
<span class="snippet"><span>Summary</span>推論タスクにおける誤りを軽減するため、外部リソースを使わずに入力に摂動を導入する手法を開発。特定のトークンをランダムにマスクすることで、Llama-2-7Bを用いたGSM8Kの精度を5％、GSM-ICの精度を10％向上させた。この手法は既存のデータ拡張手法と組み合わせることで、複数のデータセットで改善を示し、モデルが長距離依存関係を捉えるのを助ける可能性がある。コードはGithubで公開。</span>
<span class="snippet"><span>Comment</span>気になる</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1711">A Deep Dive into the Trade-Offs of Parameter-Efficient Preference  Alignment Techniques, Megh Thakkar+, arXiv'24</a>
<span class="snippet"><span>Summary</span>大規模言語モデルの整列に関する研究で、整列データセット、整列技術、モデルの3つの要因が下流パフォーマンスに与える影響を300以上の実験を通じて調査。情報量の多いデータが整列に寄与することや、監視付きファインチューニングが最適化を上回るケースを発見。研究者向けに効果的なパラメータ効率の良いLLM整列のガイドラインを提案。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/KnowledgeEditing.html">#KnowledgeEditing</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1710">Forgetting before Learning: Utilizing Parametric Arithmetic for   Knowledge Updating in Large Language Models, Shiwen Ni+, ACL'24</a>
<span class="snippet"><span>Summary</span>F-Learningという新しいファインチューニング手法を提案し、古い知識を忘却し新しい知識を学習するためにパラメトリック算術を利用。実験により、F-LearningがフルファインチューニングとLoRAファインチューニングの知識更新性能を向上させ、既存のベースラインを上回ることを示した。LoRAのパラメータを引き算することで古い知識を忘却する効果も確認。</span>
<span class="snippet"><span>Comment</span>Finetuningによって知識をアップデートしたい状況において、ベースモデルでアップデート前の該当知識を忘却してから、新しい知識を学習することで、より効果的に知識のアップデートが可能なことを示している。

<br>



<br>

古い知識のデータセットをK_old、古い知識から更新された新しい知識のデータセットをK_newとしたときに、K_oldでベースモデルを{Full-finetuning, LoRA}することで得たパラメータθ_oldを、ベースモデルのパラメータθから（古い知識を忘却することを期待して）減算し、パラメータθ'を持つ新たなベースモデルを得る。その後、パラメータθ'を持つベースモデルをk_newでFull-Finetuningすることで、新たな知識を学習させる。ただし、このような操作は、K_oldがベースモデルで学習済みである前提であることに注意する。学習済みでない場合はそもそも事前の忘却の必要がないし、減算によってベースモデルのコアとなる能力が破壊される危険がある。

<br>



<br>

<img src="https://github.com/user-attachments/assets/22c126e3-425d-4392-80ee-df319d217fd4" alt="image" loading="lazy">

<br>



<br>

結果は下記で、先行研究よりも高い性能を示している。注意点として、ベースモデルから忘却をさせる際に、Full Finetuningによってθ_oldを取得すると、ベースモデルのコアとなる能力が破壊されるケースがあるようである。一方、LoRAの場合はパラメータに対する影響が小さいため、このような破壊的な操作となりづらいようである。

<br>

<img src="https://github.com/user-attachments/assets/dfa12efc-c511-42bf-a1ab-0ef4a7749852" alt="image" loading="lazy">評価で利用されたデータセット:

<br>

・2556

<br>

・2557</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1709">NICE: To Optimize In-Context Examples or Not?, Pragya Srivastava+, ACL'24</a>
<span class="snippet"><span>Summary</span>タスク固有の指示がある場合、ICEの最適化が逆効果になることを発見。指示が詳細になるほどICE最適化の効果が減少し、タスクの学習可能性を定量化する指標「NICE」を提案。これにより、指示最適化とICE最適化の選択を支援するヒューリスティックを提供。</span>
<span class="snippet"><span>Comment</span>興味深い</span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/InstructionTuning.html">#InstructionTuning</a>
<a class="button" href="articles/PerplexityCurse.html">#PerplexityCurse</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1708">Instruction-tuned Language Models are Better Knowledge Learners, Zhengbao Jiang+, ACL'24</a>
<span class="snippet"><span>Summary</span>新しい文書からの知識更新には、事前指示調整（PIT）を提案。これは、文書の訓練前に質問に基づいて指示調整を行う手法で、LLMが新しい情報を効果的に吸収する能力を向上させ、標準的な指示調整を17.8%上回る結果を示した。</span>
<span class="snippet"><span>Comment</span>興味深い</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/KnowledgeEditing.html">#KnowledgeEditing</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1705">Learning to Edit: Aligning LLMs with Knowledge Editing, Yuxin Jiang+, ACL'24</a>
<span class="snippet"><span>Summary</span>「Learning to Edit（LTE）」フレームワークを提案し、LLMsに新しい知識を効果的に適用する方法を教える。二段階プロセスで、アライメントフェーズで信頼できる編集を行い、推論フェーズでリトリーバルメカニズムを使用。四つの知識編集ベンチマークでLTEの優位性と堅牢性を示す。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1704">Multi-Level Feedback Generation with Large Language Models for  Empowering Novice Peer Counselors, Alicja Chaszczewicz+, arXiv'24</a>
<span class="snippet"><span>Summary</span>大規模言語モデルを活用し、初心者のピアカウンセラーに文脈に応じた多層的なフィードバックを提供することを目的とした研究。上級心理療法スーパーバイザーと協力し、感情的サポートの会話に関するフィードバック注釈付きデータセットを構築。自己改善手法を設計し、フィードバックの自動生成を強化。定性的および定量的評価により、高リスクシナリオでの低品質なフィードバック生成のリスクを最小限に抑えることを示した。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1703">Learning Global Controller in Latent Space for Parameter-Efficient Fine-Tuning, Tan+, ACL'24, 2024.08</a>
<span class="snippet"><span>Summary</span>大規模言語モデル（LLMs）の高コストに対処するため、パラメータ効率の良いファインチューニング手法を提案。潜在ユニットを導入し、情報特徴を洗練することで下流タスクのパフォーマンスを向上。非対称注意メカニズムにより、トレーニングのメモリ要件を削減し、フルランクトレーニングの問題を軽減。実験結果は、自然言語処理タスクで最先端の性能を達成したことを示す。</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/MultiModal.html">#MultiModal</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1699">OlympiadBench: A Challenging Benchmark for Promoting AGI with   Olympiad-Level Bilingual Multimodal Scientific Problems, Chaoqun He+, ACL'24</a>
<span class="snippet"><span>Summary</span>大規模言語モデル（LLMs）やマルチモーダルモデル（LMMs）の能力を測定するために、オリンピアドレベルのバイリンガルマルチモーダル科学ベンチマーク「OlympiadBench」を提案。8,476の数学と物理の問題を含み、専門家レベルの注釈が付けられている。トップモデルのGPT-4Vは平均17.97%のスコアを達成したが、物理では10.74%にとどまり、ベンチマークの厳しさを示す。一般的な問題として幻覚や論理的誤謬が指摘され、今後のAGI研究に貴重なリソースとなることが期待される。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1698">DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM  Workflows, Ajay Patel+, arXiv'24</a>
<span class="snippet"><span>Summary</span>大規模言語モデル（LLMs）の利用が広がる中、標準化ツールの欠如や再現性の問題が浮上している。本論文では、研究者が簡単にLLMワークフローを実装できるオープンソースのPythonライブラリ「DataDreamer」を紹介し、オープンサイエンスと再現性を促進するためのベストプラクティスを提案する。ライブラリはGitHubで入手可能。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1697">Self-Contrast: Better Reflection Through Inconsistent Solving  Perspectives, Wenqi Zhang+, arXiv'24</a>
<span class="snippet"><span>Summary</span>LLMの反射能力に関する研究では、自己評価の質がボトルネックであることが判明。過信や高いランダム性が反射の質を低下させるため、自己対比（Self-Contrast）を提案し、多様な解決視点を探求・対比することで不一致を排除。これにより、LLMのバイアスを軽減し、より正確で安定した反射を促進。実験により、提案手法の効果と一般性が示された。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1696">Llama2Vec: Unsupervised Adaptation of Large Language Models for Dense Retrieval, Li+, ACL'24, 2024.08</a>
<span class="snippet"><span>Summary</span>Llama2Vecは、LLMを密な検索に適応させるための新しい非監視適応アプローチであり、EBAEとEBARの2つの前提タスクを用いています。この手法は、WikipediaコーパスでLLaMA-2-7Bを適応させ、密な検索ベンチマークでの性能を大幅に向上させ、特にMSMARCOやBEIRで最先端の結果を達成しました。モデルとソースコードは公開予定です。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Education.html">#Education</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1695">BIPED: Pedagogically Informed Tutoring System for ESL Education, Kwon+, ACL'24, 2024.08</a>
<span class="snippet"><span>Summary</span>大規模言語モデル（LLMs）を用いた会話型インテリジェントチュータリングシステム（CITS）は、英語の第二言語（L2）学習者に対して効果的な教育手段となる可能性があるが、既存のシステムは教育的深さに欠ける。これを改善するために、バイリンガル教育的情報を持つチュータリングデータセット（BIPED）を構築し、対話行為の語彙を考案した。GPT-4とSOLAR-KOを用いて二段階のフレームワークでCITSモデルを実装し、実験により人間の教師のスタイルを再現し、多様な教育的戦略を採用できることを示した。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1694">Beyond Memorization: The Challenge of Random Memory Access in Language  Models, Tongyao Zhu+, arXiv'24</a>
<span class="snippet"><span>Summary</span>生成型言語モデル（LM）のメモリアクセス能力を調査し、順次アクセスは可能だがランダムアクセスには課題があることを明らかに。暗唱技術がランダムメモリアクセスを向上させ、オープンドメインの質問応答においても顕著な改善を示した。実験コードは公開されている。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1693">Attribute First, then Generate: Locally-attributable Grounded Text  Generation, Aviv Slobodkin+, arXiv'24</a>
<span class="snippet"><span>Summary</span>ローカル属性付きテキスト生成アプローチを提案し、生成プロセスをコンテンツ選択、文の計画、逐次文生成の3ステップに分解。これにより、簡潔な引用を生成しつつ、生成品質と属性の正確性を維持または向上させ、事実確認にかかる時間を大幅に削減。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1692">Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to  Boost for Reasoning, Yongqi Tong+, arXiv'24</a>
<span class="snippet"><span>Summary</span>本研究では、LLMが自らの間違いから学ぶ能力を探求し、609,432の質問を含む新しいベンチマーク\textsc{CoTErrorSet}を提案。自己再考プロンプティングと間違いチューニングの2つの方法を用いて、LLMが誤りから推論能力を向上させることを実証。これにより、コスト効果の高いエラー活用戦略を提供し、今後の研究の方向性を示す。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1691">Enhancing In-Context Learning via Implicit Demonstration Augmentation, Xiaoling Zhou+, arXiv'24</a>
<span class="snippet"><span>Summary</span>インコンテキスト学習（ICL）におけるデモンストレーションの質や量がパフォーマンスに影響を与える問題に対処。デモンストレーションの深い特徴分布を活用し、表現を豊かにすることで、精度を向上させる新しいロジットキャリブレーションメカニズムを提案。これにより、さまざまなPLMやタスクでの精度向上とパフォーマンスのばらつきの減少を実現。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1690">MathGenie: Generating Synthetic Data with Question Back-translation for  Enhancing Mathematical Reasoning of LLMs, Zimu Lu+, arXiv'24</a>
<span class="snippet"><span>Summary</span>MathGenieは、少規模な問題解決データセットから多様で信頼性の高い数学問題を生成する新手法。シードデータの解答を増強し、逆翻訳モデルで新しい質問に変換。解答の正確性を確保するために根拠に基づく検証戦略を採用。MathGenieLMモデル群は、5つの数学的推論データセットでオープンソースモデルを上回り、特にGSM8Kで87.7%、MATHで55.7%の精度を達成。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1689">MELA: Multilingual Evaluation of Linguistic Acceptability, Zhang+, ACL'24, 2024.08</a>
<span class="snippet"><span>Summary</span>本研究では、46,000サンプルからなる「多言語言語的受容性評価（MELA）」ベンチマークを発表し、10言語にわたるLLMのベースラインを確立。XLM-Rを用いてクロスリンガル転送を調査し、ファインチューニングされたXLM-RとGPT-4oの性能を比較。結果、GPT-4oは多言語能力で優れ、オープンソースモデルは劣ることが判明。クロスリンガル転送実験では、受容性判断の転送が複雑であることが示され、MELAでのトレーニングがXLM-Rの構文タスクのパフォーマンス向上に寄与することが確認された。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1688">Time is Encoded in the Weights of Finetuned Language Models, Kai Nylund+, ACL'24</a>
<span class="snippet"><span>Summary</span>「時間ベクトル」を提案し、特定の時間データで言語モデルをファインチューニングする手法を示す。時間ベクトルは重み空間の方向を指定し、特定の時間帯のパフォーマンスを向上させる。隣接する時間帯に特化したベクトルは近接して配置され、補間により未来の時間帯でも良好な性能を発揮。異なるタスクやモデルサイズにおいて一貫した結果を示し、時間がモデルの重み空間にエンコードされていることを示唆。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1687">Surgical Feature-Space Decomposition of LLMs: Why, When and How?, Arnav Chavan+, arXiv'24</a>
<span class="snippet"><span>Summary</span>低ランク近似は、深層学習モデルの性能向上や推論のレイテンシ削減に寄与するが、LLMにおける有用性は未解明。本研究では、トランスフォーマーベースのLLMにおける重みと特徴空間の分解の効果を実証し、圧縮と性能のトレードオフに関する洞察を提供しつつ、常識推論性能の向上も示す。特定のネットワークセグメントの低ランク構造を特定し、モデルのバイアスへの影響も調査。これにより、低ランク近似が性能向上とバイアス修正の手段としての新たな視点を提供することを示した。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1686">MEFT: Memory-Efficient Fine-Tuning through Sparse Adapter, Jitai Hao+, arXiv'24</a>
<span class="snippet"><span>Summary</span>PEFTを用いたLLMsのファインチューニング性能は、追加パラメータの制約から限られる。これを克服するために、メモリ効率の良い大きなアダプターを導入し、CPUメモリの大容量を活用。Mixture of Expertsアーキテクチャを採用し、GPUとCPU間の通信量を削減。これにより、限られたリソース下でも高いファインチューニング性能を達成。コードはGitHubで公開。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1685">Benchmarking Knowledge Boundary for Large Language Models: A Different  Perspective on Model Evaluation, Xunjian Yin+, arXiv'24</a>
<span class="snippet"><span>Summary</span>大規模言語モデルの評価において、プロンプトに依存しない「知識境界」という新概念を提案。これにより、プロンプトの敏感さを回避し、信頼性の高い評価が可能に。新しいアルゴリズム「意味的制約を持つ投影勾配降下法」を用いて、知識境界を計算し、既存手法より優れた性能を示す。複数の言語モデルの能力を多様な領域で評価。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1683">ValueBench: Towards Comprehensively Evaluating Value Orientations and  Understanding of Large Language Models, Yuanyi Ren+, arXiv'24</a>
<span class="snippet"><span>Summary</span>本研究では、LLMsの価値観と理解を評価するための心理測定ベンチマーク「ValueBench」を提案。453の価値次元を含むデータを収集し、現実的な人間とAIの相互作用に基づく評価パイプラインを構築。6つのLLMに対する実験を通じて、共通および独自の価値観を明らかにし、価値関連タスクでの専門家の結論に近い能力を示した。ValueBenchはオープンアクセス可能。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1682">AIR-Bench: Benchmarking Large Audio-Language Models via Generative  Comprehension, Qian Yang+, arXiv'24</a>
<span class="snippet"><span>Summary</span>音声言語モデル（LALMs）の評価のために、初のベンチマークAIR-Benchを提案。これは、音声信号の理解と人間との相互作用能力を評価するもので、基本的な単一タスク能力を検査する約19,000の質問と、複雑な音声に対する理解力を評価する2,000のオープンエンド質問から構成。GPT-4を用いた評価フレームワークにより、LALMsの限界を明らかにし、今後の研究の指針を提供。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1681">Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via  Self-Evaluation, Xiaoying Zhang+, arXiv'24</a>
<span class="snippet"><span>Summary</span>自己整合性を用いてLLMの事実性を向上させるアプローチを提案。自己評価コンポーネントSelf-Evalを組み込み、生成した応答の事実性を内部知識で検証。信頼度推定を改善するSelf-Knowledge Tuningを設計し、自己注釈された応答でモデルをファインチューニング。TruthfulQAとBioGENタスクでLlamaモデルの事実精度を大幅に向上。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1679">Towards Faithful and Robust LLM Specialists for Evidence-Based  Question-Answering, Tobias Schimanski+, arXiv'24</a>
<span class="snippet"><span>Summary</span>LLMsの信頼性と追跡可能性を向上させるため、情報源の質と回答の帰属を改善するファインチューニング手法を調査。自動データ品質フィルターを用いた高品質データの合成により、パフォーマンスが向上。データ品質の改善が証拠に基づくQAにおいて重要であることを示した。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1678">AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators, Jingwei+, ACL'24, 2024.08</a>
<span class="snippet"><span>Summary</span>生成AIの普及に伴い、自動事実確認手法が重要視されているが、事実主張の検出にはスケーラビリティと一般化可能性の問題がある。これに対処するため、事実主張の統一的な定義を提案し、AFaCTAという新しいフレームワークを導入。AFaCTAはLLMsを活用し、注釈の信頼度を調整する。広範な評価により、専門家の注釈作業を効率化し、PoliClaimという包括的な主張検出データセットを作成した。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1677">Dissecting Human and LLM Preferences, Junlong Li+, arXiv'24</a>
<span class="snippet"><span>Summary</span>本研究では、人間と32種類のLLMの好みを分析し、モデルの応答の品質比較における定量的な構成を理解するための詳細なシナリオ別分析を行った。人間はエラーに対して敏感でなく、自分の立場を支持する応答を好む一方、GPT-4-Turboのような高度なLLMは正確性や無害性を重視することが分かった。また、同サイズのLLMはトレーニング方法に関係なく似た好みを示し、ファインチューニングは大きな変化をもたらさないことが明らかになった。さらに、好みに基づく評価は操作可能であり、モデルを審査員の好みに合わせることでスコアが向上することが示された。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1676">Selene: Pioneering Automated Proof in Software Verification, Lichen Zhang+, arXiv'24</a>
<span class="snippet"><span>Summary</span>ソフトウェア検証の自動化が求められる中、seL4に基づく初のプロジェクトレベルの自動証明ベンチマークSeleneを提案。Seleneは包括的な証明生成フレームワークを提供し、LLMs（GPT-3.5-turboやGPT-4）を用いた実験でその能力を示す。提案する強化策により、Seleneの課題が今後の研究で軽減可能であることを示唆。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1675">Evaluating Intention Detection Capability of Large Language Models in Persuasive Dialogues, Sakurai+, ACL'24, 2024.08</a>
<span class="snippet"><span>Summary</span>LLMsを用いてマルチターン対話における意図検出を調査。従来の研究が会話履歴を無視している中、修正したデータセットを用いて意図検出能力を評価。特に説得的対話では他者の視点を考慮することが重要であり、「フェイスアクト」の概念を取り入れることで、意図の種類に応じた分析が可能となる。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1674">Analyzing Temporal Complex Events with Large Language Models? A  Benchmark towards Temporal, Long Context Understanding, Zhihan Zhang+, arXiv'24</a>
<span class="snippet"><span>Summary</span>デジタル環境の進化に伴い、複雑なイベントの迅速かつ正確な分析が求められている。本論文では、長期間のニュース記事から「Temporal Complex Event（TCE）」を抽出・分析するために、LLMsを用いた新しいアプローチを提案。TCEは重要なポイントとタイムスタンプで特徴付けられ、読解力、時間的配列、未来のイベント予測の3つのタスクを含むベンチマーク「TCELongBench」を設立。実験では、リトリーバー強化生成（RAG）手法と長いコンテキストウィンドウを持つLLMsを活用し、適切なリトリーバーを持つモデルが長いコンテキストウィンドウを利用するモデルと同等のパフォーマンスを示すことが確認された。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1673">Feature-Adaptive and Data-Scalable In-Context Learning, Jiahao Li+, arXiv'24</a>
<span class="snippet"><span>Summary</span>FADS-ICLは、文脈内学習を強化するための特徴適応型フレームワークで、LLMの一般的な特徴を特定の下流タスクに適合させる。実験により、FADS-ICLは従来の手法を大幅に上回り、特に1.5Bモデルでの32ショット設定では平均14.3の精度向上を達成。トレーニングデータの増加により性能がさらに向上することも示された。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1672">Mitigating Catastrophic Forgetting in Large Language Models with  Self-Synthesized Rehearsal, Jianheng Huang+, arXiv'24</a>
<span class="snippet"><span>Summary</span>自己合成リハーサル（SSR）フレームワークを提案し、LLMの継続的学習における壊滅的な忘却を克服。基本のLLMで合成インスタンスを生成し、最新のLLMで洗練させることで、データ効率を高めつつパフォーマンスを向上。SSRは一般化能力を効果的に保持することが実験で示された。</span>
<a class="button" href="articles/Embeddings.html">#Embeddings</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/RAG(RetrievalAugmentedGeneration).html">#RAG(RetrievalAugmentedGeneration)</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/PostTraining.html">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1671">Grounding Language Model with Chunking-Free In-Context Retrieval, Hongjin Qian+, arXiv'24</a>
<span class="snippet"><span>Summary</span>CFICは、Retrieval-Augmented Generation（RAG）システム向けの新しいリトリーバルアプローチで、従来のチャンク化を回避し、文書のエンコードされた隠れ状態を利用して正確な証拠テキストを特定します。制約付き文のプレフィックスデコーディングとスキップデコーディングを組み込むことで、リトリーバルの効率と生成された証拠の忠実性を向上させます。CFICはオープンQAデータセットで評価され、従来の方法に対して大幅な改善を示し、RAGシステムの効率的で効果的なリトリーバルソリューションを提供します。</span>
<span class="snippet"><span>Comment</span>Chunking無しでRAGを動作させられるのは非常に魅力的。

<br>

<img src="https://github.com/user-attachments/assets/8841930a-3099-46c8-aae7-50f52473fbb1" alt="image" loading="lazy">一貫してかなり性能が向上しているように見える

<br>

<img src="https://github.com/user-attachments/assets/6eae7811-090a-4f84-aa8d-6d74a55e8427" alt="image" loading="lazy">提案手法の概要。InputとOutput全体の実例がほとんど掲載されていないので憶測を含みます。

<br>



<br>

気持ちとしては、ソーステキストが与えられたときに、Questionの回答をsupportするようなソース中のpassageの情報を活用して回答するために、重要なsentenceのprefixを回答生成前に生成させる（重要なsentenceの識別子の役割を果たす）ことで、（識別子によって重要な情報によって条件づけられて回答生成ができるやうになるのて）それら情報をより考慮しながらモデルが回答を生成できるようになる、といった話だと思われる。

<br>



<br>

Table2のようなテンプレートを用いて、ソーステキストと質問文でモデルを条件付けて、回答をsupportするsentenceのprefixを生成する。生成するprefixは各sentenceのユニークなprefixのtoken log probabilityの平均値によって決まる（トークンの対数尤度が高かったらモデルが暗黙的にその情報はQuestionにとって重要だと判断しているとみなせる）。SkipDecodingの説を読んだが、ぱっと見よく分からない。おそらく[eos]を出力させてprefix間のデリミタとして機能させたいのだと思うが、[eos]の最適なpositionはどこなのか？みたいな数式が出てきており、これがデコーディングの時にどういった役割を果たすのかがよくわからない。

<br>



<br>

また、モデルはQAと重要なPassageの三つ組のデータで提案手法によるデコーディングを適用してSFTしたものを利用する。

<br>



<br>

<img src="https://github.com/user-attachments/assets/fa4e575e-c6cb-452a-be3e-0d9bacb3cacb" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/1985754f-c21f-4904-be50-6f4f7eef56d1" alt="image" loading="lazy"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Evaluation.html">#Evaluation</a>
<a class="button" href="articles/Bias.html">#Bias</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1670">ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation  for Generative Large Language Models, Aparna Elangovan+, arXiv'24</a>
<span class="snippet"><span>Summary</span>本ポジションペーパーでは、生成的な大規模言語モデル（LLMs）の人間評価は多分野にわたる取り組みであるべきと主張し、実験デザインの信頼性を確保するためにユーザーエクスペリエンスや心理学の洞察を活用する必要性を強調します。評価には使いやすさや認知バイアスを考慮し、強力なモデルの能力と弱点を区別するための効果的なテストセットが求められます。さらに、スケーラビリティも重要であり、6つの柱から成るConSiDERS-The-Human評価フレームワークを提案します。これらの柱は、一貫性、評価基準、差別化、ユーザーエクスペリエンス、責任、スケーラビリティです。</span>
<a class="button" href="articles/Embeddings.html">#Embeddings</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/RepresentationLearning.html">#RepresentationLearning</a>
<a class="button" href="articles/STS%20(SemanticTextualSimilarity).html">#STS (SemanticTextualSimilarity)</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1669">Linguistically Conditioned Semantic Textual Similarity, Jingxuan Tu+, ACL'24</a>
<span class="snippet"><span>Summary</span>条件付きSTS（C-STS）は文の意味的類似性を測定するNLPタスクであるが、既存のデータセットには評価を妨げる問題が多い。本研究では、C-STSの検証セットを再アノテーションし、アノテーター間の不一致を55%観察。QAタスク設定を活用し、アノテーションエラーを80%以上のF1スコアで特定する自動エラー識別パイプラインを提案。また、モデル訓練によりC-STSデータのベースライン性能を向上させる新手法を示し、エンティティタイプの型特徴構造（TFS）を用いた条件付きアノテーションの可能性についても議論する。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MoE(Mixture-of-Experts).html">#MoE(Mixture-of-Experts)</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1665">DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models, Damai+, ACL'24, 2024.08</a>
<span class="snippet"><span>Summary</span>DeepSeekMoEアーキテクチャは、専門家の専門性を高めるために、専門家を細分化し柔軟な組み合わせを可能にし、共有専門家を設けて冗長性を軽減する。2BパラメータのDeepSeekMoEは、GShardと同等の性能を達成し、同じパラメータ数の密なモデルに近づく。16Bパラメータにスケールアップした際も、計算量を約40%に抑えつつ、LLaMA2と同等の性能を示した。</span>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MultiModal.html">#MultiModal</a>
<span class="issue_date">Issue Date: 2024-01-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1225">MM-LLMs: Recent Advances in MultiModal Large Language Models, Duzhen Zhang+, N_A, ACL'24 Findings</a>
<span class="snippet"><span>Summary</span>MM-LLMsは、コスト効果の高いトレーニング戦略を用いて拡張され、多様なMMタスクに対応する能力を持つことが示されている。本論文では、MM-LLMsのアーキテクチャ、トレーニング手法、ベンチマークのパフォーマンスなどについて調査し、その進歩に貢献することを目指している。</span>
<span class="snippet"><span>Comment</span>以下、論文を斜め読みしながら、ChatGPTを通じて疑問点を解消しつつ理解した内容なので、理解が不十分な点が含まれている可能性があるので注意。

<br>



<br>



<br>



<br>

まあざっくり言うと、マルチモーダルを理解できるLLMを作りたかったら、様々なモダリティをエンコーディングして得られる表現と、既存のLLMが内部的に処理可能な表現を対応づける Input Projectorという名の関数を学習すればいいだけだよ（モダリティのエンコーダ、LLMは事前学習されたものをそのままfreezeして使えば良い）。

<br>



<br>



<br>



<br>

マルチモーダルを生成できるLLMを作りたかったら、LLMがテキストを生成するだけでなく、様々なモダリティに対応する表現も追加で出力するようにして、その出力を各モダリティを生成できるモデルに入力できるように変換するOutput Projectortという名の関数を学習しようね、ということだと思われる。

<br>



<br>



<br>



<br>

概要

<br>



<br>

<img width="1093" alt="image" src="https://github.com/user-attachments/assets/c12f621b-95e6-4bff-827b-c4c5cf43b532">

<br>



<br>



<br>



<br>

ポイント

<br>



<br>

・Modality Encoder, LLM Backbone、およびModality Generatorは一般的にはパラメータをfreezeする

<br>



<br>

・optimizationの対象は「Input/Output Projector」

<br>



<br>



<br>



<br>

Modality Encoder

<br>



<br>

様々なモダリティI_Xを、特徴量F_Xに変換する。これはまあ、色々なモデルがある。

<br>



<br>

<img width="195" alt="image" src="https://github.com/user-attachments/assets/578c3bbc-0183-4d62-bf98-ee1b1bc1109c">

<br>



<br>



<br>



<br>

Input Projector

<br>



<br>

モダリティI_Xとそれに対応するテキストtのデータ {I_X, t}が与えられたとき、テキストtを埋め込み表現に変換んした結果得られる特徴量がF_Tである。Input Projectorは、F_XをLLMのinputとして利用する際に最適な特徴量P_Xに変換するθX_Tを学習することである。これは、LLM(P_X, F_T)によってテキストtがどれだけ生成できたか、を表現する損失関数を最小化することによって学習される。

<br>



<br>

<img width="451" alt="image" src="https://github.com/user-attachments/assets/a80f5453-b50f-48d5-8114-5f9f81544793">

<br>



<br>



<br>



<br>

LLM Backbone

<br>



<br>

LLMによってテキスト列tと、各モダリティに対応した表現であるS_Xを生成する。outputからt, S_Xをどのように区別するかはモデルの構造などにもよるが、たとえば異なるヘッドを用意して、t, S_Xを区別するといったことは可能であろうと思われる。

<br>



<br>

<img width="256" alt="image" src="https://github.com/user-attachments/assets/0be4e1c7-f92b-4259-a536-8ea135c1bcba">

<br>



<br>



<br>



<br>

Output Projector

<br>



<br>

S_XをModality Generatorが解釈可能な特徴量H_Xに変換する関数のことである。これは学習しなければならない。

<br>



<br>

H_XとModality Generatorのtextual encoderにtを入力した際に得られる表現τX(t)が近くなるようにOutput Projector θ_T_Xを学習する。これによって、S_XとModality Generatorがalignするようにする。

<br>



<br>

<img width="356" alt="image" src="https://github.com/user-attachments/assets/faa87be0-e738-4dc1-8e52-0787d6b973e8">

<br>



<br>



<br>



<br>

Modality Generator

<br>



<br>

各ModalityをH_Xから生成できるように下記のような損失学習する。要は、生成されたモダリティデータ（または表現）が実際のデータにどれだけ近いか、を表しているらしい。具体的には、サンプリングによって得られたノイズと、モデルが推定したノイズの値がどれだけ近いかを測る、みたいなことをしているらしい。

<br>



<br>

<img width="448" alt="image" src="https://github.com/user-attachments/assets/a18cfe29-27bf-42bf-8481-7e0afd838918">

<br>



<br>



<br>



<br>

Multi Modalを理解するモデルだけであれば、Input Projectorの損失のみが学習され、生成までするのであれば、Input/Output Projector, Modality Generatorそれぞれに示した損失関数を通じてパラメータが学習される。あと、P_XやらS_Xはいわゆるsoft-promptingみたいなものであると考えられる。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/ProgressiveLearning.html">#ProgressiveLearning</a>
<span class="issue_date">Issue Date: 2024-01-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1219">LLaMA Pro: Progressive LLaMA with Block Expansion, Chengyue Wu+, N_A, ACL'24</a>
<span class="snippet"><span>Summary</span>本研究では、大規模言語モデル（LLMs）の新しい事前学習後の手法を提案し、モデルの知識を効果的かつ効率的に向上させることを目指しました。具体的には、Transformerブロックの拡張を使用し、新しいコーパスのみを使用してモデルを調整しました。実験の結果、提案手法はさまざまなベンチマークで優れたパフォーマンスを発揮し、知的エージェントとして多様なタスクに対応できることが示されました。この研究は、自然言語とプログラミング言語を統合し、高度な言語エージェントの開発に貢献するものです。</span>
<span class="snippet"><span>Comment</span>追加の知識を導入したいときに使えるかも?事前学習したLLaMA Blockに対して、追加のLLaMA Blockをstackし、もともとのLLaMA Blockのパラメータをfreezeした上でドメインに特化したコーパスで事後学習することで、追加の知識を挿入する。LLaMA Blockを挿入するときは、Linear Layerのパラメータを0にすることで、RMSNormにおける勾配消失の問題を避けた上で、Identity Block（Blockを追加した時点では事前学習時と同様のOutputがされることが保証される）として機能させることができる。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/0ef6cc84-da38-4254-9bb3-ea4e2f9ebfab" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/a2bb221a-3ac3-4b81-9308-c114daf00401" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/QuestionAnswering.html">#QuestionAnswering</a>
<a class="button" href="articles/Chain-of-Thought.html">#Chain-of-Thought</a>
<a class="button" href="articles/Prompting.html">#Prompting</a>
<a class="button" href="articles/Verification.html">#Verification</a>
<span class="issue_date">Issue Date: 2023-09-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1044">[Paper Note] Chain-of-Verification Reduces Hallucination in Large Language Models, Shehzaad Dhuliawala+, N_A, ACL'24</a>
<span class="snippet"><span>Summary</span>私たちは、言語モデルが根拠のない情報を生成する問題に取り組んでいます。Chain-of-Verification（CoVe）メソッドを開発し、モデルが回答を作成し、検証し、最終的な回答を生成するプロセスを経ることで、幻想を減少させることができることを実験で示しました。</span>
<span class="snippet"><span>Comment</span>概要

<br>



<br>

ユーザの質問から、Verificationのための質問をplanningし、質問に対して独立に回答を得たうえでオリジナルの質問に対するaggreementを確認し、最終的に生成を実施するPrompting手法

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/18763903-2d70-4180-9384-2da55bedad2e" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

評価

<br>



<br>

dataset

<br>



<br>

・Wikidata

<br>



<br>

    ・Wikipedia APIから自動生成した「“Who are some [Profession]s who were born in [City]?”」に対するQA pairs

<br>



<br>

    ・Goldはknowledge baseから取得

<br>



<br>

    ・全56 test questions

<br>



<br>

    ・Gold Entityが大体600程度ありLLMは一部しか回答しないので、precisionで評価

<br>



<br>

・Wiki category list

<br>



<br>

    ・QUEST datasetを利用 701 

<br>



<br>

    ・回答にlogical operationが不要なものに限定して頭に"Name some"をつけて質問を生成

<br>



<br>

        ・"Name some Mexican animated horror films" or "Name some Endemic orchids of Vietnam"

<br>



<br>

    ・8個の回答を持つ55 test questionsを作成

<br>



<br>

・MultiSpanQA

<br>



<br>

    ・Reading Comprehensionに関するBenchmark dataset

<br>



<br>

    ・複数の独立した回答（回答は連続しないスパンから回答が抽出される）から構成される質問で構成

<br>



<br>

        ・特に、今回はclosed-book setting で実施

<br>



<br>

        ・すなわち、与えられた質問のみから回答しなければならず、知っている知識が問われる問題

<br>



<br>

    ・418のtest questsionsで、各回答に含まれる複数アイテムのspanが3 token未満となるようにした

<br>



<br>

    ・QA例:

<br>



<br>

        ・Q: Who invented the first printing press and in what year?

<br>



<br>

        ・A: Johannes Gutenberg, 1450.

<br>



<br>

評価結果

<br>



<br>

提案手法には、verificationの各ステップでLLMに独立したpromptingをするかなどでjoint, 2-step, Factored, Factor+Revisedの4種類のバリエーションがあることに留意。

<br>



<br>

・joint: 全てのステップを一つのpromptで実施

<br>



<br>

・2-stepは2つのpromptに分けて実施

<br>



<br>

・Factoredは各ステップを全て異なるpromptingで実施

<br>



<br>

・Factor+Revisedは異なるpromptで追加のQAに対するcross-checkをかける手法

<br>



<br>

結果を見ると、CoVEでhallucinationが軽減され、特にjointよりも2-step, factoredの方が高い性能を示すことがわかる。

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/05ff1e6c-75e7-428a-996f-61e844866391" alt="image" loading="lazy">

<br>



<br>

<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/d72aa05e-daab-4092-a6f5-9e80cdab7486" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Chain-of-Thought.html">#Chain-of-Thought</a>
<span class="issue_date">Issue Date: 2023-04-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/554">Active prompting with chain-of-thought for large language models, Diao+, The Hong Kong University of Science and Technology, ACL'24</a>
<span class="snippet"><span>Comment</span>しっかりと読めていないが、CoT-answerが存在しないtrainingデータが存在したときに、nサンプルにCoTとAnswerを与えるだけでFew-shotの予測をtestデータに対してできるようにしたい、というのがモチベーションっぽい

<br>



<br>

そのために、questionに対して、training dataに対してFew-Shot CoTで予測をさせた場合やZero-Shot CoTによって予測をさせた場合などでanswerを取得し、answerのばらつき度合いなどから不確実性を測定する。

<br>



<br>

そして、不確実性が高いCoT-Answerペアを取得し、人間が手作業でCoTと回答のペアを与え、その人間が作成したものを用いてTestデータに対してFewShotしましょう、ということだと思われる。

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/234747555-4b7bd0d5-f099-4288-a470-32206533e652.png" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/PersonalizedGeneration.html">#PersonalizedGeneration</a>
<span class="issue_date">Issue Date: 2023-04-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/536">LaMP: When Large Language Models Meet Personalization, Selemi+, University of Massachusetts Amherst （w_ Google Research）, ACL'24</a>
<span class="snippet"><span>Comment</span>概要

<br>



<br>

Personalizationはユーザのニーズや嗜好に応えるために重要な技術で、IRやRecSysで盛んに研究されてきたが、NLPではあまり実施されてこなかった。しかし、最近のタスクで、text classificationやgeneration taskでPersonalizationの重要性が指摘されている。このような中で、LLMでpersonalizedなレスポンスを生成し、評価することはあまり研究されていない。そこで、LaMPベンチマークを生成し、LLMにおけるPersonalizationをするための開発と評価をするための第一歩として提案している。

<br>



<br>

 

<br>



<br>

Personalizing LLM Outputs

<br>



<br>

LLMに対してPersonalizedなoutputをさせるためには、profileをpromptに埋め込むことが基本的なアプローチとなる。

<br>



<br>



<br>



<br>

Problem Formulation

<br>



<br>

まず、user profile（ユーザに関するrecordの集合）をユーザとみなす。データサンプルは以下の3つで構成される：

<br>



<br>

・x: モデルのinputとなるinput sequence

<br>



<br>

・y: モデルが生成することを期待するtarget output

<br>



<br>

・u: user profile（ユーザの嗜好やrequirementsを捉えるための補助的な情報）

<br>



<br>

そして、p\(y | x, u) を最大化する問題として定式化される。それぞれのユーザuに対して、モデルは{\(x\_u1, y\_u1,)...\(x\_un, y\_un)}を利用することができる。

<br>



<br>



<br>



<br>

A Retrieval Augmentation Approach for Personaliozing LLMs

<br>



<br>

user profileは基本的にめちゃめちゃ多く、promptに入れ込むことは非現実的。そこで、reteival augmentation approachと呼ばれる手法を提案している。LLMのcontext windowは限られているので、profileのうちのsubsetを利用することが現実的なアプローチとなる。また、必ずしも全てのユーザプロファイルがあるタスクを実施するために有用とは限らない。このため、retrieval augmentation approachを提案している。

<br>



<br>

retrieval augmentation approachでは、現在のテストケースに対して、relevantな部分ユーザプロファイルを選択的に抽出するフレームワークである。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/234442873-01a4961b-feab-42d3-b59c-ee26daad957f.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

(x_i, y_i)に対してpersonalizationを実現するために、3つのコンポーネントを採用している：

<br>



<br>

1. query generation function: x_iに基づきuser profileからrelevantな情報を引っ張ってくるquery qを生成するコンポーネント

<br>



<br>

2. retrieval model R(q, P_u, k): query q, プロファイルP_u, を用いて、k個のrelevantなプロファイルを引っ張ってくるモデル

<br>



<br>

3. prompt construction function: xとreteival modelが引っ張ってきたエントリからpromptを作成するコンポーネント

<br>



<br>

1, 2, 3によって生成されたprompt x^barと、yによってモデルを訓練、あるいは評価する。

<br>



<br>

この研究では、Rとして Contriever 540 , BM25, random selectionの3種類を用いている。

<br>



<br>



<br>



<br>

LaMPベンチマーク

<br>



<br>

GLUEやSuper Glue、KILT、GENといったベンチマークは、"one-size-fits-all"なモデリングと評価を前提としており、ユーザのニーズに答えるための開発を許容していない。一方で、LaMPは、以下のようなPersonalizationが必要なさまざまなタスクを統合して作成されたデータセットである。

<br>



<br>

・Personalized Text Classification

<br>



<br>

  ・Personalized Citation Identification (binary classification)

<br>



<br>

    ・Task definition

<br>



<br>

      ・user u が topic xに関する論文を書いたときに、何の論文をciteすべきかを決めるタスク

<br>



<br>

      ・user uが書いた論文のタイトルが与えられたとき、2つのcandidate paperのうちどちらをreferenceとして利用すべきかを決定する2値分類

<br>



<br>

    ・Data Collection

<br>



<br>

      ・Citation Network Datasetを利用。最低でも50本以上論文を書いているauthorを抽出し、authorの論文のうちランダムに論文と論文の引用を抽出

<br>



<br>

      ・negative document selectionとして、ランダムに共著者がciteしている論文をサンプリング

<br>



<br>

   ・Profile Specification

<br>



<br>

     ・ ユーザプロファイルは、ユーザが書いた全てのpaper

<br>



<br>

     ・titleとabstractのみをuser profileとして保持した

<br>



<br>

    ・Evaluation

<br>



<br>

      ・train/valid/testに分け、accuracyで評価する

<br>



<br>

  ・Personalized News Categorization (15 category分類)

<br>



<br>

    ・Task definition

<br>



<br>

      ・LLMが journalist uによって書かれたニュースを分類する能力を問うタスク

<br>



<br>

      ・u によって書かれたニュースxが与えられた時、uの過去の記事から得られるカテゴリの中から該当するカテゴリを予測するタスク

<br>



<br>

    ・Data Collection

<br>



<br>

      ・news categorization datasetを利用（Huff Postのニュース）

<br>



<br>

      ・記事をfirst authorでグルーピング

<br>



<br>

      ・グルーピングした記事群をtrain/valid/testに分割

<br>



<br>

      ・それぞれの記事において、記事をinputとし、その記事のカテゴリをoutputとする。そして残りの記事をuser profileとする。

<br>



<br>

    ・Profile Specification

<br>



<br>

      ・ユーザによって書かれた記事の集合

<br>



<br>

    ・Evaluation

<br>



<br>

      ・accuracy, macro-averaged F1で評価 

<br>



<br>

  ・Personalized Product Rating (5-star rating)

<br>



<br>

    ・Task definition

<br>



<br>

      ・ユーザuが記述したreviewに基づいて、LLMがユーザuの未知のアイテムに対するratingを予測する性能を問う

<br>



<br>

    ・Data Collection

<br>



<br>

      ・Amazon Reviews Datasetを利用

<br>



<br>

      ・reviewが100件未満、そしてほとんどのreviewが外れ値なユーザ1%を除外

<br>



<br>

      ・ランダムにsubsetをサンプリングし、train/valid/testに分けた

<br>



<br>

      ・input-output pairとしては、inputとしてランダムにユーザのreviewを選択し、その他のreviewをprofileとして利用する。そして、ユーザがinputのレビューで付与したratingがground truthとなる。

<br>



<br>

    ・Profile Specification

<br>



<br>

      ・ユーザのレビュ

<br>



<br>

    ・Evaluation

<br>



<br>

      ・ ttrain/valid/testに分けてRMSE, MAEで評価する

<br>



<br>

・Personalized Text Generation

<br>



<br>

  ・Personalized News Headline Generation

<br>



<br>

    ・Task definition

<br>



<br>

      ・ユーザuが記述したニュースのタイトルを生成するタスク

<br>



<br>

      ・特に、LLMが与えられたprofileに基づいてユーザのinterestsやwriting styleを捉え、適切にheadlinに反映させる能力を問う

<br>



<br>

   ・Data Collection

<br>



<br>

     ・News Categorization datasetを利用（Huff Post）

<br>



<br>

     ・データセットではauthorの情報が提供されている

<br>



<br>

     ・それぞれのfirst authorごとにニュースをグルーピングし、それぞれの記事をinput, headlineをoutputとした。そして残りの記事をprofileとした

<br>



<br>

   ・Profile Specification

<br>



<br>

     ・ユーザの過去のニュース記事とそのheadlineの集合をprofileとする

<br>



<br>

   ・Evaluation

<br>



<br>

     ・ROUGE-1, ROUGE-Lで評価

<br>



<br>

  ・Personalized Scholarly Title Generation

<br>



<br>

    ・Task Definition

<br>



<br>

      ・ユーザの過去のタイトルを考慮し、LLMがresearch paperのtitleを生成する能力を測る

<br>



<br>

    ・Data Collection

<br>



<br>

      ・Citation Network Datasetのデータを利用

<br>



<br>

      ・abstractをinput, titleをoutputとし、残りのpaperをprofileとした

<br>



<br>

    ・Profile Specification

<br>



<br>

      ・ユーザが書いたpaperの集合（abstractのみを利用）

<br>



<br>

  ・Personalized Email Subject Generation

<br>



<br>

    ・Task Definition

<br>



<br>

      ・LLMがユーザのwriting styleに合わせて、Emailのタイトルを書く能力を測る

<br>



<br>

    ・Data Collection

<br>



<br>

      ・Avocado Resaerch Email Collectionデータを利用

<br>



<br>

      ・5単語未満のsubjectを持つメール、本文が30単語未満のメールを除外、

<br>



<br>

      ・送信主のemail addressでメールをグルーピング

<br>



<br>

      ・input _outputペアは、email本文をinputとし、対応するsubjectをoutputとした。他のメールはprofile

<br>



<br>

    ・Profile Specification

<br>



<br>

      ・ ユーザのemailの集合

<br>



<br>

    ・Evaluation

<br>



<br>

      ・ROUGE-1, ROUGE-Lで評価 

<br>



<br>

  ・Personalized Tweet Paraphrasing

<br>



<br>

    ・Task Definition

<br>



<br>

      ・LLMがユーザのwriting styleを考慮し、ツイートのparaphrasingをする能力を問う

<br>



<br>

    ・Data Collection

<br>



<br>

      ・Sentiment140 datasetを利用

<br>



<br>

      ・最低10単語を持つツイートのみを利用

<br>



<br>

      ・userIDでグルーピングし、10 tweets以下のユーザは除外

<br>



<br>

      ・ランダムに1つのtweetを選択し、ChatGPT(gpt-3.5-turbo)でparaphraseした

<br>



<br>

      ・paraphrase版のtweetをinput, 元ツイートをoutputとし、input-output pairを作った。

<br>



<br>

    ・User Profile Specification

<br>



<br>

      ・ユーザの過去のツイート

<br>



<br>

    ・Evaluation

<br>



<br>

      ・ROUGE-1, ROUGE-Lで評価

<br>



<br>



<br>



<br>

実験

<br>



<br>

Experimental Setup

<br>



<br>

・FlanT5-baesをfinetuningした

<br>



<br>

・ユーザ単位でモデルが存在するのか否かが記載されておらず不明

<br>



<br>

結果

<br>



<br>

・Personalization入れた方が全てのタスクでよくなった

<br>



<br>

・Retrievalモデルとしては、randomの場合でも良くなったが、基本的にはContrirverを利用した場合が最も良かった

<br>



<br>

  ・=&gt; 適切なprofileを選択しpromptに含めることが重要であることが示された

<br>



<br>

・Rが抽出するサンプル kを増やすと、予測性能が増加する傾向もあったが、一部タスクでは性能の低下も招いた

<br>



<br>

・dev setを利用し、BM25/Contrieverのどちらを利用するか、kをいくつに設定するかをチューニングした結果、全ての結果が改善した

<br>



<br>

・FlanT5-XXLとgpt-3.5-turboを用いたZero-shotの設定でも実験。tweet paraphrasingタスクを除き、zero-shotでもuser profileをLLMで利用することでパフォーマンス改善。小さなモデルでもfinetuningすることで、zero-shotの大規模モデルにdownstreamタスクでより高い性能を獲得することを示している（ただし、めちゃめちゃ改善しているというわけでもなさそう）。

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/234452179-6fac1cd9-982f-4b48-8dfe-1d742dc1c221.png" alt="image" loading="lazy">

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/234452837-8d8cdfed-ab85-4d1e-99d7-aad35ddc8979.png" alt="image" loading="lazy">

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/234453453-a2393c95-a820-404f-b21c-3332f53cb851.png" alt="image" loading="lazy">

<br>



<br>

LaMPによって可能なResearch Problem

<br>



<br>

Prompting for Personalization

<br>



<br>

・Augmentationモデル以外のLLMへのユーザプロファイルの埋め込み方法

<br>



<br>

・hard promptingやsoft prompting 473 の活用

<br>



<br>

Evaluation of Personalized Text Generation

<br>



<br>

・テキスト生成で利用される性能指標はユーザの情報を評価のプロセスで考慮していない

<br>



<br>

・Personalizedなテキスト生成を評価するための適切なmetricはどんなものがあるか？

<br>



<br>

Learning to Retrieve from User Profiles

<br>



<br>

・Learning to RankをRetrieval modelに適用する方向性LaMPの作成に利用したテンプレート一覧

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/234457098-5c2ba78f-dc74-45e4-bf91-f07ffd99bcb4.png" alt="image" loading="lazy">

<br>



<br>

実装とleaderboard

<br>



<br>

https://lamp-benchmark.github.io/leaderboard</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Parallelism.html">#Parallelism</a>
<span class="issue_date">Issue Date: 2025-05-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1970">Sequence Parallelism: Long Sequence Training from System Perspective, Li+, ACL'23</a>
<span class="snippet"><span>Comment</span>入力系列をチャンクに分割して、デバイスごとに担当するチャンクを決めることで原理上無限の長さの系列を扱えるようにした並列化手法。系列をデバイス間で横断する場合attention scoreをどのように計算するかが課題になるが、そのためにRing Self attentionと呼ばれるアルゴリズムを提案している模様。また、MLPブロックとMulti Head Attentonブロックの計算も、BatchSize  Sequence Lengthの大きさが、それぞれ32Hidden Size, 16Attention Head size  of Attention Headよりも大きくなった場合に、Tensor Parallelismよりもメモリ効率が良くなるらしい。

<br>

<img src="https://github.com/user-attachments/assets/f3ba9010-da3a-4c3a-8515-d3715466ff59" alt="image" loading="lazy">Data Parallel, Pipeline Parallel, Tensor Parallel、全てに互換性があるとのこと（併用可能）そのほかの並列化の解説については

<br>

・1184

<br>



<br>

を参照のこと。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1707">Are Emergent Abilities in Large Language Models just In-Context  Learning?, Sheng Lu+, arXiv'23</a>
<span class="snippet"><span>Summary</span>大規模言語モデルの「出現能力」は、インコンテキスト学習やモデルの記憶、言語知識の組み合わせから生じるものであり、真の出現ではないと提案。1000以上の実験を通じてこの理論を裏付け、言語モデルの性能を理解するための基礎を提供し、能力の過大評価を警告。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1706">Boosting Language Models Reasoning with Chain-of-Knowledge Prompting, Jianing Wang+, arXiv'23</a>
<span class="snippet"><span>Summary</span>Chain-of-Thought（CoT）プロンプティングの限界を克服するために、Chain-of-Knowledge（CoK）プロンプティングを提案。CoKは、LLMsに明示的な知識の証拠を生成させ、推論の信頼性を向上させる。F^2-Verification手法を用いて、信頼性のない応答を指摘し再考を促す。実験により、常識や事実に基づく推論タスクのパフォーマンスが向上することを示した。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1702">Exploring Memorization in Fine-tuned Language Models, Shenglai Zeng+, arXiv'23</a>
<span class="snippet"><span>Summary</span>ファインチューニング中の大規模言語モデル（LLMs）の記憶を初めて包括的に分析。オープンソースのファインチューニングされたモデルを用いた結果、記憶はタスク間で不均一であることが判明。スパースコーディング理論を通じてこの不均一性を説明し、記憶と注意スコア分布の強い相関関係を明らかにした。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1701">Instruction Fusion: Advancing Prompt Evolution through Hybridization, Weidong Guo+, arXiv'23</a>
<span class="snippet"><span>Summary</span>Instruction Fusion（IF）を提案し、二つの異なるプロンプトを組み合わせることでコード生成LLMの性能を向上させる。実験により、IFが従来の手法の制約を克服し、HumanEvalなどのベンチマークで大幅な性能向上を実現することを示した。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1700">Insert or Attach: Taxonomy Completion via Box Embedding, Wei Xue+, arXiv'23</a>
<span class="snippet"><span>Summary</span>TaxBoxフレームワークは、ボックス埋め込み空間を利用して分類体系の補完を行い、挿入および付加操作に特化した幾何学的スコアラーを設計。動的ランキング損失メカニズムによりスコアを調整し、実験では従来手法を大幅に上回る性能向上を達成。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1684">SPARSEFIT: Few-shot Prompting with Sparse Fine-tuning for Jointly  Generating Predictions and Natural Language Explanations, Jesus Solano+, arXiv'23</a>
<span class="snippet"><span>Summary</span>SparseFitは、少量の自然言語による説明（NLE）データを用いて、離散的なプロンプトを活用し、予測とNLEを共同生成するスパースなfew-shot微調整戦略です。T5モデルで実験した結果、わずか6.8%のパラメータ微調整で、タスクのパフォーマンスとNLEの質が向上し、他のパラメータ効率的微調整技術よりも優れた結果を示しました。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1680">LoRAMoE: Alleviate World Knowledge Forgetting in Large Language Models  via MoE-Style Plugin, Shihan Dou+, arXiv'23</a>
<span class="snippet"><span>Summary</span>LoRAMoEフレームワークを提案し、教師ありファインチューニングにおける指示データの増加がLLMsの世界知識を損なう問題に対処。低ランクアダプターとルーターネットワークを用いて、世界知識を活用しつつ下流タスクの処理能力を向上させることを実証。</span>
<a class="button" href="articles/Survey.html">#Survey</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Chain-of-Thought.html">#Chain-of-Thought</a>
<span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1668">Navigate through Enigmatic Labyrinth A Survey of Chain of Thought  Reasoning: Advances, Frontiers and Future, Zheng Chu+, arXiv'23</a>
<span class="snippet"><span>Summary</span>推論はAIにおいて重要な認知プロセスであり、チェーン・オブ・ソートがLLMの推論能力を向上させることが注目されている。本論文では関連研究を体系的に調査し、手法を分類して新たな視点を提供。課題や今後の方向性についても議論し、初心者向けの導入を目指す。リソースは公開されている。</span>
<a class="button" href="articles/InformationRetrieval.html">#InformationRetrieval</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/RAG(RetrievalAugmentedGeneration).html">#RAG(RetrievalAugmentedGeneration)</a>
<span class="issue_date">Issue Date: 2024-11-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1498">Precise Zero-Shot Dense Retrieval without Relevance Labels, Luyu Gao+, ACL'23</a>
<span class="snippet"><span>Summary</span>本研究では、ゼロショット密な検索システムの構築において、仮想文書埋め込み（HyDE）を提案。クエリに基づき、指示に従う言語モデルが仮想文書を生成し、教師なしで学習されたエンコーダがこれを埋め込みベクトルに変換。実際のコーパスに基づく類似文書を取得することで、誤った詳細をフィルタリング。実験結果では、HyDEが最先端の密な検索器Contrieverを上回り、様々なタスクと言語で強力なパフォーマンスを示した。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/MultitaskLearning.html">#MultitaskLearning</a>
<a class="button" href="articles/Zero/Few/ManyShotPrompting.html">#Zero/Few/ManyShotPrompting</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/CrossLingual.html">#CrossLingual</a>
<a class="button" href="articles/Generalization.html">#Generalization</a>
<span class="issue_date">Issue Date: 2023-08-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/999">Crosslingual Generalization through Multitask Finetuning, Niklas Muennighoff+, N_A, ACL'23</a>
<span class="snippet"><span>Summary</span>マルチタスクプロンプトフィネチューニング（MTF）は、大規模な言語モデルが新しいタスクに汎化するのに役立つことが示されています。この研究では、マルチリンガルBLOOMとmT5モデルを使用してMTFを実施し、英語のプロンプトを使用して英語および非英語のタスクにフィネチューニングすることで、タスクの汎化が可能であることを示しました。さらに、機械翻訳されたプロンプトを使用してマルチリンガルなタスクにフィネチューニングすることも調査し、モデルのゼロショットの汎化能力を示しました。また、46言語の教師ありデータセットのコンポジットであるxP3も紹介されています。</span>
<span class="snippet"><span>Comment</span>英語タスクを英語でpromptingしてLLMをFinetuningすると、他の言語（ただし、事前学習で利用したコーパスに出現する言語に限る）で汎化し性能が向上することを示した模様。

<br>

![Image](https://github.com/user-attachments/assets/44e9cf6e-e80f-4092-af46-ad74c30fe59c)</span>
<a class="button" href="articles/PairWise.html">#PairWise</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Ensemble.html">#Ensemble</a>
<a class="button" href="articles/ModelMerge.html">#ModelMerge</a>
<span class="issue_date">Issue Date: 2023-06-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/708">LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and  Generative Fusion, Dongfu Jiang+, N_A, ACL'23</a>
<span class="snippet"><span>Summary</span>LLM-Blenderは、複数の大規模言語モデルを組み合わせたアンサンブルフレームワークであり、PairRankerとGenFuserの2つのモジュールから構成されています。PairRankerは、専門的なペアワイズ比較方法を使用して候補の出力間の微妙な違いを区別し、GenFuserは、上位ランクの候補をマージして改善された出力を生成します。MixInstructというベンチマークデータセットを導入し、LLM-Blenderは、個々のLLMsやベースライン手法を大幅に上回り、大きなパフォーマンス差を確立しました。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Zero/Few/ManyShotPrompting.html">#Zero/Few/ManyShotPrompting</a>
<a class="button" href="articles/Chain-of-Thought.html">#Chain-of-Thought</a>
<span class="issue_date">Issue Date: 2023-05-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/642">Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them, Mirac Suzgun+, N_A, ACL'23</a>
<span class="snippet"><span>Summary</span>BIG-Bench Hard (BBH) is a suite of 23 challenging tasks that current language models have not been able to surpass human performance on. This study focuses on applying chain-of-thought prompting to BBH tasks and found that PaLM and Codex were able to surpass human performance on 10 and 17 tasks, respectively. The study also found that CoT prompting is necessary for tasks that require multi-step reasoning and that CoT and model scale interact to enable new task performance on some BBH tasks.</span>
<span class="snippet"><span>Comment</span>単なるfewshotではなく、CoT付きのfewshotをすると大幅にBIG-Bench-hardの性能が向上するので、CoTを使わないanswer onlyの設定はモデルの能力の過小評価につながるよ、という話らしい

<br>

<img src="https://github.com/user-attachments/assets/0545214a-a267-489d-8af9-82d21e08ff6c" alt="image" loading="lazy">

<br>

<img src="https://github.com/user-attachments/assets/e5308c66-0bee-4d2c-b973-86478842b772" alt="image" loading="lazy"></span>
<a class="button" href="articles/MachineTranslation.html">#MachineTranslation</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Annotation.html">#Annotation</a>
<a class="button" href="articles/TransferLearning.html">#TransferLearning</a>
<a class="button" href="articles/MultiLingual.html">#MultiLingual</a>
<span class="issue_date">Issue Date: 2023-05-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/615">Frustratingly Easy Label Projection for Cross-lingual Transfer, Yang Chen+, N_A, ACL'23</a>
<span class="snippet"><span>Summary</span>多言語のトレーニングデータの翻訳は、クロスリンガル転移の改善に役立つスパンレベル注釈が必要なタスクでは、注釈付きスパンを翻訳されたテキストにマッピングするために追加のラベルプロジェクションステップが必要マーク-翻訳法を利用するアプローチが従来の注釈プロジェクションと比較してどのようになるかについての実証的な分析を行ったEasyProjectと呼ばれるマーク-翻訳法の最適化されたバージョンが多言語に簡単に適用でき、より複雑な単語アラインメントベースの方法を上回ることを示したすべてのコードとデータが公開される</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a>
<a class="button" href="articles/InstructionTuning.html">#InstructionTuning</a>
<span class="issue_date">Issue Date: 2023-03-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/513">Self-Instruct: Aligning Language Model with Self Generated Instructions, Wang+ （w_ Noah Smith）, Univesity of Washington, ACL'23</a>
<span class="snippet"><span>Comment</span>Alpacaなどでも利用されているself-instruction技術に関する論文概要

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/228716254-5f4d7451-a37a-4354-843d-7e4052ba230b.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

著者らが書いた175種のinstruction（タスクの定義 + 1種のinput/outputペア}のseedを元に、VanillaなGPT-3に新たなinstruction, input, outputのtupleを生成させ、学習データとして活用する研究。

<br>



<br>

ここで、instruction data I は以下のように定義される：

<br>



<br>

instruction dataは(I, X, Y)であり、モデルは最終的にM(I_t, x_t) = y_tとなるように学習したい。

<br>



<br>

I: instruction, X: input, Y: output

<br>



<br>



<br>



<br>

データ作成は以下のステップで構成される。なお、以下はすべてVanilla GPT-3を通じて行われる：

<br>



<br>

1. Instruction Generation

<br>



<br>

　task poolから8種類のinstructionを抽出し、 promptを構成し、最大8個新たなinstructionを生成させる

<br>



<br>

2. Classification Task Identification:

<br>



<br>

　生成されたinstructionがclassificationタスクか否かを判別する

<br>



<br>

3. Instance Generation

<br>



<br>

　いくつかの(I, X, Y)をpromptとして与え、I, Xに対応するYを生成するタスクを実行させる。このときinput-first approachを採用した結果（I-&gt;Xの順番で情報を与えYを生成するアプローチ）、特定のラベルに偏ったインスタンスが生成される傾向があることがわかった。このためoutput-first approachを別途採用し（I-&gt;Yの順番で情報を与え、各Yに対応するXを生成させる）、活用している。　

<br>



<br>

4. Filtering and Postprocessing

<br>



<br>

　最後に、既存のtask poolとROUGE-Lが0.7以上のinstructionは多様性がないため除外し、特定のキーワード（images, pictrues, graphs）等を含んでいるinstruction dataも除外して、task poolに追加する。

<br>



<br>



<br>



<br>

1-4をひたすら繰り返すことで、GPT-3がInstruction Tuningのためのデータを自動生成してくれる。

<br>



<br>



<br>



<br>

SELF-INSTRUCT Data

<br>



<br>

データセットの統計量

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/228745059-ecccadba-3e32-4f2a-9594-2459a922474b.png" alt="image" loading="lazy">

<br>



<br>

・52k instructions

<br>



<br>

・82k instances

<br>



<br>



<br>



<br>

Diversity

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/228745421-ba024963-ca6e-4e30-bac8-7224d413f8ab.png" alt="image" loading="lazy">

<br>



<br>

parserでinstructionを解析し、rootの名詞と動詞のペアを抽出して可視化した例。ただし、抽出できた例はたかだか全体の50%程度であり、その中で20の最もcommonなroot vertと4つのnounを可視化した。これはデータセット全体の14%程度しか可視化されていないが、これだけでも非常に多様なinstructionが集まっていることがわかる。

<br>



<br>

また、seed indstructionとROUGE-Lを測った結果、大半のデータは0.3~0.4程度であり、lexicalなoverlapはあまり大きくないことがわかる。instructionのlengthについても可視化した結果、多様な長さのinstructionが収集できている。

<br>



<br>



<br>



<br>

Quality

<br>



<br>

200種類のinstructionを抽出し、その中からそれぞれランダムで1つのインスタンスをサンプルした。そしてexpert annotatorに対して、それぞれのinstructionとinstance（input, outputそれぞれについて）が正しいか否かをラベル付けしてもらった。

<br>



<br>

ラベル付けの結果、ほとんどのinstructionは意味のあるinstructionであることがわかった。一方、生成されたinstanceはnoisyであることがわかった（ただし、このnoiseはある程度妥当な範囲である）。noisytではあるのだが、instanceを見ると、正しいformatであったり、部分的に正しかったりなど、modelを訓練する上で有用なguidanceを提供するものになっていることがわかった。

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/228746299-a0ffc115-3861-458b-a7b4-3a91ac94f8f5.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Experimental Results

<br>



<br>

Zero-shotでのNLPタスクに対する性能

<br>



<br>

SuperNIデータセットに含まれる119のタスク（1タスクあたり100 instance）に対して、zero-shot setupで評価を行なった。SELF-INSTRUCTによって、VanillaのGPT3から大幅に性能が向上していることがわかる。VanillaのGPT-3はほとんど人間のinstructionに応じて動いてくれないことがわかる。分析によると、GPT3は、大抵の場合、全く関係ない、あるいは繰り返しのテキストを生成していたり、そもそもいつ生成をstopするかがわかっていないことがわかった。

<br>



<br>



<br>



<br>

また、SuperNI向けにfinetuningされていないモデル間で比較した結果、非常にアノテーションコストをかけて作られたT0データでfinetuningされたモデルよりも高い性能を獲得した。また、人間がラベル付したprivateなデータによって訓練されたInstructGPT001にも性能が肉薄していることも特筆すべき点である。

<br>



<br>



<br>



<br>

SuperNIでfinetuningした場合については、SELF-INSTRUCTを使ったモデルに対して、さらに追加でSuperNIを与えた場合が最も高い性能を示した。

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/228751534-095578e0-550b-4e4c-9418-c74251e31d2a.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

User-Oriented Instructionsに対する汎化性能

<br>



<br>

SuperNIに含まれるNLPタスクは研究目的で提案されており分類問題となっている。ので、実践的な能力を証明するために、LLMが役立つドメインをブレスト（email writing, social media, productiveity tools, entertainment, programming等）し、それぞれのドメインに対して、instructionとinput-output instanceを作成した。また、instructionのスタイルにも多様性（e.g. instructionがlong/short、bullet points, table, codes, equationsをinput/outputとして持つ、など）を持たせた。作成した結果、252個のinstructionに対して、1つのinstanceのデータセットが作成された。これらが、モデルにとってunfamiliarなinstructionで多様なistructionが与えられたときに、どれだけモデルがそれらをhandleできるかを測定するテストベッドになると考えている。

<br>



<br>



<br>



<br>

これらのデータは、多様だがどれもが専門性を求められるものであり、自動評価指標で性能が測定できるものでもないし、crowdworkerが良し悪しを判定できるものでもない。このため、それぞれのinstructionに対するauthorに対して、モデルのy補足結果が妥当か否かをjudgeしてもらった。judgeは4-scaleでのratingとなっている：

<br>



<br>



<br>



<br>

・RATING-A: 応答は妥当で満足できる

<br>



<br>

・RATING-B: 応答は許容できるが、改善できるminor errorや不完全さがある。

<br>



<br>

・RATING-C: 応答はrelevantでinstructionに対して答えている。が、内容に大きなエラーがある。

<br>



<br>

・RATING-D: 応答はirrelevantで妥当ではない。

<br>



<br>



<br>



<br>

実験結果をみると、Vanilla GPT3はまったくinstructionに対して答えられていない。instruction-basedなモデルは高いパフォーマンスを発揮しているが、それらを上回る性能をSELF-INSTRUCTは発揮している（noisyであるにもかかわらず）。

<br>



<br>

また、GPT_SELF-INSTRUCTはInstructGPT001と性能が肉薄している。また、InstructGPT002, 003の素晴らしい性能を示すことにもなった。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/228755556-1c604ed8-11a5-4237-8f9c-a30960db807a.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Discussion and Limitation

<br>



<br>

なぜSELF-INSTRUCTがうまくいったか？

<br>



<br>

・LMに対する2つの極端な仮説を挙げている

<br>



<br>

  ・LM はpre-trainingでは十分に学習されなかった問題について学習する必要があるため、human feedbackはinstruction-tuningにおいて必要不可欠な側面である

<br>



<br>

  ・LM はpre-trainingからinstructionに既に精通しているため、human feedbackはinstruction-tuningにおいて必須ではない。 human feedbackを観察することは、pre-trainingにおける分布/目的を調整するための軽量なプロセスにすぎず、別のプロセスに置き換えることができる。

<br>



<br>



<br>



<br>

この2つの極端な仮説の間が実情であると筆者は考えていて、どちらかというと２つ目の仮説に近いだろう、と考えている。既にLMはpre-trainingの段階でinstructionについてある程度理解できているため、self-instructがうまくいったのではないかと推察している。

<br>



<br>



<br>



<br>

Broader Impact

<br>



<br>

InstructGPTは非常に強力なモデルだけど詳細が公表されておらず、APIの裏側に隠れている。この研究が、instruct-tuned modelの背後で何が起きているかについて、透明性を高める助けになると考えている。産業で開発されたモデルの構造や、その優れた性能の理由についてはほとんど理解されておらず、これらのモデルの成功の源泉を理解し、より優れた、オープンなモデルを作成するのはアカデミックにかかっている。この研究では、多様なinstructional dataの重要性を示していると考えており、大規模な人工的なデータセットは、より優れたinstructionに従うモデルを、構築するための第一歩だと考えている。

<br>



<br>



<br>



<br>

limitation

<br>



<br>

・Tail Phenomena

<br>



<br>

  ・LMの枠組みにとどまっているため、LMと同じ問題（Tail Phenomena）を抱えている

<br>



<br>

  ・low-frequencyなcontextに対してはうまくいかない問題

<br>



<br>

  ・SELF-INSTRUCTも、結局pre-trainingの段階で頻出するタスクやinstructionに対してgainがあると考えられ、一般的でなく、creativeなinstructionに対して脆弱性があると考えられる

<br>



<br>

・Dependence on laege models

<br>



<br>

  ・でかいモデルを扱えるだけのresourceを持っていないと使えないという問題がある

<br>



<br>

・Reinforcing LM biases

<br>



<br>

  ・アルゴリズムのiterationによって、問題のあるsocial _biasをより増幅してしまうことを懸念している（人種、種族などに対する偏見など）。また、アルゴリズムはバランスの取れたラベルを生成することが難しい。1のprompt

<br>



<br>

<img width="801" alt="image" src="https://user-images.githubusercontent.com/12249301/228717376-62648df4-e587-49f7-8e71-afd1b2269e90.png">

<br>



<br>

2のprompt

<br>



<br>

<img width="871" alt="image" src="https://user-images.githubusercontent.com/12249301/228717413-115f8ccf-b85e-4530-b489-cbf1de69341b.png">

<br>



<br>

3のprompt（input-first-approach）

<br>



<br>

<img width="853" alt="image" src="https://user-images.githubusercontent.com/12249301/228717477-58b44a4e-ce44-452f-9b3a-4a348584e40f.png">

<br>



<br>

3のprompt（output-first approach）

<br>



<br>

&lt;img width=\"803\" alt=\"image\" src=\"https://user-images.githubusercontent.com/12249301/228717535-8717405c-bdaf-455c-9d4b-480bf6494abe.png\"&gt;※ GPT3をfinetuningするのに、Instruction Dataを使った場合\$338かかったっぽい。安い・・・。LLMを使うだけでここまで研究ができる時代がきた（最近は|現在は）プロプライエタリなLLMの出力を利用して競合するモデルを訓練することは多くの場合禁止されているので注意。</span>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/KnowledgeEditing.html">#KnowledgeEditing</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<a class="button" href="articles/FactualKnowledge.html">#FactualKnowledge</a>
<a class="button" href="articles/Encoder.html">#Encoder</a>
<span class="issue_date">Issue Date: 2024-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1332">Knowledge Neurons in Pretrained Transformers, Damai Dai+, N_A, ACL'22, 2022.05</a>
<span class="snippet"><span>Summary</span>大規模な事前学習言語モデルにおいて、事実知識の格納方法についての研究を行いました。具体的には、BERTのfill-in-the-blank cloze taskを用いて、関連する事実を表現するニューロンを特定しました。また、知識ニューロンの活性化と対応する事実の表現との正の相関を見つけました。さらに、ファインチューニングを行わずに、知識ニューロンを活用して特定の事実知識を編集しようと試みました。この研究は、事前学習されたTransformers内での知識の格納に関する示唆に富んでおり、コードはhttps://github.com/Hunter-DDM/knowledge-neuronsで利用可能です。</span>
<span class="snippet"><span>Comment</span>1108 日本語解説: https://speakerdeck.com/kogoro/knowledge-neurons-in-pretrained-transformers-for-snlp2022関連:

<br>

・2140上記資料によると、特定の知識を出力する際に活性化する知識ニューロンを特定する手法を提案。MLMを用いたclozeタスクによる実験で[MASK]部分に当該知識を出力する実験をした結果、知識ニューロンの重みをゼロとすると性能が著しく劣化し、値を2倍にすると性能が改善するといった傾向がみられた。　ケーススタディとして、知識の更新と、知識の削除が可能かを検証。どちらとも更新・削除がされる方向性[^1]へモデルが変化した。

<br>



<br>

また、知識ニューロンはTransformerの層の深いところに位置している傾向にあり、異なるrelationを持つような関係知識同士では共有されない傾向にある模様。

<br>



<br>

[^1]: 他の知識に影響を与えず、完璧に更新・削除できたわけではない。知識の更新・削除に伴いExtrinsicな評価によって性能向上、あるいはPerplexityが増大した、といった結果からそういった方向性へモデルが変化した、という話</span>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/BeamSearch.html">#BeamSearch</a>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2023-08-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/997">BRIO: Bringing Order to Abstractive Summarization, Yixin Liu+, N_A, ACL'22</a>
<span class="snippet"><span>Summary</span>従来の抽象的要約モデルでは、最尤推定を使用して訓練されていましたが、この方法では複数の候補要約を比較する際に性能が低下する可能性があります。そこで、非確定論的な分布を仮定し、候補要約の品質に応じて確率を割り当てる新しい訓練パラダイムを提案しました。この手法により、CNN/DailyMailとXSumのデータセットで最高の結果を達成しました。さらに、モデルが候補要約の品質とより相関のある確率を推定できることも示されました。</span>
<span class="snippet"><span>Comment</span>ビーム内のトップがROUGEを最大化しているとは限らなかったため、ROUGEが最大となるような要約を選択するようにしたら性能爆上げしましたという研究。

<br>

実質現在のSoTA</span>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/PEFT(Adaptor/LoRA).html">#PEFT(Adaptor/LoRA)</a>
<span class="issue_date">Issue Date: 2021-09-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/405">Prefix-Tuning: Optimizing Continuous Prompts for Generation, Lisa+ （Percy Liang）, Stanford University, ACL'21</a>
<span class="snippet"><span>Comment</span>言語モデルをfine-tuningする際，エンコード時に「接頭辞」を潜在表現として与え，「接頭辞」部分のみをfine-tuningすることで（他パラメータは固定），より少量のパラメータでfine-tuningを実現する方法を提案．接頭辞を潜在表現で与えるこの方法は，GPT-3のpromptingに着想を得ている．fine-tuningされた接頭辞の潜在表現のみを配布すれば良いので，非常に少量なパラメータでfine-tuningができる．

<br>



<br>



<br>



<br>

table-to-text, summarizationタスクで，一般的なfine-tuningやAdapter（レイヤーの間にアダプターを挿入しそのパラメータだけをチューニングする手法）といった効率的なfine-tuning手法と比較．table-to-textでは、250k (元のモデルの 0.1%) ほどの数のパラメータを微調整するだけで、全パラメータをfine-tuningするのに匹敵もしくはそれ以上の性能を達成．

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/132679791-87ad130d-8a7e-4549-a311-f84400a3787b.png" alt="image" loading="lazy">

<br>



<br>

Hugging Faceの実装を利用したと論文中では記載されているが，fine-tuningする前の元の言語モデル（GPT-2）はどのように準備したのだろうか．Hugging Faceのpretrained済みのGPT-2を使用したのだろうか．autoregressive LM (GPT-2)と，encoder-decoderモデル（BART）へPrefix Tuningを適用する場合の模式図

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/132681736-0ea4b13f-71cb-41ba-ae17-027e8bf54cc0.png" alt="image" loading="lazy">

<br>



<br>

</span>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Abstractive.html">#Abstractive</a>
<a class="button" href="articles/Factuality.html">#Factuality</a>
<a class="button" href="articles/Faithfulness.html">#Faithfulness</a>
<span class="issue_date">Issue Date: 2025-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2203">[Paper Note] On Faithfulness and Factuality in Abstractive Summarization, Joshua Maynez+, ACL'20</a>
<span class="snippet"><span>Summary</span>抽象的な文書要約における言語モデルの限界を分析し、これらのモデルが入力文書に対して忠実でない内容を生成する傾向が高いことを発見。大規模な人間評価を通じて、生成される幻覚の種類を理解し、すべてのモデルで相当量の幻覚が確認された。事前学習されたモデルはROUGE指標だけでなく、人間評価でも優れた要約を生成することが示された。また、テキストの含意測定が忠実性と良好に相関することが明らかになり、自動評価指標の改善の可能性を示唆。</span>
<span class="snippet"><span>Comment</span>文書要約の文脈において `hallucination` について説明されている。

<br>

・1044 

<br>



<br>

が `hallucination` について言及する際に引用している。</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/ReviewGeneration.html">#ReviewGeneration</a>
<span class="issue_date">Issue Date: 2021-03-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/343">Unsupervised Opinion Summarization as Copycat-Review Generation, Bražinskas, ACL'20</a>
<span class="snippet"><span>Summary</span>意見要約は、製品レビューから主観的情報を自動的に要約するタスクであり、従来の研究は抽出的手法に焦点を当てていたが、本研究では新しい文を生成する抽象的要約を提案する。教師なし設定での生成モデルを定義し、新規性を制御しながら合意された意見を反映する要約を生成する。階層的変分オートエンコーダモデルを用い、実験により流暢で一貫性のある要約が生成できることを示した。</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<a class="button" href="articles/Attention.html">#Attention</a>
<a class="button" href="articles/LongSequence.html">#LongSequence</a>
<a class="button" href="articles/PositionalEncoding.html">#PositionalEncoding</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2359">[Paper Note] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context, Zihang Dai+, ACL'19</a>
<span class="snippet"><span>Summary</span>Transformer-XLは、固定長のコンテキストを超えた長期的な依存関係を学習する新しいニューラルアーキテクチャで、セグメントレベルの再帰メカニズムと新しい位置エンコーディングを採用。これにより、RNNより80%、従来のTransformersより450%長い依存関係を学習し、評価時には最大1,800倍の速度向上を実現。enwiki8やWikiText-103などで最先端のパフォーマンスを達成し、数千トークンの一貫したテキスト生成も可能。コードとモデルはTensorflowとPyTorchで利用可能。</span>
<span class="snippet"><span>Comment</span>日本語解説:

<br>

・329以下が定式化で、一つ前のセグメントのトークン・layerごとのhidden stateを、現在のセグメントの対応するトークンとlayerのhidden stateにconcatし（過去のセグメントに影響を与えないように勾配を伝搬させないStop-Gradientを適用する）、QKVのうち、KVの計算に活用する。また、絶対位置エンコーディングを利用するとモデルがセグメント間の時系列的な関係を認識できなくなるため、位置エンコーディングには相対位置エンコーディングを利用する。これにより、現在のセグメントのKVが一つ前のセグメントによって条件づけられ、contextとして考慮することが可能となり、セグメント間を跨いだ依存関係の考慮が実現される。

<br>

・<img width="634" height="563" alt="Image" src="https://github.com/user-attachments/assets/f13106d3-68ab-4126-8cdc-4c99075bc35a">

<br>



<br>

<img width="1278" height="360" alt="Image" src="https://github.com/user-attachments/assets/1435151a-2802-470b-a3c5-e7b4ea66ae05"></span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/CommentGeneration.html">#CommentGeneration</a>
<a class="button" href="articles/Personalization.html">#Personalization</a>
<span class="issue_date">Issue Date: 2019-09-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/323">Automatic Generation of Personalized Comment Based on User Profile, Zeng+, arXiv'19</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/CommentGeneration.html">#CommentGeneration</a>
<span class="issue_date">Issue Date: 2019-08-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/322">Coherent Comment Generation for Chinese Articles with a Graph-to-Sequence Model, Li+ ,ACL'19</a>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/ReviewGeneration.html">#ReviewGeneration</a>
<span class="issue_date">Issue Date: 2019-08-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/316">Automatic Generation of Personalized Comment Based on User Profile, Zeng+, ACL'19 Student Research Workshop</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/DialogueGeneration.html">#DialogueGeneration</a>
<span class="issue_date">Issue Date: 2019-01-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/302">Training Millions of Personalized Dialogue Agents, Mazaré, ACL'19</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/DataToTextGeneration.html">#DataToTextGeneration</a>
<a class="button" href="articles/TabularData.html">#TabularData</a>
<a class="button" href="articles/Encoder-Decoder.html">#Encoder-Decoder</a>
<span class="issue_date">Issue Date: 2025-08-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2367">Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data, Jhamtani+, ACL'18</a>
<span class="snippet"><span>Comment</span>データセットの日本語解説（過去の自分の資料）:https://speakerdeck.com/akihikowatanabe/data-to-text-datasetmatome-summary-of-data-to-text-datasets?slide=66</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/ReviewGeneration.html">#ReviewGeneration</a>
<span class="issue_date">Issue Date: 2019-04-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/306">Personalized Review Generation by Expanding Phrases and Attending on Aspect-Aware Representations, Ni+, ACL'18</a>
<span class="snippet"><span>Comment</span><img src="https://user-images.githubusercontent.com/12249301/56010165-8fd44a00-5d1d-11e9-8cad-81a5178d95d2.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Personalized Review Generationタスクを、user, item, short phraseがgivenな時に、それを考慮して完全なレビューを生成するタスクとして定義。

<br>



<br>

short phraseとしては、item titleやreview summaryなどを利用している。

<br>



<br>

アイテムのaspectを考慮してレビューを生成できる点が新しい。

<br>



<br>

モデルとしては、aspect-awareなrepresentationを学習することによって、ユーザ・アイテムのaspectに関する嗜好（e.g. どの部分について言及したいか、など）を捉えたレビューを生成できるようにしている。

<br>



<br>

各aspectには代表的な単語が紐づいており、aspectに紐づく単語の生成確率をaspect-aware representationから求めたattentionによって制御し、生成時に下駄を履かせている。PyTorch実装：https://github.com/nijianmo/textExpansion/tree/master/expansionNet</span>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/ReviewGeneration.html">#ReviewGeneration</a>
<a class="button" href="articles/Personalization.html">#Personalization</a>
<span class="issue_date">Issue Date: 2018-07-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/276">Personalized Review Generation by Expanding Phrases and Attending on Aspect-Aware Representations, Ni+, ACL'18</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/DialogueGeneration.html">#DialogueGeneration</a>
<span class="issue_date">Issue Date: 2018-02-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/255">Personalizing Dialogue Agents: I have a dog, do you have pets too?, Zhang+, ACL'18</a>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Document.html">#Document</a>
<a class="button" href="articles/Supervised.html">#Supervised</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/209">Coarse-to-Fine Attention Models for Document Summarization, Ling+ （with Rush）, ACL'17 Workshop on New Frontiers in Summarization</a>
<a class="button" href="articles/Single.html">#Single</a>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Document.html">#Document</a>
<a class="button" href="articles/Supervised.html">#Supervised</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Abstractive.html">#Abstractive</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/135">Get To The Point: Summarization with Pointer-Generator Networks, See+, ACL'17</a>
<span class="snippet"><span>Comment</span>解説スライド：https://www.slideshare.net/akihikowatanabe3110/get-to-the-point-summarization-with-pointergenerator-networks/1単語の生成と単語のコピーの両方を行えるハイブリッドなニューラル文書要約モデルを提案。

<br>



<br>

同じ単語の繰り返し現象(repetition)をなくすために、Coverage Mechanismも導入した。

<br>



<br>



<br>



<br>

136 などと比較するとシンプルなモデル。一般的に、PointerGeneratorと呼ばれる。

<br>



<br>

OpenNMTなどにも実装されている: https://opennmt.net/OpenNMT-py/_modules/onmt/modules/copy_generator.html（参考）Pointer Generator Networksで要約してみる：

<br>



<br>

https://qiita.com/knok/items/9a74430b279e522d5b93</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/90">Multi-Task Video Captioning with Video and Entailment Generation, Pasunuru+, ACL'17</a>
<span class="snippet"><span>Comment</span>解説スライド：https://www.slideshare.net/HangyoMasatsugu/hangyo-acl-paperreading2017multitask-video-captioning-with-video-and-entailment-generation/1multitask learningで動画（かなり短め）のキャプション生成を行なった話

<br>



<br>

(2025.05.12)

<br>

上記解説資料中のスクショがいくつか掲載されていましたが削除しました。</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/82">Learning to skim text, Yu+, ACL'17</a>
<span class="snippet"><span>Comment</span>解説スライド：http://www.lr.pi.titech.ac.jp/~haseshun/acl2017suzukake/slides/07.pdfRNNにおいて重要な部分以外は読み飛ばすことで効率を向上させる研究。いくつ読み飛ばすかも潜在変数として一緒に学習する。潜在変数（離散変数）なので、普通に尤度最大化するやり方では学習できず、おまけに離散変数なのでバックプロパゲーション使えないので、強化学習で学習する。

<br>



<br>



<br>



<br>

Vanilla LSTMと比較し、色々なタスクで実験した結果、性能も（少し）上がるし、スピードアップもする。</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Embeddings.html">#Embeddings</a>
<a class="button" href="articles/Analysis.html">#Analysis</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Word.html">#Word</a>
<span class="issue_date">Issue Date: 2017-12-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/79">Skip-Gram – Zipf + Uniform = Vector Additivity, Gittens+, ACL'17</a>
<span class="snippet"><span>Comment</span>解説スライド：http://www.lr.pi.titech.ac.jp/~haseshun/acl2017suzukake/slides/09.pdfEmbeddingの加法構成性（e.g. man+royal=king）を理論的に理由づけ

<br>



<br>

（解説スライドより）</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/MachineTranslation.html">#MachineTranslation</a>
<a class="button" href="articles/Pocket.html">#Pocket</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/67">What do Neural Machine Translation Models Learn about Morphology?, Yonatan Belinkov+, ACL'17</a>
<span class="snippet"><span>Comment</span>http://www.lr.pi.titech.ac.jp/~haseshun/acl2017suzukake/slides/06.pdf

<br>



<br>

(2025.05.12追記)

<br>

上記は2017年にすずかけ台で開催されたACL 2017読み会での解説スライドです。</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/MachineTranslation.html">#MachineTranslation</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/66">Sequence-to-Dependency Neural Machine Translation, Wu+, ACL'17</a>
<a class="button" href="articles/PersonalizedDocumentSummarization.html">#PersonalizedDocumentSummarization</a>
<a class="button" href="articles/InteractivePersonalizedSummarization.html">#InteractivePersonalizedSummarization</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/IntegerLinearProgramming%20(ILP).html">#IntegerLinearProgramming (ILP)</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/7">Joint Optimization of User-desired Content in Multi-document Summaries by Learning from User Feedback, P.V.S+, ACL'17, 2017.08</a>
<span class="snippet"><span>Comment</span>一言で言うと

<br>



<br>

ユーザとインタラクションしながら重要なコンセプトを決め、そのコンセプトが含まれるようにILPな手法で要約を生成するPDS手法。Interactive Personalized Summarizationと似ている（似ているが引用していない、引用した方がよいのでは）。

<br>



<br>



<br>



<br>

手法

<br>



<br>

要約モデルは既存のMDS手法を採用。Concept-based ILP Summarization

<br>



<br>



<br>



<br>

フィードバックをユーザからもらう際は、要約を生成し、それをユーザに提示。提示した要約から重要なコンセプトをユーザに選択してもらう形式（ユーザが重要と判断したコンセプトには定数重みが与えられる）。

<br>



<br>

ユーザに対して、τ回フィードバックをもらうまでは、フィードバックをもらっていないコンセプトの重要度が高くなるようにし、フィードバックをもらったコンセプトの重要度が低くなるように目的関数を調整する。これにより、まだフィードバックを受けていないコンセプトが多く含まれる要約が生成されるため、これをユーザに提示することでユーザのフィードバックを得る。τ回を超えたら、ユーザのフィードバックから決まったweightが最大となるように目的関数を修正する。

<br>



<br>



<br>



<br>

ユーザからコンセプトのフィードバックを受ける際は、効率的にフィードバックを受けられると良い（最小のインタラクションで）。そこで、Active Learningを導入する。コンセプトの重要度の不確実性をSVMで判定し、不確実性が高いコンセプトを優先的に含むように目的関数を修正する手法（AL）、SVMで重要度が高いと推定されたコンセプトを優先的に要約に含むように目的関数を修正する手法（AL+）を提案している。

<br>



<br>



<br>



<br>

評価

<br>



<br>

oracle-based approachというものを使っている。要は、要約をシステムが提示しリファレンスと被っているコンセプトはユーザから重要だとフィードバックがあったコンセプトだとみなすというもの。

<br>



<br>

評価結果を見ると、ベースラインのMDSと比べてupper bound近くまでROUGEスコアが上がっている。フィードバックをもらうためのイテレーションは最大で１０回に絞っている模様（これ以上ユーザとインタラクションするのは非現実的）。

<br>



<br>



<br>



<br>

実際にユーザがシステムを使用する場合のコンテキストに沿った評価になっていないと思う。

<br>



<br>

この評価で示せているのは、ReferenceSummary中に含まれる単語にバイアスをかけて要約を生成していくと、ReferenceSummaryと同様な要約が最終的に作れます、ということと、このときPool-basedなActiveLearningを使うと、より少ないインタラクションでこれが実現できますということ。

<br>



<br>

これを示すのは別に良いと思うのだが、feedbackをReferenceSummaryから与えるのは少し現実から離れすぎている気が。たとえばユーザが新しいことを学ぶときは、ある時は一つのことを深堀し、そこからさらに浅いところに戻って別のところを深堀するみたいなプロセスをする気がするが、この深堀フェーズなどはReferenceSummaryからのフィードバックからでは再現できないのでは。

<br>



<br>



<br>



<br>

所感

<br>



<br>

評価が甘いと感じる。十分なサイズのサンプルを得るのは厳しいからorable-based approachとりましたと書いてあるが、なんらかの人手評価もあったほうが良いと思う。

<br>



<br>



<br>



<br>

ユーザに数百単語ものフィードバックをもらうというのはあまり現時的ではない気が。

<br>



<br>



<br>



<br>

oracle-based approachでユーザのフィードバックをシミュレーションしているが、oracleの要約は、人がそのドキュメントクラスタの内容を完璧に理解した上で要約しているものなので、これを評価に使うのも実際のコンテキストと違うと思う。実際にユーザがシステムを使うときは、ドキュメントクラスタの内容なんてなんも知らないわけで、そのユーザからもらえるフィードバックをoracle-based approachでシミュレーションするのは無理がある。仮に、ドキュメントクラスタの内容を完璧に理解しているユーザのフィードバックをシミュレーションするというのなら、わかる。が、そういうユーザのために要約作って提示したいわけではないはず。</span>
<a class="button" href="articles/RecommenderSystems.html">#RecommenderSystems</a>
<a class="button" href="articles/Citations.html">#Citations</a>
<a class="button" href="articles/LearningToRank.html">#LearningToRank</a>
<span class="issue_date">Issue Date: 2018-01-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/181">News Citation Recommendation with Implicit and Explicit Semantics, Peng+, ACL'16</a>
<span class="snippet"><span>Comment</span>target text中に記述されているイベントや意見に対して、それらをサポートするような他のニュース記事を推薦する研究。

<br>



<br>



<br>



<br>

たとえば、target text中に「北朝鮮が先日ミサイルの発射に失敗したが...」、といった記述があったときに、このイベントについて報道しているニュース記事を推薦するといったことを、target text中の様々なcontextに対して行う。

<br>



<br>



<br>



<br>

このようなシステムの利用により、target textの著者の執筆支援（自身の主張をサポートするためのreferenceの自動獲得）や、target textの読者の読解支援（text中の記述について詳細な情報を知りたい場合に、検索の手間が省ける）などの利点があると主張。

<br>



<br>



<br>



<br>

タスクとしては、target text中のあるcontextと、推薦の候補となるニュース記事の集合が与えられたときに、ニュース記事をre-rankingする タスク。

<br>



<br>



<br>



<br>

提案手法はシンプルで、contextとニュース記事間で、様々な指標を用いてsimilarityを測り、それらをlearning-to-rankで学習した重みで組み合わせてre-rankingを行うだけ。 similarityを測る際は、表記揺れや曖昧性の問題に対処するためにEmbeddingを用いる手法と、groundingされたentityの情報を用いる手法を提案。

<br>



<br>



<br>



<br>

Bing news中のAnchor textと、hyperlink先のニュース記事の対から、contextと正解ニュース記事の対を取得し、30000件規模の実験データを作成し、評価。その結果、baselineよりも提案手法の性能が高いことを示した。</span>
<a class="button" href="articles/Single.html">#Single</a>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Document.html">#Document</a>
<a class="button" href="articles/Supervised.html">#Supervised</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Abstractive.html">#Abstractive</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/136">Incorporating Copying Mechanism in Sequence-to-Sequence Learning, Gu+, ACL'16</a>
<span class="snippet"><span>Comment</span>解説スライド：https://www.slideshare.net/akihikowatanabe3110/incorporating-copying-mechanism-in-sequene-to-sequence-learning単語のコピーと生成、両方を行えるネットワークを提案。

<br>



<br>

location based addressingなどによって、生成された単語がsourceに含まれていた場合などに、copy-mode, generate-modeを切り替えるような仕組みになっている。

<br>



<br>



<br>



<br>

65 と同じタイミングで発表</span>
<a class="button" href="articles/Single.html">#Single</a>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Document.html">#Document</a>
<a class="button" href="articles/Supervised.html">#Supervised</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Extractive.html">#Extractive</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/131">Neural Summarization by Extracting Sentences and Words, Cheng+, ACL'16</a>
<span class="snippet"><span>Comment</span>ExtractiveかつNeuralな単一文書要約ならベースラインとして使用した方がよいかも</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Sentence.html">#Sentence</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/76">Larger-context language modelling with recurrent neural networks, Wang+, ACL'16</a>
<span class="snippet"><span>Comment</span>概要

<br>



<br>

通常のNeural Language Modelはsentence間に独立性の仮定を置きモデル化されているが、この独立性を排除し、preceding sentencesに依存するようにモデル化することで、言語モデルのコーパスレベルでのPerplexityが改善したという話。提案した言語モデルは、contextを考慮することで特に名詞や動詞、形容詞の予測性能が向上。Late-Fusion methodと呼ばれるRNNのoutputの計算にcontext vectorを組み込む手法が、Perplexityの改善にもっとも寄与していた。

<br>



<br>



<br>



<br>

手法

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34412713-1e16da94-ec22-11e7-830c-0d0b6247207c.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

sentence間の独立性を排除し、Corpusレベルのprobabilityを下図のように定義。（普通はP(Slが条件付けされていない)）

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34412980-e2425afa-ec23-11e7-86cd-148f85dccc07.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

preceding sentence (context)をモデル化するために、3種類の手法を提案。

<br>



<br>



<br>



<br>

[1. bag-of-words context]

<br>



<br>

　ナイーブに、contextに現れた単語の（単一の）bag-of-wordsベクトルを作り、linear layerをかませてcontext vectorを生成する手法。

<br>



<br>



<br>



<br>

[2. context recurrent neural network]

<br>



<br>

　preceding sentencesをbag-of-wordsベクトルの系列で表現し、これらのベクトルをsequentialにRNN-LSTMに読み込ませ、最後のhidden stateをcontext vectorとする手法。これにより、sentenceが出現した順番が考慮される。

<br>



<br>



<br>



<br>

[3. attention based context representation]

<br>



<br>

　Attentionを用いる手法も提案されており、context recurrent neural networkと同様にRNNにbag-of-wordsのsequenceを食わせるが、各時点におけるcontext sentenceのベクトルを、bi-directionalなRNNのforward, backward stateをconcatしたもので表現し、attention weightの計算に用いる。context vectorは1, 2ではcurrent sentence中では共通のものを用いるが、attention basedな場合はcurrent sentenceの単語ごとに異なるcontext vectorを生成して用いる。

<br>



<br>



<br>



<br>

生成したcontext vectorをsentence-levelのRNN言語モデルに組み合わせる際に、二種類のFusion Methodを提案している。

<br>



<br>



<br>



<br>

[1. Early Fusion]

<br>



<br>

　ナイーブに、RNNLMの各時点でのinputにcontext vectorの情報を組み込む方法。

<br>



<br>

[2. Late Fusion]

<br>



<br>

　よりうまくcontext vectorの情報を組み込むために、current sentence内の単語のdependency(intra-sentence dependency)と、current sentenceとcontextの関係を別々に考慮する。context vectorとmemory cellの情報から、context vector中の不要箇所をフィルタリングしたcontrolled context vectorを生成し、LSTMのoutputの計算に用いる。Later Fusionはシンプルだが、corpusレベルのlanguage modelingの勾配消失問題を緩和することもできる。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34413898-99efbaf8-ec29-11e7-94f5-db82eee399b3.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

評価

<br>



<br>

IMDB, BBC, PennTreebank, Fil9 (cleaned wikipedia corpus)の4種類のデータで学習し、corpus levelでPerplexityを測った。

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34414121-b75b2996-ec2a-11e7-9716-dbbb9006b1b5.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

Late FusionがPerplexityの減少に大きく寄与している。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/34414218-596b373a-ec2b-11e7-85ad-cf98df04ce57.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

PoSタグごとのperplexityを測った結果、contextを考慮した場合に名詞や形容詞、動詞のPerplexityに改善が見られた。一方、Coordinate Conjungtion (And, Or, So, Forなど)や限定詞、Personal Pronouns (I, You, It, Heなど)のPerplexityは劣化した。前者はopen-classな内容語であり、後者はclosed-classな機能語である。機能語はgrammaticalなroleを決めるのに対し、内容語はその名の通り、sentenceやdiscourseの内容を決めるものなので、文書の内容をより捉えることができると考察している。</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/MachineTranslation.html">#MachineTranslation</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/65">Pointing the unknown words, Gulcehre+, ACL'16</a>
<span class="snippet"><span>Comment</span>テキストを生成する際に、source textからのコピーを行える機構を導入することで未知語問題に対処した話CopyNetと同じタイミングで（というか同じconferenceで）発表</span>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/IJCNLP.html">#IJCNLP</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2018-03-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/266">Unsupervised prediction of acceptability judgements, Lau+, ACL-IJCNLP'15</a>
<span class="snippet"><span>Comment</span>文のacceptability（容認度）論文。

<br>



<br>

文のacceptabilityとは、native speakerがある文を読んだときに、その文を正しい文として容認できる度合いのこと。

<br>



<br>

acceptabilityスコアが低いと、Readabilityが低いと判断できる。

<br>



<br>

言語モデルをトレーニングし、トレーニングした言語モデルに様々な正規化を施すことで、acceptabilityスコアを算出する。</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2018-02-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/257">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks, Tai+, ACL'15</a>
<span class="snippet"><span>Comment</span>Tree-LSTM論文</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/Document.html">#Document</a>
<a class="button" href="articles/Embeddings.html">#Embeddings</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/RepresentationLearning.html">#RepresentationLearning</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/74">A hierarchical neural autoencoder for paragraphs and documents, Li+, ACL'15</a>
<span class="snippet"><span>Comment</span>複数文を生成(今回はautoencoder)するために、standardなseq2seq LSTM modelを、拡張したという話。

<br>



<br>



<br>



<br>

要は、paragraph/documentのrepresentationが欲しいのだが、アイデアとしては、word-levelの情報を扱うLSTM layerとsentenc-levelの情報を扱うLSTM layerを用意し、それらのcompositionによって、paragraph/documentを表現しましたという話。

<br>



<br>



<br>



<br>

sentence-levelのattentionを入れたらよくなっている。

<br>



<br>



<br>



<br>

trip advisorのreviewとwikipediaのparagraphを使ってtrainingして、どれだけ文書を再構築できるか実験。

<br>



<br>

MetricはROUGE, BLEUおよびcoherence(sentence order代替)を測るために、各sentence間のgapがinputとoutputでどれだけ一致しているかで評価。

<br>



<br>



<br>



<br>

hierarchical lstm with attention &gt; hierarchical lstm &gt; standard lstm の順番で高性能。

<br>



<br>



<br>



<br>

学習には、tesla K40を積んだマシンで、standard modelが2-3 weeks, hierarchical modelsが4-6週間かかるらしい。</span>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/Others.html">#Others</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/DataToTextGeneration.html">#DataToTextGeneration</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/108">Comparing Multi-label Classification with Reinforcement Learning for Summarization of Time-series Data, Gkatzia+, ACL'14</a>
<a class="button" href="articles/Multi.html">#Multi</a>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Extractive.html">#Extractive</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/58">Hierarchical Summarization: Scaling Up Multi-Document Summarization, Christensen+, ACL'14</a>
<span class="snippet"><span>Comment</span>概要

<br>



<br>

だいぶ前に読んだ。好きな研究。

<br>



<br>

テキストのsentenceを階層的にクラスタリングすることで、抽象度が高い情報から、関連する具体度の高いsentenceにdrill downしていけるInteractiveな要約を提案している。

<br>



<br>



<br>



<br>

手法

<br>



<br>

通常のMDSでのデータセットの規模よりも、実際にMDSを使う際にはさらに大きな規模のデータを扱わなければならないことを指摘し（たとえばNew York Timesで特定のワードでイベントを検索すると数千、数万件の記事がヒットしたりする）そのために必要な事項を検討。

<br>



<br>

これを実現するために、階層的なクラスタリングベースのアプローチを提案。

<br>



<br>

提案手法では、テキストのsentenceを階層的にクラスタリングし、下位の層に行くほどより具体的な情報になるようにsentenceを表現。さらに、上位、下位のsentence間にはエッジが張られており、下位に紐付けられたsentence

<br>



<br>

は上位に紐付けられたsentenceの情報をより具体的に述べたものとなっている。

<br>



<br>

これを活用することで、drill down型のInteractiveな要約を実現。</span>
<a class="button" href="articles/Multi.html">#Multi</a>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Dataset.html">#Dataset</a>
<a class="button" href="articles/QueryBiased.html">#QueryBiased</a>
<a class="button" href="articles/Extractive.html">#Extractive</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/57">Query-Chain Focused Summarization, Baumel+, ACL'14</a>
<span class="snippet"><span>Comment</span>（管理人が作成した過去の紹介資料）

<br>

[Query-Chain Focused Summarization.pdf](https://github.com/AkihikoWatanabe/paper_notes/files/1590916/Query-Chain.Focused.Summarization.pdf)

<br>



<br>

上記スライドは私が当時作成した論文紹介スライドです。スライド中のスクショは説明のために論文中のものを引用しています。</span>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/Others.html">#Others</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/ConceptToTextGeneration.html">#ConceptToTextGeneration</a>
<a class="button" href="articles/IJCNLP.html">#IJCNLP</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/113">Learning semantic correspondences with less supervision, Liang+, ACL-IJCNLP'09</a>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a>
<a class="button" href="articles/DomainAdaptation.html">#DomainAdaptation</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/126">Frustratingly easy domain adaptation, Daum'e, ACL'07</a>
<span class="snippet"><span>Comment</span><img src="https://user-images.githubusercontent.com/12249301/34462211-f3428130-ee81-11e7-8a06-36e66bd19b2f.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

domain adaptationをする際に、Source側のFeatureとTarget側のFeatureを上式のように、Feature Vectorを拡張し独立にコピーし表現するだけで、お手軽にdomain adaptationができることを示した論文。

<br>



<br>



<br>



<br>

イメージ的には、SourceとTarget、両方に存在する特徴は、共通部分の重みが高くなり、Source, Targetドメイン固有の特徴は、それぞれ拡張した部分のFeatureに重みが入るような感じ。</span>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/RuleBased.html">#RuleBased</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/DataToTextGeneration.html">#DataToTextGeneration</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/105">Design of a knowledge-based report generator, Kukich, ACL'83</a>
<span class="snippet"><span>Comment</span>タスク

<br>



<br>

numerical stock market dataからstock market reportsを生成，我々と同様なタスク．システム名: ANA

<br>



<br>



<br>



<br>

手法概要

<br>



<br>

ルールベースな手法，

<br>



<br>

1) fact-generator,

<br>



<br>

2) message generator, 

<br>



<br>

3) discourse organizer, 

<br>



<br>

4) text generatorの4コンポーネントから成る． 

<br>



<br>



<br>



<br>

2), 3), 4)はそれぞれ120, 16, 109個のルールがある. 4)ではphrasal dictionaryも使う． 

<br>



<br>

1)では，入力されたpriceデータから，closing averageを求めるなどの数値的な演算などを行う. 

<br>



<br>

2)では，1)で計算された情報に基づいて，メッセージの生成を行う(e.g. market was mixed). 

<br>



<br>

3)では，メッセージのparagraph化，orderの決定，priorityの設定などを行う． 

<br>



<br>

4)では，辞書からフレーズを選択したり，適切なsyntactic formを決定するなどしてテキストを生成．Data2Textの先駆け論文。引用すべし。多くの研究で引用されている。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/LanguageModel.html">#LanguageModel</a>
<a class="button" href="articles/SyntheticData.html">#SyntheticData</a>
<a class="button" href="articles/Slide.html">#Slide</a>
<span class="issue_date">Issue Date: 2025-08-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2361">Synthetic Data in the Era of LLMs, Tutorial at ACL 2025</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/gneubig/status/1952876206388359186?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/Tutorial.html">#Tutorial</a>
<a class="button" href="articles/Slide.html">#Slide</a>
<span class="issue_date">Issue Date: 2025-05-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1948">ACL 2024 参加報告, 張+, 株式会社サイバーエージェント AI Lab, 2024.08</a>
<span class="snippet"><span>Comment</span>業界のトレンドを把握するのに非常に参考になる:

<br>

・Reasoning, KnowledgeGraph, KnowledgeEditing, Distillation

<br>

・PEFT, Bias, Fairness, Ethics

<br>

・Multimodal(QA, Benchmarking, Summarization)

<br>

などなど。

<br>



<br>

投稿数5000件は多いなあ…</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/Transformer.html">#Transformer</a>
<span class="issue_date">Issue Date: 2021-06-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/384">FastSeq: Make Sequence Generation Faster, Yan+, ACL’21</a>
<span class="snippet"><span>Comment</span>BART, DistilBART, T5, GPT2等のさまざまなTransformer-basedな手法で、4-9倍Inference speedを向上させる手法を提案。</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2021-06-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/372">Incorporating Copying Mechanism in Sequence-to-Sequence Learning, Gu+, ACL’16</a>
<span class="snippet"><span>Comment</span>371 と同様コピーメカニズムを提案した論文。Joint Copy ModelやCOPYNETと呼ばれる。

<br>



<br>

次の単語が "生成" されるのか "コピー" されるのかをスコアリングし、各単語がコピーされる確率と生成される確率をMixtureした同時確率分布で表現する（ 207 等でも説明されている）。

<br>



<br>

コピーメカニズムを導入せるなら引用すべき。

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/120571719-ad148700-c455-11eb-8e93-8d9be799aad5.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

コピーメカニズム部分の説明

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/120571852-efd65f00-c455-11eb-9063-872103738e2f.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/120571874-fa90f400-c455-11eb-885f-5b1a08d7d528.png" alt="image" loading="lazy">

<br>



<br>



<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/120572859-a6870f00-c457-11eb-9744-e1ff5ab5d253.png" alt="image" loading="lazy">

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/120572917-bd2d6600-c457-11eb-8c76-5bb48988a5f9.png" alt="image" loading="lazy">

<br>



<br>

<img src="https://user-images.githubusercontent.com/12249301/120572936-c585a100-c457-11eb-822b-e70f0e857ac9.png" alt="image" loading="lazy">

<br>



<br>

解説資料: http://www.lr.pi.titech.ac.jp/~sasano/acl2016suzukake/slides/08.pdf</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a>
<a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<span class="issue_date">Issue Date: 2021-06-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/371">Pointing the Unknown Words, Gulcehre+, ACL’16</a>
<span class="snippet"><span>Comment</span>Conditional Copy Model （Pointer Softmax）を提案した論文。

<br>

単語を生成する際に、語彙内の単語から生成する分布、原文の単語から生成する分布を求める。後者はattention distributionから。コピーするか否かを決める確率変数を導入し（sigmoid）、両生成確率を重み付けする。

<br>

コピーメカニズム入れるなら引用すべき。解説スライド:https://www.slideshare.net/hytae/pointing-the-unknown-words</span>
<a class="button" href="articles/Article.html">#Article</a>
<a class="button" href="articles/PersonalizedDocumentSummarization.html">#PersonalizedDocumentSummarization</a>
<a class="button" href="articles/DocumentSummarization.html">#DocumentSummarization</a>
<a class="button" href="articles/NLP.html">#NLP</a>
<a class="button" href="articles/COLING.html">#COLING</a>
<span class="issue_date">Issue Date: 2017-12-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/18">Automatic Text Summarization based on the Global Document Annotation, COLING-ACL, [Nagao+, 1998], 1998.08</a>
<span class="snippet"><span>Comment</span>Personalized summarizationの評価はしていない。提案のみ。以下の3種類の手法を提案

<br>



<br>

・keyword-based customization

<br>



<br>

  ・関心のあるキーワードをユーザが入力し、コーパスやwordnet等の共起関係から関連語を取得し要約に利用する

<br>



<br>

・文書の要素をinteractiveに選択することによる手法

<br>



<br>

  ・文書中の関心のある要素（e.g. 単語、段落等）

<br>



<br>

・browsing historyベースの手法

<br>



<br>

  ・ユーザのbrowsing historyのドキュメントから、yahooディレクトリ等からカテゴリ情報を取得し、また、トピック情報も取得し（要約技術を活用するとのこと）特徴量ベクトルを作成

<br>



<br>

  ・ユーザがアクセスするたびに特徴ベクトルが更新されることを想定している？</span>
<button onclick="hideContent(0)" style="display: none;">hide</button>
</div>


    </div>

</article>
<div class="post-nav">
<a class="previous" href="/paper_notes/articles/AACL.html" title="AACLに関する論文・技術記事メモの一覧">AACLに関する論文・技術記事メモの一覧</a><a class="next" href="/paper_notes/articles/ACMMM.html" title="ACMMMに関する論文・技術記事メモの一覧">ACMMMに関する論文・技術記事メモの一覧</a>
</div>
<div class="post-related">
      <div>Related Articles</div>
      <ul>
        <li class="">
          <a class="post-link" href="/paper_notes/articles/ICCV.html" title="ICCVに関する論文・技術記事メモの一覧">
            ICCVに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/VisionLanguageModel.html" title="VisionLanguageModelに関する論文・技術記事メモの一覧">
            VisionLanguageModelに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/AffectDetection.html" title="AffectDetectionに関する論文・技術記事メモの一覧">
            AffectDetectionに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/Tools.html" title="Toolsに関する論文・技術記事メモの一覧">
            Toolsに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
</ul>
    </div>
<div class="post-comments"></div></section>
</div>


  </section>
  <section class="sidebar" style="margin-left: 15px;">
    <!-- Get sidebar items --><style type="text/css" media="screen">
.post-menu ul {
  list-style: none;
  padding: 0;
  margin: 0;
}
</style>

<div class="post-menu">
  <div class="post-menu-title">TOC</div>
  <div class="post-menu-content"></div>
</div>

<script>
  function generateContent() {
    var menu = document.querySelector(".post-menu");
    var menuContent =  menu.querySelector(".post-menu-content");
    var headings = document.querySelector(".post-content").querySelectorAll("h2, h3, h4, h5, h6");

    // Hide menu when no headings
    if (headings.length === 0) {
      return menu.style.display = "none";
    }

    // Generate post menu
    var menuHTML = '';
    for (var i = 0; i < headings.length; i++) {
      var h = headings[i];
      menuHTML += (
        '<li class="h-' + h.tagName.toLowerCase() + '">'
        + '<a href="#h-' + h.getAttribute('id') + '">' + h.textContent + '</a></li>');
    }

    menuContent.innerHTML = '<ul>' + menuHTML + '</ul>';

    // The header element
    var header = document.querySelector('header.site-header');

    function doMenuCollapse(index, over_items) {
      var items = menuContent.firstChild.children;

      if (over_items == undefined) {
        over_items = 20;
      }

      if (items.length < over_items) {
        return;
      }

      var activeItem = items[index];
      var beginItem = activeItem
      var endItem = activeItem
      var beginIndex = index;
      var endIndex = index + 1;
      while (beginIndex >= 0
        && !items[beginIndex].classList.contains('h-h2')) {
        beginIndex -= 1;
      }
      while (endIndex < items.length
        && !items[endIndex].classList.contains('h-h2')) {
        endIndex += 1;
      }
      for (var i = 0; i < beginIndex; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
      for (var i = beginIndex + 1; i < endIndex; i++) {
        item = items[i]
        // if (!item.classList.contains('h-h2')) {
          item.style.display = '';
        // }
      }
      for (var i = endIndex; i < items.length; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
    }

    // Init menu collapsed
    doMenuCollapse(-1);

    // Active the menu item
    window.addEventListener('scroll', function (event) {
      var lastActive = menuContent.querySelector('.active');
      var changed = true;
      var activeIndex = -1;
      for (var i = headings.length - 1; i >= 0; i--) {
        var h = headings[i];
        var headingRect = h.getBoundingClientRect();
        var headerRect = header.getBoundingClientRect();
        var headerTop = Math.floor(headerRect.top);
        var headerHeight = Math.floor(headerRect.height);
        var headerHeight = headerTop + headerHeight + 20;
        if (headingRect.top <= headerHeight) {
          var id = 'h-' + h.getAttribute('id');
          var a = menuContent.querySelector('a[href="#' + id  + '"]');
          var curActive = a.parentNode;
          if (curActive) {
            curActive.classList.add('active');
            activeIndex = i;
          }
          if (lastActive == curActive) {
            changed = false;
          }
          break;
        }
      }
      if (changed) {
        if (lastActive) {
          lastActive.classList.remove('active');
        }
        doMenuCollapse(activeIndex);
      }
      event.preventDefault();
    });
  }
  generateContent();
</script>
</section>
</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/paper_notes/"></data>

  <div class="wrapper">
    <div class="site-footer-inner">
<div>Copyright © 2023-current AkihikoWATANABE. The header images and any thumbnail images for the posts were generated by ChatGPT's DALL-E3.</div>
      <div>Powered by <a title="Jekyll is a simple, blog-aware, static site
      generator." href="https://jekyllrb.com/">Jekyll</a> &amp; <a title="Yat, yet
      another theme." href="https://github.com/jeffreytse/jekyll-theme-yat">Yat Theme</a>.</div>
      <div class="footer-col rss-subscribe">Subscribe <a href="/paper_notes/feed.xml">via RSS</a>
</div>
    </div>
  </div>
</footer>
</body>
</html>
