<!DOCTYPE html>
<html lang="ja">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-translate-customization" content="108d9124921d80c3-80e20d618ff053c8-g4f02ec6f3dba68b7-c">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Chain-of-Thoughtに関する論文・技術記事メモの一覧 | わたしのべんきょうノート</title>
<meta name="generator" content="Jekyll v4.3.2">
<meta property="og:title" content="Chain-of-Thoughtに関する論文・技術記事メモの一覧">
<meta name="author" content="AkihikoWATANABE">
<meta property="og:locale" content="ja">
<meta name="description" content="Chain-of-Thought #Pocket#NLP#LanguageModel#Reasoning#SafetyIssue Date: 2025-07-16 Paper Note Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety, Tomek Korbak+, arXiv25 Summary人間の言語で「考える」AIシステムは、安全性向上のために思考の連鎖（CoT）を監視することで悪意のある意図を検出する機会を提供する。しかし、CoT監視は完璧ではなく、一部の不正行為が見逃される可能性がある。研究を進め、既存の安全手法と併せてCoT監視への投資を推奨する。モデル開発者は、開発の決定がCoTの監視可能性に与える影響を考慮すべきである。 Comment元ポスト:https://x.com/gdb/status/1945350912668737701?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QCoTを監視することで、たとえばモデルのよろしくない挙動（e.g., misalignmentなどの意図しない動作や、prompt injection等の不正行為)を検知することができ、特にAIがより長期的な課題に取り組む際にはより一層その内部プロセスを監視する手段が必要不可欠となるため、CoTの忠実性や解釈性が重要となる。このため、CoTの監視可能性が維持される（モデルのアーキテクチャや学習手法（たとえばCoTのプロセス自体は一見真っ当なことを言っているように見えるが、実はRewardHackingしている、など）によってはそもそもCoTが難読化し監視できなかったりするので、現状は脆弱性がある）、より改善していく方向にコミュニティとして動くことを推奨する。そして、モデルを研究開発する際にはモデルのCoT監視に関する評価を実施すべきであり、モデルのデプロイや開発の際にはCoTの監視に関する決定を組み込むべき、といったような提言のようである。関連:https://x.com/dongxi_nlp/status/1945606266027426048?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q #Analysis#Pocket#NLP#LanguageModelIssue Date: 2025-06-18 Paper Note Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought, Hanlin Zhu+, arXiv25 Summary本研究では、連続CoTsを用いた二層トランスフォーマーが有向グラフ到達可能性問題を解決できることを証明。連続CoTsは複数の探索フロンティアを同時にエンコードし、従来の離散CoTsよりも効率的に解を導く。実験により、重ね合わせ状態が自動的に現れ、モデルが複数のパスを同時に探索することが確認された。 Comment元ポスト:https://x.com/tydsh/status/1935206012799303817?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q #Survey#Pocket#NLP#LanguageModel#COLINGIssue Date: 2025-05-29 Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs, Yu Xia+, COLING25 SummaryChain-of-Thought（CoT）を基にしたChain-of-X（CoX）手法の調査を行い、LLMsの課題に対処するための多様なアプローチを分類。ノードの分類とアプリケーションタスクに基づく分析を通じて、既存の手法の意義と今後の可能性を議論。研究者にとって有用なリソースを提供することを目指す。 more">
<meta property="og:description" content="Chain-of-Thought #Pocket#NLP#LanguageModel#Reasoning#SafetyIssue Date: 2025-07-16 Paper Note Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety, Tomek Korbak+, arXiv25 Summary人間の言語で「考える」AIシステムは、安全性向上のために思考の連鎖（CoT）を監視することで悪意のある意図を検出する機会を提供する。しかし、CoT監視は完璧ではなく、一部の不正行為が見逃される可能性がある。研究を進め、既存の安全手法と併せてCoT監視への投資を推奨する。モデル開発者は、開発の決定がCoTの監視可能性に与える影響を考慮すべきである。 Comment元ポスト:https://x.com/gdb/status/1945350912668737701?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QCoTを監視することで、たとえばモデルのよろしくない挙動（e.g., misalignmentなどの意図しない動作や、prompt injection等の不正行為)を検知することができ、特にAIがより長期的な課題に取り組む際にはより一層その内部プロセスを監視する手段が必要不可欠となるため、CoTの忠実性や解釈性が重要となる。このため、CoTの監視可能性が維持される（モデルのアーキテクチャや学習手法（たとえばCoTのプロセス自体は一見真っ当なことを言っているように見えるが、実はRewardHackingしている、など）によってはそもそもCoTが難読化し監視できなかったりするので、現状は脆弱性がある）、より改善していく方向にコミュニティとして動くことを推奨する。そして、モデルを研究開発する際にはモデルのCoT監視に関する評価を実施すべきであり、モデルのデプロイや開発の際にはCoTの監視に関する決定を組み込むべき、といったような提言のようである。関連:https://x.com/dongxi_nlp/status/1945606266027426048?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q #Analysis#Pocket#NLP#LanguageModelIssue Date: 2025-06-18 Paper Note Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought, Hanlin Zhu+, arXiv25 Summary本研究では、連続CoTsを用いた二層トランスフォーマーが有向グラフ到達可能性問題を解決できることを証明。連続CoTsは複数の探索フロンティアを同時にエンコードし、従来の離散CoTsよりも効率的に解を導く。実験により、重ね合わせ状態が自動的に現れ、モデルが複数のパスを同時に探索することが確認された。 Comment元ポスト:https://x.com/tydsh/status/1935206012799303817?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q #Survey#Pocket#NLP#LanguageModel#COLINGIssue Date: 2025-05-29 Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs, Yu Xia+, COLING25 SummaryChain-of-Thought（CoT）を基にしたChain-of-X（CoX）手法の調査を行い、LLMsの課題に対処するための多様なアプローチを分類。ノードの分類とアプリケーションタスクに基づく分析を通じて、既存の手法の意義と今後の可能性を議論。研究者にとって有用なリソースを提供することを目指す。 more">
<link rel="canonical" href="http://akihikowatanabe.github.io/paper_notes/articles/Chain-of-Thought.html">
<meta property="og:url" content="http://akihikowatanabe.github.io/paper_notes/articles/Chain-of-Thought.html">
<meta property="og:site_name" content="わたしのべんきょうノート">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-07-26T00:52:37+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="Chain-of-Thoughtに関する論文・技術記事メモの一覧">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"AkihikoWATANABE"},"dateModified":"2025-07-26T00:52:37+00:00","datePublished":"2025-07-26T00:52:37+00:00","description":"Chain-of-Thought #Pocket#NLP#LanguageModel#Reasoning#SafetyIssue Date: 2025-07-16 Paper Note Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety, Tomek Korbak+, arXiv25 Summary人間の言語で「考える」AIシステムは、安全性向上のために思考の連鎖（CoT）を監視することで悪意のある意図を検出する機会を提供する。しかし、CoT監視は完璧ではなく、一部の不正行為が見逃される可能性がある。研究を進め、既存の安全手法と併せてCoT監視への投資を推奨する。モデル開発者は、開発の決定がCoTの監視可能性に与える影響を考慮すべきである。 Comment元ポスト:https://x.com/gdb/status/1945350912668737701?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QCoTを監視することで、たとえばモデルのよろしくない挙動（e.g., misalignmentなどの意図しない動作や、prompt injection等の不正行為)を検知することができ、特にAIがより長期的な課題に取り組む際にはより一層その内部プロセスを監視する手段が必要不可欠となるため、CoTの忠実性や解釈性が重要となる。このため、CoTの監視可能性が維持される（モデルのアーキテクチャや学習手法（たとえばCoTのプロセス自体は一見真っ当なことを言っているように見えるが、実はRewardHackingしている、など）によってはそもそもCoTが難読化し監視できなかったりするので、現状は脆弱性がある）、より改善していく方向にコミュニティとして動くことを推奨する。そして、モデルを研究開発する際にはモデルのCoT監視に関する評価を実施すべきであり、モデルのデプロイや開発の際にはCoTの監視に関する決定を組み込むべき、といったような提言のようである。関連:https://x.com/dongxi_nlp/status/1945606266027426048?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q #Analysis#Pocket#NLP#LanguageModelIssue Date: 2025-06-18 Paper Note Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought, Hanlin Zhu+, arXiv25 Summary本研究では、連続CoTsを用いた二層トランスフォーマーが有向グラフ到達可能性問題を解決できることを証明。連続CoTsは複数の探索フロンティアを同時にエンコードし、従来の離散CoTsよりも効率的に解を導く。実験により、重ね合わせ状態が自動的に現れ、モデルが複数のパスを同時に探索することが確認された。 Comment元ポスト:https://x.com/tydsh/status/1935206012799303817?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q #Survey#Pocket#NLP#LanguageModel#COLINGIssue Date: 2025-05-29 Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs, Yu Xia+, COLING25 SummaryChain-of-Thought（CoT）を基にしたChain-of-X（CoX）手法の調査を行い、LLMsの課題に対処するための多様なアプローチを分類。ノードの分類とアプリケーションタスクに基づく分析を通じて、既存の手法の意義と今後の可能性を議論。研究者にとって有用なリソースを提供することを目指す。 more","headline":"Chain-of-Thoughtに関する論文・技術記事メモの一覧","mainEntityOfPage":{"@type":"WebPage","@id":"http://akihikowatanabe.github.io/paper_notes/articles/Chain-of-Thought.html"},"url":"http://akihikowatanabe.github.io/paper_notes/articles/Chain-of-Thought.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="icon" href="">
  <link rel="canonical" href="http://akihikowatanabe.github.io">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/main.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/custom_button.css">
  <script src="/paper_notes/assets/js/main.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script><link type="application/atom+xml" rel="alternate" href="http://akihikowatanabe.github.io/paper_notes/feed.xml" title="わたしのべんきょうノート">
<script>
  function showMore(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "block";
          
      // このボタンの参照を取得して非表示にします
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "none";
      }
          
      // hideボタンの参照を取得して表示します
      const hideButton = contentDiv.querySelector('button[onclick^="hideContent"]');
      if (hideButton) {
        hideButton.style.display = "block";
      }
    }
  }

  function hideContent(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "none";
  
      // moreボタンの参照を取得して表示します
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "block";
      }
  
      // このボタンを隠します
      const hideButtons = document.querySelectorAll('button[onclick^="hideContent"]');
      const hideButton = hideButtons[index];
      if (hideButton) {
        hideButton.style.display = "none";
      }
    }
  }
</script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script charset="UTF-8" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/go.min.js" async></script>



















<script>
// Init highlight js
document.addEventListener('DOMContentLoaded', function(event) {
  var els = document.querySelectorAll('pre code')

  function addLangData(block) {
    var outer = block.parentElement.parentElement.parentElement;
    var lang = block.getAttribute('data-lang');
    for (var i = 0; i < outer.classList.length; i++) {
      var cls = outer.classList[i];
      if (cls.startsWith('language-')) {
        lang = cls;
        break;
      }
    }
    if (!lang) {
      cls = block.getAttribute('class');
      lang = cls ? cls.replace('hljs ', '') : '';
    }
    if (lang.startsWith('language-')) {
      lang = lang.substr(9);
    }
    block.setAttribute('class', 'hljs ' + lang);
    block.parentNode.setAttribute('data-lang', lang);
  }

  function addBadge(block) {
    var enabled = ('true' || 'true').toLowerCase();
    if (enabled == 'true') {
      var pre = block.parentElement;
      pre.classList.add('badge');
    }
  }

  function handle(block) {
    addLangData(block);
    addBadge(block)
    hljs.highlightBlock(block);
  }

  for (var i = 0; i < els.length; i++) {
    var el = els[i];
    handle(el);
  }
});
</script>

<style>
  /* code language badge */
  pre.badge::before {
    content: attr(data-lang);
    color: #fff;
    background-color: #ff4e00;
    padding: 0 .5em;
    border-radius: 0 2px;
    text-transform: uppercase;
    text-align: center;
    min-width: 32px;
    display: inline-block;
    position: absolute;
    right: 0;
  }

  /* fix wrong badge display for firefox browser */
  code > table pre::before {
    display: none;
  }
</style>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe-lightbox.umd.min.js" async></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe.umd.min.js" async></script>
<link href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/photoswipe.min.css" rel="stylesheet">
<style>
  .pswp .pswp__container .pswp__img {
    background-color: white;
  }
</style>

<script>
  function initPhotoSwipe() {
    let customOptions = {};

    try {
      const data = `{"gallery"=>"section.main", "children"=>"a.photo-swipe", "bgOpacity"=>0.8, "padding"=>{"top"=>20, "bottom"=>40, "left"=>100, "right"=>100}}`.replaceAll("=>", ":");
      customOptions = JSON.parse(data);
    } catch (e) {
      console.info("Invalid custom photo previewer options! " + e.message);
    }

    // Define object and gallery options
    const options = Object.assign(
      {
        gallery: "section.main",
        children: "a.photo-swipe",
        photo_scale: 2,
        // dynamic import is not supported in UMD version
        pswpModule: PhotoSwipe,
      },
      customOptions
    );

    const galleryEl = document.querySelector(options.gallery);
    if (!galleryEl) {
      return;
    }

    const imgEls = [];
    const els = galleryEl.querySelectorAll("img:not(.emoji)");
    els.forEach((el) => {
      if (el.src.trim() == "") {
        return;
      }
      if (!imgEls.includes(el)) {
        imgEls.push(el);
      }
    });

    if (imgEls.length === 0) {
      return;
    }

    imgEls.forEach((imgEl) => {
      imgEl.outerHTML = `
      <a class="photo-swipe"
        href="${imgEl.src}"
        data-pswp-width="${
          Math.max(imgEl.naturalWidth, imgEl.width) * options.photo_scale
        }"
        data-pswp-height="${
          Math.max(imgEl.naturalHeight, imgEl.height) * options.photo_scale
        }"
        data-pswp-caption="${imgEl.getAttribute("caption") || imgEl.alt}"
        target="_blank">
        ${imgEl.outerHTML}
      </a>`;
    });

    // Initialize PhotoSwipe 5
    var lightbox = new PhotoSwipeLightbox(options);

    lightbox.init();
  }

  window.addEventListener("load", initPhotoSwipe);
</script>
<meta name="google-site-verification" content="u_DTTPcCZ806iq51zgirHyWq3556HUKGq8AQfH91iFI">
</head>
<body>





































































































































<header class="site-header site-header-transparent" role="banner">

  <div class="wrapper">
    <div class="site-header-inner">
<span class="site-brand"><a class="site-brand-inner" rel="author" href="/paper_notes/">
  <img class="site-favicon" title="わたしのべんきょうノート" src="" onerror="this.style.display='none'">
  わたしのべんきょうノート
</a>
</span><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger">
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewbox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
              </svg>
            </span>
          </label>

          <div class="trigger">









<span class="page-link">



<div id="google_translate_element" style="display: none;">
</div>

<span class="ct-language">
  <ul class="list-unstyled ct-language-dropdown">
    
      <li>
        <a href="#" class="lang-select" data-lang="en">
          
          <img src="https://cdn.countryflags.com/thumbs/united-states-of-america/flag-400.png" title="English">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="fr">
          
          <img src="https://cdn.countryflags.com/thumbs/france/flag-400.png" title="French">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="zh-CN">
          
          <img src="https://cdn.countryflags.com/thumbs/china/flag-400.png" title="Chinese(Simple)">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ja">
          
          <img src="https://cdn.countryflags.com/thumbs/japan/flag-400.png" title="Japanese">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ko">
          
          <img src="https://cdn.countryflags.com/thumbs/south-korea/flag-400.png" title="Korean">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ru">
          
          <img src="https://cdn.countryflags.com/thumbs/russia/flag-400.png" title="Russian">
          
        </a>
      </li>
    
  </ul>
</span>

<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({
    pageLanguage: 'ja',
    autoDisplay: false,
    layout: google.translate.TranslateElement.InlineLayout.VERTICAL
  }, 'google_translate_element');

  // Links to cross-origin destinations are unsafe
  var gll = document.getElementsByClassName('goog-logo-link')[0];
  if (gll) {
    gll.setAttribute('rel', 'noopener');
  }

  function restoreLang() {
    var iframe = document.getElementsByClassName('goog-te-banner-frame')[0];
    if (!iframe) return;

    var innerDoc = iframe.contentDocument || iframe.contentWindow.document;
    var restore_el = innerDoc.getElementsByTagName("button");

    for (var i = 0; i < restore_el.length; i++) {
      if (restore_el[i].id.indexOf("restore") >= 0) {
        restore_el[i].click();
        var close_el = innerDoc.getElementsByClassName("goog-close-link");
        close_el[0].click();
        return;
      }
    }
  }

  function triggerHtmlEvent(element, eventName) {
    var event;
    if (document.createEvent) {
      event = document.createEvent('HTMLEvents');
      event.initEvent(eventName, true, true);
      element.dispatchEvent(event);
    } else {
      event = document.createEventObject();
      event.eventType = eventName;
      element.fireEvent('on' + event.eventType, event);
    }
  }

  var googleCombo = document.querySelector("select.goog-te-combo");
  var langSelect = document.querySelector('.ct-language');
  langSelect.addEventListener('click', function(event) {
    if (!event.target) {
      return;
    }

    var selected = document.querySelector('.ct-language .ct-language-selected');
    if (selected) {
      selected.classList.remove('ct-language-selected');
    }

    var target = event.target;
    while (target && target !== langSelect ) {
      if (target.matches('.lang-select')) {
        break;
      }
      target = target.parentElement;
    }

    if (target && target.matches('.lang-select')) {
      var lang = target.getAttribute('data-lang');
      if (googleCombo.value == lang) {
        restoreLang();
      } else {
        target.parentElement.classList.add('ct-language-selected');
        googleCombo.value = lang;
        triggerHtmlEvent(googleCombo, 'change');
      }
    }

    event.preventDefault();
  });
}
</script>

<script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit" async></script>
</span>
</div>
        </nav>
</div>
  </div>
</header>

<script>
  function initHeader() {
    var lastScrollY = getScrollPos().y;
    var documentElement = document.documentElement;

    function storeScrollData() {
      var y = getScrollPos().y;documentElement.setAttribute("data-header-transparent", "");var scrollStatus = "";

      if (y <= 0) {
        scrollStatus = "top";
      } else if ((window.innerHeight + y) >= document.body.offsetHeight) {
        scrollStatus = "bottom";
      } else {
        var isScrollDown = (y - lastScrollY > 0) ? true : false;
        scrollStatus = isScrollDown ? "down" : "up";
      }

      lastScrollY = y;
      documentElement.setAttribute("data-scroll-status", scrollStatus);
    }

    window.addEventListener('scroll', function(e) {
      storeScrollData();
    });

    storeScrollData();
  }
  document.addEventListener('DOMContentLoaded', initHeader);
</script>


























































































































































<style>
    html .page-banner .page-banner-img > *:first-child {
      opacity: 0.4;
    }

    html[data-theme="dark"] .page-banner .page-banner-img > *:first-child {
      opacity: 0.2872;
    }
  </style>
<section class="page-banner">
    <div class="page-banner-img">
<div style="background-image: url(/paper_notes/assets/images/banner.png)"></div>
        <img class="img-placeholder" src="/paper_notes/assets/images/banner.png">
</div>
    <div class="wrapper">
      <div class="page-banner-inner">
<header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">わたしのべんきょうノート</h1>
  <h2 class="post-subtitle">勉強した論文や技術等の情報をGithubのIssueにメモっているひとのブログ。
それなりにメモの量が蓄積されてきたので、一度整理したいなと思いブログはじめてみました！
自然言語処理(NLP), 推薦システム(RecommenderSystem), Educational Data Mining (EDM), Learning Analytics (LA)などの分野のメモが多いと思います。
最近は特にLLMの勉強が多めです :)</h2>

  <div class="post-meta">
    <time class="dt-published" datetime="2025-07-26T00:52:37+00:00" itemprop="datePublished"><i class="fa fa-calendar"></i> Jul 26, 2025
    </time><span class="post-author left-vsplit"><i class="fa fa-pencil"></i> AkihikoWATANABE</span>
    
































    <span class="post-reading-time left-vsplit"><i class="fa fa-clock-o"></i> About 2 hours 7 mins</span>
  </div></header>
</div>
    </div>
  </section><script>
  function hashLocate(hashValue) {
    hashValue = hashValue.replace(/^.*#h-/, '');
    hashValue = decodeURIComponent(hashValue);
    var element = document.getElementById(hashValue);

    if (!element) {
      return;
    }

    var header = document.querySelector('header.site-header');
    var headerRect = header.getBoundingClientRect();
    var headerTop = Math.floor(headerRect.top);
    var headerHeight = Math.floor(headerRect.height);
    var scrollPos = getScrollPos();
    var offsetY = element.offsetTop - (headerTop + headerHeight + 20);

    if (offsetY == scrollPos.y) {
      return;
    }

    if (headerTop == 0  && offsetY > scrollPos.y) {
      offsetY += headerHeight + 2;
    } else if (headerTop < 0  && offsetY < scrollPos.y) {
      offsetY -= headerHeight - 2;
    }

    smoothScrollTo(offsetY);
  }

  // The first event occurred
  window.addEventListener('load', function(event) {
    if (window.location.hash) {
      hashLocate(window.location.hash);
    }
  });

  // The first event occurred
  window.addEventListener('click', function(event) {
    if (event.target.tagName.toLowerCase() == 'a') {
      hashLocate(event.target.getAttribute('href'));
    }
  });
</script>
<div class="theme-toggle">
  <input type="checkbox" id="theme-switch">
  <label for="theme-switch">
    <div class="toggle"></div>
    <div class="names">
      <p class="light">Light</p>
      <p class="dark">Dark</p>
    </div>
  </label>
</div>




<script>
  (function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var nightModeOption = ('off' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
      storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
      var data = storage.getItem('theme');
      try {
        data = JSON.parse(data ? data : '');
      } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
      }
      return data;
    }

    function handleThemeToggle(nightShift) {
      themeData.nightShift = nightShift;
      saveThemeData(themeData);
      html.dataset.theme = nightShift ? 'dark' : 'light';
      setTimeout(function() {
        sw.checked = nightShift ? true : false;
      }, 50);
    }

    function autoThemeToggle() {
      // Next time point of theme toggle
      var now = new Date();
      var toggleAt = new Date();
      var hours = now.getHours();
      var nightShift = hours >= 19 || hours <=7;

      if (nightShift) {
        if (hours > 7) {
          toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
      } else {
        toggleAt.setHours(19);
      }

      toggleAt.setMinutes(0);
      toggleAt.setSeconds(0);
      toggleAt.setMilliseconds(0)

      var delay = toggleAt.getTime() - now.getTime();

      // auto toggle theme mode
      setTimeout(function() {
        handleThemeToggle(!nightShift);
      }, delay);

      return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
      };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
      handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
      var data = autoThemeToggle();

      // Toggle theme by local setting
      if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
      } else {
        handleThemeToggle(themeData.nightShift);
      }
    } else if (nightModeOption == 'manual') {
      handleThemeToggle(themeData.nightShift);
    } else {
      var nightShift = themeData.nightShift;
      if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
      }
      handleThemeToggle(nightShift);
    }
  })();
</script>
<div id="click-to-top" class="click-to-top">
  <i class="fa fa-arrow-up"></i>
</div>
<script>
  (function () {
    const clickToTop = document.getElementById('click-to-top');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 100) {
        clickToTop.classList.add('show')
      }else {
        clickToTop.classList.remove('show')
      }
    });
    clickToTop.addEventListener('click', () => {
      window.smoothScrollTo(0);
    });
  })();
</script>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="framework">
  <section class="main">

     <div class="post">
  <section>









<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="post-content e-content" itemprop="articleBody">

      <h2 id="chain-of-thought">Chain-of-Thought</h2>
<div class="visible-content">
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Reasoning.html">#Reasoning</a><a class="button" href="articles/Safety.html">#Safety</a><br><span class="issue_date">Issue Date: 2025-07-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2238">Paper Note Chain of Thought Monitorability: A New and Fragile Opportunity for AI  Safety, Tomek Korbak+, arXiv25</a>
<span class="snippet"><span>Summary</span>人間の言語で「考える」AIシステムは、安全性向上のために思考の連鎖（CoT）を監視することで悪意のある意図を検出する機会を提供する。しかし、CoT監視は完璧ではなく、一部の不正行為が見逃される可能性がある。研究を進め、既存の安全手法と併せてCoT監視への投資を推奨する。モデル開発者は、開発の決定がCoTの監視可能性に与える影響を考慮すべきである。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/gdb/status/1945350912668737701?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QCoTを監視することで、たとえばモデルのよろしくない挙動（e.g., misalignmentなどの意図しない動作や、prompt injection等の不正行為)を検知することができ、特にAIがより長期的な課題に取り組む際にはより一層その内部プロセスを監視する手段が必要不可欠となるため、CoTの忠実性や解釈性が重要となる。このため、CoTの監視可能性が維持される（モデルのアーキテクチャや学習手法（たとえばCoTのプロセス自体は一見真っ当なことを言っているように見えるが、実はRewardHackingしている、など）によってはそもそもCoTが難読化し監視できなかったりするので、現状は脆弱性がある）、より改善していく方向にコミュニティとして動くことを推奨する。そして、モデルを研究開発する際にはモデルのCoT監視に関する評価を実施すべきであり、モデルのデプロイや開発の際にはCoTの監視に関する決定を組み込むべき、といったような提言のようである。関連:https://x.com/dongxi_nlp/status/1945606266027426048?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Analysis.html">#Analysis</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br><span class="issue_date">Issue Date: 2025-06-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2059">Paper Note Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought, Hanlin Zhu+, arXiv25</a>
<span class="snippet"><span>Summary</span>本研究では、連続CoTsを用いた二層トランスフォーマーが有向グラフ到達可能性問題を解決できることを証明。連続CoTsは複数の探索フロンティアを同時にエンコードし、従来の離散CoTsよりも効率的に解を導く。実験により、重ね合わせ状態が自動的に現れ、モデルが複数のパスを同時に探索することが確認された。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/tydsh/status/1935206012799303817?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Survey.html">#Survey</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/COLING.html">#COLING</a><br><span class="issue_date">Issue Date: 2025-05-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2000">Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs, Yu Xia+, COLING25</a>
<span class="snippet"><span>Summary</span>Chain-of-Thought（CoT）を基にしたChain-of-X（CoX）手法の調査を行い、LLMsの課題に対処するための多様なアプローチを分類。ノードの分類とアプリケーションタスクに基づく分析を通じて、既存の手法の意義と今後の可能性を議論。研究者にとって有用なリソースを提供することを目指す。</span>
</div>
<p><button onclick="showMore(0)">more</button></p>

<div class="hidden-content">
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a><a class="button" href="articles/Reasoning.html">#Reasoning</a><br><span class="issue_date">Issue Date: 2025-05-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1980">AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via  Reinforcement Learning, Chenwei Lou+, arXiv25</a>
<span class="snippet"><span>Summary</span>AdaCoT（Adaptive Chain-of-Thought）は、LLMsが推論を適応的に行う新しいフレームワークで、CoTの呼び出しタイミングを最適化します。強化学習を用いて、クエリの複雑さに基づいてCoTの必要性を判断し、計算コストを削減します。実験では、AdaCoTがCoTトリガー率を3.18%に低下させ、応答トークンを69.06%減少させつつ、高い性能を維持することが示されました。</span>
<span class="snippet"><span>Comment</span>RLのRewardにおいて、bassのリワードだけでなく、
- reasoningをなくした場合のペナルティ項
- reasoningをoveruseした場合のペナルティ項
- formattingに関するペナルティ項
を設定し、reasoningの有無を適切に判断できた場合にrewardが最大化されるような形にしている。(2.2.2)

が、multi-stageのRLでは（stageごとに利用するデータセットを変更するが）、データセットの分布には歪みがあり、たとえば常にCoTが有効なデータセットも存在しており（数学に関するデータなど）、その場合常にCoTをするような分布を学習してしまい、AdaptiveなCoT decisionが崩壊したり、不安定になってしまう（decision boundary collapseと呼ぶ）。特にこれがfinal stageで起きると最悪で、これまでAdaptiveにCoTされるよう学習されてきたものが全て崩壊してしまう。これを防ぐために、Selective Loss Maskingというlossを導入している。具体的には、decision token [^1]のlossへの貢献をマスキングするようにすることで、CoTが生じるratioにバイアスがかからないようにする。今回は、Decision tokenとして、`<think>`トークン直後のトークンをdecision tokenとみなし、lossに対する貢献をマスクしている（Selective Loss Masking）。

[^1]: CoTするかどうかは多くの場合このDecision Tokenによって決まる、といったことがどっかの研究に示されていたはずいつか必要になったらしっかり読むが、全てのステージでSelective Loss Maskingをしたら、SFTでwarm upした段階からあまりCoTのratioが変化しないような学習のされ方になる気がするが、どのステージに対してapplyするのだろうか。&lt;/span&gt;
<a class="button" href="articles/Survey.html">#Survey</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a><a class="button" href="articles/InstructionTuning.html">#InstructionTuning</a><a class="button" href="articles/PPO%20(ProximalPolicyOptimization).html">#PPO (ProximalPolicyOptimization)</a><a class="button" href="articles/Reasoning.html">#Reasoning</a><a class="button" href="articles/LongSequence.html">#LongSequence</a><a class="button" href="articles/RewardHacking.html">#RewardHacking</a><a class="button" href="articles/GRPO.html">#GRPO</a><a class="button" href="articles/Contamination.html">#Contamination</a><a class="button" href="articles/VerifiableRewards.html">#VerifiableRewards</a><a class="button" href="articles/CurriculumLearning.html">#CurriculumLearning</a><br><span class="issue_date">Issue Date: 2025-05-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1925">100 Days After DeepSeek-R1: A Survey on Replication Studies and More  Directions for Reasoning Language Models, Chong Zhang+, arXiv25</a>
<span class="snippet"><span>Summary</span>最近の推論言語モデル（RLM）の進展を受けて、DeepSeek-R1が注目を集めているが、その実装詳細は完全にはオープンソース化されていない。これにより、多くの再現研究が行われ、DeepSeek-R1のパフォーマンスを再現しようとする試みが続いている。特に、監視付きファインチューニング（SFT）と強化学習（RLVR）の戦略が探求され、貴重な洞察が得られている。本報告では、再現研究の概要を提供し、データ構築やトレーニング手順の詳細を紹介し、今後の研究の促進を目指す。また、RLMを強化するための追加技術や開発上の課題についても考察する。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/_philschmid/status/1918898257406709983?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

サーベイのtakeawayが箇条書きされている。</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a><a class="button" href="articles/Embeddings.html">#Embeddings</a><a class="button" href="articles/Analysis.html">#Analysis</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/RepresentationLearning.html">#RepresentationLearning</a><a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/SSM%20(StateSpaceModel).html">#SSM (StateSpaceModel)</a><a class="button" href="articles/ICML.html">#ICML</a><a class="button" href="articles/PostTraining.html">#PostTraining</a><a class="button" href="articles/read-later.html">#read-later</a><br><span class="issue_date">Issue Date: 2025-05-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1924">Layer by Layer: Uncovering Hidden Representations in Language Models, Oscar Skean+, ICML25</a>
<span class="snippet"><span>Summary</span>中間層の埋め込みが最終層を超えるパフォーマンスを示すことを分析し、情報理論や幾何学に基づくメトリクスを提案。32のテキスト埋め込みタスクで中間層が強力な特徴を提供することを実証し、AIシステムの最適化における中間層の重要性を強調。</span>
<span class="snippet"><span>Comment</span>現代の代表的な言語モデルのアーキテクチャ（decoder-only model, encoder-only model, SSM）について、最終層のembeddingよりも中間層のembeddingの方がdownstream task（MTEBの32Taskの平均）に、一貫して（ただし、これはMTEBの平均で見たらそうという話であり、個別のタスクで一貫して強いかは読んでみないとわからない）強いことを示した研究。

このこと自体は経験的に知られているのであまり驚きではないのだが（ただ、SSMでもそうなのか、というのと、一貫して強いというのは興味深い）、この研究はMatrix Based Entropyと呼ばれるものに基づいて、これらを分析するための様々な指標を定義し理論的な根拠を示し、Autoregressiveな学習よりもMasked Languageによる学習の方がこのようなMiddle Layerのボトルネックが緩和され、同様のボトルネックが画像の場合でも起きることを示し、CoTデータを用いたFinetuningについても分析している模様。この辺の貢献が非常に大きいと思われるのでここを理解することが重要だと思われる。あとで読む。

![image](https://github.com/user-attachments/assets/bda00c50-c97b-45e0-97a5-d98dd98599fd)</span>
<a class="button" href="articles/Analysis.html">#Analysis</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/ICLR.html">#ICLR</a><br><span class="issue_date">Issue Date: 2025-04-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1918">When More is Less: Understanding Chain-of-Thought Length in LLMs, Yuyang Wu+, ICLR25</a>
<span class="snippet"><span>Summary</span>Chain-of-thought (CoT)推論は、LLMsの多段階推論能力を向上させるが、CoTの長さが増すと最初は性能が向上するものの、最終的には低下することが観察される。長い推論プロセスがノイズに脆弱であることを示し、理論的に最適なCoTの長さを導出。Length-filtered Voteを提案し、CoTの長さをモデルの能力とタスクの要求に合わせて調整する必要性を強調。</span>
<span class="snippet"><span>Comment</span>ICLR 2025 Best Paper Runner Up Award
元ポスト:https://x.com/yifeiwang77/status/1916873981979660436?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Transformer.html">#Transformer</a><a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a><a class="button" href="articles/SSM%20(StateSpaceModel).html">#SSM (StateSpaceModel)</a><a class="button" href="articles/ICLR.html">#ICLR</a><br><span class="issue_date">Issue Date: 2025-04-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1905">RNNs are not Transformers （Yet）: The Key Bottleneck on In-context   Retrieval, Kaiyue Wen+, ICLR25</a>
<span class="snippet"><span>Summary</span>本論文では、RNNとトランスフォーマーの表現力の違いを調査し、特にRNNがChain-of-Thought（CoT）プロンプトを用いてトランスフォーマーに匹敵するかを分析。結果、CoTはRNNを改善するが、トランスフォーマーとのギャップを埋めるには不十分であることが判明。RNNの情報取得能力の限界がボトルネックであるが、Retrieval-Augmented Generation（RAG）やトランスフォーマー層の追加により、RNNはCoTを用いて多項式時間で解決可能な問題を解決できることが示された。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/yuma_1_or/status/1915968478735130713?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q関連:
- #1210

↑とはどういう関係があるだろうか？</span>
<a class="button" href="articles/Analysis.html">#Analysis</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a><a class="button" href="articles/Reasoning.html">#Reasoning</a><a class="button" href="articles/LongSequence.html">#LongSequence</a><a class="button" href="articles/RewardHacking.html">#RewardHacking</a><a class="button" href="articles/PostTraining.html">#PostTraining</a><br><span class="issue_date">Issue Date: 2025-02-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1746">Demystifying Long Chain-of-Thought Reasoning in LLMs, Edward Yeo+, arXiv25</a>
<span class="snippet"><span>Summary</span>本研究では、大規模言語モデル（LLMs）における長い思考の連鎖（CoTs）推論のメカニズムを調査し、重要な要因を特定。主な発見は、(1) 教師ありファインチューニング（SFT）は必須ではないが効率を向上させる、(2) 推論能力は計算の増加に伴い現れるが、報酬の形状がCoTの長さに影響、(3) 検証可能な報酬信号のスケーリングが重要で、特に分布外タスクに効果的、(4) エラー修正能力は基本モデルに存在するが、RLを通じて効果的に奨励するには多くの計算が必要。これらの洞察は、LLMsの長いCoT推論を強化するためのトレーニング戦略の最適化に役立つ。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/xiangyue96/status/1887332772198371514?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q元ポストのスレッド中に論文の11個の知見が述べられている。どれも非常に興味深い。DeepSeek-R1のテクニカルペーパーと同様、

- Long CoTとShort CoTを比較すると前者の方が到達可能な性能のupper bonudが高いことや、
- SFTを実施してからRLをすると性能が向上することや、
- RLの際にCoTのLengthに関する報酬を入れることでCoTの長さを抑えつつ性能向上できること、
- 数学だけでなくQAペアなどのノイジーだが検証可能なデータをVerifiableな報酬として加えると一般的なreasoningタスクで数学よりもさらに性能が向上すること、
- より長いcontext window sizeを活用可能なモデルの訓練にはより多くの学習データが必要なこと、
- long CoTはRLによって学習データに類似したデータが含まれているためベースモデルの段階でその能力が獲得されていることが示唆されること、
- aha momentはすでにベースモデル時点で獲得されておりVerifiableな報酬によるRLによって強化されたわけではなさそう、

など、興味深い知見が盛りだくさん。非常に興味深い研究。あとで読む。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Prompting.html">#Prompting</a><br><span class="issue_date">Issue Date: 2025-01-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1728">Perspective Transition of Large Language Models for Solving Subjective  Tasks, Xiaolong Wang+, arXiv25</a>
<span class="snippet"><span>Summary</span>視点の移行を通じた推論（RPT）を提案し、LLMsが主観的な問題に対して動的に視点を選択できる手法を紹介。広範な実験により、従来の固定視点手法を上回り、文脈に応じた適切な応答を提供する能力を示す。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/rohanpaul_ai/status/1882739526361370737?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-QOpenReview: https://openreview.net/forum?id=cFGPlRony5"Subjective Task"とは例えば「メタファーの認識」や「ダークユーモアの検知」などがあり、これらは定量化しづらい認知的なコンテキストや、ニュアンスや感情などが強く関連しており、現状のLLMではチャレンジングだと主張している。
Subjective Taskでは、Reasoningモデルのように自動的にCoTのpathwayを決めるのは困難で、手動でpathwayを記述するのはチャレンジングで一貫性を欠くとした上で、複数の視点を組み合わせたPrompting（direct perspective, role-perspective, third-person perspectivfe）を実施し、最もConfidenceの高いanswerを採用することでこの課題に対処すると主張している。イントロしか読めていないが、自動的にCoTのpathwayを決めるのも手動で決めるのも難しいという風にイントロで記述されているが、手法自体が最終的に3つの視点から回答を生成させるという枠組みに則っている（つまりSubjective Taskを解くための形式化できているので、自動的な手法でもできてしまうのではないか？と感じた）ので、イントロで記述されている主張の”難しさ”が薄れてしまっているかも・・・？と感じた。論文が解こうとしている課題の”難しさ”をサポートする材料がもっとあった方がよりmotivationが分かりやすくなるかもしれない、という感想を持った。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/QuestionAnswering.html">#QuestionAnswering</a><a class="button" href="articles/Zero/FewShotPrompting.html">#Zero/FewShotPrompting</a><a class="button" href="articles/RAG(RetrievalAugmentedGeneration).html">#RAG(RetrievalAugmentedGeneration)</a><a class="button" href="articles/Reasoning.html">#Reasoning</a><br><span class="issue_date">Issue Date: 2025-01-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1649">AutoReason: Automatic Few-Shot Reasoning Decomposition, Arda Sevinc+, arXiv24</a>
<span class="snippet"><span>Summary</span>Chain of Thought（CoT）を用いて、暗黙のクエリを明示的な質問に分解することで、LLMの推論能力を向上させる自動生成システムを提案。StrategyQAとHotpotQAデータセットで精度向上を確認し、特にStrategyQAで顕著な成果を得た。ソースコードはGitHubで公開。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/dair_ai/status/1868299926897074309?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/PostTraining.html">#PostTraining</a><a class="button" href="articles/LatentReasoning.html">#LatentReasoning</a><br><span class="issue_date">Issue Date: 2024-12-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1586">Training Large Language Models to Reason in a Continuous Latent Space, Shibo Hao+, arXiv24</a>
<span class="snippet"><span>Summary</span>新しい推論パラダイム「Coconut」を提案し、LLMの隠れ状態を連続的思考として利用。これにより、次の入力を連続空間でフィードバックし、複数の推論タスクでLLMを強化。Coconutは幅優先探索を可能にし、特定の論理推論タスクでCoTを上回る性能を示す。潜在的推論の可能性を探る重要な洞察を提供。</span>
<span class="snippet"><span>Comment</span>Chain of Continuous Thought...?通常のCoTはRationaleをトークン列で生成するが、Coconutは最終的なhidden state（まだ読んでないのでこれが具体的に何を指すか不明）をそのまま入力に追加することで、トークンに制限されずにCoTさせるということらしい。あとでしっかり読む
![image](https://github.com/user-attachments/assets/b930f44b-96f4-47cd-aa1a-0b5fabde54a5)まだ読んでいないが、おそらく学習の際に工夫が必要なので既存モデルをこねくり回してできます系の話ではないかもOpenReview:https://openreview.net/forum?id=tG4SgayTtk

ICLR'25にrejectされている。
ざっと最初のレビューに書かれているWeaknessを読んだ感じ
- 評価データが合成データしかなく、よりrealisticなデータで評価した方が良い
- CoTら非常に一般的に適用可能な技術なので、もっと広範なデータで評価すべき
- GSM8Kでは大幅にCOCONUTはCoTに性能が負けていて、ProsQAでのみにしかCoTに勝てていない
- 特定のデータセットでの追加の学習が必要で、そこで身につけたreasoning能力が汎化可能か明らかでない

といった感じに見える</span>
<a class="button" href="articles/Analysis.html">#Analysis</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br><span class="issue_date">Issue Date: 2024-11-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1509">A Theoretical Understanding of Chain-of-Thought: Coherent Reasoning and  Error-Aware Demonstration, Yingqian Cui+, arXiv24</a>
<span class="snippet"><span>Summary</span>Few-shot Chain-of-Thought (CoT) プロンプティングはLLMsの推論能力を向上させるが、従来の研究は推論プロセスを分離された文脈内学習に依存している。本研究では、初期ステップからの一貫した推論（Coherent CoT）を統合することで、トランスフォーマーのエラー修正能力と予測精度を向上させることを理論的に示す。実験により、正しい推論経路と誤った推論経路を組み込むことでCoTを改善する提案の有効性を検証する。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/_philschmid/status/1855926845855699311?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

おもしろそうな研究</span>
<a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Prompting.html">#Prompting</a><br><span class="issue_date">Issue Date: 2024-09-29</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1429">Logic-of-Thought: Injecting Logic into Contexts for Full Reasoning in  Large Language Models, Tongxuan Liu+, N_A, arXiv24</a>
<span class="snippet"><span>Summary</span>Logic-of-Thought（LoT）プロンプティングを提案し、命題論理を用いて入力から拡張された論理情報を生成。これにより、LLMsの論理推論能力を向上させ、既存のプロンプト手法と統合可能。実験により、LoTが5つの論理推論タスクで顕著な性能向上を示し、特にReClorで+4.35%、LogiQAで+5%、ProofWriterで+8%の改善を達成。</span>
<span class="snippet"><span>Comment</span>SNSで話題になっているようだがGPT-3.5-TurboとGPT-4でしか比較していない上に、いつの時点のモデルかも記述されていないので、unreliableに見える
![image](https://github.com/user-attachments/assets/9ca6fc62-2691-40c8-a578-554c0083df8f)
</span>
<a class="button" href="articles/Analysis.html">#Analysis</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br><span class="issue_date">Issue Date: 2024-09-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1406">To CoT or not to CoT? Chain-of-thought helps mainly on math and symbolic  reasoning, Zayne Sprague+, N_A, arXiv24</a>
<span class="snippet"><span>Summary</span>Chain-of-thought（CoT）プロンプティングはLLMsの推論能力を引き出す手法であり、100以上の論文を対象にしたメタ分析により、主に数学や論理タスクでのパフォーマンス向上が確認された。一方、他のタスクでは効果が限定的で、MMLUでは直接回答生成がCoTと同等の精度を示した。計画と実行を分離し、ツール強化LLMsと比較した結果、CoTの利点は記号的実行の改善に起因し、記号ソルバーには劣ることが分かった。CoTの選択的適用により、推論コストを節約しつつパフォーマンスを維持できる可能性が示唆され、LLMアプリケーション全体での中間計算の活用が求められている。</span>
<span class="snippet"><span>Comment</span>CoTを100個以上の先行研究でmeta-analysisし（i.e. CoTを追加した場合のgainとタスクのプロット）、20個超えるデータセットで著者らが実験した結果、mathはsymbolic reasoning（12*4のように、シンボルを認識し、何らかの操作をして回答をする問題）が必要なタスクで、CoTは大きなgainが得られることがわかった（他はほとんどgainがない）。
![image](https://github.com/user-attachments/assets/a399306f-bda9-45c9-a756-2a83a9727e63)</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a><a class="button" href="articles/PostTraining.html">#PostTraining</a><br><span class="issue_date">Issue Date: 2024-09-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1391">ReFT: Reasoning with Reinforced Fine-Tuning, Trung Quoc Luong+, N_A, ACL24</a>
<span class="snippet"><span>Summary</span>強化ファインチューニング（ReFT）を提案し、LLMsの推論能力を向上。SFTでモデルをウォームアップ後、PPOアルゴリズムを用いてオンライン強化学習を行い、豊富な推論パスを自動サンプリング。GSM8K、MathQA、SVAMPデータセットでSFTを大幅に上回る性能を示し、追加のトレーニング質問に依存せず優れた一般化能力を発揮。</span>
<span class="snippet"><span>Comment</span>![image](https://github.com/user-attachments/assets/ab5ed92d-6a5c-48dc-a607-3f652b2c9b3f)
![image](https://github.com/user-attachments/assets/e34e5a62-c055-4586-87ee-5ece7e0cbffb)
</span>
<a class="button" href="articles/InformationRetrieval.html">#InformationRetrieval</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/RAG(RetrievalAugmentedGeneration).html">#RAG(RetrievalAugmentedGeneration)</a><br><span class="issue_date">Issue Date: 2024-04-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1282">RAT: Retrieval Augmented Thoughts Elicit Context-Aware Reasoning in  Long-Horizon Generation, Zihao Wang+, N_A, arXiv24</a>
<span class="snippet"><span>Summary</span>大規模言語モデルの推論および生成能力を向上させ、幻覚を軽減する方法として、情報検索を利用して思考の連鎖を修正する「retrieval-augmented thoughts（RAT）」が提案された。この方法は、ゼロショットのCoTが生成された後、取得した情報を使用して各思考ステップを修正する。GPT-3.5、GPT-4、およびCodeLLaMA-7bにRATを適用することで、コード生成、数学的推論、創造的な執筆、具体的なタスク計画などのタスクでパフォーマンスが大幅に向上した。デモページはhttps://craftjarvis.github.io/RATで利用可能。</span>
<span class="snippet"><span>Comment</span>RAGにおいてCoTさせる際に、各reasoningのstepを見直させることでより質の高いreasoningを生成するRATを提案。Hallucinationが低減し、生成のパフォーマンスも向上するとのこと。
![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/785f22e8-15b3-4dd1-997b-7186a4a9d399)コンセプト自体はそりゃそうだよねという話なので、RAGならではの課題があり、それを解決した、みたいな話があるのかが気になる。</span>
<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/785f22e8-15b3-4dd1-997b-7186a4a9d399" alt="image" loading="lazy"><a class="button" href="articles/ComputerVision.html">#ComputerVision</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br><span class="issue_date">Issue Date: 2024-04-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1275">Visualization-of-Thought Elicits Spatial Reasoning in Large Language  Models, Wenshan Wu+, N_A, arXiv24</a>
<span class="snippet"><span>Summary</span>LLMsの空間推論能力を向上させるために、Visualization-of-Thought（VoT）プロンプティングを提案。VoTは、LLMsの推論トレースを可視化し、空間推論タスクで使用することで、既存のMLLMsを上回る性能を示す。VoTは、空間推論を促進するために「メンタルイメージ」を生成する能力を持ち、MLLMsでの有効性を示唆する。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Prompting.html">#Prompting</a><br><span class="issue_date">Issue Date: 2024-03-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1247">Chain-of-Thought Reasoning Without Prompting, Xuezhi Wang+, N_A, arXiv24</a>
<span class="snippet"><span>Summary</span>LLMsの推論能力を向上させるための新しいアプローチに焦点を当てた研究が行われている。この研究では、LLMsがプロンプトなしで効果的に推論できるかどうかを検証し、CoT推論パスをデコーディングプロセスを変更することで引き出す方法を提案している。提案手法は、従来の貪欲なデコーディングではなく、代替トークンを調査することでCoTパスを見つけることができることを示しており、様々な推論ベンチマークで有効性を示している。</span>
<span class="snippet"><span>Comment</span>以前にCoTを内部的に自動的に実施されるように事前学習段階で学習する、といった話があったと思うが、この研究はデコーディング方法を変更することで、promptingで明示的にinstructionを実施せずとも、CoTを実現するもの、ということだと思われる。
![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/afb3a31e-3d85-4b7e-affa-fccc00b7321e)</span>
<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/afb3a31e-3d85-4b7e-affa-fccc00b7321e" alt="image" loading="lazy"><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br><span class="issue_date">Issue Date: 2024-01-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1208">The Impact of Reasoning Step Length on Large Language Models, Mingyu Jin+, N_A, arXiv24</a>
<span class="snippet"><span>Summary</span>Chain of Thought（CoT）の推論ステップの長さとLLMsの推論能力の関係を調査した。推論ステップを延長すると、プロンプトに新しい情報を追加せずにLLMsの推論能力が向上することがわかった。逆に、キーとなる情報を保持しながら推論ステップを短縮すると、推論能力が低下する。また、誤った根拠でも推論の必要な長さを保つ限り、好ましい結果が得られることも示された。さらに、タスクによって推論ステップの増加の利点が異なることも観察された。</span>
<a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/QuestionAnswering.html">#QuestionAnswering</a><a class="button" href="articles/Prompting.html">#Prompting</a><a class="button" href="articles/ACL.html">#ACL</a><a class="button" href="articles/Verification.html">#Verification</a><br><span class="issue_date">Issue Date: 2023-09-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1044">Paper Note Chain-of-Verification Reduces Hallucination in Large Language Models, Shehzaad Dhuliawala+, N_A, ACL24</a>
<span class="snippet"><span>Summary</span>私たちは、言語モデルが根拠のない情報を生成する問題に取り組んでいます。Chain-of-Verification（CoVe）メソッドを開発し、モデルが回答を作成し、検証し、最終的な回答を生成するプロセスを経ることで、幻想を減少させることができることを実験で示しました。</span>
<span class="snippet"><span>Comment</span># 概要
ユーザの質問から、Verificationのための質問をplanningし、質問に対して独立に回答を得たうえでオリジナルの質問に対するaggreementを確認し、最終的に生成を実施するPrompting手法
![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/18763903-2d70-4180-9384-2da55bedad2e)

# 評価
## dataset
- Wikidata
    - Wikipedia APIから自動生成した「“Who are some [Profession]s who were born in [City]?”」に対するQA pairs
    - Goldはknowledge baseから取得
    - 全56 test questions
    - Gold Entityが大体600程度ありLLMは一部しか回答しないので、precisionで評価
- Wiki category list
    - QUEST datasetを利用 #701 
    - 回答にlogical operationが不要なものに限定して頭に"Name some"をつけて質問を生成
        - "Name some Mexican animated horror films" or "Name some Endemic orchids of Vietnam"
    - 8個の回答を持つ55 test questionsを作成
- MultiSpanQA
    - Reading Comprehensionに関するBenchmark dataset
    - 複数の独立した回答（回答は連続しないスパンから回答が抽出される）から構成される質問で構成
        - 特に、今回はclosed-book setting で実施
        - すなわち、与えられた質問のみから回答しなければならず、知っている知識が問われる問題
    - 418のtest questsionsで、各回答に含まれる複数アイテムのspanが3 token未満となるようにした
    - QA例:
        - Q: Who invented the first printing press and in what year?
        - A: Johannes Gutenberg, 1450.
# 評価結果
提案手法には、verificationの各ステップでLLMに独立したpromptingをするかなどでjoint, 2-step, Factored, Factor+Revisedの4種類のバリエーションがあることに留意。
- joint: 全てのステップを一つのpromptで実施
- 2-stepは2つのpromptに分けて実施
- Factoredは各ステップを全て異なるpromptingで実施
- Factor+Revisedは異なるpromptで追加のQAに対するcross-checkをかける手法
結果を見ると、CoVEでhallucinationが軽減され、特にjointよりも2-step, factoredの方が高い性能を示すことがわかる。
![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/05ff1e6c-75e7-428a-996f-61e844866391)
![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/d72aa05e-daab-4092-a6f5-9e80cdab7486)
</span>
<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/18763903-2d70-4180-9384-2da55bedad2e" alt="image" loading="lazy"><a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/ACL.html">#ACL</a><br><span class="issue_date">Issue Date: 2023-04-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/554">Active prompting with chain-of-thought for large language models, Diao+, The Hong Kong University of Science and Technology, ACL24</a>
<span class="snippet"><span>Comment</span>しっかりと読めていないが、CoT-answerが存在しないtrainingデータが存在したときに、nサンプルにCoTとAnswerを与えるだけでFew-shotの予測をtestデータに対してできるようにしたい、というのがモチベーションっぽい
そのために、questionに対して、training dataに対してFew-Shot CoTで予測をさせた場合やZero-Shot CoTによって予測をさせた場合などでanswerを取得し、answerのばらつき度合いなどから不確実性を測定する。
そして、不確実性が高いCoT-Answerペアを取得し、人間が手作業でCoTと回答のペアを与え、その人間が作成したものを用いてTestデータに対してFewShotしましょう、ということだと思われる。
![image](https://user-images.githubusercontent.com/12249301/234747555-4b7bd0d5-f099-4288-a470-32206533e652.png)
</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Prompting.html">#Prompting</a><a class="button" href="articles/AutomaticPromptEngineering.html">#AutomaticPromptEngineering</a><a class="button" href="articles/NAACL.html">#NAACL</a><br><span class="issue_date">Issue Date: 2023-04-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/532">Enhancing LLM Chain-of-Thought with Iterative Bootstrapping, Sun+, Xiamen University （w_ MSRA et al.）, NAACL24</a>
<span class="snippet"><span>Comment</span>Zero shot CoTからスタートし、正しく問題に回答できるようにreasoningを改善するようにpromptをreviseし続けるループを回す。最終的にループした結果を要約し、それらをプールする。テストセットに対しては、プールの中からNshotをサンプルしinferenceを行う。
![image](https://user-images.githubusercontent.com/12249301/234311707-0d6f3443-681a-4309-917b-d21fd1a8c024.jpeg)できそうだなーと思っていたけど、早くもやられてしまった実装: https://github.com/GasolSun36/Iter-CoT# モチベーション: 既存のCoT Promptingの問題点
## Inappropriate Examplars can Reduce Performance
まず、既存のCoT prompting手法は、sampling examplarがシンプル、あるいは極めて複雑な（hop-based criterionにおいて; タスクを解くために何ステップ必要かという情報; しばしば人手で付与されている？）サンプルをサンプリングしてしまう問題がある。シンプルすぎるサンプルを選択すると、既にLLMは適切にシンプルな回答には答えられるにもかかわらず、demonstrationが冗長で限定的になってしまう。加えて、極端に複雑なexampleをサンプリングすると、複雑なquestionに対しては性能が向上するが、シンプルな問題に対する正答率が下がってしまう。

続いて、demonstration中で誤ったreasoning chainを生成してしまうと、inference時にパフォーマンスが低下する問題がある。下図に示した通り、誤ったdemonstrationが増加するにつれて、最終的な予測性能が低下する傾向にある。

これら2つの課題は、現在のメインストリームな手法（questionを選択し、reasoning chainを生成する手法）に一般的に存在する。
#556 , #555 のように推論時に適切なdemonstrationを選択するような取り組みは行われてきているが、test questionに対して推論するために、適切なexamplarsを選択するような方法は計算コストを増大させてしまう。
これら研究は誤った例の利用を最小限に抑えて、その悪影響を防ぐことを目指している。

一方で、この研究では、誤った例がLLMに対してcomplexityのlevelを提供し、貴重な学習をもたらすことができることを見出した。これは学生が難解だが回答可能な問題に取り組むことによって、問題解決スキルを向上させる方法に類似している。従って、誤った例を活用してLLMのパフォーマンスを向上させる方法を調査することは価値がある。
![image](https://user-images.githubusercontent.com/12249301/234752168-fe1d83a4-8d29-4f6c-8aa1-6bde1706beea.png)

## Large Language Models can self-Correct with Bootstrapping
Zero-Shot CoTでreasoning chainを生成し、誤ったreasoning chainを生成したpromptを**LLMに推敲させ(self-correction)**正しい出力が得られるようにする。こういったプロセスを繰り返し、correct sampleを増やすことでどんどん性能が改善していった。これに基づいて、IterCoTを提案。

![image](https://user-images.githubusercontent.com/12249301/234786084-5a6055f6-6f42-4546-bcbf-686b1d759ca9.png)
# IterCoT: Iterative Bootstrapping in Chain-of-Thought Prompting
IterCoTはweak bootstrappingとstrong bootstrappingによって構成される。

## Weak bootstrapping
- Initialization
  - Training setに対してZero-shot CoTを実施し、reasoning chainとanswerを得
- Bootstrapping 
  - 回答が誤っていた各サンプルに対して、Revise-Promptを適用しLLMに誤りを指摘し、新しい回答を生成させる。
  - 回答が正確になるまでこれを繰り返す。
- Summarization
  - 正しい回答が得られたら、Summary-Promptを利用して、これまでの誤ったrationaleと、正解のrationaleを利用し、最終的なreasoning chain (Iter-CoT)を生成する。
  - 全体のcontextual informationが加わることで、LLMにとって正確でわかりやすいreasoning chainを獲得する。
- Inference
  - questionとIter-Cotを組み合わせ、demonstration poolに加える
  - inference時はランダムにdemonstraction poolからサンプリングし、In context learningに利用し推論を行う

## Strong Bootstrapping
コンセプトはweak bootstrappingと一緒だが、Revise-Promptでより人間による介入を行う。具体的には、reasoning chainのどこが誤っているかを明示的に指摘し、LLMにreasoning chainをreviseさせる。
これは従来のLLMからの推論を必要としないannotationプロセスとは異なっている。何が違うかというと、人間によるannnotationをLLMの推論と統合することで、文脈情報としてreasoning chainを修正することができるようになる点で異なっている。# 実験
Manual-CoT #551 , Random-CoT #551, Auto-CoT #554 と比較。
Iter-CoTが11個のデータセット全てでoutperformした。

![image](https://user-images.githubusercontent.com/12249301/234792846-e8fd2f8b-6e26-48fc-9e5d-785bcf2a6b6a.png)

weak bootstrapingのiterationは4回くらいで頭打ちになった
![image](https://user-images.githubusercontent.com/12249301/234793570-f57e56e4-7320-4be4-9c93-ee3be01ad389.png)

また、手動でreasoning chainを修正した結果と、contextにannotation情報を残し、最後にsummarizeする方法を比較した結果、後者の方が性能が高かった。このため、contextの情報を利用しsummarizeすることが効果的であることがわかる。</span>
<a class="button" href="articles/Survey.html">#Survey</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/ACL.html">#ACL</a><br><span class="issue_date">Issue Date: 2025-01-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1668">Navigate through Enigmatic Labyrinth A Survey of Chain of Thought  Reasoning: Advances, Frontiers and Future, Zheng Chu+, arXiv23</a>
<span class="snippet"><span>Summary</span>推論はAIにおいて重要な認知プロセスであり、チェーン・オブ・ソートがLLMの推論能力を向上させることが注目されている。本論文では関連研究を体系的に調査し、手法を分類して新たな視点を提供。課題や今後の方向性についても議論し、初心者向けの導入を目指す。リソースは公開されている。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br><span class="issue_date">Issue Date: 2025-01-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1657">Program of Thoughts Prompting: Disentangling Computation from Reasoning   for Numerical Reasoning Tasks, Wenhu Chen+, TMLR23</a>
<span class="snippet"><span>Summary</span>段階的な推論を用いた数値推論タスクにおいて、Chain-of-thoughts prompting（CoT）の進展があり、推論をプログラムとして表現する「Program of Thoughts」（PoT）を提案。PoTは外部コンピュータで計算を行い、5つの数学問題データセットと3つの金融QAデータセットで評価した結果、少数ショットおよびゼロショット設定でCoTに対して約12％の性能向上を示した。自己一貫性デコーディングとの組み合わせにより、数学問題データセットで最先端の性能を達成。データとコードはGitHubで公開。</span>
<span class="snippet"><span>Comment</span>1. LLMsは算術演算を実施する際にエラーを起こしやすく、特に大きな数に対する演算を実施する際に顕著
2. LLMsは複雑な数式（e.g. 多項式, 微分方程式）を解くことができない
3. LLMsはiterationを表現するのが非常に非効率

の3点を解決するために、外部のインタプリタに演算処理を委譲するPoTを提案。PoTでは、言語モデルにreasoning stepsをpython programで出力させ、演算部分をPython Interpreterに実施させる。

![image](https://github.com/user-attachments/assets/ccaeee09-ca6f-45ec-aef4-c65960d52692)テキスト、テーブル、対話などの多様なinputをサポートする5つのMath Word Problem （MWP）, 3つのFinancial Datasetで評価した結果、zero-shot, few-shotの両方の設定において、PoTはCoTをoutpeformし、また、Self-Consistencyと組み合わせた場合も、PoTはCoTをoutperformした。
![image](https://github.com/user-attachments/assets/6b380fab-ab60-4f21-bce1-532167c8c8f2)</span>
<a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Reasoning.html">#Reasoning</a><br><span class="issue_date">Issue Date: 2025-01-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1656">Recursion of Thought: A Divide-and-Conquer Approach to Multi-Context  Reasoning with Language Models, Soochan Lee+, arXiv23</a>
<span class="snippet"><span>Summary</span>Recursion of Thought（RoT）という新しい推論フレームワークを提案し、言語モデル（LM）が問題を複数のコンテキストに分割することで推論能力を向上させる。RoTは特別なトークンを導入し、コンテキスト関連の操作をトリガーする。実験により、RoTがLMの推論能力を劇的に向上させ、数十万トークンの問題を解決できることが示された。</span>
<span class="snippet"><span>Comment</span>divide-and-conquerで複雑な問題に回答するCoT手法。生成過程でsubquestionが生じた際にモデルに特殊トークン（GO）を出力させ、subquestionの回答部分に特殊トークン（THINK）を出力させるようにSupervisedに学習させる。最終的にTHINKトークン部分は、subquestionを別途モデルによって解いた回答でreplaceして、最終的な回答を得る。
subquestionの中でさらにsubquestionが生じることもあるため、再帰的に処理される。
![image](https://github.com/user-attachments/assets/6a5a5155-b3dd-4a6a-a9f5-0975dddcedb7)四則演算と4種類のアルゴリズムに基づくタスクで評価。アルゴリズムに基づくタスクは、2つの数のlongest common subsequenceを見つけて、そのsubsequenceとlengthを出力するタスク（LCS）、0-1 knapsack問題、行列の乗算、数値のソートを利用。x軸が各タスクの問題ごとの問題の難易度を表しており、難易度が上がるほど提案手法によるgainが大きくなっているように見える。

Without Thoughtでは直接回答を出力させ、CoTではground truthとなるrationaleを1つのcontextに与えて回答を生成している。RoTではsubquestionごとに回答を別途得るため、より長いcontextを活用して最終的な回答を得る点が異なると主張している。

![image](https://github.com/user-attachments/assets/8e713c76-5f79-40c7-87b0-d69f6fac3ee3)
感想としては、ベースラインが弱すぎると感じる。詳細が書かれていないが、おそらくRoTはSFTによって各タスクに特化した学習をしていると考えられる（タスクごとの特殊トークンが存在するため）。ベースラインとしてRoT無しでSFTしたモデルあった方が良いのではないか？と感じる。

また、学習データにおけるsubquestionとsubquestionに対するground truthのデータ作成方法は書かれているが、そもそも元データとして何を利用したかや、その統計量も書かれていないように見える。あと、そもそも機械的に学習データを作成できない場合どうすれば良いのか？という疑問は残る。読んでいた時にAuto-CoTとの違いがよくわからなかったが、Related Workの部分にはAuto-CoTは動的、かつ多様なデモンストレーションの生成にフォーカスしているが、AutoReasonはquestionを分解し、few-shotの promptingでより詳細なrationaleを生成することにフォーカスしている点が異なるという主張のようである。

- #556Auto-CoTとの差別化は上記で理解できるが、G-Evalが実施しているAuto-CoTとの差別化はどうするのか？という風にふと思った。論文中でもG-Evalは引用されていない。

素朴にはAutoReasonはSFTをして学習をしています、さらにRecursiveにquestionをsubquestionを分解し、分解したsubquestionごとに回答を得て、subquestionの回答結果を活用して最終的に複雑なタスクの回答を出力する手法なので、G-Evalが実施している同一context内でrationaleをzeroshotで生成する手法よりも、より複雑な問題に回答できる可能性が高いです、という主張にはなりそうではある。

- #1223ICLR 2023 OpenReview:https://openreview.net/forum?id=PTUcygUoxuc

- 提案手法は一般的に利用可能と主張しているが、一般的に利用するためには人手でsubquestionの学習データを作成する必要があるため十分に一般的ではない
- 限られたcontext長に対処するために再帰を利用するというアイデアは新しいものではなく、数学の定理の証明など他の設定で利用されている

という理由でrejectされている。</span>
<a class="button" href="articles/Tutorial.html">#Tutorial</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br><span class="issue_date">Issue Date: 2023-11-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1152">Igniting Language Intelligence: The Hitchhikers Guide From  Chain-of-Thought Reasoning to Language Agents, Zhuosheng Zhang+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>大規模言語モデル（LLMs）は、言語知能の分野で劇的な進歩を遂げており、複雑な推論タスクにおいて高いパフォーマンスを示しています。特に、chain-of-thought（CoT）推論技術を活用することで、中間ステップを形成し、解釈可能性や制御可能性を向上させることができます。この論文では、CoT技術の基本的なメカニズムやその効果について詳しく解説し、言語エージェントの開発における応用例を紹介しています。将来の研究の展望にも触れており、初心者から経験豊富な研究者まで幅広い読者に対応しています。関連論文のリポジトリも提供されています。</span>
<span class="snippet"><span>Comment</span>CoTに関するチュートリアル論文</span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br><span class="issue_date">Issue Date: 2023-11-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1147">Implicit Chain of Thought Reasoning via Knowledge Distillation, Yuntian Deng+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>本研究では、言語モデルの内部の隠れ状態を使用して暗黙的な推論を行う手法を提案します。明示的なチェーン・オブ・ソートの推論ステップを生成する代わりに、教師モデルから抽出した暗黙的な推論ステップを使用します。実験により、この手法が以前は解決できなかったタスクを解決できることが示されました。</span>
<span class="snippet"><span>Comment</span>これは非常に興味深い話</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Prompting.html">#Prompting</a><br><span class="issue_date">Issue Date: 2023-11-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1144">Contrastive Chain-of-Thought Prompting, Yew Ken Chia+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>言語モデルの推論を改善するために、対照的なchain of thoughtアプローチを提案する。このアプローチでは、有効な推論デモンストレーションと無効な推論デモンストレーションの両方を提供し、モデルが推論を進める際にミスを減らすようにガイドする。また、自動的な方法を導入して対照的なデモンストレーションを構築し、汎化性能を向上させる。実験結果から、対照的なchain of thoughtが一般的な改善手法として機能することが示された。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Prompting.html">#Prompting</a><a class="button" href="articles/RAG(RetrievalAugmentedGeneration).html">#RAG(RetrievalAugmentedGeneration)</a><br><span class="issue_date">Issue Date: 2023-11-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1140">Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language  Models, Wenhao Yu+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>検索補完言語モデル（RALM）は、外部の知識源を活用して大規模言語モデルの性能を向上させるが、信頼性の問題や知識の不足による誤った回答がある。そこで、Chain-of-Noting（CoN）という新しいアプローチを導入し、RALMの頑健性を向上させることを目指す。CoNは、順次の読み取りノートを生成し、関連性を評価して最終的な回答を形成する。ChatGPTを使用してCoNをトレーニングし、実験結果はCoNを装備したRALMが標準的なRALMを大幅に上回ることを示している。特に、ノイズの多いドキュメントにおいてEMスコアで平均+7.9の改善を達成し、知識範囲外のリアルタイムの質問に対する拒否率で+10.5の改善を達成している。</span>
<span class="snippet"><span>Comment</span>一番重要な情報がappendixに載っている
![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/58dc0468-e3f5-4893-8173-fc891893519f)
CoNによって、ノイズがあった場合にゲインが大きい。
![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/556a20e1-f687-4f08-bd8a-6ba3cd197cd7)</span>
<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/58dc0468-e3f5-4893-8173-fc891893519f" alt="image" loading="lazy"><a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Prompting.html">#Prompting</a><br><span class="issue_date">Issue Date: 2023-11-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1135">Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads  to Answers Faster, Hongxuan Zhang+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>この研究では、FastCoTというフレームワークを提案します。FastCoTは、LLMを使用して並列デコーディングと自己回帰デコーディングを同時に行い、計算リソースを最大限に活用します。また、FastCoTは推論時間を約20%節約し、性能の低下がほとんどないことを実験で示しました。さらに、異なるサイズのコンテキストウィンドウに対しても頑健性を示すことができました。</span>
<span class="snippet"><span>Comment</span>論文中の図を見たが、全くわからなかった・・・。ちゃんと読まないとわからなそうである。</span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Prompting.html">#Prompting</a><br><span class="issue_date">Issue Date: 2023-10-24</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1085">Eliminating Reasoning via Inferring with Planning: A New Framework to  Guide LLMs Non-linear Thinking, Yongqi Tong+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>本研究では、大規模言語モデル（LLMs）に非線形の思考を促すために、新しいプロンプティング方法であるInferential Exclusion Prompting（IEP）を提案する。IEPは、計画を立てて可能な解を推論し、逆推論を行うことで広い視点を得ることができる。IEPは他の手法と比較して複雑な人間の思考プロセスをシミュレートできることを実証し、LLMsのパフォーマンス向上にも貢献することを示した。さらに、Mental-Ability Reasoning Benchmark（MARB）を導入し、LLMsの論理と言語推論能力を評価するための新しいベンチマークを提案した。IEPとMARBはLLMsの研究において有望な方向性であり、今後の進展が期待される。</span>
<span class="snippet"><span>Comment</span>元論文は読んでいないのだが、CoTが線形的だという主張がよくわからない。
CoTはAutoregressiveな言語モデルに対して、コンテキストを自己生成したテキストで利用者の意図した方向性にバイアスをかけて補完させ、
利用者が意図した通りのアウトプットを最終的に得るためのテクニック、だと思っていて、
線形的だろうが非線形的だろうがどっちにしろCoTなのでは。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Prompting.html">#Prompting</a><br><span class="issue_date">Issue Date: 2023-10-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1078">Meta-CoT: Generalizable Chain-of-Thought Prompting in Mixed-task  Scenarios with Large Language Models, Anni Zou+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>本研究では、大規模言語モデル（LLMs）を使用して、推論のためのチェーン・オブ・ソート（CoT）プロンプトを生成する方法を提案しています。従来のCoTの方法では、一般的なプロンプトや手作業デモンストレーションに依存していましたが、本研究では入力質問のタイプに基づいて自動的にプロンプトを生成するMeta-CoTを提案しています。Meta-CoTは、10のベンチマーク推論タスクで優れたパフォーマンスを示し、SVAMPでは最先端の結果を達成しました。また、分布外データセットでも安定性と汎用性が確認されました。</span>
<span class="snippet"><span>Comment</span>色々出てきたがなんかもう色々組み合わせれば最強なんじゃね?って気がしてきた。
![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/bb51c119-c1bc-4033-a7d4-f403d3c82d30)
![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/1c450a01-5cd6-4af3-af76-323e8c8d3769)
</span>
<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/bb51c119-c1bc-4033-a7d4-f403d3c82d30" alt="image" loading="lazy"><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Prompting.html">#Prompting</a><br><span class="issue_date">Issue Date: 2023-10-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1076">Take a Step Back: Evoking Reasoning via Abstraction in Large Language  Models, Huaixiu Steven Zheng+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>Step-Back Promptingは、大規模言語モデル（LLMs）を使用して推論の手順をガイドするシンプルなプロンプティング技術です。この技術により、LLMsは具体的な詳細から高レベルの概念や基本原則を抽象化し、正しい推論経路をたどる能力を向上させることができます。実験により、Step-Back PromptingはSTEM、Knowledge QA、Multi-Hop Reasoningなどのタスクにおいて大幅な性能向上が観察されました。具体的には、MMLU Physics and Chemistryで7%、11%、TimeQAで27%、MuSiQueで7%の性能向上が確認されました。</span>
<span class="snippet"><span>Comment</span>また新しいのが出た![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/aac94123-7c39-4938-889f-feb5cff9317c)
![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/a793ff97-3d5c-4707-9ec6-3884b143182b)
</span>
<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/aac94123-7c39-4938-889f-feb5cff9317c" alt="image" loading="lazy"><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Prompting.html">#Prompting</a><br><span class="issue_date">Issue Date: 2023-10-07</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1056">Large Language Models as Analogical Reasoners, Michihiro Yasunaga+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>本研究では、言語モデルの推論プロセスを自動的にガイドするための新しいプロンプティング手法であるアナロジカルプロンプティングを提案しています。この手法は、関連する過去の経験を引用して新しい問題に取り組む認知プロセスに倣い、問題を解決する前に文脈内で関連する例示や知識を自己生成させるように言語モデルに促します。この手法は、例示のラベリングや検索の必要性を排除し、一般性と適応性を提供します。実験結果は、この手法がさまざまな推論タスクで他の手法を上回ることを示しています。</span>
<span class="snippet"><span>Comment</span>以下、著者ツイートのざっくり翻訳: https://x.com/michiyasunaga/status/1709582150025240854?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q

人間は新しい問題に取り組む時、過去に解いた類義の問題を振り返り、その経験を活用する。これをLLM上で実践できないか?というのがアイデア。
Analogical Promptingでは、問題を解く前に、適切なexamplarを自動生成（problemとsolution）させ、コンテキストとして利用する。

これにより、examplarは自己生成されるため、既存のCoTで必要なexamplarのラベリングや検索が不要となることと、解こうとしている問題に合わせてexamplarを調整し、推論に対してガイダンスを提供することが可能となる。

実験の結果、数学、コード生成、BIG-Benchでzero-shot CoT、few-shot CoTを上回った。

![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/8aae5d9d-d8d8-4c86-b55f-0fcde5d5381c)
![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/8544d7e2-bae3-4a1e-a867-ab655785c725)LLMが知っており、かつ得意な問題に対してならうまく働きそう。一方で、LLMが苦手な問題などは人手作成したexamplarでfew-shotした方が（ある程度）うまくいきそうな予感がする。うまくいきそうと言っても、そもそもLLMが苦手な問題なのでfew-shotした程度では焼石に水だとは思うが。</span>
<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/8aae5d9d-d8d8-4c86-b55f-0fcde5d5381c" alt="image" loading="lazy"><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Prompting.html">#Prompting</a><br><span class="issue_date">Issue Date: 2023-09-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1030">Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language  Models, Bilgehan Sel+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>大規模言語モデル（LLMs）の推論能力を向上させるために、新しい戦略「Algorithm of Thoughts」を提案している。この戦略では、LLMsをアルゴリズム的な推論経路に導き、わずか1つまたは数個のクエリでアイデアの探索を拡大する。この手法は、以前の単一クエリ手法を上回り、マルチクエリ戦略と同等の性能を発揮する。また、LLMを指導するアルゴリズムを使用することで、アルゴリズム自体を上回るパフォーマンスが得られる可能性があり、LLMが最適化された検索に自己の直感を織り込む能力を持っていることを示唆している。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Prompting.html">#Prompting</a><br><span class="issue_date">Issue Date: 2023-08-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1015">Large Language Model Guided Tree-of-Thought, Jieyi Long, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>この論文では、Tree-of-Thought（ToT）フレームワークを紹介し、自己回帰型の大規模言語モデル（LLM）の問題解決能力を向上させる新しいアプローチを提案しています。ToTは、人間の思考方法に触発された技術であり、複雑な推論タスクを解決するためにツリー状の思考プロセスを使用します。提案手法は、LLMにプロンプターエージェント、チェッカーモジュール、メモリモジュール、およびToTコントローラーなどの追加モジュールを組み込むことで実現されます。実験結果は、ToTフレームワークがSudokuパズルの解決成功率を大幅に向上させることを示しています。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Prompting.html">#Prompting</a><br><span class="issue_date">Issue Date: 2023-08-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1012">Graph of Thoughts: Solving Elaborate Problems with Large Language Models, Maciej Besta+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>私たちは、Graph of Thoughts（GoT）というフレームワークを紹介しました。これは、大規模言語モデル（LLMs）のプロンプティング能力を進化させるもので、任意のグラフとしてモデル化できることが特徴です。GoTは、思考の組み合わせやネットワーク全体の本質の抽出、思考の強化などを可能にします。さまざまなタスクで最先端の手法に比べて利点を提供し、LLMの推論を人間の思考に近づけることができます。</span>
<span class="snippet"><span>Comment</span>Chain of Thought #551 
=&gt; Self-consistency #558 
=&gt; Thought Decomposition #1013 
=&gt; Tree of Thoughts #684 Tree of Thought #1015 
=&gt; Graph of Thought</span>
<a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Distillation.html">#Distillation</a><br><span class="issue_date">Issue Date: 2023-07-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/867">Teaching Small Language Models to Reason, ACL23</a>
<span class="snippet"><span>Summary</span>本研究では、大規模な言語モデルの推論能力を小さなモデルに転送するための知識蒸留を探求しました。具体的には、大きな教師モデルによって生成された出力を用いて学生モデルを微調整し、算術、常識、象徴的な推論のタスクでのパフォーマンスを向上させることを示しました。例えば、T5 XXLの正解率は、PaLM 540BとGPT-3 175Bで生成された出力を微調整することで、それぞれ8.11％から21.99％および18.42％に向上しました。</span>
<a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Distillation.html">#Distillation</a><br><span class="issue_date">Issue Date: 2023-07-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/829">SCOTT: Self-Consistent Chain-of-Thought Distillation, ACL23</a>
<span class="snippet"><span>Summary</span>本研究では、大規模な言語モデル（LM）から小さなCoTモデルを学習するための知識蒸留手法であるSCOTTを提案しています。SCOTTは、教師モデルからゴールドアンサーをサポートする根拠を引き出し、より信憑性のあるトークンを生成するように学習を促します。さらに、学生モデルはカウンターファクトリーニングの目的で教師が生成した根拠を使用して学習されます。実験結果は、提案手法がベースラインよりも忠実なモデルを導くことを示しています。また、根拠を尊重することで意思決定を改善することも可能です。</span>
<span class="snippet"><span>Comment</span>CoTのパフォーマンス向上がパラメータ数が大きいモデルでないと発揮せれないことは元論文 #551 で考察されており、それをより小さいモデルに蒸留し発揮できるようにする、おもしろい</span>
<a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/NumericReasoning.html">#NumericReasoning</a><br><span class="issue_date">Issue Date: 2023-07-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/797">Teaching Arithmetic to Small Transformers, Nayoung Lee+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>本研究では、GPT-4のような大規模言語モデルが、教師なしのトークン予測目的に明示的にエンコードされていないにもかかわらず、算術演算や基本的な関数を効率的に学習できることを示しています。訓練データのフォーマットの変更やchain-of-thoughtスタイルのデータの使用により、精度や収束速度が改善されます。また、訓練中の算術とテキストデータの相互作用やモデルのスケールの影響も研究されています。この研究は、高品質な指導的なデータが算術能力の引き出しにおいて重要であることを強調しています。</span>
<span class="snippet"><span>Comment</span>小規模なtransformerに算術演算を学習させ、どのような学習データが効果的か調査。CoTスタイルの詳細なスクラッチパッドを学習データにすることで、plainなもの等と比較して、予測性能や収束速度などが劇的に改善した

![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/42e60fc0-d04b-4338-922c-5a46b69890b9)結局next token predictionで学習させているみたいだけど、本当にそれで算術演算をモデルが理解しているのだろうか?という疑問がいつもある</span>
<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/42e60fc0-d04b-4338-922c-5a46b69890b9" alt="image" loading="lazy"><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br><span class="issue_date">Issue Date: 2023-06-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/754">OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities, Yuanzhen Xie+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>本論文では、人間の認知フレームワークを模倣することで、複雑な推論問題を解決するための新しい知的フレームワークであるOlaGPTを提案しています。OlaGPTは、注意、記憶、推論、学習などの異なる認知モジュールを含み、以前の誤りや専門家の意見を動的に参照する学習ユニットを提供しています。また、Chain-of-Thought（COT）テンプレートと包括的な意思決定メカニズムも提案されています。OlaGPTは、複数の推論データセットで厳密に評価され、最先端のベンチマークを上回る優れた性能を示しています。OlaGPTの実装はGitHubで利用可能です。</span>
<a class="button" href="articles/Analysis.html">#Analysis</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Faithfulness.html">#Faithfulness</a><a class="button" href="articles/NeurIPS.html">#NeurIPS</a><br><span class="issue_date">Issue Date: 2023-05-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/666">Language Models Dont Always Say What They Think: Unfaithful   Explanations in Chain-of-Thought Prompting, Miles Turpin+, N_A, NeurIPS23</a>
<span class="snippet"><span>Summary</span>LLMsによる推論において、chain-of-thought reasoning（CoT）と呼ばれる説明を生成することができるが、この説明がモデルの予測の真の理由を誤って表現することがあることがわかった。バイアスのある特徴をモデルの入力に追加することで、CoT説明が大きく影響を受けることが示された。この結果は、LLMsに対する信頼を高めるために、説明の忠実度を評価し、改善する必要があることを示唆している。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Zero/FewShotPrompting.html">#Zero/FewShotPrompting</a><a class="button" href="articles/ACL.html">#ACL</a><br><span class="issue_date">Issue Date: 2023-05-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/642">Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them, Mirac Suzgun+, N_A, ACL23</a>
<span class="snippet"><span>Summary</span>BIG-Bench Hard (BBH) is a suite of 23 challenging tasks that current language models have not been able to surpass human performance on. This study focuses on applying chain-of-thought prompting to BBH tasks and found that PaLM and Codex were able to surpass human performance on 10 and 17 tasks, respectively. The study also found that CoT prompting is necessary for tasks that require multi-step reasoning and that CoT and model scale interact to enable new task performance on some BBH tasks.</span>
<span class="snippet"><span>Comment</span>単なるfewshotではなく、CoT付きのfewshotをすると大幅にBIG-Bench-hardの性能が向上するので、CoTを使わないanswer onlyの設定はモデルの能力の過小評価につながるよ、という話らしい
![image](https://github.com/user-attachments/assets/0545214a-a267-489d-8af9-82d21e08ff6c)
![image](https://github.com/user-attachments/assets/e5308c66-0bee-4d2c-b973-86478842b772)</span>
<a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/QuestionAnswering.html">#QuestionAnswering</a><a class="button" href="articles/Prompting.html">#Prompting</a><br><span class="issue_date">Issue Date: 2023-04-28</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/568">Answering Questions by Meta-Reasoning over Multiple Chains of Thought, Yoran+, Tel Aviv University （w_ Allen Institute for AI）, arXiv23</a>
<span class="snippet"><span>Comment</span>self-consistency #558 のようなvoting basedなアルゴリズムは、複数のCoTのintermediate stepを捨ててしまい、結果だけを採用するが、この研究は複数のCoTの中からquestionに回答するために適切なfactual informationを抽出するMeta Reasonerを導入し、複数のCoTの情報を適切に混在させて適切な回答を得られるようにした。

7個のMulti Hop QAデータでstrong baselineをoutperformし、人間が回答をverificationするための高品質な説明を生成できることを示した。

![image](https://user-images.githubusercontent.com/12249301/235135436-11dca529-771a-402b-a4ef-9b6deacec32e.jpeg)</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br><span class="issue_date">Issue Date: 2023-04-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/558">Self-consistency improves chain of thought reasoning in language models, Wang+, Google Research, ICLR23</a>
<span class="snippet"><span>Comment</span>self-consistencyと呼ばれる新たなCoTのデコーディング手法を提案。
これは、難しいreasoningが必要なタスクでは、複数のreasoningのパスが存在するというintuitionに基づいている。

self-consistencyではまず、普通にCoTを行う。そしてgreedyにdecodingする代わりに、以下のようなプロセスを実施する：
1. 多様なreasoning pathをLLMに生成させ、サンプリングする。
2. 異なるreasoning pathは異なるfinal answerを生成する（= final answer set）。
3. そして、最終的なanswerを見つけるために、reasoning pathをmarginalizeすることで、final answerのsetの中で最も一貫性のある回答を見出す。

これは、もし異なる考え方によって同じ回答が導き出されるのであれば、その最終的な回答は正しいという経験則に基づいている。
self-consistencyを実現するためには、複数のreasoning pathを取得した上で、最も多いanswer a_iを選択する（majority vote）。これにはtemperature samplingを用いる（temperatureを0.5やら0.7に設定して、より高い信頼性を保ちつつ、かつ多様なoutputを手に入れる）。
temperature samplingについては[こちら](https://openreview.net/pdf?id=rygGQyrFvH)の論文を参照のこと。
sampling数は増やせば増やすほど性能が向上するが、徐々にサチってくる。サンプリング数を増やすほどコストがかかるので、その辺はコスト感との兼ね合いになると思われる。

![image](https://user-images.githubusercontent.com/12249301/234754605-6316223f-4290-45d5-bf7c-64675f07d0c3.png)
![image](https://user-images.githubusercontent.com/12249301/234779335-478f2431-67ea-4b24-9c1b-fa1dd6ac6b45.png)
Self-consistencyは回答が閉じた集合であるような問題に対して適用可能であり、open-endなquestionでは利用できないことに注意が必要。ただし、open-endでも回答間になんらかの関係性を見出すような指標があれば実現可能とlimitationで言及している。</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/ICLR.html">#ICLR</a><br><span class="issue_date">Issue Date: 2023-04-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/556">Automatic Chain of Thought Prompting in Large Language Models, Zhang+, Shanghai Jiao Tong University, ICLR23</a>
<span class="snippet"><span>Comment</span>LLMによるreasoning chainが人間が作成したものよりも優れていることを示しているとのこと #532 よりclusteringベースな手法を利用することにより、誤りを含む例が単一のクラスタにまとめられうことを示し、これにより過剰な誤ったデモンストレーションが軽減されることを示した。手法の概要。questionを複数のクラスタに分割し、各クラスタから代表的なquestionをサンプリングし、zero-shot CoTでreasoning chainを作成しpromptに組み込む。最終的に回答を得たいquestionに対しても、上記で生成した複数のquestion-reasoningで条件付けした上で、zeroshot-CoTでrationaleを生成する。
![image](https://github.com/user-attachments/assets/35213747-9b5f-4d38-a525-1deafe86cd0c)</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br><span class="issue_date">Issue Date: 2023-04-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/555">Automatic prompt augmentation and selection with chain-of-thought from labeled data, Shum+, The Hong Kong University of Science and Technology, arXiv23</a>
<span class="snippet"><span>Comment</span>LLMによるreasoning chainが人間が作成したものよりも優れていることを示しているとのこと #532 よりselection phaseで誤ったexampleは直接排除する手法をとっている。そして、強化学習によって、demonstrationのselection modelを訓練している。</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Prompting.html">#Prompting</a><br><span class="issue_date">Issue Date: 2023-04-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/553">Large Language Models are Zero-Shot Reasoners, Kojima+, University of Tokyo, NeurIPS22</a>
<span class="snippet"><span>Comment</span>Zero-Shot CoT (Let's think step-by-step.)論文<img width="856" alt="image" src="https://user-images.githubusercontent.com/12249301/234746367-2cd80e23-8dcb-4244-b56c-e28120629027.png">
Zero-Shot-CoTは2つのステップで構成される：
- STEP1: Reasoning Extraction
  - 元のquestionをxとし、zero-shot-CoTのtrigger sentenceをtとした時に、テンプレート "Q: [X]. A. [T]" を用いてprompt　x'を作成
  - このprompt x'によって得られる生成テキストzはreasoningのrationaleとなっている。
- STEP2: Answer Extraction
  - STEP1で得られたx'とzを用いて、テンプレート "[X'] [Z] [A]" を用いてpromptを作成し、quiestionに対する回答を得る
  - このとき、Aは回答を抽出するためのtrigger sentenceである。
  - Aはタスクに応じて変更するのが効果的であり、たとえば、multi-choice QAでは "Therefore, among A through E, the answer is" といったトリガーを用いたり、数学の問題では "Therefore, the answer (arabic numerals) is" といったトリガーを用いる。

![image](https://user-images.githubusercontent.com/12249301/236404426-ed936908-3771-4eef-9871-c6ae04c896bf.png)

# 実験結果
表中の性能指標の左側はタスクごとにAnswer Triggerをカスタマイズしたもので、右側はシンプルに"The answer is"をAnswer Triggerとした場合。Zero-shot vs. Zero-shot-CoTでは、Zero-Shot-CoTが多くのb現地マークにおいて高い性能を示している。ただし、commonsense reasoningではperformance gainを得られなかった。これは #551 で報告されている通り、commonsense reasoningタスクでは、Few-Shot CoTでもLambda135Bで性能が向上せず、Palm540Bで性能が向上したように、モデルのparameter数が足りていない可能性がある（本実験では17種類のモデルを用いているが、特に注釈がなければtext-davinci-002を利用した結果）。

![image](https://user-images.githubusercontent.com/12249301/236405336-fe5e1f7f-9d2f-457f-9e25-98afe4ae0ec1.png)

## 他ベースラインとの比較
他のベースラインとarithmetic reasoning benchmarkで性能比較した結果。Few-Shot-CoTには勝てていないが、standard Few-shot Promptingtを大幅に上回っている。
![image](https://user-images.githubusercontent.com/12249301/236406621-7862823f-e019-4551-be96-1c97265ca5ba.png)

## zero-shot reasoningにおけるモデルサイズの影響
さまざまな言語モデルに対して、zero-shotとzero-shot-CoTを実施した場合の性能比較。#551 と同様にモデルサイズが小さいとZero-shot-CoTによるgainは得られないが、モデルサイズが大きくなると一気にgainが大きくなる。
![image](https://user-images.githubusercontent.com/12249301/236407727-f29e6f67-8ca1-4623-8341-73bbf2029e67.png)

## Zero-shot CoTにおけるpromptの選択による影響
input promptに対するロバスト性を確認した。instructiveカテゴリ（すなわち、CoTを促すトリガーであれば）性能が改善している。特に、どのようなsentenceのトリガーにするかで性能が大きくかわっている。今回の実験では、"Let's think step by step"が最も高い性能を占め最多。
![image](https://user-images.githubusercontent.com/12249301/236408268-8dbc32f3-76c7-4e41-aa1b-a19008aa680c.png)

## Few-shot CoTのprompt選択における影響
CommonsenseQAのexampleを用いて、AQUA-RAT, MultiArithをFew-shot CoTで解いた場合の性能。どちらのケースもドメインは異なるが、前者は回答のフォーマットは共通である。異なるドメインでも、answer format（multiple choice）の場合、ドメインが異なるにもかかわらず、zero-shotと比較して性能が大幅に向上した。一方、answer formatが異なる場合はperformance gainが小さい。このことから、LLMはtask自体よりも、exampleにおけるrepeated formatを活用していることを示唆している。また、CommonSennseをExamplarとして用いたFew-Shot-CoTでは、どちらのデータセットでもZero-Shot-CoTよりも性能が劣化している。つまり、Few-Shot-CoTでは、タスク特有のサンプルエンジニアリングが必要であることがわかる（一方、Zero-shot CoTではそのようなエンジニアリングは必要ない）。

![image](https://user-images.githubusercontent.com/12249301/236408978-b292ea0f-0a17-42fc-8e3c-6eee35780ca4.png)
</span>
<a class="button" href="articles/NeuralNetwork.html">#NeuralNetwork</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Zero/FewShotPrompting.html">#Zero/FewShotPrompting</a><a class="button" href="articles/Prompting.html">#Prompting</a><a class="button" href="articles/NeurIPS.html">#NeurIPS</a><br><span class="issue_date">Issue Date: 2023-04-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/551">Chain of thought prompting elicits reasoning in large language models, Wei+, Google Research, NeurIPS22</a>
<span class="snippet"><span>Comment</span>Chain-of-Thoughtを提案した論文。CoTをする上でパラメータ数が100B未満のモデルではあまり効果が発揮されないということは念頭に置いた方が良さそう。
![image](https://user-images.githubusercontent.com/12249301/234739470-be1c9299-0dd6-4483-901a-0bf855e73f0f.png)
先行研究では、reasoningが必要なタスクの性能が低い問題をintermediate stepを明示的に作成し、pre-trainedモデルをfinetuningすることで解決していた。しかしこの方法では、finetuning用の高品質なrationaleが記述された大規模データを準備するのに多大なコストがかかるという問題があった。
このため、few-shot promptingによってこの問題を解決することが考えられるが、reasoning能力が必要なタスクでは性能が悪いという問題あがった。そこで、両者の強みを組み合わせた手法として、chain-of-thought promptingは提案された。# CoTによる実験結果
以下のベンチマークを利用
- math word problem: GSM8K, SVAMP, ASDiv, AQuA, MAWPS
- commonsense reasoning: CSQA, StrategyQA, Big-bench Effort (Date, Sports), SayCan
- Symbolic Reasoning: Last Letter concatenation, Coin Flip
  - Last Letter concatnation: 名前の単語のlast wordをconcatするタスク（"Amy Brown" -&gt; "yn"）
  - Coin Flip: コインをひっくり返す、 あるいはひっくり返さない動作の記述の後に、コインが表向きであるかどうかをモデルに回答するよう求めるタスク
 
## math word problem benchmark
- モデルのサイズが大きくなるにつれ性能が大きく向上（emergent ability）することがあることがわかる
  - 言い換えるとCoTは&lt;100Bのモデルではパフォーマンスに対してインパクトを与えない
  - モデルサイズが小さいと、誤ったCoTを生成してしまうため
- 複雑な問題になればなるほど、CoTによる恩恵が大きい
  - ベースラインの性能が最も低かったGSM8Kでは、パフォーマンスの2倍向上しており、1 stepのreasoningで解決できるSingleOpやMAWPSでは、性能の向上幅が小さい
- Task specificなモデルをfinetuningした以前のSoTAと比較してcomparable, あるいはoutperformしている
- ![image](https://user-images.githubusercontent.com/12249301/236394200-826ba167-8ec7-4abb-ba4d-fe44bf247b41.png)
## Ablation Study
CoTではなく、他のタイプのpromptingでも同じような効果が得られるのではないか？という疑問に回答するために、3つのpromptingを実施し、CoTと性能比較した：
- Equation Only: 回答するまえに数式を記載するようなprompt
  - promptの中に数式が書かれているから性能改善されているのでは？という疑問に対する検証
  - =&gt; GSM8Kによる結果を見ると、equation onlyでは性能が低かった。これは、これは数式だけでreasoning stepsを表現できないことに起因している
- Variable compute only: dotのsequence (...) のみのprompt
  - CoTは難しい問題に対してより多くの計算（intermediate token）をすることができているからでは？という疑問に対する検証
  - variable computationとCoTの影響を分離するために、dotのsequence (...) のみでpromptingする方法を検証
  - =&gt; 結果はbaselineと性能変わらず。このことから、variableの計算自体が性能向上に寄与しているわけではないことがわかる。
- Chain of Thought after answer: 回答の後にCoTを出力するようなprompting
  - 単にpretrainingの際のrelevantな知識にアクセスしやすくなっているだけなのでは？という疑問を検証
  - =&gt; baselineと性能は変わらず、単に知識を活性化させるだけでは性能が向上しないことがわかる。

![image](https://user-images.githubusercontent.com/12249301/236396383-877a26ae-20c2-42a4-a023-1eb66abf8320.png)

## CoTのロバスト性
人間のAnnotatorにCoTを作成させ、それらを利用したCoTpromptingとexamplarベースな手法によって性能がどれだけ変わるかを検証。standard promptingを全ての場合で上回る性能を獲得した。このことから、linguisticなstyleにCoTは影響を受けていないことがわかる。
![image](https://user-images.githubusercontent.com/12249301/236397864-073dd88f-95c0-47f0-af3c-ed7288ca967d.png)

# commonsense reasoning
全てのデータセットにおいて、CoTがstandard promptingをoutperformした。
![image](https://user-images.githubusercontent.com/12249301/236398447-6c58a3f3-7461-4109-9a96-8f8092831dd1.png)

# Symbolic Reasoning
in-domain test setとout-of-domain test setの2種類を用意した。前者は必要なreasoning stepがfew-shot examplarと同一のもの、後者は必要なreasoning stepがfew-shot examplarよりも多いものである。
CoTがStandard proimptingを上回っている。特に、standard promptingではOOV test setではモデルをスケールさせても性能が向上しなかったのに対し、CoTではより大きなgainを得ている。このことから、CoTにはreasoning stepのlengthに対しても汎化能力があることがわかる。

![image](https://user-images.githubusercontent.com/12249301/236399389-30e62218-3e59-4912-983c-818de457fa04.png)
</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Tutorial.html">#Tutorial</a><a class="button" href="articles/Pretraining.html">#Pretraining</a><a class="button" href="articles/MachineLearning.html">#MachineLearning</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Transformer.html">#Transformer</a><a class="button" href="articles/In-ContextLearning.html">#In-ContextLearning</a><a class="button" href="articles/Attention.html">#Attention</a><a class="button" href="articles/DiffusionModel.html">#DiffusionModel</a><a class="button" href="articles/SSM%20(StateSpaceModel).html">#SSM (StateSpaceModel)</a><a class="button" href="articles/Scaling%20Laws.html">#Scaling Laws</a><a class="button" href="articles/PostTraining.html">#PostTraining</a><br><span class="issue_date">Issue Date: 2025-05-31</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2001">2025年度人工知能学会全国大会チュートリアル講演「深層基盤モデルの数理」, Taiji Suzuki, 2025.05</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/btreetaiji/status/1927678122817921442?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Tools.html">#Tools</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Blog.html">#Blog</a><a class="button" href="articles/Reasoning.html">#Reasoning</a><br><span class="issue_date">Issue Date: 2025-03-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1822">The think tool: Enabling Claude to stop and think in complex tool use situations, Anthropic, 2025.03</a>
<span class="snippet"><span>Comment</span>"考える"ことをツールとして定義し利用することで、externalなthinkingを明示的に実施した上でタスクを遂行させる方法を紹介している</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/python.html">#python</a><a class="button" href="articles/StructuredData.html">#StructuredData</a><br><span class="issue_date">Issue Date: 2025-01-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1721">Structured Outputs OpenAI Platform, 2025.01</a>
<span class="snippet"><span>Comment</span>pydanticを用いて、CoT＋構造化されたoutputを実施するサンプル</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Tutorial.html">#Tutorial</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Alignment.html">#Alignment</a><a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/Reasoning.html">#Reasoning</a><a class="button" href="articles/Mathematics.html">#Mathematics</a><a class="button" href="articles/PostTraining.html">#PostTraining</a><br><span class="issue_date">Issue Date: 2024-12-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1617">LLMを数学タスクにアラインする手法の系譜 - GPT-3からQwen2.5まで, bilzard, 2024.12</a>
<span class="snippet"><span>Comment</span>- #1618

において、数学においてモデルのパラメータ数のスケーリングによって性能改善が見込める学習手法として、モデルとは別にVerifierを学習し、モデルが出力した候補の中から良いものを選択できるようにする、という話の気持ちが最初よくわからなかったのだが、後半のなぜsample&amp;selectがうまくいくのか？節を読んでなんとなく気持ちが理解できた。SFTを進めるとモデルが出力する解放の多様性が減っていくというのは、興味深かった。

しかし、特定の学習データで学習した時に、全く異なるUnseenなデータに対しても解法は減っていくのだろうか？という点が気になった。あとは、学習データの多様性をめちゃめちゃ増やしたらどうなるのか？というのも気になる。特定のデータセットを完全に攻略できるような解法を出力しやすくなると、他のデータセットの性能が悪くなる可能性がある気がしており、そうするとそもそもの1shotの性能自体も改善していかなくなりそうだが、その辺はどういう設定で実験されているのだろうか。

たとえば、
- #1475

などでは、

- #1474

のような1600を超えるようなNLPタスクのデータでLoRAによりSFTすると、LoRAのパラメータ数を非常に大きくするとUnseenタスクに対する性能がfull-parameter tuningするよりも向上することが示されている。この例は数学に特化した例ではないが、SFTによって解法の多様性が減ることによって学習データに過剰適合して汎化性能が低下する、というのであれば、この論文のことを鑑みると「学習データにoverfittingした結果他のデータセットで性能が低下してしまう程度の多様性の学習データしか使えていないのでは」と感じてしまうのだが、その辺はどうなんだろうか。元論文を読んで確認したい。
とても勉強になった。記事中で紹介されている
&gt; LLMを使って複数解法の候補をサンプリングし、その中から最適な1つを選択する

のルーツは #1618 とのことなので是非読みたい。

この辺はSelf-Consistency #558 あたりが最初なのかと思っていた。</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Reasoning.html">#Reasoning</a><a class="button" href="articles/Test-Time%20Scaling.html">#Test-Time Scaling</a><br><span class="issue_date">Issue Date: 2024-09-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1390">OpenAI o1, 2024.09</a>
<span class="snippet"><span>Comment</span>Jason Wei氏のポスト:
https://x.com/_jasonwei/status/1834278706522849788?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q#1072 や　#1147 で似たような考えはすでに提案されていたが、どのような点が異なるのだろうか？

たとえば前者は、pauseトークンと呼ばれるoutputとは関係ないトークンを生成することで、outputを生成する前にモデル内部で推論する前により多くのベクトル操作を加える（=ベクトルを縦方向と横方向に混ぜ合わせる; 以後ベクトルをこねくりまわすと呼称する）、といった挙動を実現しているようだが、明示的にCoTの教師データを使ってSFTなどをしているわけではなさそうに見える（ざっくりとしか読んでないが）。
一方、Jason Wei氏のポストからは、RLで明示的により良いCoTができるように学習をしている点が違うように見える。学習の計算量だけでなく、inferenceの計算量に対しても、新たなスケーリング則が見出されている模様。
![image](https://github.com/user-attachments/assets/85a39908-7db8-4f97-9b5d-4bfdc8439577)

テクニカルレポート中で言われている time spent thinking （test-time compute）というのは、具体的には何なのだろうか。

上の研究でいうところの、inference時のpauseトークンの生成のようなものだろうか。モデルがベクトルをこねくり回す回数（あるいは生成するトークン数）が増えると性能も良くなるのか？
しかしそれはオリジナルのCoT研究である #551 のdotのみの文字列をpromptに追加して性能が向上しなかった、という知見と反する。

おそらく、**モデル学習のデコーディング時に**、ベクトルをこねくり回す回数（あるいは生成するトークン数）を増やすこと＝time spent thinking (test-time compute) 、ということなのだろうか？
そしてそのように学習されたモデルは、推論時にベクトルをこねくり回す回数（あるいは生成するトークン数）を増やすと性能が上がる、ということなのだろうか。
もしそうだとすると、これは #1072 のpauseトークンの生成をしながらfinetuningすると性能が向上する、という主張とも合致するように思うが、うーん。

実際暗号解読のexampleを見ると、とてつもなく長いCoT（トークンの生成数が多い）が行われている。RLでReasoningを学習させる関連研究: #1391 #1392以下o1の動きに関して考えている下記noteからの引用。

&gt;これによって、LLMはモデルサイズやデータ量をスケールさせる時代から推論時間をスケールさせる（つまり、沢山の推論ステップを探索する）時代に移っていきそうです。

なるほど。test-compute timeとは、推論ステップ数とその探索に要する時間という見方もあるのですね。

またnote中では、CoTの性能向上のために、Process Reward Model（PRM）を学習させ、LLMが生成した推論ステップを評価できるようにし、PRMを報酬モデルとし強化学習したモデルがo1なのではないか、と推測している。
PRMを提案した研究では、推論ステップごとに0,1の正誤ラベルが付与されたデータから学習しているとのこと。
なるほど、勉強になります。

note: https://note.com/hatti8/n/nf4f3ce63d4bc?sub_rt=share_pbnote（詳細編）:https://note.com/hatti8/n/n867c36ffda45?sub_rt=share_pbこちらのリポジトリに関連論文やXポスト、公式ブログなどがまとめられている: https://github.com/hijkzzz/Awesome-LLM-Strawberry

これはすごい。論文全部読みたい</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Prompting.html">#Prompting</a><a class="button" href="articles/Faithfulness.html">#Faithfulness</a><br><span class="issue_date">Issue Date: 2023-07-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/896">Measuring Faithfulness in Chain-of-Thought Reasoning, Anthropic, 2023</a>
<span class="snippet"><span>Summary</span>大規模言語モデル（LLMs）は、Chain-of-Thought（CoT）推論を生成することで質問に答える性能を向上させるが、その推論が実際の推論を忠実に表しているかは不明である。本研究では、CoT推論の忠実さを調査し、CoTに介入することでモデルの予測がどのように変化するかを調べる。結果は、モデルのサイズやタスクによってCoTの忠実さが異なることを示唆している。</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Pretraining.html">#Pretraining</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Supervised-FineTuning%20(SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a><a class="button" href="articles/Evaluation.html">#Evaluation</a><a class="button" href="articles/Blog.html">#Blog</a><a class="button" href="articles/Reasoning.html">#Reasoning</a><br><span class="issue_date">Issue Date: 2023-05-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/626">Towards Complex Reasoning: the Polaris of Large Language Models, Yao Fu, 2023.05</a>
<button onclick="hideContent(0)" style="display: none;">hide</button>
&lt;/div&gt;
</think></span>
</div>


    </div>

</article>
<div class="post-nav">
<a class="previous" href="/paper_notes/articles/Catastrophic%20Forgetting.html" title="Catastrophic Forgettingに関する論文・技術記事メモの一覧">Catastrophic Forgettingに関する論文・技術記事メモの一覧</a><a class="next" href="/paper_notes/articles/ChatGPT.html" title="ChatGPTに関する論文・技術記事メモの一覧">ChatGPTに関する論文・技術記事メモの一覧</a>
</div>
<div class="post-related">
      <div>Related Articles</div>
      <ul>
        <li class="">
          <a class="post-link" href="/paper_notes/articles/SelfCorrection.html" title="SelfCorrectionに関する論文・技術記事メモの一覧">
            SelfCorrectionに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/Metrics.html" title="Metricsに関する論文・技術記事メモの一覧">
            Metricsに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/Distillation.html" title="Distillationに関する論文・技術記事メモの一覧">
            Distillationに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/RepresentationLearning.html" title="RepresentationLearningに関する論文・技術記事メモの一覧">
            RepresentationLearningに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li>
</ul>
    </div>
<div class="post-comments"></div></section>
</div>


  </section>
  <section class="sidebar" style="margin-left: 15px;">
    <!-- Get sidebar items --><style type="text/css" media="screen">
.post-menu ul {
  list-style: none;
  padding: 0;
  margin: 0;
}
</style>

<div class="post-menu">
  <div class="post-menu-title">TOC</div>
  <div class="post-menu-content"></div>
</div>

<script>
  function generateContent() {
    var menu = document.querySelector(".post-menu");
    var menuContent =  menu.querySelector(".post-menu-content");
    var headings = document.querySelector(".post-content").querySelectorAll("h2, h3, h4, h5, h6");

    // Hide menu when no headings
    if (headings.length === 0) {
      return menu.style.display = "none";
    }

    // Generate post menu
    var menuHTML = '';
    for (var i = 0; i < headings.length; i++) {
      var h = headings[i];
      menuHTML += (
        '<li class="h-' + h.tagName.toLowerCase() + '">'
        + '<a href="#h-' + h.getAttribute('id') + '">' + h.textContent + '</a></li>');
    }

    menuContent.innerHTML = '<ul>' + menuHTML + '</ul>';

    // The header element
    var header = document.querySelector('header.site-header');

    function doMenuCollapse(index, over_items) {
      var items = menuContent.firstChild.children;

      if (over_items == undefined) {
        over_items = 20;
      }

      if (items.length < over_items) {
        return;
      }

      var activeItem = items[index];
      var beginItem = activeItem
      var endItem = activeItem
      var beginIndex = index;
      var endIndex = index + 1;
      while (beginIndex >= 0
        && !items[beginIndex].classList.contains('h-h2')) {
        beginIndex -= 1;
      }
      while (endIndex < items.length
        && !items[endIndex].classList.contains('h-h2')) {
        endIndex += 1;
      }
      for (var i = 0; i < beginIndex; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
      for (var i = beginIndex + 1; i < endIndex; i++) {
        item = items[i]
        // if (!item.classList.contains('h-h2')) {
          item.style.display = '';
        // }
      }
      for (var i = endIndex; i < items.length; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
    }

    // Init menu collapsed
    doMenuCollapse(-1);

    // Active the menu item
    window.addEventListener('scroll', function (event) {
      var lastActive = menuContent.querySelector('.active');
      var changed = true;
      var activeIndex = -1;
      for (var i = headings.length - 1; i >= 0; i--) {
        var h = headings[i];
        var headingRect = h.getBoundingClientRect();
        var headerRect = header.getBoundingClientRect();
        var headerTop = Math.floor(headerRect.top);
        var headerHeight = Math.floor(headerRect.height);
        var headerHeight = headerTop + headerHeight + 20;
        if (headingRect.top <= headerHeight) {
          var id = 'h-' + h.getAttribute('id');
          var a = menuContent.querySelector('a[href="#' + id  + '"]');
          var curActive = a.parentNode;
          if (curActive) {
            curActive.classList.add('active');
            activeIndex = i;
          }
          if (lastActive == curActive) {
            changed = false;
          }
          break;
        }
      }
      if (changed) {
        if (lastActive) {
          lastActive.classList.remove('active');
        }
        doMenuCollapse(activeIndex);
      }
      event.preventDefault();
    });
  }
  generateContent();
</script>
</section>
</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/paper_notes/"></data>

  <div class="wrapper">
    <div class="site-footer-inner">
<div>Copyright © 2023-current AkihikoWATANABE. The header images and any thumbnail images for the posts were generated by ChatGPT's DALL-E3.</div>
      <div>Powered by <a title="Jekyll is a simple, blog-aware, static site
      generator." href="https://jekyllrb.com/">Jekyll</a> &amp; <a title="Yat, yet
      another theme." href="https://github.com/jeffreytse/jekyll-theme-yat">Yat Theme</a>.</div>
      <div class="footer-col rss-subscribe">Subscribe <a href="/paper_notes/feed.xml">via RSS</a>
</div>
    </div>
  </div>
</footer>
</body>
</html>
