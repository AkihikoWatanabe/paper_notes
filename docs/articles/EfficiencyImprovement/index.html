<!DOCTYPE html>
<html lang="ja">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-translate-customization" content="108d9124921d80c3-80e20d618ff053c8-g4f02ec6f3dba68b7-c">
  <link rel="preconnect" href="https://cdnjs.cloudflare.com" crossorigin>
  <link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <link rel="preconnect" href="https://www.googletagmanager.com" crossorigin>
  <link rel="preconnect" href="https://www.google-analytics.com" crossorigin>
  <link rel="preconnect" href="https://platform.twitter.com">
  <link rel="preconnect" href="https://pbs.twimg.com">
  <link rel="dns-prefetch" href="https://cdnjs.cloudflare.com">
  <link rel="dns-prefetch" href="https://cdn.jsdelivr.net">
  <link rel="dns-prefetch" href="https://platform.twitter.com">
  <link rel="dns-prefetch" href="https://pbs.twimg.com">
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>EfficiencyImprovementに関する論文・技術記事メモの一覧 | わたしのべんきょうノート</title>
<meta name="generator" content="Jekyll v3.10.0">
<meta property="og:title" content="EfficiencyImprovementに関する論文・技術記事メモの一覧">
<meta name="author" content="AkihikoWATANABE">
<meta property="og:locale" content="ja">
<meta name="description" content="EfficiencyImprovement [Paper Note] Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations, Chancharik Mitra+, arXiv'25, 2025.11 Paper/Blog Link My Issue #Pocket #Supervised-FineTuning (SFT) #PEFT(Adaptor/LoRA) #Robotics #VisionLanguageActionModel #EmbodiedAI #One-Line Notes Issue Date: 2025-12-28 GPT Summary- VLAモデルはロボティクスにおける視覚と言語の統合を目指すが、物理的要因へのファインチューニングが必要。既存手法は特異性に欠けるため、タスク特異的な注意ヘッドを選択的にファインチューニングする「Robotic Steering」を提案。Franka Emikaロボットアームでの評価により、Robotic SteeringがLoRAを上回り、堅牢性、計算コスト削減、解釈可能性の向上を実現することを示した。 Commentpj page:">
<meta property="og:description" content="EfficiencyImprovement [Paper Note] Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations, Chancharik Mitra+, arXiv'25, 2025.11 Paper/Blog Link My Issue #Pocket #Supervised-FineTuning (SFT) #PEFT(Adaptor/LoRA) #Robotics #VisionLanguageActionModel #EmbodiedAI #One-Line Notes Issue Date: 2025-12-28 GPT Summary- VLAモデルはロボティクスにおける視覚と言語の統合を目指すが、物理的要因へのファインチューニングが必要。既存手法は特異性に欠けるため、タスク特異的な注意ヘッドを選択的にファインチューニングする「Robotic Steering」を提案。Franka Emikaロボットアームでの評価により、Robotic SteeringがLoRAを上回り、堅牢性、計算コスト削減、解釈可能性の向上を実現することを示した。 Commentpj page:">
<link rel="canonical" href="http://akihikowatanabe.github.io/paper_notes/articles/EfficiencyImprovement/">
<meta property="og:url" content="http://akihikowatanabe.github.io/paper_notes/articles/EfficiencyImprovement/">
<meta property="og:site_name" content="わたしのべんきょうノート">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2025-12-28T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta property="twitter:title" content="EfficiencyImprovementに関する論文・技術記事メモの一覧">
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"AkihikoWATANABE"},"dateModified":"2025-12-28T00:00:00+00:00","datePublished":"2025-12-28T00:00:00+00:00","description":"EfficiencyImprovement [Paper Note] Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations, Chancharik Mitra+, arXiv&#39;25, 2025.11 Paper/Blog Link My Issue #Pocket #Supervised-FineTuning (SFT) #PEFT(Adaptor/LoRA) #Robotics #VisionLanguageActionModel #EmbodiedAI #One-Line Notes Issue Date: 2025-12-28 GPT Summary- VLAモデルはロボティクスにおける視覚と言語の統合を目指すが、物理的要因へのファインチューニングが必要。既存手法は特異性に欠けるため、タスク特異的な注意ヘッドを選択的にファインチューニングする「Robotic Steering」を提案。Franka Emikaロボットアームでの評価により、Robotic SteeringがLoRAを上回り、堅牢性、計算コスト削減、解釈可能性の向上を実現することを示した。 Commentpj page:","headline":"EfficiencyImprovementに関する論文・技術記事メモの一覧","mainEntityOfPage":{"@type":"WebPage","@id":"http://akihikowatanabe.github.io/paper_notes/articles/EfficiencyImprovement/"},"url":"http://akihikowatanabe.github.io/paper_notes/articles/EfficiencyImprovement/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="icon" href="">
  <link rel="canonical" href="http://akihikowatanabe.github.io">

  <link rel="preload" href="/paper_notes/assets/css/main.css" as="style">
  <link rel="preload" href="/paper_notes/assets/js/main.js" as="script">

  <link rel="stylesheet" href="/paper_notes/assets/css/main.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/custom_button.css">
  
  <link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css"></noscript>
  <link rel="preload" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css" as="style" onload="this. onload=null;this.rel='stylesheet'">
  <noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css"></noscript>
  
  <script src="/paper_notes/assets/js/main.js"></script><link type="application/atom+xml" rel="alternate" href="http://akihikowatanabe.github.io/paper_notes/feed.xml" title="わたしのべんきょうノート">
<script>
  function showMore(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "block";
          
      // このボタンの参照を取得して非表示にします
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "none";
      }
          
      // hideボタンの参照を取得して表示します
      const hideButton = contentDiv.querySelector('button[onclick^="hideContent"]');
      if (hideButton) {
        hideButton.style.display = "block";
      }
    }
  }

  function hideContent(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "none";
  
      // moreボタンの参照を取得して表示します
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "block";
      }
  
      // このボタンを隠します
      const hideButtons = document.querySelectorAll('button[onclick^="hideContent"]');
      const hideButton = hideButtons[index];
      if (hideButton) {
        hideButton.style.display = "none";
      }
    }
  }
</script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script charset="UTF-8" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/go.min.js" async></script>



















<script>
// Init highlight js
document.addEventListener('DOMContentLoaded', function(event) {
  var els = document.querySelectorAll('pre code')

  function addLangData(block) {
    var outer = block.parentElement.parentElement.parentElement;
    var lang = block.getAttribute('data-lang');
    for (var i = 0; i < outer.classList.length; i++) {
      var cls = outer.classList[i];
      if (cls.startsWith('language-')) {
        lang = cls;
        break;
      }
    }
    if (!lang) {
      cls = block.getAttribute('class');
      lang = cls ? cls.replace('hljs ', '') : '';
    }
    if (lang.startsWith('language-')) {
      lang = lang.substr(9);
    }
    block.setAttribute('class', 'hljs ' + lang);
    block.parentNode.setAttribute('data-lang', lang);
  }

  function addBadge(block) {
    var enabled = ('true' || 'true').toLowerCase();
    if (enabled == 'true') {
      var pre = block.parentElement;
      pre.classList.add('badge');
    }
  }

  function handle(block) {
    addLangData(block);
    addBadge(block)
    hljs.highlightBlock(block);
  }

  for (var i = 0; i < els.length; i++) {
    var el = els[i];
    handle(el);
  }
});
</script>

<style>
  /* code language badge */
  pre.badge::before {
    content: attr(data-lang);
    color: #fff;
    background-color: #ff4e00;
    padding: 0 .5em;
    border-radius: 0 2px;
    text-transform: uppercase;
    text-align: center;
    min-width: 32px;
    display: inline-block;
    position: absolute;
    right: 0;
  }

  /* fix wrong badge display for firefox browser */
  code > table pre::before {
    display: none;
  }
</style>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe-lightbox.umd.min.js" async></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe.umd.min.js" async></script>
<link href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/photoswipe.min.css" rel="stylesheet">
<style>
  .pswp .pswp__container .pswp__img {
    background-color: white;
  }
</style>

<script>
  function initPhotoSwipe() {
    let customOptions = {};

    try {
      const data = `{"gallery"=>"section.main", "children"=>"a.photo-swipe", "bgOpacity"=>0.8, "padding"=>{"top"=>20, "bottom"=>40, "left"=>100, "right"=>100}}`.replaceAll("=>", ":");
      customOptions = JSON.parse(data);
    } catch (e) {
      console.info("Invalid custom photo previewer options! " + e.message);
    }

    // Define object and gallery options
    const options = Object.assign(
      {
        gallery: "section.main",
        children: "a.photo-swipe",
        photo_scale: 2,
        // dynamic import is not supported in UMD version
        pswpModule: PhotoSwipe,
      },
      customOptions
    );

    const galleryEl = document.querySelector(options.gallery);
    if (!galleryEl) {
      return;
    }

    const imgEls = [];
    const els = galleryEl.querySelectorAll("img:not(.emoji)");
    els.forEach((el) => {
      if (el.src.trim() == "") {
        return;
      }
      if (!imgEls.includes(el)) {
        imgEls.push(el);
      }
    });

    if (imgEls.length === 0) {
      return;
    }

    imgEls.forEach((imgEl) => {
      imgEl.outerHTML = `
      <a class="photo-swipe"
        href="${imgEl.src}"
        data-pswp-width="${
          Math.max(imgEl.naturalWidth, imgEl.width) * options.photo_scale
        }"
        data-pswp-height="${
          Math.max(imgEl.naturalHeight, imgEl.height) * options.photo_scale
        }"
        data-pswp-caption="${imgEl.getAttribute("caption") || imgEl.alt}"
        target="_blank">
        ${imgEl.outerHTML}
      </a>`;
    });

    // Initialize PhotoSwipe 5
    var lightbox = new PhotoSwipeLightbox(options);

    lightbox.init();
  }

  window.addEventListener("load", initPhotoSwipe);
</script>
<meta name="google-site-verification" content="u_DTTPcCZ806iq51zgirHyWq3556HUKGq8AQfH91iFI">
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-P70KSB88WH"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-P70KSB88WH');
  </script>

<script>MathJax={"tex":{"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"svg":{"fontCache":"global"},"svg":{"fontCache":"global"}}</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>


<body>





































































































































<header class="site-header site-header-transparent" role="banner">

  <div class="wrapper">
    <div class="site-header-inner">
<span class="site-brand"><a class="site-brand-inner" rel="author" href="/paper_notes/">
  <img class="site-favicon" title="わたしのべんきょうノート" src="" onerror="this.style.display='none'">
  わたしのべんきょうノート
</a>
</span><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger">
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewbox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
              </svg>
            </span>
          </label>

          <div class="trigger">
<a class="page-link" href="/paper_notes/">論文や技術メモの一覧（随時更新）</a><a class="page-link" href="/paper_notes/archives.html">ARCHIVES</a>









<span class="page-link">



<div id="google_translate_element" style="display: none;">
</div>

<span class="ct-language">
  <ul class="list-unstyled ct-language-dropdown">
    
      <li>
        <a href="#" class="lang-select" data-lang="en">
          
          <img src="https://cdn.countryflags.com/thumbs/united-states-of-america/flag-400.png" title="English">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="fr">
          
          <img src="https://cdn.countryflags.com/thumbs/france/flag-400.png" title="French">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="zh-CN">
          
          <img src="https://cdn.countryflags.com/thumbs/china/flag-400.png" title="Chinese(Simple)">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ja">
          
          <img src="https://cdn.countryflags.com/thumbs/japan/flag-400.png" title="Japanese">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ko">
          
          <img src="https://cdn.countryflags.com/thumbs/south-korea/flag-400.png" title="Korean">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ru">
          
          <img src="https://cdn.countryflags.com/thumbs/russia/flag-400.png" title="Russian">
          
        </a>
      </li>
    
  </ul>
</span>

<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({
    pageLanguage: 'ja',
    autoDisplay: false,
    layout: google.translate.TranslateElement.InlineLayout.VERTICAL
  }, 'google_translate_element');

  // Links to cross-origin destinations are unsafe
  var gll = document.getElementsByClassName('goog-logo-link')[0];
  if (gll) {
    gll.setAttribute('rel', 'noopener');
  }

  function restoreLang() {
    var iframe = document.getElementsByClassName('goog-te-banner-frame')[0];
    if (!iframe) return;

    var innerDoc = iframe.contentDocument || iframe.contentWindow.document;
    var restore_el = innerDoc.getElementsByTagName("button");

    for (var i = 0; i < restore_el.length; i++) {
      if (restore_el[i].id.indexOf("restore") >= 0) {
        restore_el[i].click();
        var close_el = innerDoc.getElementsByClassName("goog-close-link");
        close_el[0].click();
        return;
      }
    }
  }

  function triggerHtmlEvent(element, eventName) {
    var event;
    if (document.createEvent) {
      event = document.createEvent('HTMLEvents');
      event.initEvent(eventName, true, true);
      element.dispatchEvent(event);
    } else {
      event = document.createEventObject();
      event.eventType = eventName;
      element.fireEvent('on' + event.eventType, event);
    }
  }

  var googleCombo = document.querySelector("select.goog-te-combo");
  var langSelect = document.querySelector('.ct-language');
  langSelect.addEventListener('click', function(event) {
    if (!event.target) {
      return;
    }

    var selected = document.querySelector('.ct-language .ct-language-selected');
    if (selected) {
      selected.classList.remove('ct-language-selected');
    }

    var target = event.target;
    while (target && target !== langSelect ) {
      if (target.matches('.lang-select')) {
        break;
      }
      target = target.parentElement;
    }

    if (target && target.matches('.lang-select')) {
      var lang = target.getAttribute('data-lang');
      if (googleCombo.value == lang) {
        restoreLang();
      } else {
        target.parentElement.classList.add('ct-language-selected');
        googleCombo.value = lang;
        triggerHtmlEvent(googleCombo, 'change');
      }
    }

    event.preventDefault();
  });
}
</script>

<script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit" async></script>
</span>
</div>
        </nav>
</div>
  </div>
</header>

<script>
  function initHeader() {
    var lastScrollY = getScrollPos().y;
    var documentElement = document.documentElement;
    var ticking = false;

    function storeScrollData() {
      var y = getScrollPos().y;documentElement.setAttribute("data-header-transparent", "");var scrollStatus = "";

      if (y <= 0) {
        scrollStatus = "top";
      } else if ((window.innerHeight + y) >= document.body.offsetHeight) {
        scrollStatus = "bottom";
      } else {
        var isScrollDown = (y - lastScrollY > 0);
        scrollStatus = isScrollDown ? "down" : "up";
      }

      lastScrollY = y;
      documentElement.setAttribute("data-scroll-status", scrollStatus);
      
      // 処理完了フラグをリセット
      ticking = false;
    }

    function requestTick() {
      if (!ticking) {
        // 次の描画フレームで実行をスケジュール
        window.requestAnimationFrame(storeScrollData);
        ticking = true;
      }
    }

    // passive:  true でスクロールパフォーマンスを向上
    window.addEventListener('scroll', requestTick, { passive: true });

    // 初期実行
    storeScrollData();
  }
  
  document.addEventListener('DOMContentLoaded', initHeader);
</script>


























































































































































<style>
    html .page-banner .page-banner-img > *:first-child {
      opacity: 0.4;
    }

    html[data-theme="dark"] .page-banner .page-banner-img > *:first-child {
      opacity: 0.2872;
    }
  </style>
<section class="page-banner">
    <div class="page-banner-img">
<div style="background-image: url(/paper_notes/assets/images/banner.png)"></div>
        <img class="img-placeholder" src="/paper_notes/assets/images/banner.png">
</div>
    <div class="wrapper">
      <div class="page-banner-inner">
<header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">わたしのべんきょうノート</h1>
  <h2 class="post-subtitle">勉強した論文や技術等の情報をGithubのIssueにメモっているひとのブログ。
それなりにメモの量が蓄積されてきたので、一度整理したいなと思いブログはじめてみました！
自然言語処理(NLP), 推薦システム(RecommenderSystem), Educational Data Mining (EDM), Learning Analytics (LA)などの分野のメモが多いと思います。
最近は特にLLMの勉強が多めです :)</h2>

  <div class="post-meta">
    <time class="dt-published" datetime="2025-12-28T00:00:00+00:00" itemprop="datePublished"><i class="fa fa-calendar"></i> Dec 28, 2025
    </time><span class="post-author left-vsplit"><i class="fa fa-pencil"></i> AkihikoWATANABE</span>
    
































    <span class="post-reading-time left-vsplit"><i class="fa fa-clock-o"></i> About 8 hours 44 mins</span>
  </div></header>
</div>
    </div>
  </section><script>
  function hashLocate(hashValue) {
    hashValue = hashValue.replace(/^.*#h-/, '');
    hashValue = decodeURIComponent(hashValue);
    var element = document.getElementById(hashValue);

    if (!element) {
      return;
    }

    var header = document.querySelector('header.site-header');
    var headerRect = header.getBoundingClientRect();
    var headerTop = Math.floor(headerRect.top);
    var headerHeight = Math.floor(headerRect.height);
    var scrollPos = getScrollPos();
    var offsetY = element.offsetTop - (headerTop + headerHeight + 20);

    if (offsetY == scrollPos.y) {
      return;
    }

    if (headerTop == 0  && offsetY > scrollPos.y) {
      offsetY += headerHeight + 2;
    } else if (headerTop < 0  && offsetY < scrollPos.y) {
      offsetY -= headerHeight - 2;
    }

    smoothScrollTo(offsetY);
  }

  // The first event occurred
  window.addEventListener('load', function(event) {
    if (window.location.hash) {
      hashLocate(window.location.hash);
    }
  });

  // The first event occurred
  window.addEventListener('click', function(event) {
    if (event.target.tagName.toLowerCase() == 'a') {
      hashLocate(event.target.getAttribute('href'));
    }
  });
</script>
<div class="theme-toggle">
  <input type="checkbox" id="theme-switch">
  <label for="theme-switch">
    <div class="toggle"></div>
    <div class="names">
      <p class="light">Light</p>
      <p class="dark">Dark</p>
    </div>
  </label>
</div>




<script>
  (function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var nightModeOption = ('off' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
      storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
      var data = storage.getItem('theme');
      try {
        data = JSON.parse(data ? data : '');
      } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
      }
      return data;
    }

    function handleThemeToggle(nightShift) {
      themeData.nightShift = nightShift;
      saveThemeData(themeData);
      html.dataset.theme = nightShift ? 'dark' : 'light';
      setTimeout(function() {
        sw.checked = nightShift ? true : false;
      }, 50);
    }

    function autoThemeToggle() {
      // Next time point of theme toggle
      var now = new Date();
      var toggleAt = new Date();
      var hours = now.getHours();
      var nightShift = hours >= 19 || hours <=7;

      if (nightShift) {
        if (hours > 7) {
          toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
      } else {
        toggleAt.setHours(19);
      }

      toggleAt.setMinutes(0);
      toggleAt.setSeconds(0);
      toggleAt.setMilliseconds(0)

      var delay = toggleAt.getTime() - now.getTime();

      // auto toggle theme mode
      setTimeout(function() {
        handleThemeToggle(!nightShift);
      }, delay);

      return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
      };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
      handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
      var data = autoThemeToggle();

      // Toggle theme by local setting
      if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
      } else {
        handleThemeToggle(themeData.nightShift);
      }
    } else if (nightModeOption == 'manual') {
      handleThemeToggle(themeData.nightShift);
    } else {
      var nightShift = themeData.nightShift;
      if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
      }
      handleThemeToggle(nightShift);
    }
  })();
</script>
<div id="click-to-top" class="click-to-top">
  <i class="fa fa-arrow-up"></i>
</div>
<script>
  (function () {
    const clickToTop = document.getElementById('click-to-top');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 100) {
        clickToTop.classList.add('show')
      }else {
        clickToTop.classList.remove('show')
      }
    });
    clickToTop.addEventListener('click', () => {
      window.smoothScrollTo(0);
    });
  })();
</script>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="framework">
  <section class="main">

     <div class="post">
  <section>









<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="post-content e-content" itemprop="articleBody">

      <h2 id="EfficiencyImprovement" class="paper-head"> EfficiencyImprovement</h2>
<div class="visible-content">
<article class="paper-entry">
<h3 id="mechanistic-finetuning-4084" class="title-link">[Paper Note] Mechanistic Finetuning of Vision-Language-Action Models via Few-Shot Demonstrations, Chancharik Mitra+, arXiv'25, 2025.11</h3>
<br><a href="https://arxiv.org/abs/2511.22697" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/4084" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="PEFT(Adaptor_LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="Robotics.html" target="_blank" rel="noopener noreferrer">#Robotics</a>
<a class="button" href="VisionLanguageActionModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageActionModel</a>
<a class="button" href="EmbodiedAI.html" target="_blank" rel="noopener noreferrer">#EmbodiedAI</a>
<a class="button" href="One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-12-28</span>
<span class="snippet"><span>GPT Summary</span>- VLAモデルはロボティクスにおける視覚と言語の統合を目指すが、物理的要因へのファインチューニングが必要。既存手法は特異性に欠けるため、タスク特異的な注意ヘッドを選択的にファインチューニングする「Robotic Steering」を提案。Franka Emikaロボットアームでの評価により、Robotic SteeringがLoRAを上回り、堅牢性、計算コスト削減、解釈可能性の向上を実現することを示した。</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://chancharikmitra.github.io/robosteering/" target="_blank" rel="noopener noreferrer">https://chancharikmitra.github.io/robosteering/</a>


</p>
<p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/chancharikm/status/2004597444730929596?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>VLAにおいて学習したいタスクと関連する(sparseな） attention headsだけをfinetuningすることで、効率的に、忘却を防ぎつつ、overfitを防ぐような手法を提案。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="sonicmoe-accelerating-4002" class="title-link">[Paper Note] SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations, Wentao Guo+, arXiv'25, 2025.12</h3>
<br><a href="https://arxiv.org/abs/2512.14080" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/4002" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<a class="button" href="mid-training.html" target="_blank" rel="noopener noreferrer">#mid-training</a>
<a class="button" href="PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-12-19</span>
<span class="snippet"><span>GPT Summary</span>- SonicMoEは、MoEモデルのフォワードおよびバックワードパスをメモリ効率良く計算するアルゴリズムを提案し、活性化メモリを45%削減。Hopper GPU上で7B MoEモデルの計算スループットを1.86倍改善し、トレーニングスループットは2130億トークン/日を達成。新しいトークンラウンディング手法により、カーネル実行時間で1.16倍のスピードアップを実現。すべてのカーネルはオープンソース化され、MoEモデルのトレーニングを加速。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/tri_dao/status/2001785266873499875?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>MoEモデルの学習速度、メモリ使用が最大2倍効率化される実装らしい。ただしHopperに特化している模様。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="fast-and-3998" class="title-link">[Paper Note] Fast and Accurate Causal Parallel Decoding using Jacobi Forcing, Lanxiang Hu+, arXiv'25, 2025.12</h3>
<br><a href="https://arxiv.org/abs/2512.14681" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3998" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-12-18</span>
<span class="snippet"><span>GPT Summary</span>- マルチトークン生成において、Jacobi Forcingを導入し、ARモデルから効率的な並列デコーダーへの移行を実現。これにより、コーディングと数学のベンチマークで3.8倍の速度向上を達成し、マルチブロックデコーディングで最大4.5倍のトークン受け入れ数を実現。推論のレイテンシを低下させることが可能に。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/haozhangml/status/2001409875516211558?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>pj page:


<a href="https://hao-ai-lab.github.io/blogs/jacobi-forcing/" target="_blank" rel="noopener noreferrer">https://hao-ai-lab.github.io/blogs/jacobi-forcing/</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="fb-rag-improving-3994" class="title-link">[Paper Note] FB-RAG: Improving RAG with Forward and Backward Lookup, Kushal Chawla+, AACL'25 Findings, 2025.05</h3>
<br><a href="https://arxiv.org/abs/2505.17206" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3994" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="InformationRetrieval.html" target="_blank" rel="noopener noreferrer">#InformationRetrieval</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="RAG(RetrievalAugmentedGeneration).html" target="_blank" rel="noopener noreferrer">#RAG(RetrievalAugmentedGeneration)</a>
<a class="button" href="SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="AACL.html" target="_blank" rel="noopener noreferrer">#AACL</a>
<a class="button" href="SpeculativeDecoding.html" target="_blank" rel="noopener noreferrer">#SpeculativeDecoding</a>
<a class="button" href="One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-12-18</span>
<span class="snippet"><span>GPT Summary</span>- FB-RAGは、複雑なクエリに対するRAGの課題を解決する新しいフレームワークで、軽量のLLMを用いて関連性の高いコンテキストを特定。従来のファインチューニングなしで性能向上を実現し、レイテンシを削減。EN.QAデータセットでは、リーディングベースラインに匹敵し、性能向上とレイテンシ削減を達成。小さなLLMが大きなLLMの性能を向上させる可能性を示す。</span>
<span class="snippet"><span>Comment</span><p>元ポスト: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dair_ai/status/2001306474803540415?s=20"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>使いやすそうなアプローチなので覚えておくと実用上は良いかもしれない</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="routerag-efficient-3989" class="title-link">[Paper Note] RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning, Yucan Guo+, arXiv'25, 2025.12</h3>
<br><a href="https://arxiv.org/abs/2512.09487" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3989" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Multi.html" target="_blank" rel="noopener noreferrer">#Multi</a>
<a class="button" href="InformationRetrieval.html" target="_blank" rel="noopener noreferrer">#InformationRetrieval</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="RAG(RetrievalAugmentedGeneration).html" target="_blank" rel="noopener noreferrer">#RAG(RetrievalAugmentedGeneration)</a>
<a class="button" href="KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-12-17</span>
<span class="snippet"><span>GPT Summary</span>- Retrieval-Augmented Generation (RAG)を用いた新しいRLベースのフレームワーク\model{}を提案。これにより、LLMsがマルチターンのグラフ-テキストハイブリッドRAGを実行し、推論のタイミングや情報取得を学習。二段階のトレーニングフレームワークにより、ハイブリッド証拠を活用しつつリトリーバルのオーバーヘッドを回避。実験結果は、\model{}が既存のRAGベースラインを大幅に上回ることを示し、複雑な推論における効率的なリトリーバルの利点を強調。</span>
<span class="snippet"><span>Comment</span><p>元ポスト: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dair_ai/status/2000400449355325806?s=20"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>モデル自身が何を、いつ、どこからretrievalし、いつやめるかをするかを動的にreasoningできるようRLで学習することで、コストの高いretrievalを削減し、マルチターンRAGの性能を保ちつつ効率をあげる手法（最大で検索のターン数が20パーセント削減）とのこと。<br><br>学習は2ステージで、最初のステージでanswerに正しく辿り着けるよう学習することでreasoning能力を向上させ、次のステージで不要な検索が削減されるような効率に関するrewardを組み込み、accuracyとcostのバランスをとる。モデルはツールとして検索を利用できるが、ツールはpassage, graph, hybridの3つの検索方法を選択できる。<br><br><img src="https://github.com/user-attachments/assets/1cb7e1f6-30ed-4f00-875d-a4c7b187fb87" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="one-layer-3946" class="title-link">[Paper Note] One Layer Is Enough: Adapting Pretrained Visual Encoders for Image Generation, Yuan Gao+, arXiv'25, 2025.12</h3>
<br><a href="https://arxiv.org/abs/2512.07829" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3946" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<a class="button" href="2D%20(Image).html" target="_blank" rel="noopener noreferrer">#2D (Image)</a>
<a class="button" href="AutoEncoder.html" target="_blank" rel="noopener noreferrer">#AutoEncoder</a>
<span class="issue_date">Issue Date: 2025-12-15</span>
<span class="snippet"><span>GPT Summary</span>- 視覚生成モデルにおける潜在空間の不一致を解消するため、FAE（Feature Auto-Encoder）を提案。FAEは、再構成と生成の両方に必要な情報を保持しつつ、1つのアテンション層で実現。2つの深層デコーダを組み合わせ、さまざまな自己教師ありエンコーダに対応。拡散モデルや正規化フローと接続可能で、ImageNetでのベンチマークにおいて優れた性能を示す。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/askalphaxiv/status/2000288992458424770?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="budget-aware-tool-use-3945" class="title-link">[Paper Note] Budget-Aware Tool-Use Enables Effective Agent Scaling, Tengxiao Liu+, arXiv'25, 2025.11</h3>
<br><a href="https://arxiv.org/abs/2511.17006" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3945" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="Test-Time%20Scaling.html" target="_blank" rel="noopener noreferrer">#Test-Time Scaling</a>
<a class="button" href="One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-12-15</span>
<span class="snippet"><span>GPT Summary</span>- 大規模言語モデル（LLMs）のエージェントにおけるツールコールのスケーリングを研究。単にツールコール予算を増やすだけでは効果がなく、予算意識が必要。軽量プラグイン「Budget Tracker」を導入し、動的に計画を適応させる「BATS」を開発。コストとパフォーマンスを共同で考慮する指標を定式化し、予算意識のある手法がより良いスケーリングを実現することを示す。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/2000225781927325863?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>AI Agentにplug-and-playでbudgetに関する情報をinternalなreasoning token中に出力させる(budget tracker)ことで、余剰なtoken消費、tool callのコストを自律的に調整させながらタスクを遂行させる手法に見える。<br><br>budget trackerは非常にシンプルなpromptで以下のようなブロックで表現され、ツールごとにbudgetがスタート時点に決められており、個々のツールごとに残りのbudgetをブロック中に動的に出力させる。たとえばtool1は検索（budgetはクエリの発行数）、tool2はブラウジング（budgetはurl数）のようなものである。<br><br>```<br><budget><br>Tool1 Budget Used: ##, Tool1 Budget Remaining: ##<br>Tool2 Budget Used: ##, Tool2 Budget Remaining: ##<br>Make the best use of the available resources.<br></budget><br>```<br><br>自律的に制御すると記述したが、AppendixCを見る限りは、promptingに応じてbudgetの残量に応じた方向性はgivenな設定なようである。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="thinking-free-policy-3941" class="title-link">[Paper Note] Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners, Xin Xu+, arXiv'25, 2025.09</h3>
<br><a href="https://arxiv.org/abs/2509.26226" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3941" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="RLVR.html" target="_blank" rel="noopener noreferrer">#RLVR</a>
<span class="issue_date">Issue Date: 2025-12-13</span>
<span class="snippet"><span>GPT Summary</span>- TFPI（Thinking-Free Policy Initialization）は、強化学習における長いコンテキスト長の問題を解決するための手法で、思考内容を破棄する*ThinkFree*操作を用いてトークン使用量を削減します。これにより、トレーニングの効率が向上し、RLの収束を加速し、より高い性能を達成します。TFPIを用いた4Bモデルは、AIME24で89.0%、LiveCodeBenchで65.5%の精度を記録しました。</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=RKYO6R8Jgb" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=RKYO6R8Jgb</a>


</p>
<p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/viemccoy/status/1999556573044310353?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="can-you-3926" class="title-link">[Paper Note] Can You Learn to See Without Images? Procedural Warm-Up for Vision Transformers, Zachary Shinnick+, arXiv'25, 2025.11</h3>
<br><a href="https://arxiv.org/abs/2511.13945" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3926" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="2D%20(Image).html" target="_blank" rel="noopener noreferrer">#2D (Image)</a>
<a class="button" href="KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<a class="button" href="WarmUp.html" target="_blank" rel="noopener noreferrer">#WarmUp</a>
<span class="issue_date">Issue Date: 2025-12-11</span>
<span class="snippet"><span>GPT Summary</span>- 視覚トランスフォーマー（ViTs）を手続き生成データで事前学習する新しい方法を提案。これにより、モデルは抽象的な計算的知識を内在化し、標準的な画像トレーニングでデータ効率やパフォーマンスが向上。ImageNet-1kで1%の手続き生成データを使用することで、精度が1.7%以上向上し、28%のデータに相当する効果を示す。新しい事前学習戦略の可能性を示唆。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/damienteney/status/1998660264892264572?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>特定のgrammarを持つ（意味情報を持たない予測可能な）シンボルトークン列（e.g.,規則的なアルファベットの羅列, 括弧による階層構造; 非画像データ）を用いてViTのTransformerブロックを事前学習することによって、MLPやattention Layerに対して構造情報を捉える能力がwarmupされ、その後実画像で事前学習をするとサンプル効率が上がる、という話らしい。<br><br>warmupでは、ViTにおける入力機構（画像パッチ+linear layer）は一切用いず、discreteなトークンと、それらをランダムに初期化したlookup table を用いる。このとき、embeddingとpositional encodingをfreezeすることで、MLP, Attention Layerに知識が埋め込まれることを保証する。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="threadweaver-adaptive-3915" class="title-link">[Paper Note] ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models, Long Lian+, arXiv'25, 2025.11</h3>
<br><a href="https://arxiv.org/abs/2512.07843" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3915" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<a class="button" href="Parallel.html" target="_blank" rel="noopener noreferrer">#Parallel</a>
<span class="issue_date">Issue Date: 2025-12-10</span>
<span class="snippet"><span>GPT Summary</span>- ThreadWeaverは、適応型並列推論のフレームワークで、逐次推論モデルと同等の精度を保ちながら推論の遅延を大幅に削減します。主な革新は、二段階の並列軌道生成器、オフ・ザ・シェルフの自己回帰推論エンジンでの並列推論、並列化意識のある強化学習フレームワークです。これにより、数学的推論ベンチマークで高い精度を維持しつつ、最大1.53倍のスピードアップを達成しました。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_akhaliq/status/1998614936835035576?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="xrouter-training-3798" class="title-link">[Paper Note] xRouter: Training Cost-Aware LLMs Orchestration System via Reinforcement Learning, Cheng Qian+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.08439" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3798" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="Routing.html" target="_blank" rel="noopener noreferrer">#Routing</a>
<span class="issue_date">Issue Date: 2025-11-25</span>
<span class="snippet"><span>GPT Summary</span>- xRouterは、コストとパフォーマンスのトレードオフを考慮したルーティングシステムで、学習されたルーターが直接回答するか外部モデルを呼び出す。強化学習により訓練され、手動ルールの必要がない。多様なベンチマークでコスト削減とタスク完了率の向上を実現し、LLMオーケストレーションの進展に寄与することを目指す。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscreamnearby/status/1993142931428196539?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="think-or-3787" class="title-link">[Paper Note] Think or Not? Selective Reasoning via Reinforcement Learning for Vision-Language Models, Jiaqi Wang+, NeurIPS'25, 2025.05</h3>
<br><a href="https://arxiv.org/abs/2505.16854" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3787" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-11-25</span>
<span class="snippet"><span>GPT Summary</span>- 強化学習を用いて視覚と言語モデルの推論を強化するために、TONという二段階のトレーニング戦略を提案。簡単な質問には推論をスキップし、必要な時に考える人間の思考プロセスを模倣。実験により、TONは従来の手法に比べて推論ステップを最大90％削減し、性能を向上させることが示された。モデルはトレーニングを通じて不要な推論を回避することを学習。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/kevinqhlin/status/1993120942399033679?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>著者ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/kevinqhlin/status/1925799774835392871?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>いつ思考をするか/しないかを学習することでCoTのtrajectoryを節約する。選択的に思考しないということをモデルは基本的に学習していないのでSFTで模倣学習することでコールドスタートを脱っし、その後RLによって選択的に思考しないことも含めて思考を最適化する、といった話に見える。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="apriel-h1-towards-3777" class="title-link">[Paper Note] Apriel-H1: Towards Efficient Enterprise Reasoning Models, Oleksiy Ostapenko+, arXiv'25, 2025.11</h3>
<br><a href="https://arxiv.org/abs/2511.02651" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3777" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="RecurrentModels.html" target="_blank" rel="noopener noreferrer">#RecurrentModels</a>
<span class="issue_date">Issue Date: 2025-11-22</span>
<span class="snippet"><span>GPT Summary</span>- 大規模言語モデル（LLMs）は、トランスフォーマーアーキテクチャの限界を克服するために、状態空間モデル（SSMs）と注意メカニズムを組み合わせたハイブリッドモデルApriel-H1を提案。これにより、推論性能を維持しつつ、スループットを2倍以上向上させることに成功。蒸留を通じて、重要度の低い注意層をSSMに置き換え、効率的な推論を実現。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/tscholak/status/1991586600241807517?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>blog:


<a href="https://huggingface.co/blog/ServiceNow-AI/apriel-h1" target="_blank" rel="noopener noreferrer">https://huggingface.co/blog/ServiceNow-AI/apriel-h1</a>


<br>HF:


<a href="https://huggingface.co/collections/ServiceNow-AI/apriel-h1" target="_blank" rel="noopener noreferrer">https://huggingface.co/collections/ServiceNow-AI/apriel-h1</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="taming-the-3772" class="title-link">[Paper Note] Taming the Long-Tail: Efficient Reasoning RL Training with Adaptive Drafter, Qinghao Hu+, arXiv'25, 2025.11</h3>
<br><a href="https://arxiv.org/abs/2511.16665" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3772" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-11-21</span>
<span class="snippet"><span>GPT Summary</span>- 大規模言語モデル（LLMs）の推論能力を向上させるため、TLTを提案。TLTは適応的な推測デコーディングを用いて、強化学習（RL）トレーニングの効率を向上させる。主なコンポーネントは、アイドルGPUでトレーニングされるアダプティブドラフターと、メモリ効率の良いプールを維持するアダプティブロールアウトエンジン。TLTは、最先端システムに対して1.7倍のトレーニング速度向上を実現し、モデルの精度を保持しつつ高品質なドラフトモデルを生成。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1991732743663940048?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>ロングテールのrolloutをする際にspeculative decodingをすることでボトルネックを改善しon-policy RLの速度を改善する話らしいが、Inflight Weight Updatesがもしうまく機能するならこちらの方が簡単な気がするが、果たしてどうなのだろうか。<br>関連:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3130" target="_blank" rel="noopener noreferrer">PipelineRL, Piche+, ServiceNow, 2025.04</a>
</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="seer-online-3748" class="title-link">[Paper Note] Seer: Online Context Learning for Fast Synchronous LLM Reinforcement Learning, Ruoyu Qin+, arXiv'25, 2025.11</h3>
<br><a href="https://arxiv.org/abs/2511.14617" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3748" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="Off-Policy.html" target="_blank" rel="noopener noreferrer">#Off-Policy</a>
<a class="button" href="On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<span class="issue_date">Issue Date: 2025-11-20</span>
<span class="snippet"><span>GPT Summary</span>- 強化学習における性能ボトルネックを解消するために、新しいオンラインコンテキスト学習システム「Seer」を提案。Seerは、出力の類似性を活用し、分割ロールアウト、コンテキストに基づくスケジューリング、適応的グループ化推測デコーディングを導入。これにより、ロールアウトの待機時間を大幅に短縮し、リソース効率を向上。評価結果では、エンドツーエンドのロールアウトスループットを74%から97%向上させ、待機時間を75%から93%削減した。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/kimi_moonshot/status/1991409529930617093?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="kandinsky-5.0-3745" class="title-link">[Paper Note] Kandinsky 5.0: A Family of Foundation Models for Image and Video Generation, Vladimir Arkhipkin+, arXiv'25, 2025.11</h3>
<br><a href="https://arxiv.org/abs/2511.14993" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3745" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<a class="button" href="DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="TextToImageGeneration.html" target="_blank" rel="noopener noreferrer">#TextToImageGeneration</a>
<a class="button" href="SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="VideoGeneration_Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-11-20</span>
<span class="snippet"><span>GPT Summary</span>- Kandinsky 5.0は、高解像度画像と10秒動画合成のための最先端モデルで、3つのコアモデル（Image Lite、Video Lite、Video Pro）から構成される。データキュレーションライフサイクルのレビューや、自己教師ありファインチューニングや強化学習を用いた品質向上技術を取り入れ、高い生成速度とパフォーマンスを実現。オープンソースコードとトレーニングチェックポイントの提供により、研究コミュニティの発展に寄与することを目指す。</span>
<span class="snippet"><span>Comment</span><p>HF:


<a href="https://huggingface.co/kandinskylab" target="_blank" rel="noopener noreferrer">https://huggingface.co/kandinskylab</a>


</p>
<p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1991477026910531742?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="lightrag-simple-3713" class="title-link">[Paper Note] LightRAG: Simple and Fast Retrieval-Augmented Generation, Zirui Guo+, EMNLP'25, 2024.10</h3>
<br><a href="https://arxiv.org/abs/2410.05779" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3713" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="GraphBased.html" target="_blank" rel="noopener noreferrer">#GraphBased</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="RAG(RetrievalAugmentedGeneration).html" target="_blank" rel="noopener noreferrer">#RAG(RetrievalAugmentedGeneration)</a>
<a class="button" href="EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<span class="issue_date">Issue Date: 2025-11-18</span>
<span class="snippet"><span>GPT Summary</span>- LightRAGは、グラフ構造を取り入れたRetrieval-Augmented Generation (RAG)システムで、文脈に関連した応答を提供します。二重レベルの検索システムにより、知識発見を強化し、関連エンティティの効率的な検索を実現。増分更新アルゴリズムにより、急速に変化するデータ環境でも応答性を維持。実験により、既存のアプローチと比較して精度と効率が大幅に改善されたことが示されました。LightRAGはオープンソースで公開されています。</span>
<span class="snippet"><span>Comment</span><p>github:


<a href="https://github.com/HKUDS/LightRAG" target="_blank" rel="noopener noreferrer">https://github.com/HKUDS/LightRAG</a>


</p>
<p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huang_chao4969/status/1990277067431424465?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="optimizing-mixture-3700" class="title-link">[Paper Note] Optimizing Mixture of Block Attention, Guangxuan Xiao+, arXiv'25, 2025.11</h3>
<br><a href="https://arxiv.org/abs/2511.11571" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3700" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2025-11-17</span>
<span class="snippet"><span>GPT Summary</span>- Mixture of Block Attention (MoBA)は、LLMにおける長いコンテキスト処理を効率化するが、その設計原則やGPU実装が不十分である。本研究では、MoBAのメカニズムを分析し、クエリとキーの親和性に基づくブロックの識別能力が性能に影響することを明らかにする。改善策として、小さなブロックサイズの使用とキーに対する短い畳み込みの適用を提案。これを実現するために、FlashMoBAを導入し、効率的なMoBA実行を可能にするCUDAカーネルを開発。FlashMoBAは、最大14.7倍のスピードアップを達成し、理論に基づく改善を実用化した。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/papers_anon/status/1990266669206536330?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>flash_attention2に対して最大で14.7倍👀どういう条件、実験だろうか</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="virtual-width-3699" class="title-link">[Paper Note] Virtual Width Networks, Seed+, arXiv'25, 2025.11</h3>
<br><a href="https://arxiv.org/abs/2511.11238" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3699" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-11-17</span>
<span class="snippet"><span>GPT Summary</span>- Virtual Width Networks (VWN)は、隠れ層のサイズを増やすことなく、より広い表現を可能にするフレームワークである。VWNはバックボーンの計算をほぼ一定に保ちながら埋め込み空間を拡張し、8倍の拡張でトークン予測の最適化を加速することを示した。トレーニングが進むにつれてこの利点は増幅され、仮想幅と損失削減の間には対数線形のスケーリング関係があることが確認された。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1990270269873864796?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>ポイント解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/papers_anon/status/1990269413195743671?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>重要論文に見える。transformerのバックボーンの次元は変えないでベクトルのwidthを広げることと同等の効力を得るためのアーキテクチャを提案している模様。<br><br>ざっくり言うとembeddingをN倍（over-width)し、提案手法であるGHCを用いてバックボーンに流せるサイズにベクトルを圧縮しtransformerブロックで処理しover-widthした次元に戻す処理をする機構と、over-widthしたembeddingを次元数は変えずに変換するlinearを噛ませた結果を足し合わせるような機構を用意して最大のボトルネックであるtransformerブロックの計算量は変えずに表現力を向上させる、といった感じの手法な模様<br><br><img src="https://github.com/user-attachments/assets/8c2fc967-54ff-43fa-a469-ece0cdaa0131" alt="image" loading="lazy" width="400" height="300"><br><img src="https://github.com/user-attachments/assets/8d9a123f-747f-48f7-b04b-167e98ed2350" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="tidar-think-3658" class="title-link">[Paper Note] TiDAR: Think in Diffusion, Talk in Autoregression, Jingyu Liu+, arXiv'25, 2025.11</h3>
<br><a href="https://arxiv.org/abs/2511.08923" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3658" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-11-13</span>
<span class="snippet"><span>GPT Summary</span>- TiDARは、拡散言語モデルと自己回帰モデルの利点を融合したハイブリッドアーキテクチャで、トークンのドラフトとサンプリングを単一のフォワードパスで実行します。これにより、高スループットとARモデルに匹敵する品質を両立させ、推測的デコーディングを上回る効率を実現しました。TiDARは、1秒あたり4.71倍から5.91倍のトークン生成を可能にし、ARモデルとの品質ギャップを初めて埋めました。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1988852973938889104?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>解説:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1990928212726288562?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="teaching-pretrained-3655" class="title-link">[Paper Note] Teaching Pretrained Language Models to Think Deeper with Retrofitted Recurrence, Sean McLeish+, arXiv'25, 2025.11</h3>
<br><a href="https://arxiv.org/abs/2511.07384" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3655" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="LatentReasoning.html" target="_blank" rel="noopener noreferrer">#LatentReasoning</a>
<a class="button" href="RecurrentModels.html" target="_blank" rel="noopener noreferrer">#RecurrentModels</a>
<a class="button" href="RecursiveModels.html" target="_blank" rel="noopener noreferrer">#RecursiveModels</a>
<span class="issue_date">Issue Date: 2025-11-12</span>
<span class="snippet"><span>GPT Summary</span>- 深層再帰言語モデルの進展により、再帰の計算量を訓練時とテスト時で切り離すことが可能に。本研究では、非再帰言語モデルを深層再帰モデルに変換する方法を提案し、再帰のカリキュラムを用いることで性能を維持しつつ計算コストを削減できることを示した。数学実験では、再帰モデルへの変換がポストトレーニングよりも優れた性能を発揮することが確認された。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/seanmcleish/status/1988268805479690364?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>関連:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/micahgoldblum/status/1988265009508655528?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="analyzing-uncertainty-3636" class="title-link">[Paper Note] Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with   Conformal Prediction, Huanxin Sheng+, EMNLP'25 SAC Highlights, 2025.09</h3>
<br><a href="https://arxiv.org/abs/2509.18658" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3636" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="LLM-as-a-Judge.html" target="_blank" rel="noopener noreferrer">#LLM-as-a-Judge</a>
<a class="button" href="EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="Stability.html" target="_blank" rel="noopener noreferrer">#Stability</a>
<span class="issue_date">Issue Date: 2025-11-10</span>
<span class="snippet"><span>GPT Summary</span>- LLMを用いた自然言語生成の評価における不確実性を分析するためのフレームワークを提案。適合予測を通じて予測区間を構築し、中央値に基づくスコアを低バイアスの代替手段として提示。実験により、適合予測が有効な予測区間を提供できることを示し、判断の向上に向けた中央値や再プロンプトの有用性も探求。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huanxinshe5254/status/1987186857545969698?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>実用上非常に重要な話に見える</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="infini-gram-mini-3633" class="title-link">[Paper Note] Infini-gram mini: Exact n-gram Search at the Internet Scale with   FM-Index, Hao Xu+, EMNLP'25 Best Paper, 2025.06</h3>
<br><a href="https://arxiv.org/abs/2506.12229" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3633" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Search.html" target="_blank" rel="noopener noreferrer">#Search</a>
<a class="button" href="Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Contamination-free.html" target="_blank" rel="noopener noreferrer">#Contamination-free</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-11-09</span>
<span class="snippet"><span>GPT Summary</span>- 「infini-gram mini」は、ペタバイトレベルのテキストコーパスを効率的に検索可能にするシステムで、FM-indexデータ構造を用いてインデックスを作成し、ストレージオーバーヘッドを44%に削減。インデックス作成速度やメモリ使用量を大幅に改善し、83TBのインターネットテキストを99日でインデックス化。大規模なベンチマーク汚染の分析を行い、主要なLM評価ベンチマークがインターネットクローリングで汚染されていることを発見。汚染率を共有する公報をホストし、検索クエリ用のウェブインターフェースとAPIも提供。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/liujc1998/status/1986821544405045497?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>pj page:


<a href="https://infini-gram-mini.io" target="_blank" rel="noopener noreferrer">https://infini-gram-mini.io</a>


</p>
<p>benchmarmk contamination monitoring system: 


<a href="https://huggingface.co/spaces/infini-gram-mini/Benchmark-Contamination-Monitoring-System" target="_blank" rel="noopener noreferrer">https://huggingface.co/spaces/infini-gram-mini/Benchmark-Contamination-Monitoring-System</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="edgetam-on-device-3631" class="title-link">[Paper Note] EdgeTAM: On-Device Track Anything Model, Chong Zhou+, arXiv'25, 2025.01</h3>
<br><a href="https://arxiv.org/abs/2501.07256" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3631" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="ImageSegmentation.html" target="_blank" rel="noopener noreferrer">#ImageSegmentation</a>
<a class="button" href="SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="Video.html" target="_blank" rel="noopener noreferrer">#Video</a>
<a class="button" href="2D%20(Image).html" target="_blank" rel="noopener noreferrer">#2D (Image)</a>
<span class="issue_date">Issue Date: 2025-11-09</span>
<span class="snippet"><span>GPT Summary</span>- SAM 2は動画セグメンテーションの基盤モデルであり、メモリバンクメカニズムを通じて性能を向上させています。本研究では、モバイルデバイス上での効率を高めるために、EdgeTAMを提案し、2D空間パーセプターを用いて計算コストを削減します。これにより、メモリの空間構造を保持しつつ、推論オーバーヘッドなしで性能を向上させる蒸留パイプラインも導入。EdgeTAMは複数のデータセットで高いJ&amp;Fスコアを達成し、iPhone 15 Pro Maxで16 FPSで動作します。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/mervenoyann/status/1986785795424788812?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>SAM2より性能は少し劣るが、edge-deviceてわ動作可能で非常に高速なモデル（promptによって制御可能なsegmentation)とのこと<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3632" target="_blank" rel="noopener noreferrer">[Paper Note] SAM 2: Segment Anything in Images and Videos, Nikhila Ravi+, ICLR'25, 2024.08</a>
</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="scaling-agent-3617" class="title-link">[Paper Note] Scaling Agent Learning via Experience Synthesis, Zhaorun Chen+, arXiv'25, 2025.11</h3>
<br><a href="https://arxiv.org/abs/2511.03773" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3617" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<span class="issue_date">Issue Date: 2025-11-07</span>
<span class="snippet"><span>GPT Summary</span>- DreamGymは、強化学習（RL）エージェントのオンライントレーニングを効率化するための統一フレームワークであり、高コストのロールアウトや不安定な報酬信号の課題に対処します。環境のダイナミクスを推論に基づく経験モデルに蒸留し、安定した状態遷移とフィードバックを提供します。オフラインデータを活用した経験リプレイバッファにより、エージェントのトレーニングを強化し、新しいタスクを適応的に生成することでオンラインカリキュラム学習を実現します。実験により、DreamGymは合成設定とリアルなシナリオでRLトレーニングを大幅に改善し、非RL準備タスクでは30％以上の性能向上を示しました。合成経験のみでトレーニングされたポリシーは、実環境RLにおいても優れたパフォーマンスを発揮し、スケーラブルなウォームスタート戦略を提供します。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jaseweston/status/1986613046047846569?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="pipelinerl-faster-3615" class="title-link">[Paper Note] PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence  Generation, Alexandre Piché+, arXiv'25, 2025.09</h3>
<br><a href="https://arxiv.org/abs/2509.19128" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3615" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-11-07</span>
<span class="snippet"><span>GPT Summary</span>- 強化学習（RL）を用いて大規模言語モデル（LLMs）の推論能力を向上させるための新しいアプローチ、PipelineRLを提案。PipelineRLは非同期データ生成とモデル更新を同時に行い、トレーニングデータの新鮮さを保ちながら、GPUの利用率を最大化。実験では、従来のRL手法に比べて約2倍の学習速度を達成。PipelineRLのオープンソース実装も公開。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/finbarrtimbers/status/1986481494823739581?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="culture-cartography-3605" class="title-link">[Paper Note] Culture Cartography: Mapping the Landscape of Cultural Knowledge, Caleb Ziems+, EMNLP'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.27672" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3605" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="DPO.html" target="_blank" rel="noopener noreferrer">#DPO</a>
<a class="button" href="Cultural.html" target="_blank" rel="noopener noreferrer">#Cultural</a>
<span class="issue_date">Issue Date: 2025-11-06</span>
<span class="snippet"><span>GPT Summary</span>- LLMは文化特有の知識を必要とし、CultureCartographyという混合イニシアティブを提案。LLMが自信の低い質問をアノテーションし、人間がそのギャップを埋めることで重要なトピックに導く。CultureExplorerツールを用いた実験で、従来のモデルよりも効果的に知識を生成し、Llama-3.1-8Bの精度を最大19.2%向上させることが示された。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/diyi_yang/status/1985826293053866234?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>効率的にLLMにとって未知、かつ重要な文化的な知識バンクを作成する話な模様。アクティブラーニングに似たような思想に見える。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="opportunistic-expert-3576" class="title-link">[Paper Note] Opportunistic Expert Activation: Batch-Aware Expert Routing for Faster  Decode Without Retraining, Costin-Andrei Oncescu+, arXiv'25, 2025.11</h3>
<br><a href="https://arxiv.org/abs/2511.02237" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3576" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<span class="issue_date">Issue Date: 2025-11-05</span>
<span class="snippet"><span>GPT Summary</span>- MoEアーキテクチャを用いたLLMのデコードレイテンシを低下させるため、トークンから専門家へのマッピングを動的に再ルーティングするフレームワークを提案。バッチ認識ルーティングを活用し、メモリに既にロードされている専門家を利用することで、精度を維持しつつ、Qwen3-30BおよびQwen3-235Bモデルでそれぞれ39%と15%のレイテンシ削減を達成。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1985955439029227927?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="emu3.5-native-3542" class="title-link">[Paper Note] Emu3.5: Native Multimodal Models are World Learners, Yufeng Cui+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.26583" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3542" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="2D%20(Image).html" target="_blank" rel="noopener noreferrer">#2D (Image)</a>
<a class="button" href="UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<a class="button" href="text.html" target="_blank" rel="noopener noreferrer">#text</a>
<span class="issue_date">Issue Date: 2025-11-01</span>
<span class="snippet"><span>GPT Summary</span>- Emu3.5は、視覚と言語の両方に基づく次の状態を予測する大規模なマルチモーダルワールドモデルで、10兆トークン以上のデータで事前訓練されています。双方向の並列予測を用いた「Discrete Diffusion Adaptation（DiDA）」により、推論を約20倍加速し、強力なマルチモーダル能力を発揮します。Emu3.5は、画像生成や編集タスクで優れたパフォーマンスを示し、オープンソースとして提供されています。</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://emu.world/" target="_blank" rel="noopener noreferrer">https://emu.world/</a>


</p>
<p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1984190340279234888?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>ポイント解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/bilzrd/status/1994638829861687710?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="defeating-the-3532" class="title-link">[Paper Note] Defeating the Training-Inference Mismatch via FP16, Penghui Qi+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.26788" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3532" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="Stability.html" target="_blank" rel="noopener noreferrer">#Stability</a>
<a class="button" href="Reference%20Collection.html" target="_blank" rel="noopener noreferrer">#Reference Collection</a>
<a class="button" href="train-inference-gap.html" target="_blank" rel="noopener noreferrer">#train-inference-gap</a>
<span class="issue_date">Issue Date: 2025-11-01</span>
<span class="snippet"><span>GPT Summary</span>- 強化学習による大規模言語モデルのファインチューニングにおける不安定性は、トレーニングポリシーと推論ポリシーの数値的不一致に起因する。従来の対策は効果が薄かったが、本研究ではFP16に戻すことでこの問題を解決できることを示した。この変更は簡単で、モデルやアルゴリズムの修正を必要とせず、安定した最適化と速い収束を実現し、多様なタスクで強力なパフォーマンスを発揮することが確認された。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/mavenlin/status/1984130875307782257?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>RL学習時の浮動小数点数表現をbf16からfp16に変更するシンプルな変更で、訓練-推論時のgapが小さくなり学習が改善する、という話らしい。</p>
<p>ポイント解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dimitrispapail/status/1984286681022050373?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>所見:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenhuchen/status/1984470768688759000?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1984395743696994736?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1984750121255313462?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>verlはFP16での学習をサポートしていないので著者がパッチを出した模様:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/qphutu/status/1984268030558519737?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="think-just-3507" class="title-link">[Paper Note] Think Just Enough: Sequence-Level Entropy as a Confidence Signal for LLM  Reasoning, Aman Sharma+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.08146v3" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3507" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<a class="button" href="Inference.html" target="_blank" rel="noopener noreferrer">#Inference</a>
<a class="button" href="Entropy.html" target="_blank" rel="noopener noreferrer">#Entropy</a>
<span class="issue_date">Issue Date: 2025-10-30</span>
<span class="snippet"><span>GPT Summary</span>- エントロピーに基づく新しいフレームワークを提案し、推論タスクにおける大規模言語モデルのトークン効率を向上。シャノンエントロピーを信頼度信号として利用し、早期停止を実現することで、計算コストを25-50%削減。モデルごとに異なるエントロピー閾値を用いて、正しい答えを早期に得ることを認識し、トークン節約とレイテンシ削減を可能にする。精度を維持しつつ一貫したパフォーマンスを示し、現代の推論システムの特徴を明らかに。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/lossfunk/status/1983509644355268908?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>デコード時のエントロピーに応じて、reasoningを打ち切るか否か判定してコスト削減しつつ推論する話な模様</p>
<p>vLLMとかでデフォルトでサポートされてスループット上がったら嬉しいなあ</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="memory-efficient-backpropagation-3505" class="title-link">[Paper Note] Memory-Efficient Backpropagation for Fine-Tuning LLMs on  Resource-Constrained Mobile Devices, Congzheng Song+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.03425" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3505" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Personalization.html" target="_blank" rel="noopener noreferrer">#Personalization</a>
<a class="button" href="SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-10-30</span>
<span class="snippet"><span>GPT Summary</span>- モバイルデバイス向けに、メモリ効率の良いバックプロパゲーション実装（MeBP）を提案。これにより、メモリ使用量と計算時間のトレードオフを改善し、ゼロ次最適化よりも速く収束し、優れたパフォーマンスを実現。iPhone 15 Pro Maxでの検証により、0.5Bから4Bのパラメータを持つLLMが1GB未満のメモリでファインチューニング可能であることを示した。実装例は公開済み。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1983377112208986329?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>iPhone上で4BモデルまでFinetuningができるようになった模様。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="farmer-flow-3488" class="title-link">[Paper Note] FARMER: Flow AutoRegressive Transformer over Pixels, Guangting Zheng+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.23588" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3488" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="NormalizingFlow.html" target="_blank" rel="noopener noreferrer">#NormalizingFlow</a>
<a class="button" href="Compression.html" target="_blank" rel="noopener noreferrer">#Compression</a>
<span class="issue_date">Issue Date: 2025-10-28</span>
<span class="snippet"><span>GPT Summary</span>- FARMERという新しい生成フレームワークを提案し、正規化フローと自己回帰モデルを統合して高品質な画像合成と尤度推定を実現。潜在シーケンスへの変換や自己教師あり次元削減により、ARモデリングの効率を向上。推論速度を加速する蒸留スキームと画像生成品質を向上させる分類器フリーガイダンスを導入。実験により、FARMERは既存モデルと比較して競争力のある性能を示した。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1983015650139222334?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>ポイント解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1983034143580795131?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>これは...👀👀👀</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="dler-doing-3458" class="title-link">[Paper Note] DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per  Token via Reinforcement Learning, Shih-Yang Liu+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.15110v1" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3458" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<span class="issue_date">Issue Date: 2025-10-27</span>
<span class="snippet"><span>GPT Summary</span>- 推論言語モデルは長い出力を生成することが多く、応答の長さに対する精度向上が課題である。本研究では、切り捨てを用いた強化学習（RL）の再考を行い、精度低下の原因は不十分なRL最適化にあることを示す。3つの課題（バイアス、エントロピーの崩壊、スパースな報酬信号）に対処するため、DLERというトレーニング手法を提案し、出力の長さを70％以上削減しつつ精度を向上させた。さらに、Difficulty-Aware DLERを導入し、簡単な質問に対して適応的に切り捨てを厳しくすることで効率を向上させる手法も提案した。</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://nvlabs.github.io/DLER/" target="_blank" rel="noopener noreferrer">https://nvlabs.github.io/DLER/</a>


</p>
<p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/simonxindong/status/1982217071728435519?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>reasoningをトークン数の観点で効率化する話</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="asynchzp-hierarchical-3425" class="title-link">[Paper Note] AsyncHZP: Hierarchical ZeRO Parallelism with Asynchronous Scheduling for  Scalable LLM Training, Huawei Bai+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.20111" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3425" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<a class="button" href="mid-training.html" target="_blank" rel="noopener noreferrer">#mid-training</a>
<a class="button" href="PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="Parallelism.html" target="_blank" rel="noopener noreferrer">#Parallelism</a>
<span class="issue_date">Issue Date: 2025-10-25</span>
<span class="snippet"><span>GPT Summary</span>- 非同期階層ゼロ並列処理（AsyncHZP）を提案し、シンプルさとメモリ効率を保ちながら、トレーニング効率を向上。従来のZeROの通信オーバーヘッドを削減し、パラメータや勾配の再シャーディングを適応的に行う。マルチストリーム非同期スケジューリングにより通信と計算を重ね合わせ、メモリの断片化を最小限に抑える。DenseおよびMixture-of-Expertsモデルでの評価により、AsyncHZPが従来のND並列処理を上回る性能を示した。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1981756482128671166?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="every-attention-3413" class="title-link">[Paper Note] Every Attention Matters: An Efficient Hybrid Architecture for  Long-Context Reasoning, Ling Team+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.19338" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3413" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="Hybrid.html" target="_blank" rel="noopener noreferrer">#Hybrid</a>
<span class="issue_date">Issue Date: 2025-10-24</span>
<span class="snippet"><span>GPT Summary</span>- Ring-linearモデルシリーズ、特にRing-mini-linear-2.0（16Bパラメータ）とRing-flash-linear-2.0（104Bパラメータ）を紹介。両モデルはハイブリッドアーキテクチャを採用し、長いコンテキストの推論でI/Oと計算オーバーヘッドを削減。推論コストは32億パラメータの密なモデルと比較して1/10、元のRingシリーズと比べて50%以上削減。最適なモデル構造を特定し、高性能FP8オペレーターライブラリ「linghe」によりトレーニング効率が50%向上。複数の複雑推論ベンチマークでSOTAパフォーマンスを維持。</span>
<span class="snippet"><span>Comment</span><p>HF:


<a href="https://huggingface.co/inclusionAI/Ring-flash-linear-2.0-128k" target="_blank" rel="noopener noreferrer">https://huggingface.co/inclusionAI/Ring-flash-linear-2.0-128k</a>


</p>
<p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1981442875192987882?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>所見:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/grad62304977/status/1981571978382500203?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="text-or-3375" class="title-link">[Paper Note] Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text  Inputs in Multimodal LLMs, Yanhong Li+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.18279" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3375" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="Pixel-based.html" target="_blank" rel="noopener noreferrer">#Pixel-based</a>
<span class="issue_date">Issue Date: 2025-10-22</span>
<span class="snippet"><span>GPT Summary</span>- テキストを画像として提供することで、LLMのトークン使用量を削減しつつ性能を維持できることを示す。長いテキストを画像にレンダリングし、デコーダーに直接入力することで、必要なトークン数を大幅に減少させる。実験により、RULERとCNN/DailyMailのベンチマークで性能を損なうことなく、トークンの節約が実現できることを確認。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1980942325573648703?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="prompt-mii-meta-learning-3362" class="title-link">[Paper Note] Prompt-MII: Meta-Learning Instruction Induction for LLMs, Emily Xiao+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.16932" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3362" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<a class="button" href="AutomaticPromptOptimization.html" target="_blank" rel="noopener noreferrer">#AutomaticPromptOptimization</a>
<span class="issue_date">Issue Date: 2025-10-21</span>
<span class="snippet"><span>GPT Summary</span>- PROMPT-MIIという新しい指示誘導モデルを提案し、トレーニング例をコンパクトなプロンプトに縮小することで、インコンテキスト学習（ICL）と同等のパフォーマンスを実現。3,000以上の分類データセットでトレーニングし、90の未見タスクで評価した結果、下流モデルの品質を4-9 F1ポイント向上させ、必要なトークン数を3-13倍削減。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gneubig/status/1980644772902789603?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>タスクのexamplar/demonstrationからタスクに関するdescription）＝instruction)を生成するモデルを学習し、生成されたinstructionを用いることで、manyshotでICLするよりも、少ないトークン数で同等以上の性能を達成するといった話に見える。どういうinstructionになるのかが非常に興味がある。A.6参照のこと。細かく具体的だがコンパクトな指示が記述されているようなinstructionとなっている。<br><br><img src="https://github.com/user-attachments/assets/ee02024a-725d-47a1-9b8c-46fed1dbcb8f" alt="image" loading="lazy" width="400" height="300"><br><img src="https://github.com/user-attachments/assets/363dd369-79d3-4b37-b4de-f880e2fb746d" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="glyph-scaling-3357" class="title-link">[Paper Note] Glyph: Scaling Context Windows via Visual-Text Compression, Jiale Cheng+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.17800" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3357" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="ContextWindow.html" target="_blank" rel="noopener noreferrer">#ContextWindow</a>
<a class="button" href="LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-10-21</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、長いコンテキストを持つ大規模言語モデル（LLMs）の実用性を向上させるため、Glyphというフレームワークを提案し、テキストを画像に変換して視覚と言語のモデル（VLMs）で処理します。このアプローチにより、3-4倍のトークン圧縮を実現し、精度を維持しつつ処理速度を約4倍向上させます。さらに、128KコンテキストのVLMが1Mトークンのテキストタスクを処理可能になることを示しました。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1980474912168112471?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>所見:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/arankomatsuzaki/status/1980722682246398069?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>テキストを画像にレンダリングしてVLMに入力することでtextと比較して3.2倍KV Cache (context)を圧縮し、prefillingとデコード速度も4.8, 4.4倍高速化するフレームワークらしい<br><br><img src="https://github.com/user-attachments/assets/e65f880d-0d04-434f-9a51-accc84d44a6f" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="synthesizing-agentic-3348" class="title-link">[Paper Note] Synthesizing Agentic Data for Web Agents with Progressive Difficulty  Enhancement Mechanisms, Shrey Pandit+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.13913v1" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3348" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="Diversity.html" target="_blank" rel="noopener noreferrer">#Diversity</a>
<a class="button" href="Verification.html" target="_blank" rel="noopener noreferrer">#Verification</a>
<a class="button" href="DeepResearch.html" target="_blank" rel="noopener noreferrer">#DeepResearch</a>
<a class="button" href="LongHorizon.html" target="_blank" rel="noopener noreferrer">#LongHorizon</a>
<span class="issue_date">Issue Date: 2025-10-21</span>
<span class="snippet"><span>GPT Summary</span>- Webベースの「ディープリサーチ」エージェントは、長期的なインタラクションを通じて複雑な質問応答タスクを解決することを目指すが、従来の方法は推論の複雑さを捉えきれない。そこで、タスクの複雑さを段階的に増加させる二段階のデータ合成パイプラインを導入し、ベースラインエージェントが質問に挑戦し、事実確認を行う。実験により、提案したデータセットが既存のものよりも効果的な訓練を可能にし、ツール使用アクションの多様性が2倍であることが示された。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/caimingxiong/status/1980302240163488057?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="spatial-forcing-3328" class="title-link">[Paper Note] Spatial Forcing: Implicit Spatial Representation Alignment for  Vision-language-action Model, Fuhao Li+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.12276" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3328" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="3D%20(Scene).html" target="_blank" rel="noopener noreferrer">#3D (Scene)</a>
<a class="button" href="Robotics.html" target="_blank" rel="noopener noreferrer">#Robotics</a>
<a class="button" href="VisionLanguageActionModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageActionModel</a>
<a class="button" href="SpatialUnderstanding.html" target="_blank" rel="noopener noreferrer">#SpatialUnderstanding</a>
<span class="issue_date">Issue Date: 2025-10-20</span>
<span class="snippet"><span>GPT Summary</span>- Spatial Forcing (SF)という新しい整合戦略を提案し、VLAモデルが3D空間理解能力を向上させることを促進。SFは3D入力や深度推定器に依存せず、VLAの中間視覚埋め込みを3D基盤モデルの幾何学的表現と整合させる。実験により、SFは最先端の結果を達成し、トレーニングを最大3.8倍加速、データ効率を改善。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1979911820401086613?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="attention-is-3319" class="title-link">[Paper Note] Attention Is All You Need for KV Cache in Diffusion LLMs, Quan Nguyen-Tri+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.14973" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3319" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<a class="button" href="KV%20Cache.html" target="_blank" rel="noopener noreferrer">#KV Cache</a>
<span class="issue_date">Issue Date: 2025-10-19</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、拡散型大規模言語モデル（DLMs）のデコーディング待機時間を最小化しつつ予測精度を最大化するために、適応的なKVキャッシュ再計算手法「Elastic-Cache」を提案。これにより、浅いレイヤーの冗長性を削減し、重要なトークンに基づいてキャッシュのリフレッシュを動的に行う。実験では、GSM8KやHumanEvalでの速度向上を示し、生成品質を維持しながら高いスループットを達成した。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/1979180865520570615?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>DLMにおいて、denoisingの各ステップにおいて全てのKVを再計算するのではなく、attention scoreが大きくドリフトしていない部分についてはKV Cacheを再利用し、大きくドリフトした部分だけ再計算するような仕組みを学習することで、品質を損なうことなく推論速度を高速化した模様</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="dr.llm-dynamic-3298" class="title-link">[Paper Note] Dr.LLM: Dynamic Layer Routing in LLMs, Ahmed Heakl+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.12773" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3298" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="DynamicNetworks.html" target="_blank" rel="noopener noreferrer">#DynamicNetworks</a>
<a class="button" href="Routing.html" target="_blank" rel="noopener noreferrer">#Routing</a>
<a class="button" href="One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-10-17</span>
<span class="snippet"><span>GPT Summary</span>- Dr.LLMは、LLMsに動的な層ルーティングを導入し、計算効率を向上させるフレームワーク。モンテカルロ木探索を用いて高品質な層構成を導出し、ARCやDARTで精度を最大+3.4%向上させ、平均5層を節約。ドメイン外タスクでもわずか0.85%の精度低下で従来手法を上回る。明示的な監視下でのルーターがLLMsを効率的に活用できることを示す。</span>
<span class="snippet"><span>Comment</span><p>LayerごとにMLPのrouterを用意し、（元のLLMのパラメータはfreezeして）Layerをskip, execute, repeatするかを追加で学習することで、クエリに応じて動的に計算コストとpathを調整する能力を身につけさせ、性能を向上させつつも計算量も削減できます、といった話な模様。routerが学習されているのでinference時にsearchは不要。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="representation-based-exploration-3280" class="title-link">[Paper Note] Representation-Based Exploration for Language Models: From Test-Time to  Post-Training, Jens Tuyls+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.11686" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3280" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="Test-Time%20Scaling.html" target="_blank" rel="noopener noreferrer">#Test-Time Scaling</a>
<a class="button" href="PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="Diversity.html" target="_blank" rel="noopener noreferrer">#Diversity</a>
<span class="issue_date">Issue Date: 2025-10-16</span>
<span class="snippet"><span>GPT Summary</span>- 強化学習（RL）が言語モデルの行動発見に与える影響を調査。事前学習されたモデルの隠れ状態を基にした表現ベースのボーナスを用いることで、多様性とpass@k率が大幅に改善されることを発見。推論時における探索が効率を向上させ、ポストトレーニングにおいてもRLパイプラインとの統合により性能が向上。意図的な探索が新しい行動の発見に寄与する可能性を示唆。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/canondetortugas/status/1978245046366319048?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>探索の多様性をあげてRLこ学習効率、test time scalingの効率を上げるという話</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="expert-as-a-service-towards-3277" class="title-link">[Paper Note] Expert-as-a-Service: Towards Efficient, Scalable, and Robust Large-scale  MoE Serving, Ziming Liu+, arXiv'25, 2025.09</h3>
<br><a href="https://arxiv.org/abs/2509.17863v1" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3277" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<span class="issue_date">Issue Date: 2025-10-16</span>
<span class="snippet"><span>GPT Summary</span>- EaaSという新しいサービングシステムを提案し、Mixture-of-Experts (MoE)モデルの効率的でスケーラブルな展開を実現。MoEモジュールを独立したステートレスサービスに分解し、リソースの細かいスケーリングとフォールトトレランスを提供。実験により、EaaSはモノリシックシステムと同等のパフォーマンスを維持しつつ、スループットの減少を2%未満に抑え、最大37.5%の計算リソースを節約することが確認された。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1978286917159624888?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="streamingvlm-real-time-3270" class="title-link">[Paper Note] StreamingVLM: Real-Time Understanding for Infinite Video Streams, Ruyi Xu+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.09608" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3270" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="AttentionSinks.html" target="_blank" rel="noopener noreferrer">#AttentionSinks</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="VideoGeneration_Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-10-15</span>
<span class="snippet"><span>GPT Summary</span>- StreamingVLMは、無限のビデオストリームをリアルタイムで理解するためのモデルで、トレーニングと推論を統一したフレームワークを採用。アテンションシンクの状態を再利用し、短いビジョントークンと長いテキストトークンのウィンドウを保持することで、計算コストを抑えつつ高い性能を実現。新しいベンチマークInf-Streams-Evalで66.18%の勝率を達成し、一般的なVQA能力を向上させることに成功。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/teortaxestex/status/1978324546370343088?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>これは興味深い</p>
<p>保持するKV Cacheの上限を決め、Sink Token[^1]は保持し[^2]（512トークン）、textual tokenは長距離で保持、visual tokenは短距離で保持、またpositional encodingとしてはRoPEを採用するが、固定されたレンジの中で動的にindexを更新することで、位相を学習時のrangeに収めOODにならないような工夫をすることで、memoryと計算コストを一定に保ちながらlong contextでの一貫性とリアルタイムのlatencyを実現する、といった話にみえる。<br><img src="https://github.com/user-attachments/assets/4d063c90-e10a-4d07-9095-f87ee85c33fb" alt="image" loading="lazy" width="400" height="300"><br><br>学習時はフレームがoverlapした複数のチャンクに分けて、それぞれをfull attentionで学習する（Sink Tokenは保持する）。これは上述のinference時のパターンと整合しており学習時とinference時のgapが最小限になる。また、わざわざlong videoで学習する必要がない。（美しい解決方法）<br><img src="https://github.com/user-attachments/assets/98b50d1b-b9c4-427a-93f5-d385b2bc35a1" alt="image" loading="lazy" width="400" height="300"><br><br>[^1]: decoder-only transformerの余剰なattention scoreの捨て場として機能するsequence冒頭の数トークン(3--4トークン程度）のこと。本論文では512トークンと大きめのSink Tokenを保持している。<br>[^2]: Attention Sinksによって、long contextの性能が改善され <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1860" target="_blank" rel="noopener noreferrer">Why do LLMs attend to the first token?, Federico Barbero+, COLM'25</a>
 decoder-only transformerの層が深い部分でのトークンの表現が均一化されてしまうover-mixingを抑制する <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1861" target="_blank" rel="noopener noreferrer">Efficient Streaming Language Models with Attention Sinks, Guangxuan Xiao+, ICLR'24</a>
 ことが報告されている</p>
<p>AttentionSink関連リンク:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1861" target="_blank" rel="noopener noreferrer">Efficient Streaming Language Models with Attention Sinks, Guangxuan Xiao+, ICLR'24</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1860" target="_blank" rel="noopener noreferrer">Why do LLMs attend to the first token?, Federico Barbero+, COLM'25</a>
</p>
<p>↑これは元ポストを読んで（と論文斜め読み）の感想のようなものなので、詳細は後で元論文を読む。</p>
<p>関連:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/yukangchen_/status/1978653384539341287?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="qerl-beyond-3259" class="title-link">[Paper Note] QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning  for LLMs, Wei Huang+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.11696" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3259" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="Quantization.html" target="_blank" rel="noopener noreferrer">#Quantization</a>
<a class="button" href="PEFT(Adaptor_LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="Entropy.html" target="_blank" rel="noopener noreferrer">#Entropy</a>
<span class="issue_date">Issue Date: 2025-10-14</span>
<span class="snippet"><span>GPT Summary</span>- QeRLは、LLMs向けの量子化強化学習フレームワークで、NVFP4量子化とLoRAを組み合わせてRLのロールアウトを加速し、メモリ使用量を削減します。量子化ノイズがポリシーエントロピーを増加させ、探索を強化することを示し、AQNメカニズムでノイズを動的に調整します。実験により、ロールアウトフェーズで1.5倍のスピードアップを達成し、32B LLMのRLトレーニングを単一のH100 80GB GPUで可能にしました。QeRLは、報酬の成長と最終精度で優れた結果を示し、LLMsにおけるRLトレーニングの効率的なフレームワークとしての地位を確立しました。</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://github.com/NVlabs/QeRL" target="_blank" rel="noopener noreferrer">https://github.com/NVlabs/QeRL</a>


</p>
<p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1977949560149315847?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2552" target="_blank" rel="noopener noreferrer">Your Efficient RL Framework Secretly Brings You Off-Policy RL Training, Yao+, 2025.08</a>
<br><br>のようなロールアウトする際のエンジンと学習のエンジンのgapによる問題は生じたりしないのだろうか。</p>
<p>解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1979325188581007627?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="diffusion-transformers-3258" class="title-link">[Paper Note] Diffusion Transformers with Representation Autoencoders, Boyang Zheng+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.11690" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3258" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-10-14</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、従来のVAEエンコーダを事前学習された表現エンコーダに置き換えたRepresentation Autoencoders（RAE）を提案。これにより、高品質な再構成と豊かな潜在空間を実現し、拡散トランスフォーマーの性能向上を図る。RAEは、補助的な表現整合損失なしで早い収束を達成し、ImageNetで優れた画像生成結果を示した。RAEは、拡散トランスフォーマーの新しいデフォルトとしての利点を提供する。</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://rae-dit.github.io" target="_blank" rel="noopener noreferrer">https://rae-dit.github.io</a>


</p>
<p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1978001535717216751?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>U-NetをBackboneとしたVAEの代わりにViTに基づく（down, up- scaling無しの）アーキテクチャを用いることで、より少ない計算量で高い性能を達成しました、といった話に見える。</p>
<p>ポイント解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1977967098736549990?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1978018195953848384?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="part-ii-3254" class="title-link">[Paper Note] Part II: ROLL Flash -- Accelerating RLVR and Agentic Training with  Asynchrony, Han Lu+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.11345" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3254" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="RLVR.html" target="_blank" rel="noopener noreferrer">#RLVR</a>
<span class="issue_date">Issue Date: 2025-10-14</span>
<span class="snippet"><span>GPT Summary</span>- 非同期RL後処理をサポートする「ROLL Flash」を提案。細粒度の並列性とロールアウト・トレインのデカップリングに基づき、効率的なトレーニングアーキテクチャを実現。ROLL Flashはリソース利用効率とスケーラビリティを大幅に改善し、RLVRタスクで最大2.24倍、エージェントタスクで最大2.72倍のスピードアップを達成。非同期トレーニングが同期トレーニングと同等のパフォーマンスを示すことを確認。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1977959866699513889?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>RLのロールアウト中のGPUのアイドルタイムを削減します系の話も最近結構見るような<br>たとえば<br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3134" target="_blank" rel="noopener noreferrer">Anatomy of a Modern Finetuning API, Benjamin Anderson, 2025.10</a>
</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="dinfer-an-3248" class="title-link">[Paper Note] dInfer: An Efficient Inference Framework for Diffusion Language Models, Yuxin Ma+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.08666" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3248" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-10-14</span>
<span class="snippet"><span>GPT Summary</span>- dLLMの推論を効率化するフレームワークdInferを提案。dInferは4つのモジュールに分解され、新しいアルゴリズムと最適化を統合。これにより、出力品質を維持しつつ、推論速度を大幅に向上。HumanEvalで1秒あたり1,100トークンを超え、従来のシステムに比べて10倍のスピードアップを実現。dInferはオープンソースで公開。</span>
<span class="snippet"><span>Comment</span><p>code:


<a href="https://github.com/inclusionAI/dInfer" target="_blank" rel="noopener noreferrer">https://github.com/inclusionAI/dInfer</a>


</p>
<p>とうとうdLLMを高速でinferenceできるフレームワークが出た模様。inclusionAIより。</p>
<p>ポイント解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1978662709773373856?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="deepprune-parallel-3232" class="title-link">[Paper Note] DeepPrune: Parallel Scaling without Inter-trace Redundancy, Shangqing Tu+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.08483" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3232" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Pruning.html" target="_blank" rel="noopener noreferrer">#Pruning</a>
<a class="button" href="Test-Time%20Scaling.html" target="_blank" rel="noopener noreferrer">#Test-Time Scaling</a>
<a class="button" href="Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<a class="button" href="Parallel.html" target="_blank" rel="noopener noreferrer">#Parallel</a>
<span class="issue_date">Issue Date: 2025-10-12</span>
<span class="snippet"><span>GPT Summary</span>- DeepPruneという新しいフレームワークを提案し、並列スケーリングの計算非効率を解決。80%以上の推論トレースが同一の回答を生成する問題に対処し、焦点損失とオーバーサンプリング技術を用いた判定モデルで同等性を予測。オンラインの貪欲クラスタリングで冗長な経路をプルーニングし、80%以上のトークン削減を達成しつつ、精度を維持。効率的な並列推論の新基準を確立。</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://deepprune.github.io" target="_blank" rel="noopener noreferrer">https://deepprune.github.io</a>


</p>
<p>HF:


<a href="https://huggingface.co/collections/THU-KEG/deepprune-68e5c1ea71f789a6719b2c1c" target="_blank" rel="noopener noreferrer">https://huggingface.co/collections/THU-KEG/deepprune-68e5c1ea71f789a6719b2c1c</a>


</p>
<p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1977134276966789446?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="artificial-hippocampus-3195" class="title-link">[Paper Note] Artificial Hippocampus Networks for Efficient Long-Context Modeling, Yunhao Fang+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.07318" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3195" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="memory.html" target="_blank" rel="noopener noreferrer">#memory</a>
<a class="button" href="RecurrentModels.html" target="_blank" rel="noopener noreferrer">#RecurrentModels</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<span class="snippet"><span>GPT Summary</span>- 長大なシーケンスモデリングにおけるメモリのトレードオフを解決するため、人工海馬ネットワーク（AHN）を提案。AHNは短期メモリを維持しつつ、長期メモリを圧縮。実験により、AHNを用いたモデルが従来のベースラインを上回り、計算とメモリ要件を大幅に削減しつつ、パフォーマンスを向上させることを示した。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1976107974939816308?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>所見:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/teortaxestex/status/1976451334951063854?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="the-markovian-3191" class="title-link">[Paper Note] The Markovian Thinker, Milad Aghajohari+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.06557v1" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3191" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-10-09</span>
<span class="snippet"><span>GPT Summary</span>- 強化学習を用いて長い思考の連鎖を生成するための新しいパラダイム「マルコフ的思考」を提案。これにより、状態を一定のサイズに制限し、思考の長さをコンテキストのサイズから切り離すことで、線形計算を実現。新しいRL環境「Delethink」を構築し、モデルは短い持ち越しで推論を継続することを学習。訓練されたモデルは、長い推論を効率的に行い、コストを大幅に削減。思考環境の再設計が、効率的でスケーラブルな推論LLMの実現に寄与することを示した。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/maghajohari/status/1976296195438887012?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>ポイント解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1976466786565656986?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1976798665038758377?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="generative-representational-3178" class="title-link">[Paper Note] Generative Representational Instruction Tuning, Niklas Muennighoff+, ICLR'25, 2024.02</h3>
<br><a href="https://arxiv.org/abs/2402.09906" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3178" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="RepresentationLearning.html" target="_blank" rel="noopener noreferrer">#RepresentationLearning</a>
<a class="button" href="RAG(RetrievalAugmentedGeneration).html" target="_blank" rel="noopener noreferrer">#RAG(RetrievalAugmentedGeneration)</a>
<a class="button" href="ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-10-08</span>
<span class="snippet"><span>GPT Summary</span>- 生成的表現指示チューニング（GRIT）を用いて、大規模言語モデルが生成タスクと埋め込みタスクを同時に処理できる手法を提案。GritLM 7BはMTEBで新たな最先端を達成し、GritLM 8x7Bはすべてのオープン生成モデルを上回る性能を示す。GRITは生成データと埋め込みデータの統合による性能損失がなく、RAGを60%以上高速化する利点もある。モデルは公開されている。</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=BC4lIvfSzv" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=BC4lIvfSzv</a>


</p>
<p>従来はgemerativeタスクとembeddingタスクは別々にモデリングされていたが、それを統一的な枠組みで実施し、両方のタスクで同等のモデルサイズの他モデルと比較して高い性能を達成した研究。従来のgenerativeタスク用のnext-token-prediction lossとembeddingタスク用のconstastive lossを組み合わせて学習する（式3）。タスクの区別はinstructionにより実施し、embeddingタスクの場合はすべてのトークンのlast hidden stateのmean poolingでrepresentationを取得する。また、embeddingの時はbi-directional attention / generativeタスクの時はcausal maskが適用される。これらのattentionの適用のされ方の違いが、どのように管理されるかはまだしっかり読めていないのでよくわかっていないが、非常に興味深い研究である。<br><br><img width="603" height="349" alt="Image" src="&lt;a%20href=" https: target="_blank" rel="noopener noreferrer">https://github.com/user-attachments/assets/acb2cbcd-364d-43c7-b51a-6c5ea9866415"


/&gt;</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="ssdd-single-step-3177" class="title-link">[Paper Note] SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization, Théophane Vallaeys+, arXiv'25, 2025.10</h3>
<br><a href="https://arxiv.org/abs/2510.04961" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3177" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="Tokenizer.html" target="_blank" rel="noopener noreferrer">#Tokenizer</a>
<a class="button" href="Decoder.html" target="_blank" rel="noopener noreferrer">#Decoder</a>
<span class="issue_date">Issue Date: 2025-10-08</span>
<span class="snippet"><span>GPT Summary</span>- 新しいピクセル拡散デコーダアーキテクチャ（SSDD）を提案し、KL-VAEに依存せずに高品質な画像再構成を実現。SSDDは敵対的損失なしで訓練され、再構成FIDを改善し、サンプリング速度を向上させる。これにより、KL-VAEの代替として迅速かつ高品質な生成モデルの構築が可能となる。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1975484440475505039?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="free-draft-and-verification-3132" class="title-link">[Paper Note] Free Draft-and-Verification: Toward Lossless Parallel Decoding for  Diffusion Large Language Models, Shutong Wu+, arXiv'25, 2025.09</h3>
<br><a href="https://arxiv.org/abs/2510.00294" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3132" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<span class="issue_date">Issue Date: 2025-10-06</span>
<span class="snippet"><span>GPT Summary</span>- Diffusion Large Language Models (DLLMs)は、双方向の注意メカニズムにより文脈を捉える能力が高いが、推論効率が自己回帰モデルに劣る。既存の並列デコーディングアルゴリズムは性能低下を伴う。これを解決するために、損失のない並列デコーディングを実現する新しいアルゴリズム「Free Draft-and-Verification（Freedave）」を提案。Freedaveにより、DLLMsのスループットは数学的推論タスクで最大2.8倍向上する。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/lingyang_pu/status/1974754204473536620?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="limited-preference-3131" class="title-link">[Paper Note] Limited Preference Data? Learning Better Reward Model with Latent Space  Synthesis, Leitian Tao+, arXiv'25, 2025.09</h3>
<br><a href="https://arxiv.org/abs/2509.26074" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3131" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Alignment.html" target="_blank" rel="noopener noreferrer">#Alignment</a>
<a class="button" href="SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="VariationalAutoEncoder.html" target="_blank" rel="noopener noreferrer">#VariationalAutoEncoder</a>
<a class="button" href="NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="RewardModel.html" target="_blank" rel="noopener noreferrer">#RewardModel</a>
<span class="issue_date">Issue Date: 2025-10-06</span>
<span class="snippet"><span>GPT Summary</span>- 報酬モデリングのために、LLMの潜在埋め込み空間で好みデータを合成する新フレームワークLENSを提案。VAEを用いて埋め込みの構造化された表現を学習し、コストのかかるテキスト生成を回避しつつ、多様で一貫した合成好みペアを生成。実験では、合成ペアが元の好みの順序を保持し、報酬モデルの一般化を改善。生成速度は18倍速く、16,000倍小さいモデルで優れた結果を達成。効率的なデータ拡張を通じて報酬モデリングを強化する効果的な手法を提供。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/sharonyixuanli/status/1974871491117187099?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="ia2-alignment-3128" class="title-link">[Paper Note] IA2: Alignment with ICL Activations Improves Supervised Fine-Tuning, Aayush Mishra+, arXiv'25, 2025.09</h3>
<br><a href="https://arxiv.org/abs/2509.22621" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3128" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2025-10-05</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、インコンテキスト学習（ICL）の活性化パターンを利用して、監視付きファインチューニング（SFT）の品質を向上させる手法を提案。ICLとSFTの異なる適応メカニズムを示し、ICL活性化アライメント（IA2）という自己蒸留技術を導入。IA2をSFTの前に実行することで、モデルの出力精度とキャリブレーションが向上することを12のベンチマークで実証。これにより、モデル適応の内部メカニズムに対する新たな視点も提供される。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/danielkhashabi/status/1974119053728919790?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="vela-an-3059" class="title-link">[Paper Note] VELA: An LLM-Hybrid-as-a-Judge Approach for Evaluating Long Image   Captions, Kazuki Matsuda+, EMNLP'25, 2025.09</h3>
<br><a href="https://arxiv.org/abs/2509.25818" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3059" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="ImageCaptioning.html" target="_blank" rel="noopener noreferrer">#ImageCaptioning</a>
<a class="button" href="LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="LLM-as-a-Judge.html" target="_blank" rel="noopener noreferrer">#LLM-as-a-Judge</a>
<a class="button" href="EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="MultiDimensional.html" target="_blank" rel="noopener noreferrer">#MultiDimensional</a>
<span class="issue_date">Issue Date: 2025-10-01</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、長い画像キャプションの自動評価に特化した新しい指標VELAを提案し、マルチモーダル大規模言語モデル（MLLMs）を活用した評価フレームワークを構築。さらに、評価指標を検証するためのLongCap-Arenaベンチマークを導入し、7,805枚の画像と32,246件の人間の判断を用いて、VELAが既存の指標を上回る性能を示した。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/yuigawada/status/1973309026546098355?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="pretraining-large-3047" class="title-link">[Paper Note] Pretraining Large Language Models with NVFP4, NVIDIA+, arXiv'25, 2025.09</h3>
<br><a href="https://arxiv.org/abs/2509.25149" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3047" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Quantization.html" target="_blank" rel="noopener noreferrer">#Quantization</a>
<span class="issue_date">Issue Date: 2025-09-30</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、NVFP4フォーマットを用いた大規模言語モデル（LLMs）の安定かつ正確なトレーニング手法を提案。ランダムハダマード変換や二次元量子化スキームを取り入れ、偏りのない勾配推定を実現。10兆トークンでのトレーニングにより、FP8と同等の性能を達成し、狭い精度のLLMトレーニングにおける進展を示した。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/arankomatsuzaki/status/1972869149102858441?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1975344045754097685?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="sparse-videogen2-3003" class="title-link">[Paper Note] Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via   Semantic-Aware Permutation, Shuo Yang+, NeurIPS'25 Spotlight, 2025.05</h3>
<br><a href="https://arxiv.org/abs/2505.18875" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3003" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="VideoGeneration_Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="Sparse.html" target="_blank" rel="noopener noreferrer">#Sparse</a>
<a class="button" href="SparseAttention.html" target="_blank" rel="noopener noreferrer">#SparseAttention</a>
<span class="issue_date">Issue Date: 2025-09-27</span>
<span class="snippet"><span>GPT Summary</span>- Diffusion Transformers（DiTs）の動画生成におけるレイテンシーの問題を解決するため、重要トークンの特定精度を最大化し計算の無駄を最小化するトレーニング不要のフレームワークSVG2を提案。SVG2は意味に基づくトークンのクラスタリングと再配置を行い、計算効率を向上させる。これにより、HunyuanVideoおよびWan 2.1でそれぞれ最大2.30倍および1.89倍のスピードアップを達成し、PSNRを維持。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/chenfeng_x/status/1971343654662046184?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>pj page:


<a href="https://svg-project.github.io/v2/" target="_blank" rel="noopener noreferrer">https://svg-project.github.io/v2/</a>


</p>
<p>Q, Kそれぞれについて独立してkmeansクラスタリングを実施し、意味的に類似したQ, Kをクラスタ化し、map上で散らばっているトークンの配置を整頓して計算機上で効率的に扱えるようにし、各クラスタのcentroidをattention scoreの計算に用いてクラスタ内のトークンのスコアを近似することで計算を効率化します、といった話な模様。また、クリティカルなクラスタとそうでは無いものがあるので、p個のクリティカルなクラスタを選択しさらに効率化をする模様。<br><img src="https://github.com/user-attachments/assets/862cf5c8-5583-4f94-8b67-59177c444176" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="angles-don't-3002" class="title-link">[Paper Note] Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's   Own Signals, Qinsi Wang+, NeurIPS'25 Spotlight, 2025.06</h3>
<br><a href="https://arxiv.org/abs/2506.02281" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3002" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<span class="issue_date">Issue Date: 2025-09-27</span>
<span class="snippet"><span>GPT Summary</span>- 大規模言語モデル（LLMs）の強化学習微調整（RFT）におけるサンプル効率の低下を改善するため、モデル固有の信号「角度集中」を特定。これに基づき、勾配駆動型角度情報ナビゲート強化学習フレームワーク（GAIN-RL）を提案し、トレーニングデータを動的に選択することで効率を向上。実証評価では、GAIN-RLがトレーニング効率を2.5倍以上向上させ、元のデータの半分でより良いパフォーマンスを達成したことが示された。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/chenfeng_x/status/1971343654662046184?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>ヒューリスティックや特定の難易度に基づくラベルからRLのサンプルをサンプリングするのではなく、モデル自身の現在の学習の状態に基づいて動的に選択し学習効率を向上させるアプローチな模様。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="expanding-reasoning-2997" class="title-link">[Paper Note] Expanding Reasoning Potential in Foundation Model by Learning Diverse  Chains of Thought Patterns, Xuemiao Zhang+, arXiv'25, 2025.09</h3>
<br><a href="https://arxiv.org/abs/2509.21124" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2997" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="mid-training.html" target="_blank" rel="noopener noreferrer">#mid-training</a>
<span class="issue_date">Issue Date: 2025-09-26</span>
<span class="snippet"><span>GPT Summary</span>- 大規模推論モデルの進展は強化学習によって促進され、CoTデータの利用が推論の深さを向上させることが示されている。しかし、どのデータタイプが最も効果的かは未解決の問題である。本研究では、推論ポテンシャルを独立した試行の数の逆数として定義し、これを拡張するために高価値の推論パターンを用いた多様なデータの利用を提案。具体的には、CoTシーケンスから原子的な推論パターンを抽象化し、コアリファレンスセットを構築。二重粒度アルゴリズムを用いて高価値のCoTデータを効率的に選択し、モデルの推論能力を向上させる。10BトークンのCoTPデータにより、85A6B Mixture-of-ExpertsモデルはAIME 2024および2025で9.58%の改善を達成した。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1971461991039631691?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>細かいところは読めていないのだが、学習データの中から高品質な推論パターンを持つものを選んで学習に使いたいというモチベーション。そのためにまず価値の高い推論パターンを含むコアセットを作り、コアセットと類似した推論パターンや、推論中のトークンのエントロピー列を持つサンプルを学習データから収集するみたいな話な模様。類似度は重みつきDynamic Time Warping (DTW)で、原始的な推論パターンの系列とエントロピー系列のDTWの線型結合によっめ求める。原始的な推論パターンのアノテーションや、CoT sequence中のトークンのエントロピー列はDeepSeek-V3によって生成する。<br><br>コアセットを作るためには、問題タイプや問題の難易度に基づいて人手で問題を選び、それらに対してstrong reasoning modelでCoTを生成。各CoTに対して（おそらく）DeepSeek-V3でreasoningのパターン（パターンは原始的なCoTパターンの系列で構成される）をアノテーションし、各パターンに対してTF-IDFによって重要度を決定する。最終的に、問題に正答しているサンプルについて、人手で高品質でdiscriminativeなCoTパターンを持つものを選択し、各CoTパターンに重みをつけた上でコアセットを作成した、みたいな感じに見える。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="limi-less-2938" class="title-link">[Paper Note] LIMI: Less is More for Agency, Yang Xiao+, arXiv'25, 2025.09</h3>
<br><a href="https://arxiv.org/abs/2509.17567" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2938" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<span class="issue_date">Issue Date: 2025-09-23</span>
<span class="snippet"><span>GPT Summary</span>- AIシステムのエージェンシーを、自律的に問題を発見し解決策を実行する能力と定義。急速に変化する業界のニーズに応じて、単なる推論を超えた自律的なエージェントが求められている。LIMI（Less Is More for Intelligent Agency）は、最小限のトレーニングサンプルで高いエージェンシーを実現する新たな原則を提案し、78サンプルで73.5%の成果を達成。これは、従来のデータ量に依存するアプローチに対する挑戦であり、高品質なデモの戦略的キュレーションが重要であることを示している。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/arankomatsuzaki/status/1970328242688246160?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>LLM AgentのSFTにおけるLess is more<br><br>参考:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/700" target="_blank" rel="noopener noreferrer">LIMA: Less Is More for Alignment, Chunting Zhou+, N/A, NeurIPS'23</a>
</p>
<p>ポイント解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1971777010658955436?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="bread-branched-2898" class="title-link">[Paper Note] BREAD: Branched Rollouts from Expert Anchors Bridge SFT &amp; RL for   Reasoning, Xuechen Zhang+, NeurIPS'25</h3>
<br><a href="https://arxiv.org/pdf/2506.17211" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2898" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<span class="issue_date">Issue Date: 2025-09-19</span>
<span class="snippet"><span>GPT Summary</span>- 小型言語モデル（SLMs）は、トレースが不足している場合に複雑な推論を学ぶのが難しい。本研究では、SFT + RLの限界を調査し、BREADという新しい手法を提案。BREADは、専門家のガイダンスを用いてSFTとRLを統合し、失敗したトレースに対して短いヒントを挿入することで成功を促進。これにより、トレーニングが約3倍速くなり、標準的なGRPOを上回る性能を示す。BREADは、SLMの推論能力を大幅に向上させることが確認された。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/sametoymac/status/1968892463382200391?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="websailor-navigating-2855" class="title-link">[Paper Note] WebSailor: Navigating Super-human Reasoning for Web Agent, Kuan Li+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2507.02592" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2855" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<span class="issue_date">Issue Date: 2025-09-18</span>
<span class="snippet"><span>GPT Summary</span>- WebSailorは、LLMのトレーニングにおいて人間の認知的限界を超えるためのポストトレーニング手法であり、複雑な情報探索タスクでの性能を向上させる。構造化サンプリングや情報の難読化、DUPOを用いて高不確実性タスクを生成し、オープンソースエージェントの能力を大幅に上回ることを目指す。</span>
</article>
<article class="paper-entry">
<h3 id="openvision-2-2820" class="title-link">[Paper Note] OpenVision 2: A Family of Generative Pretrained Visual Encoders for  Multimodal Learning, Yanqing Liu+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2509.01644" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2820" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="OpenSource.html" target="_blank" rel="noopener noreferrer">#OpenSource</a>
<a class="button" href="Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<a class="button" href="Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-09-16</span>
<span class="snippet"><span>GPT Summary</span>- 本論文では、OpenVisionのアーキテクチャを簡素化し、トレーニング効率を向上させる方法を提案。テキストエンコーダーと対照損失を削除し、キャプショニング損失のみを使用したOpenVision 2を導入。初期結果は、トレーニング時間を約1.5倍短縮し、メモリ使用量を約1.8倍削減することを示し、10億以上のパラメータにスケールアップ可能であることを強調。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1967865921399296286?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>事前学習時にtext, image encoderのcontrastive lossで学習していたが、text encoderを無くしimage encoderに入力されたimageからcaptionを生成するcaption lossのみにすることで性能を落とすことなく効率を改善</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="adaptive-computation-2816" class="title-link">[Paper Note] Adaptive Computation Pruning for the Forgetting Transformer, Zhixuan Lin+, COLM'25</h3>
<br><a href="https://arxiv.org/abs/2504.06949" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2816" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Pruning.html" target="_blank" rel="noopener noreferrer">#Pruning</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-09-16</span>
<span class="snippet"><span>GPT Summary</span>- Forgeting Transformer（FoX）は、忘却ゲートを用いたソフトマックスアテンションを特徴とし、従来のTransformerと比較して優れた性能を示す。FoXの特性を活かし、適応計算プルーニング（ACP）を提案し、計算を動的にプルーニングすることで、FLOPsとメモリアクセスを約70%削減。これにより、アテンションの実行時間を50%から70%短縮し、トレーニングスループットを10%から40%向上させた。性能の劣化はなく、長い文脈長ではさらなる計算コストの節約が可能である。</span>
<span class="snippet"><span>Comment</span><p>code:


<a href="https://github.com/zhixuan-lin/forgetting-transformer" target="_blank" rel="noopener noreferrer">https://github.com/zhixuan-lin/forgetting-transformer</a>


</p>
<p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/zhxlin/status/1967596994362220761?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>openreview:


<a href="https://openreview.net/forum?id=xNj14CY5S1#discussion" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=xNj14CY5S1#discussion</a>


</p>
<p>先行研究:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2817" target="_blank" rel="noopener noreferrer">[Paper Note] Forgetting Transformer: Softmax Attention with a Forget Gate, Zhixuan Lin+, ICLR'25</a>
</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="efficient-context-2748" class="title-link">[Paper Note] Efficient Context Selection for Long-Context QA: No Tuning, No  Iteration, Just Adaptive-$k$, Chihiro Taguchi+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2506.08479" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2748" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="InformationRetrieval.html" target="_blank" rel="noopener noreferrer">#InformationRetrieval</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="ContextWindow.html" target="_blank" rel="noopener noreferrer">#ContextWindow</a>
<a class="button" href="RAG(RetrievalAugmentedGeneration).html" target="_blank" rel="noopener noreferrer">#RAG(RetrievalAugmentedGeneration)</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-09-10</span>
<span class="snippet"><span>GPT Summary</span>- Adaptive-$k$ retrievalを提案し、クエリと候補パッセージの類似度に基づいて適応的にパッセージ数を選択。これにより、固定サイズのベースラインと同等以上の性能を発揮し、トークン使用量を最大10倍削減しつつ70%の関連パッセージを取得。LCLMsと埋め込みモデルで精度向上を実現し、動的なコンテキストサイズ調整が効率的なQAに寄与することを示す。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/megagonlabs/status/1965430004667613305?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>実務上コストを抑えられるのは非常に嬉しい。あとで読む。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="spikingbrain-technical-2726" class="title-link">[Paper Note] SpikingBrain Technical Report: Spiking Brain-inspired Large Models, Yuqi Pan+, arXiv'25</h3>
<br><a href="https://arxiv.org/pdf/2509.05276" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2726" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-09-08</span>
<span class="snippet"><span>GPT Summary</span>- SpikingBrainは、長いコンテキストの効率的なトレーニングと推論のために設計された脳にインスパイアされたモデルで、MetaX GPUクラスターを活用。線形およびハイブリッド線形アーキテクチャを採用し、非NVIDIAプラットフォーム上での大規模LLM開発を実現。SpikingBrain-7BとSpikingBrain-76Bを開発し、約150BトークンでオープンソースのTransformerと同等の性能を達成。トレーニング効率を大幅に改善し、低消費電力での運用を可能にすることを示した。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/f14bertolotti/status/1964949822429069761?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>TTFTが4Mコンテキストの時にQwen2.5と比べて100倍高速化…？</p>
<p>中国のMetaX社のGPUが利用されている。<br><br>


<a href="https://www.metax-tech.com/en/goods/prod.html?cid=3" target="_blank" rel="noopener noreferrer">https://www.metax-tech.com/en/goods/prod.html?cid=3</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="refrag-rethinking-2716" class="title-link">[Paper Note] REFRAG: Rethinking RAG based Decoding, Xiaoqiang Lin+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2509.01092" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2716" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="RAG(RetrievalAugmentedGeneration).html" target="_blank" rel="noopener noreferrer">#RAG(RetrievalAugmentedGeneration)</a>
<a class="button" href="LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="SpeculativeDecoding.html" target="_blank" rel="noopener noreferrer">#SpeculativeDecoding</a>
<span class="issue_date">Issue Date: 2025-09-07</span>
<span class="snippet"><span>GPT Summary</span>- REFRAGは、RAGアプリケーションにおける遅延を改善するための効率的なデコーディングフレームワークであり、スパース構造を利用して初回トークンまでの時間を30.85倍加速します。これにより、LLMsのコンテキストサイズを16まで拡張可能にし、さまざまな長コンテキストタスクで精度を損なうことなくスピードアップを実現しました。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dr_singularity/status/1964453705430036982?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>興味深い。Speculative Decodingの新手法ともみなせそう。</p>
<p>同時期に出た下記研究と比較してどのようなpros/consがあるだろうか？<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2699" target="_blank" rel="noopener noreferrer">[Paper Note] Set Block Decoding is a Language Model Inference Accelerator, Itai Gat+, arXiv'25</a>
</p>
<p>解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1966292095242744186?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="set-block-2699" class="title-link">[Paper Note] Set Block Decoding is a Language Model Inference Accelerator, Itai Gat+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2509.04185" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2699" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-09-05</span>
<span class="snippet"><span>GPT Summary</span>- Set Block Decoding（SBD）を提案し、次トークン予測とマスクトークン予測を統合して生成を加速。SBDは複数の未来のトークンを並行してサンプリング可能で、従来の手法よりも速度向上を実現。アーキテクチャ変更なしで既存モデルをファインチューニングし、フォワードパスの数を3-5倍削減しつつ同等のパフォーマンスを達成。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/arankomatsuzaki/status/1963817987506643350?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="gso-challenging-2676" class="title-link">[Paper Note] GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents, Manish Shetty+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2505.23671" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2676" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="Coding.html" target="_blank" rel="noopener noreferrer">#Coding</a>
<a class="button" href="SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<span class="issue_date">Issue Date: 2025-09-03</span>
<span class="snippet"><span>GPT Summary</span>- 高性能ソフトウェア開発における言語モデルの能力を評価するためのベンチマークGSOを提案。102の最適化タスクを特定する自動化パイプラインを開発し、主要なソフトウェアエンジニアリングエージェントの成功率は5%未満であることを示した。定性的分析により、低レベル言語や最適化戦略の課題が明らかになった。研究の進展のために、ベンチマークのコードとエージェントのデータを公開。</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://gso-bench.github.io" target="_blank" rel="noopener noreferrer">https://gso-bench.github.io</a>


</p>
<p>ソフトウェアの高速化に関するベンチ</p>
<p>元ポストに掲載されているリーダーボードはどこにあるのだろう。ざっと見た感じ見当たらない。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="r-4b-incentivizing-2645" class="title-link">[Paper Note] R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs  via Bi-Mode Annealing and Reinforce Learning, Jie Jiang+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2508.21113" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2645" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="GRPO.html" target="_blank" rel="noopener noreferrer">#GRPO</a>
<a class="button" href="VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-09-02</span>
<span class="snippet"><span>GPT Summary</span>- R-4Bは、問題の複雑さに応じて思考を行うかどうかを適応的に判断する自動思考型のマルチモーダル大規模言語モデル（MLLM）である。思考能力と非思考能力を持たせ、バイモードポリシー最適化（BPO）を用いて思考プロセスの起動を精度良く判断する。訓練には多様なトピックのデータセットを使用し、実験結果はR-4Bが25のベンチマークで最先端のパフォーマンスを達成し、特に推論集約型タスクで低コストで高い性能を示したことを示している。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1962445854654288036?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>VLMにthinking, non-thinkingを入力に応じて使い分けさせる手法</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="aworld-orchestrating-2630" class="title-link">[Paper Note] AWorld: Orchestrating the Training Recipe for Agentic AI, Chengyue Yu+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2508.20404" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2630" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<span class="issue_date">Issue Date: 2025-08-31</span>
<span class="snippet"><span>GPT Summary</span>- AWorldというオープンソースシステムを導入し、エージェントと環境の相互作用を効率化。経験収集を14.6倍加速し、Qwen3-32Bベースのエージェントを訓練してGAIAの精度を21.59%から32.23%に向上。最難関レベルで商用モデルを超える性能を達成。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/aicia_solid/status/1961999098032328902?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1963005182817808721?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="moe++-accelerating-2622" class="title-link">[Paper Note] MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation   Experts, Peng Jin+, ICLR'25</h3>
<br><a href="https://arxiv.org/abs/2410.07348" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2622" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-08-31</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、Mixture-of-Experts（MoE）手法の効果と効率を向上させるために、MoE++フレームワークを提案。ゼロ計算エキスパートを導入し、低計算オーバーヘッド、高パフォーマンス、デプロイメントの容易さを実現。実験結果により、MoE++は従来のMoEモデルに比べて1.1-2.1倍のスループットを提供し、優れた性能を示す。</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=t7P5BUKcYv" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=t7P5BUKcYv</a>


</p>
<p>従来のMoEと比べて、専門家としてzero computation expertsを導入することで、性能を維持しながら効率的にinferenceをする手法(MoEにおいて全てのトークンを均一に扱わない）を提案している模様。<br><br><img src="https://github.com/user-attachments/assets/b71d01ed-92b1-4c4f-ab90-bf9b01c461be" alt="image" loading="lazy" width="400" height="300"><br><br>zero computation expertsは3種類で<br>- Zero Experts: 入力をゼロベクトルに落とす<br>- Copy Experts: 入力xをそのままコピーする<br>- Constant Experts: learnableな定数ベクトルvを学習し、xと線形結合して出力する。W_cによって入力xを変換することで線形補　結合の係数a1,a2を入力に応じて動的に決定する。<br><br><img src="https://github.com/user-attachments/assets/8c2f8f4c-d8d2-44ad-b3f0-4951f9fb2cfb" alt="image" loading="lazy" width="400" height="300"><br><br>Routingの手法やgating residual、学習手法の工夫もなされているようなので、後で読む。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="shortcut-connected-expert-2621" class="title-link">[Paper Note] Shortcut-connected Expert Parallelism for Accelerating   Mixture-of-Experts, Weilin Cai+, ICLR'25</h3>
<br><a href="https://arxiv.org/abs/2404.05019" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2621" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<span class="issue_date">Issue Date: 2025-08-31</span>
<span class="snippet"><span>GPT Summary</span>- ScMoEは、スパースゲート混合専門家モデルの計算負荷を分散させる新しいアーキテクチャで、通信と計算の重複を最大100%可能にし、全対全通信のボトルネックを解消。これにより、トレーニングで1.49倍、推論で1.82倍のスピードアップを実現し、モデル品質も既存手法と同等またはそれ以上を達成。</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=GKly3FkxN4&noteId=4tfWewv7R2" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=GKly3FkxN4&noteId=4tfWewv7R2</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="skip-a-2610" class="title-link">[Paper Note] Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs, Ziyue Li+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2507.07996" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2610" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Controllable.html" target="_blank" rel="noopener noreferrer">#Controllable</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Search.html" target="_blank" rel="noopener noreferrer">#Search</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Test-Time%20Scaling.html" target="_blank" rel="noopener noreferrer">#Test-Time Scaling</a>
<a class="button" href="Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<a class="button" href="KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-08-30</span>
<span class="snippet"><span>GPT Summary</span>- 事前学習済みのLLMの層をモジュールとして操作し、各サンプルに最適なアーキテクチャを構築する手法を提案。モンテカルロ木探索を用いて、数学および常識推論のベンチマークで最適な層の連鎖（CoLa）を特定。CoLaは柔軟で動的なアーキテクチャを提供し、推論効率を改善する可能性を示唆。75%以上の正しい予測に対して短いCoLaを見つけ、60%以上の不正確な予測を正すことができることが明らかに。固定アーキテクチャの限界を克服する道を開く。</span>
<span class="snippet"><span>Comment</span><p>解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1961749826028347602?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>事前学習済み言語モデルのforward pathにおける各layerをbuilding blocksとみなして、入力に応じてスキップ、あるいは再帰的な利用をMCTSによって選択することで、test time時のモデルの深さや、モデルの凡化性能をタスクに対して適用させるような手法を提案している模様。モデルのパラメータの更新は不要。k, r ∈ {1,2,3,4} の範囲で、"k個のlayerをskip"、あるいはk個のlayerのブロックをr回再帰する、とすることで探索範囲を限定的にしtest時の過剰な計算を抑止している。また、MCTSにおけるsimulationの回数は200回。length penaltyを大きくすることでcompactなforward pathになるように調整、10%の確率でまだ探索していない子ノードをランダムに選択することで探索を促すようにしている。オリジナルと比較して実行時間がどの程度増えてしまうのか？に興味があったが、モデルの深さという観点で推論効率は考察されているように見えたが、実行時間という観点ではざっと見た感じ記載がないように見えた。<br><br><img width="948" height="301" alt="Image" src="&lt;a%20href=" https: target="_blank" rel="noopener noreferrer">https://github.com/user-attachments/assets/0a03cdc2-141b-40a1-a11e-9560187ff7b6"


/&gt;<br><br>以下の広範なQA、幅広い難易度を持つ数学に関するデータで評価（Appendix Bに各データセットごとに500 sampleを利用と記載がある）をしたところ、大幅に性能が向上している模様。ただし、8B程度のサイズのモデルでしか実験はされていない。<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2613" target="_blank" rel="noopener noreferrer">[Paper Note] Think you have Solved Question Answering? Try ARC, the AI2 Reasoning
  Challenge, Peter Clark+, arXiv'18</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2615" target="_blank" rel="noopener noreferrer">[Paper Note] DART-Math: Difficulty-Aware Rejection Tuning for Mathematical  Problem-Solving, Yuxuan Tong+, NeurIPS'24</a>
<br><img width="986" height="682" alt="Image" src="&lt;a%20href=" https: target="_blank" rel="noopener noreferrer">https://github.com/user-attachments/assets/c6d88c0a-4ae0-41b7-8526-17d041692f49"


/&gt;</p>
<p>関連:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2611" target="_blank" rel="noopener noreferrer">[Paper Note] Looped Transformers are Better at Learning Learning Algorithms, Liu Yang+, ICLR'24</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2612" target="_blank" rel="noopener noreferrer">[Paper Note] Looped Transformers for Length Generalization, Ying Fan+, ICLR'25</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2605" target="_blank" rel="noopener noreferrer">[Paper Note] Universal Transformers, Mostafa Dehghani+, ICLR'19</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2241" target="_blank" rel="noopener noreferrer">[Paper Note] Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive   Token-Level Computation, Sangmin Bae+, NeurIPS'25</a>
</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="ultra-sparse-memory-2585" class="title-link">[Paper Note] Ultra-Sparse Memory Network, Zihao Huang+, ICLR'25</h3>
<br><a href="https://arxiv.org/abs/2411.12364" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2585" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="memory.html" target="_blank" rel="noopener noreferrer">#memory</a>
<span class="issue_date">Issue Date: 2025-08-29</span>
<span class="snippet"><span>GPT Summary</span>- UltraMemは、大規模で超スパースなメモリ層を組み込むことで、Transformerモデルの推論レイテンシを削減しつつ性能を維持する新しいアーキテクチャを提案。実験により、UltraMemはMoEを上回るスケーリング特性を示し、最大2000万のメモリスロットを持つモデルが最先端の推論速度と性能を達成することを実証。</span>
</article>
<article class="paper-entry">
<h3 id="jet-nemotron-efficient-2548" class="title-link">[Paper Note] Jet-Nemotron: Efficient Language Model with Post Neural Architecture  Search, Yuxian Gu+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2508.15884" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2548" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="NeuralArchitectureSearch.html" target="_blank" rel="noopener noreferrer">#NeuralArchitectureSearch</a>
<a class="button" href="SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="Reference%20Collection.html" target="_blank" rel="noopener noreferrer">#Reference Collection</a>
<span class="issue_date">Issue Date: 2025-08-26</span>
<span class="snippet"><span>GPT Summary</span>- Jet-Nemotronは新しいハイブリッドアーキテクチャの言語モデルで、フルアテンションモデルと同等以上の精度を持ちながら生成スループットを大幅に改善します。Post Neural Architecture Search（PostNAS）を用いて開発され、事前トレーニングされたモデルから効率的にアテンションブロックを探索します。Jet-Nemotron-2Bモデルは、他の先進モデルに対して高い精度を達成し、生成スループットを最大53.6倍向上させました。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1959832287073403137?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>著者ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hancai_hm/status/1960000017235902722?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jacksonatkinsx/status/1960090774122483783?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>所見:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/webbigdata/status/1960392071384326349?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/1960724749790929009?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>続報:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hancai_hm/status/1972794734747033985?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<br><br>コードとチェックポイントがリリース<br><br>code:


<a href="https://github.com/NVlabs/Jet-Nemotron" target="_blank" rel="noopener noreferrer">https://github.com/NVlabs/Jet-Nemotron</a>


<br>HF:


<a href="https://huggingface.co/collections/jet-ai/jet-nemotron-68ac76e8356b5399ef83ac9c" target="_blank" rel="noopener noreferrer">https://huggingface.co/collections/jet-ai/jet-nemotron-68ac76e8356b5399ef83ac9c</a>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="tokenskip-controllable-2536" class="title-link">[Paper Note] TokenSkip: Controllable Chain-of-Thought Compression in LLMs, Heming Xia+, EMNLP'25</h3>
<br><a href="https://arxiv.org/abs/2502.12067" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2536" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<a class="button" href="Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="Length.html" target="_blank" rel="noopener noreferrer">#Length</a>
<a class="button" href="Inference.html" target="_blank" rel="noopener noreferrer">#Inference</a>
<span class="issue_date">Issue Date: 2025-08-24</span>
<span class="snippet"><span>GPT Summary</span>- Chain-of-Thought (CoT)はLLMの推論能力を向上させるが、長いCoT出力は推論遅延を増加させる。これに対処するため、重要度の低いトークンを選択的にスキップするTokenSkipを提案。実験により、TokenSkipはCoTトークンの使用を削減しつつ推論性能を維持することを示した。特に、Qwen2.5-14B-InstructでGSM8Kにおいて推論トークンを40%削減し、性能低下は0.4%未満であった。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hemingkx/status/1891873475545137245?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="pushing-the-2535" class="title-link">[Paper Note] Pushing the Envelope of LLM Inference on AI-PC, Evangelos Georganas+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2508.06753" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2535" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Inference.html" target="_blank" rel="noopener noreferrer">#Inference</a>
<span class="issue_date">Issue Date: 2025-08-24</span>
<span class="snippet"><span>GPT Summary</span>- 超低ビットLLMモデルの登場により、リソース制約のある環境でのLLM推論が可能に。1ビットおよび2ビットのマイクロカーネルを設計し、PyTorch-TPPに統合することで、推論効率を最大2.2倍向上。これにより、AI PCやエッジデバイスでの超低ビットLLMモデルの効率的な展開が期待される。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1959379120577826935?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="hard-examples-2530" class="title-link">[Paper Note] Hard Examples Are All You Need: Maximizing GRPO Post-Training Under  Annotation Budgets, Benjamin Pikus+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2508.14094" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2530" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="GRPO.html" target="_blank" rel="noopener noreferrer">#GRPO</a>
<span class="issue_date">Issue Date: 2025-08-23</span>
<span class="snippet"><span>GPT Summary</span>- リソースが制約された状況での言語モデルのファインチューニングにおいて、難易度の異なるトレーニング例の優先順位を検討。実験により、最も難しい例でのトレーニングが最大47%のパフォーマンス向上をもたらすことが示され、難しい例が学習機会を多く提供することが明らかに。これにより、予算制約下での効果的なトレーニング戦略として、難しい例を優先することが推奨される。</span>
<span class="snippet"><span>Comment</span><p>ベースモデルのpass@kが低いhardestなサンプルでGRPOを学習するのがデータ効率が良く、OODに対する汎化性能も発揮されます、というのをQwen3-4B, 14B, Phi4で実験して示しました、という話っぽい？<br><br>小規模モデル、およびGSM8K、BIG Bench hardでの、Tracking Shuffled Objectのみでの実験な模様？大規模モデルやコーディングなどのドメインでもうまくいくかはよく分からない。OODの実験もAIME2025でのみの実験しているようなのでそこは留意した方が良いかも。<br>rewardとして何を使ったのかなどの細かい内容を追えていない。</p>
<p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/pratyushrt/status/1958947577216524352?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="beyond-gpt-5-2527" class="title-link">[Paper Note] Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency  Optimized Routing, Yiqun Zhang+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2508.12631" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2527" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-23</span>
<span class="snippet"><span>GPT Summary</span>- LLMのパフォーマンスと効率のバランスを取るために、テスト時ルーティングフレームワーク「Avengers-Pro」を提案。クエリを埋め込み、クラスタリングし、最適なモデルにルーティングすることで、6つのベンチマークで最先端の結果を達成。最強の単一モデルを平均精度で+7%上回り、コストを27%削減しつつ約90%のパフォーマンスを実現。すべての単一モデルの中で最高の精度と最低のコストを提供するパレートフロンティアを達成。コードは公開中。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/1958897458408563069?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>クエリをkmeansでクラスタリングし、各クラスタごとにモデルごとのperformanceとcostを事前に算出しておく。そして新たなクエリが来た時にクエリが割り当てられるtop pのクラスタのperformanae-cost efficiencyを合計し、スコアが高い一つのモデルを選択（＝routing)しinferenceを実施する。クエリはQwenでembedding化してクラスタリングに活用する。ハイパーパラメータα∈[0,1]によって、performance, costどちらを重視するかのバランスを調整する。<br><br>シンプルな手法だが、GPT-5 mediumと同等のコスト/性能　でより高い　性能/コスト　を実現。<br><img src="https://github.com/user-attachments/assets/203f99a3-79b3-4465-985b-2bbd124d3972" alt="image" loading="lazy" width="400" height="300"></p>
<p>性能向上、コスト削減でダメ押ししたい時に使えそうだが、発行するクエリがプロプライエタリデータ、あるいはそもそも全然データないんです、みたいな状況の場合、クエリの割当先となるクラスタを適切に確保する（クラスタリングに用いる十分な量のデータを準備する）のが大変な場面があるかもしれない。</p>
<p>（全然本筋と関係ないが、最近論文のタイトルにBeyondつけるの流行ってる…？）</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="liteasr-efficient-2525" class="title-link">[Paper Note] LiteASR: Efficient Automatic Speech Recognition with Low-Rank  Approximation, Keisuke Kamahori+, EMNLP'25</h3>
<br><a href="https://arxiv.org/abs/2502.20583" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2525" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="AutomaticSpeechRecognition(ASR).html" target="_blank" rel="noopener noreferrer">#AutomaticSpeechRecognition(ASR)</a>
<a class="button" href="EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<a class="button" href="Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<span class="issue_date">Issue Date: 2025-08-22</span>
<span class="snippet"><span>GPT Summary</span>- LiteASRは、現代の自動音声認識モデルのエンコーダを低ランク圧縮する手法で、推論コストを大幅に削減しつつ転写精度を維持します。主成分分析を用いて低ランク行列の乗算を近似し、自己注意機構を最適化することで、Whisper large-v3のエンコーダサイズを50%以上圧縮し、Whisper mediumと同等のサイズでより良い転写精度を実現しました。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/keisukekamahori/status/1958695752810864754?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>現代のASRモデルはencoderが計算効率の上でボトルネックとなっていたが、Forward Passにおける activatrion Y を PCA （式2, 3）に基づいて2つの低ランク行列の積（とバイアス項の加算; 式5）によって近似し計算効率を大幅に向上させた、という話な模様。weightを低ランクに写像するV_kとバイアス項のY_M（データセット全体に対するactivation Yの平均）はcalibrfationデータによって事前に計算可能とのこと。また、PCAのrank kがattention headの次元数より小さい場合、self-attentionの計算もより（QWKへ写像するWを低ランク行列で近似することで）効率的な手法を採用でき、そちらについても提案されている模様。（ざっくりしか読めていないので誤りがあるかもしれない。）<br><br><img width="592" height="449" alt="Image" src="&lt;a%20href=" https: target="_blank" rel="noopener noreferrer">https://github.com/user-attachments/assets/38c8aa6a-cad3-42d1-af6a-9102ed1df3f5"


/&gt;<br><br><img width="484" height="415" alt="Image" src="&lt;a%20href=" https: target="_blank" rel="noopener noreferrer">https://github.com/user-attachments/assets/f8fa8cd1-2b6a-405a-88ec-3bfd2158dffb"


/&gt;</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="chain-of-agents-end-to-end-2503" class="title-link">[Paper Note] Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent  Distillation and Agentic RL, Weizhen Li+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2508.13167" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2503" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Single.html" target="_blank" rel="noopener noreferrer">#Single</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-08-21</span>
<span class="snippet"><span>GPT Summary</span>- Chain-of-Agents（CoA）という新しいLLM推論パラダイムを提案し、マルチエージェントシステムの協力を単一モデル内でエンドツーエンドに実現。マルチエージェント蒸留フレームワークを用いて、エージェント的な教師ありファインチューニングを行い、強化学習で能力を向上。得られたエージェント基盤モデル（AFMs）は、ウェブエージェントやコードエージェントの設定で新たな最先端性能を示す。研究成果はオープンソース化され、今後の研究の基盤を提供。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/1958186531161853995?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>マルチエージェントのように振る舞うシングルエージェントを、マルチエージェントから得られたtrajectoryを通じて蒸留することめ実現する手法を提案。SFTでcold startに対して訓練した後、verifiable reward (タスクを正常に完了できたか否か)でRLする模様。<br><br><img src="https://github.com/user-attachments/assets/b4cafaba-488e-4d8b-a6d3-faf98733d134" alt="image" loading="lazy" width="400" height="300"><br><br><img src="https://github.com/user-attachments/assets/80a934e9-db47-401b-809e-394ab5e20585" alt="image" loading="lazy" width="400" height="300"></p>
<p>データセットも公開されている模様</p>
<p>所見:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dongxi_nlp/status/1958604404338147417?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1959877518972137667?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="less-is-2428" class="title-link">[Paper Note] Less Is More: Training-Free Sparse Attention with Global Locality for  Efficient Reasoning, Lijie Yang+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2508.07101" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2428" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2025-08-14</span>
<span class="snippet"><span>GPT Summary</span>- 「LessIsMore」という新しいスパースアテンションメカニズムを提案。これは、トレーニング不要でグローバルアテンションパターンを活用し、トークン選択を効率化。精度を維持しつつ、デコーディング速度を1.1倍向上させ、トークン数を2倍削減。既存手法と比較して1.13倍のスピードアップを実現。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/lijieyyang/status/1955139186530328633?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>トレーニングフリーで1.1倍のデコーディング速度で性能もFull Attentionと同等以上のSparse Attentionらしい</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="beyond-ten-2426" class="title-link">[Paper Note] Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale  Asynchronous RL, Jiaxuan Gao+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2508.07976" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2426" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Search.html" target="_blank" rel="noopener noreferrer">#Search</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<a class="button" href="Reference%20Collection.html" target="_blank" rel="noopener noreferrer">#Reference Collection</a>
<span class="issue_date">Issue Date: 2025-08-14</span>
<span class="snippet"><span>GPT Summary</span>- ASearcherは、LLMベースの検索エージェントの大規模なRLトレーニングを実現するオープンソースプロジェクトであり、高効率な非同期RLトレーニングと自律的に合成された高品質なQ&amp;Aデータセットを用いて、検索能力を向上させる。提案されたエージェントは、xBenchで46.7%、GAIAで20.8%の改善を達成し、長期的な検索能力を示した。モデルとデータはオープンソースで提供される。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1955603041518035358?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>著者ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jxwuyi/status/1955487396344238486?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>解説ポスト: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/1955266026498855354?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>関連ベンチマーク:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2466" target="_blank" rel="noopener noreferrer">[Paper Note] xbench: Tracking Agents Productivity Scaling with Profession-Aligned
  Real-World Evaluations, Kaiyuan Chen+, arXiv'25</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1158" target="_blank" rel="noopener noreferrer">GAIA: a benchmark for General AI Assistants, Grégoire Mialon+, N/A, arXiv'23</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1461" target="_blank" rel="noopener noreferrer">[Paper Note] Fact, Fetch, and Reason: A Unified Evaluation of Retrieval-Augmented  Generation, Satyapriya Krishna+, N/A, NAACL'25</a>
</p>
<p>既存のモデルは &lt;= 10 turnsのデータで学習されており、大規模で高品質なQAデータが不足している問題があったが、シードQAに基づいてQAを合成する手法によって1.4万シードQAから134kの高品質なQAを合成した（うち25.6kはツール利用が必要）。具体的には、シードのQAを合成しエージェントがQAの複雑度をiterationをしながら向上させていく手法を提案。事実情報は常にverificationをされ、合成プロセスのiterationの中で保持され続ける。個々のiterationにおいて、現在のQAと事実情報に基づいて、エージェントは<br>- Injection: 事実情報を新たに注入しQAをよりリッチにすることで複雑度を上げる<br>- Fuzz: QA中の一部の詳細な情報をぼかすことで、不確実性のレベルを向上させる。<br>の2種類の操作を実施する。その上で、QAに対してQuality verificationを実施する:<br>- Basic Quality: LLMでqualityを評価する<br>- Difficulty Measurement: LRMによって、複数の回答候補を生成する<br>- Answer Uniqueness: Difficulty Measurementで生成された複数の解答情報に基づいて、mismatched answersがvalid answerとなるか否かを検証し、正解が単一であることを担保する<br><br><img width="907" height="561" alt="Image" src="&lt;a%20href=" https: target="_blank" rel="noopener noreferrer">https://github.com/user-attachments/assets/d020fc8f-b1da-4425-981a-6759cba5824b"


/&gt;<br><br>また、複雑なタスク、特にtool callsが非常に多いタスクについては、多くのターン数（long trajectories）が必要となるが、既存のバッチに基づいた学習手法ではlong trajectoriesのロールアウトをしている間、他のサンプルの学習がブロックされてしまい学習効率が非常に悪いので、バッチ内のtrajectoryのロールアウトとモデルの更新を分離（ロールアウトのリクエストが別サーバに送信されサーバ上のInference Engineで非同期に実行され、モデルをアップデートする側は十分なtrajectoryがバッチ内で揃ったらパラメータを更新する、みたいな挙動？）することでIdleタイムを無くすような手法を提案した模様。<br><br><img width="873" height="466" alt="Image" src="&lt;a%20href=" https: target="_blank" rel="noopener noreferrer">https://github.com/user-attachments/assets/65d7e7b1-25fb-4288-a85e-07ae7a5eea2f"


/&gt;</p>
<p>既存の手法ベンチマークの性能は向上している。学習が進むにつれて、trajectory中のURL参照回数やsearch query数などが増大していく曲線は考察されている。他モデルと比較して、より多いターン数をより高い正確性を以って実行できるといった定量的なデータはまだ存在しないように見えた。<br><br><img width="891" height="778" alt="Image" src="&lt;a%20href=" https: target="_blank" rel="noopener noreferrer">https://github.com/user-attachments/assets/70644da8-b862-4bcb-bb05-d915c815b885"


/&gt;</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="difficulty-based-preference-2405" class="title-link">[Paper Note] Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap, Xuan Qi+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2508.04149" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2405" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Alignment.html" target="_blank" rel="noopener noreferrer">#Alignment</a>
<a class="button" href="DPO.html" target="_blank" rel="noopener noreferrer">#DPO</a>
<a class="button" href="PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-08-12</span>
<span class="snippet"><span>GPT Summary</span>- LLMの好みを人間に合わせるための新しいデータ選択戦略を提案。DPOの暗黙的報酬ギャップが小さいデータを選ぶことで、データ効率とモデルの整合性を向上。元のデータの10％で5つのベースラインを上回るパフォーマンスを達成。限られたリソースでのLLM整合性向上に寄与。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/zhijingjin/status/1954535751489667173?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>preference pair dataを学習効率の良いサンプルのみに圧縮することで学習効率を上げたい系の話で、chosen, rejectedなサンプルのそれぞれについて、¥frac{現在のポリシーの尤度}{参照ポリシーの尤度}によってreward rを定義し（おそらく参照ポリシーの尤度によってサンプルの重要度を重みづけしている）、r_chosenとr_rejectedの差をreward gapと定義し、gapが大きいものは難易度が低いと判断してフィルタリングする、といった話に見える。<br><img src="https://github.com/user-attachments/assets/1b930f5e-8db4-4c20-b7ca-59fb452f9056" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="fast-and-2396" class="title-link">[Paper Note] Fast and Simplex: 2-Simplicial Attention in Triton, Aurko Roy+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2507.02754" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2396" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-08-11</span>
<span class="snippet"><span>GPT Summary</span>- 2-シンプリシアルトランスフォーマーを用いることで、トークン効率を向上させ、標準的なトランスフォーマーよりも優れた性能を発揮することを示す。固定されたトークン予算内で、数学や推論タスクにおいてドット積アテンションを上回る結果を得た。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/scaling01/status/1954682957798715669?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="on-the-2352" class="title-link">[Paper Note] On the Expressiveness of Softmax Attention: A Recurrent Neural Network  Perspective, Gabriel Mongaras+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2507.23632" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2352" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、ソフトマックスアテンションの再帰的な形式を導出し、線形アテンションがその近似であることを示す。これにより、ソフトマックスアテンションの各部分をRNNの言語で説明し、構成要素の重要性と相互作用を理解する。これにより、ソフトマックスアテンションが他の手法よりも表現力が高い理由を明らかにする。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1952485214162407644?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>LinearAttention関連の研究は下記あたりがありそう？<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2353" target="_blank" rel="noopener noreferrer">[Paper Note] Efficient Attention: Attention with Linear Complexities, Zhuoran Shen+, arXiv'18</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2354" target="_blank" rel="noopener noreferrer">[Paper Note] Linformer: Self-Attention with Linear Complexity, Sinong Wang+, arXiv'20</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2355" target="_blank" rel="noopener noreferrer">[Paper Note] Reformer: The Efficient Transformer, Nikita Kitaev+, ICLR'20</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2356" target="_blank" rel="noopener noreferrer">[Paper Note] Transformers are RNNs: Fast Autoregressive Transformers with Linear  Attention, Angelos Katharopoulos+, ICML'20</a>
</p>
<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1271" target="_blank" rel="noopener noreferrer">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head
  Checkpoints, Joshua Ainslie+, N/A, arXiv'23</a>
<br><br>たとえばGQAはQwen3で利用されているが、本研究の知見を活用してscaled-dot product attention計算時のSoftmax計算の計算量が削減できたら、さらに計算量が削減できそう？</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="srpo-a-2341" class="title-link">[Paper Note] SRPO: A Cross-Domain Implementation of Large-Scale Reinforcement  Learning on LLM, Xiaojiang Zhang+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2504.14286" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2341" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<a class="button" href="CrossDomain.html" target="_blank" rel="noopener noreferrer">#CrossDomain</a>
<span class="issue_date">Issue Date: 2025-08-03</span>
<span class="snippet"><span>GPT Summary</span>- 二段階履歴再サンプリングポリシー最適化（SRPO）を提案し、DeepSeek-R1-Zero-32Bを上回る性能をAIME24およびLiveCodeBenchで達成。SRPOはトレーニングステップを約1/10に削減し、効率性を示す。二つの革新として、クロスドメイントレーニングパラダイムと履歴再サンプリング技術を導入し、LLMの推論能力を拡張するための実験を行った。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1914920300359377232?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>GRPOよりもより効率的な手法な模様。最初に数学のデータで学習をしReasoning Capabilityを身につけさせ、その後別のドメインのデータで学習させることで、その能力を発揮させるような二段階の手法らしい。<br><br>Datamixingよりも高い性能（ただし、これは数学とコーディングのCoT Lengthのドメイン間の違いに起因してこのような2 stageな手法にしているようなのでその点には注意が必要そう）？しっかりと読めていないので、読み違いの可能性もあるので注意。<br><img src="https://github.com/user-attachments/assets/cf00de8b-1923-4f23-b575-0a889517ec9e" alt="image" loading="lazy" width="400" height="300"></p>
<p>なんたらRPO多すぎ問題</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="wsm-decay-free-2340" class="title-link">[Paper Note] WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM  Pre-training, Changxin Tian+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2507.17634" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2340" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Optimizer.html" target="_blank" rel="noopener noreferrer">#Optimizer</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="ModelMerge.html" target="_blank" rel="noopener noreferrer">#ModelMerge</a>
<a class="button" href="Stability.html" target="_blank" rel="noopener noreferrer">#Stability</a>
<span class="issue_date">Issue Date: 2025-08-02</span>
<span class="snippet"><span>GPT Summary</span>- 学習率スケジューリングの新たなアプローチとして、Warmup-Stable and Merge（WSM）を提案。WSMは、学習率の減衰とモデルマージの関係を確立し、さまざまな減衰戦略を統一的に扱う。実験により、マージ期間がモデル性能において重要であることを示し、従来のWSDアプローチを上回る性能向上を達成。特に、MATHで+3.5%、HumanEvalで+2.9%、MMLU-Proで+5.5%の改善を記録。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/stochasticchasm/status/1951427541803106714?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>Weight Decayを無くせるらしい</p>
<p>エッセンスの解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenhaocha1/status/1951790366900019376?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<br><br>チェックポイントさえ保存しておいて事後的に活用することだで、細かなハイパラ調整のための試行錯誤する手間と膨大な計算コストがなくなるのであれば相当素晴らしいのでは…？<p>解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1965893163152793728?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="efficient-attention-2325" class="title-link">[Paper Note] Efficient Attention Mechanisms for Large Language Models: A Survey, Yutao Sun+, arXiv'25</h3>
<br><a href="https://arxiv.org/pdf/2507.19595" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2325" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2025-07-31</span>
<span class="snippet"><span>GPT Summary</span>- Transformerアーキテクチャの自己注意の複雑さが長文コンテキストモデリングの障害となっている。これに対処するため、線形注意手法とスパース注意技術が導入され、計算効率を向上させつつコンテキストのカバレッジを保持する。本研究は、これらの進展を体系的にまとめ、効率的な注意を大規模言語モデルに組み込む方法を分析し、理論と実践を統合したスケーラブルなモデル設計の基礎を提供することを目指す。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/omarsar0/status/1950287053046022286?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p><img src="https://github.com/user-attachments/assets/df56fa40-4206-4d12-9172-39f7b36f19c7" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="group-sequence-2299" class="title-link">[Paper Note] Group Sequence Policy Optimization, Chujie Zheng+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2507.18071" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2299" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="On-Policy.html" target="_blank" rel="noopener noreferrer">#On-Policy</a>
<a class="button" href="Stability.html" target="_blank" rel="noopener noreferrer">#Stability</a>
<span class="issue_date">Issue Date: 2025-07-26</span>
<span class="snippet"><span>GPT Summary</span>- Group Sequence Policy Optimization (GSPO)は、大規模言語モデルのための新しい強化学習アルゴリズムで、シーケンスの尤度に基づく重要度比を用いてトレーニングを行う。GSPOは、従来のGRPOアルゴリズムよりも効率的で高性能であり、Mixture-of-Experts (MoE) のトレーニングを安定化させる。これにより、最新のQwen3モデルにおいて顕著な改善が見られる。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1948904443749302785?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>公式ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/alibaba_qwen/status/1949412072942612873?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>GRPOとGSPOの違いのGIF:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1953976551424634930?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="swe-perf-can-2251" class="title-link">[Paper Note] SWE-Perf: Can Language Models Optimize Code Performance on Real-World  Repositories?, Xinyi He+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2507.12415" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2251" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="Evaluation.html" target="_blank" rel="noopener noreferrer">#Evaluation</a>
<a class="button" href="SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<span class="issue_date">Issue Date: 2025-07-18</span>
<span class="snippet"><span>GPT Summary</span>- コードのパフォーマンス最適化は重要であり、LLMsのリポジトリレベルでの能力は未探求。これに対処するため、SWE-Perfという初のベンチマークを導入。140のインスタンスを用いて、LLMsと専門家の最適化パフォーマンスのギャップを評価し、研究機会を示す。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/sivil_taram/status/1945855374336446577?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>これまでのSWE系のベンチマークはBug Fixなどにフォーカスされてきたが、こちらのベンチマークはソフトウェアのパフォーマンス（i.e., 実行時間）を改善させられるかにフォーカスしているとのこと。<br>実際にリポジトリからPRを収集し、パッチ前後の実行時間を比較。20回のrunを通じて統計的に有意な実行時間の差があるもののみにフィルタリングをしているとのこと。<br><br>Human Expertsは平均10.9%のgainを得たが、エージェントは2.3%にとどまっており、ギャップがあるとのこと。<br><br>傾向として、LLMはlow levelなインフラストラクチャ（環境構築, 依存関係のハンドリング, importのロジック）を改善するが、Human Expertsはhigh levelなロジックやデータ構造を改善する（e.g., アルゴリズムや、データハンドリング）。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="mixture-of-recursions-learning-2241" class="title-link">[Paper Note] Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive   Token-Level Computation, Sangmin Bae+, NeurIPS'25</h3>
<br><a href="https://arxiv.org/abs/2507.10524" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2241" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="memory.html" target="_blank" rel="noopener noreferrer">#memory</a>
<a class="button" href="RecurrentModels.html" target="_blank" rel="noopener noreferrer">#RecurrentModels</a>
<a class="button" href="RecursiveModels.html" target="_blank" rel="noopener noreferrer">#RecursiveModels</a>
<span class="issue_date">Issue Date: 2025-07-17</span>
<span class="snippet"><span>GPT Summary</span>- Mixture-of-Recursions（MoR）というフレームワークを提案し、再帰型トランスフォーマー内でパラメータ共有と適応計算を同時に実現。MoRは、レイヤーの再利用とトークンごとの再帰深さの動的割り当てにより、メモリアクセス効率を向上させる。135Mから1.7Bパラメータのモデルで、トレーニングFLOPsを維持しつつ、困惑度を低下させ、少数ショット精度を向上。MoRは大規模モデルのコストを抑えつつ、品質向上に寄与することを示す。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1945632764650533048?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1961593983114907806?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>関連:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2605" target="_blank" rel="noopener noreferrer">[Paper Note] Universal Transformers, Mostafa Dehghani+, ICLR'19</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2612" target="_blank" rel="noopener noreferrer">[Paper Note] Looped Transformers for Length Generalization, Ying Fan+, ICLR'25</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2611" target="_blank" rel="noopener noreferrer">[Paper Note] Looped Transformers are Better at Learning Learning Algorithms, Liu Yang+, ICLR'24</a>
 </p>
<p>著者ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/reza_byt/status/1980335836542677210?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="singlora-low-2194" class="title-link">[Paper Note] SingLoRA: Low Rank Adaptation Using a Single Matrix, David Bensaïd+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2507.05566" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2194" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="PEFT(Adaptor_LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="Stability.html" target="_blank" rel="noopener noreferrer">#Stability</a>
<span class="issue_date">Issue Date: 2025-07-12</span>
<span class="snippet"><span>GPT Summary</span>- SingLoRAは、LoRAの低ランク適応を再定式化し、単一の低ランク行列とその転置の積を用いることで、トレーニングの安定性を向上させ、パラメータ数をほぼ半減させる手法です。実験により、常識推論タスクでLLama 7Bを用いたファインチューニングで91.3%の精度を達成し、LoRAやLoRA+を上回る結果を示しました。また、画像生成においてもStable Diffusionのファインチューニングで高い忠実度を実現しました。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1943701154497732765?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>LoRAは低ランク行列BAの積を計算するが、オリジナルのモデルと同じ挙動から学習をスタートするために、Bをzeroで初期化し、Aはランダムに初期化する。このAとBの不均衡さが、勾配消失、爆発、あるいはsub-optimalな収束の要因となってしまっていた（inter-matrix scale conflicts)。特に、LoRAはモデルのwidthが大きくなると不安定になるという課題があった。このため、低ランク行列を2つ使うのではなく、1つの低ランク行列（とその転置）およびoptimizationのstep tごとにtrainableなパラメータがどの程度影響を与えるかを調整する度合いを決めるscalar function u(t)を導入することで、低ランク行列間の不均衡を解消しつつ、パラメータ数を半減し、学習の安定性と性能を向上させる。たとえばu(t)を学習開始時にzeroにすれば、元のLoRAにおいてBをzeroに初期化するのと同じ挙動（つまり元のモデルと同じ挙動から学習スタートができたりする。みたいな感じだろうか？<br><br><img src="https://github.com/user-attachments/assets/2dcd4ec1-59d3-43c0-ab8d-5c1c37e5ec3d" alt="image" loading="lazy" width="400" height="300"><br><br><img src="https://github.com/user-attachments/assets/c73b8715-e0c8-45c8-a7fa-ea55ac8ca3ce" alt="image" loading="lazy" width="400" height="300"><br><br><img src="https://github.com/user-attachments/assets/cf034dcd-37c4-48f1-a0a3-1d836db37820" alt="image" loading="lazy" width="400" height="300"><br><br><img src="https://github.com/user-attachments/assets/82999835-ac1e-4380-8bd0-00d14022abf5" alt="image" loading="lazy" width="400" height="300"></p>
<p>関連:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1956" target="_blank" rel="noopener noreferrer">LoRA: Low-Rank Adaptation of Large Language Models, Edward J. Hu+, ICLR'22</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1245" target="_blank" rel="noopener noreferrer">LoRA+: Efficient Low Rank Adaptation of Large Models, Soufiane Hayou+, N/A, ICML'24</a>
</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="first-return-2184" class="title-link">[Paper Note] First Return, Entropy-Eliciting Explore, Tianyu Zheng+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2507.07017" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2184" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="RLVR.html" target="_blank" rel="noopener noreferrer">#RLVR</a>
<span class="issue_date">Issue Date: 2025-07-10</span>
<span class="snippet"><span>GPT Summary</span>- FR3E（First Return, Entropy-Eliciting Explore）は、強化学習における不安定な探索を改善するための構造化された探索フレームワークであり、高不確実性の意思決定ポイントを特定し、中間フィードバックを提供します。実験結果は、FR3Eが安定したトレーニングを促進し、一貫した応答を生成することを示しています。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/f14bertolotti/status/1943201406271328524?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>RLVRのロールアウトにおいて、reasoning traceにおける各トークンを出力する際にエントロピーが高い部分を特定し（つまり、複数の候補がありモデルが迷っている）、その部分について異なる意図的に異なる生成パスを実行することで探索を促すようにするとRLVRがよりreliableになるといった話のようである<br><img src="https://github.com/user-attachments/assets/fc8adfcf-f6fc-4631-ba0a-04fa1401e96a" alt="image" loading="lazy" width="400" height="300"><br><br><img src="https://github.com/user-attachments/assets/fabf56a8-20f3-4782-a07b-3c854f01dfd5" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="naturalthoughts-selecting-2129" class="title-link">[Paper Note] NaturalThoughts: Selecting and Distilling Reasoning Traces for General  Reasoning Tasks, Yang Li+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2507.01921" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2129" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="Distillation.html" target="_blank" rel="noopener noreferrer">#Distillation</a>
<span class="issue_date">Issue Date: 2025-07-03</span>
<span class="snippet"><span>GPT Summary</span>- 教師モデルからの推論トレースを用いて生徒モデルの能力を向上させる方法を体系的に研究。NaturalReasoningに基づく高品質な「NaturalThoughts」をキュレーションし、サンプル効率とスケーラビリティを分析。データサイズの拡大が性能向上に寄与し、多様な推論戦略を必要とする例が効果的であることを発見。LlamaおよびQwenモデルでの評価により、NaturalThoughtsが既存のデータセットを上回り、STEM推論ベンチマークで優れた性能を示した。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jaseweston/status/1940656092054204498?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>関連:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1768" target="_blank" rel="noopener noreferrer">NaturalReasoning: Reasoning in the Wild with 2.8M Challenging Questions, Weizhe Yuan+, arXiv'25</a>
</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="chain-of-experts-unlocking-2110" class="title-link">[Paper Note] Chain-of-Experts: Unlocking the Communication Power of  Mixture-of-Experts Models, Zihan Wang+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2506.18945" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2110" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-06-28</span>
<span class="snippet"><span>GPT Summary</span>- Chain-of-Experts（CoE）は、逐次的な専門家間のコミュニケーションを導入した新しいMixture-of-Experts（MoE）アーキテクチャで、トークンを反復的に処理する。各反復ステップで専用のルーターを使用し、動的な専門家選択を可能にすることで、モデルの表現能力を向上させる。CoEは数学的推論タスクにおいて、従来のMoEと比較して検証損失を低下させ、メモリ使用量を削減する。反復的残差構造と専門家の専門化が、より表現力豊かな結果をもたらすことが示されている。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1938728784351658087?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="fineweb2-one-2109" class="title-link">[Paper Note] FineWeb2: One Pipeline to Scale Them All -- Adapting Pre-Training Data  Processing to Every Language, Guilherme Penedo+, COLM'25</h3>
<br><a href="https://arxiv.org/abs/2506.20920" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2109" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="MultiLingual.html" target="_blank" rel="noopener noreferrer">#MultiLingual</a>
<a class="button" href="COLM.html" target="_blank" rel="noopener noreferrer">#COLM</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-06-28</span>
<span class="snippet"><span>GPT Summary</span>- 多言語LLMsの性能向上のために、FineWebに基づく新しい事前学習データセットキュレーションパイプラインを提案。9つの言語に対して設計選択肢を検証し、非英語コーパスが従来のデータセットよりも高性能なモデルを生成できることを示す。データセットの再バランス手法も導入し、1000以上の言語にスケールアップした20テラバイトの多言語データセットFineWeb2を公開。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gui_penedo/status/1938631842720022572?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>v1<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1942" target="_blank" rel="noopener noreferrer">The FineWeb Datasets: Decanting the Web for the Finest Text Data at   Scale, Guilherme Penedo+, NeurIPS'24</a>
</p>
<p>abstを見る限りFinewebを多言語に拡張した模様</p>
<p>openreview:


<a href="https://openreview.net/forum?id=jnRBe6zatP#discussion" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=jnRBe6zatP#discussion</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="openvision-a-2105" class="title-link">[Paper Note] OpenVision: A Fully-Open, Cost-Effective Family of Advanced Vision  Encoders for Multimodal Learning, Xianhang Li+, ICCV'25</h3>
<br><a href="https://arxiv.org/abs/2505.04601" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2105" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="OpenSource.html" target="_blank" rel="noopener noreferrer">#OpenSource</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<a class="button" href="Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<a class="button" href="Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-06-26</span>
<span class="snippet"><span>GPT Summary</span>- OpenVisionは、完全にオープンでコスト効果の高いビジョンエンコーダーのファミリーを提案し、CLIPと同等以上の性能を発揮します。既存の研究を基に構築され、マルチモーダルモデルの進展に実用的な利点を示します。5.9Mから632.1Mパラメータのエンコーダーを提供し、容量と効率の柔軟なトレードオフを実現します。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/cihangxie/status/1920575141849030882?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>v2へアップデート:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/cihangxie/status/1963297223753494832?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<br><br>事前学習時にtext, image encoderのcontrastive lossで学習していたが、text encoderを無くしimage encoderに入力されたimageからcaptionを生成するcaption lossのみにすることで性能を落とすことなく効率を改善<br><br>テクニカルペーパーが出た模様<br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2820" target="_blank" rel="noopener noreferrer">[Paper Note] OpenVision 2: A Family of Generative Pretrained Visual Encoders for
  Multimodal Learning, Yanqing Liu+, arXiv'25</a>
<p>HF:


<a href="https://huggingface.co/collections/UCSC-VLAA/openvision-681a4c27ee1f66411b4ae919" target="_blank" rel="noopener noreferrer">https://huggingface.co/collections/UCSC-VLAA/openvision-681a4c27ee1f66411b4ae919</a>


<br>pj page: 


<a href="https://ucsc-vlaa.github.io/OpenVision/" target="_blank" rel="noopener noreferrer">https://ucsc-vlaa.github.io/OpenVision/</a>


</p>
<p>CLIP, SigLIPとは異なり完全にオープンなVision Encoder<br><img src="https://github.com/user-attachments/assets/b7c8eb07-45df-4ab3-9cd2-6b31af46e761" alt="image" loading="lazy" width="400" height="300"></p>
<p>v2の解説:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1963442911108084161?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="vamba-understanding-2099" class="title-link">[Paper Note] Vamba: Understanding Hour-Long Videos with Hybrid Mamba-Transformers, Weiming Ren+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2503.11579" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2099" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<a class="button" href="VideoGeneration_Understandings.html" target="_blank" rel="noopener noreferrer">#VideoGeneration/Understandings</a>
<a class="button" href="ICCV.html" target="_blank" rel="noopener noreferrer">#ICCV</a>
<span class="issue_date">Issue Date: 2025-06-26</span>
<span class="snippet"><span>GPT Summary</span>- VAMBAモデルは、Mamba-2ブロックを用いてビデオトークンを線形にエンコードし、トークン削減なしで1024フレームを処理可能。これにより、GPUメモリ使用量を50%削減し、トレーニング速度を倍増。1時間のビデオ理解ベンチマークLVBenchで4.3%の精度向上を達成し、様々なビデオ理解タスクで優れた性能を示す。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenhuchen/status/1938064510369280136?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="drop-upcycling-training-2088" class="title-link">[Paper Note] Drop-Upcycling: Training Sparse Mixture of Experts with Partial   Re-initialization, Taishi Nakamura+, ICLR'25</h3>
<br><a href="https://arxiv.org/abs/2502.19261" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2088" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<span class="issue_date">Issue Date: 2025-06-25</span>
<span class="snippet"><span>GPT Summary</span>- Drop-Upcycling手法を提案し、MoEモデルのトレーニング効率を向上。事前にトレーニングされた密なモデルの知識を活用しつつ、一部の重みを再初期化することで専門家の専門化を促進。大規模実験により、5.9BパラメータのMoEモデルが13B密なモデルと同等の性能を達成し、トレーニングコストを約1/4に削減。すべての実験リソースを公開。</span>
<span class="snippet"><span>Comment</span><p>OpenReview:


<a href="https://openreview.net/forum?id=gx1wHnf5Vp" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=gx1wHnf5Vp</a>


</p>
<p>関連:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1546" target="_blank" rel="noopener noreferrer">Sparse Upcycling: Training Mixture-of-Experts from Dense Checkpoints, Aran Komatsuzaki+, ICLR'23</a>
</p>
<p>提案手法の全体像とDiversity re-initializationの概要。元のUpcyclingでは全てidenticalな重みでreplicateされていたため、これが個々のexpertがlong termでの学習で特化することの妨げになり、最終的に最大限のcapabilityを発揮できず、収束が遅い要因となっていた。これを、Upcyclingした重みのうち、一部のindexのみを再初期化することで、replicate元の知識を保持しつつ、expertsの多様性を高めることで解決する。<br><img src="https://github.com/user-attachments/assets/46ec75a2-30b1-4f48-9f21-cf5f6e30df95" alt="image" loading="lazy" width="400" height="300"><br><img src="https://github.com/user-attachments/assets/ef3c66b2-32a5-46ab-bb31-828fb4570b53" alt="image" loading="lazy" width="400" height="300"><br><br>提案手法は任意のactivation function適用可能。今回はFFN Layerのactivation functionとして一般的なSwiGLUを採用した場合で説明している。<br><br>Drop-Upcyclingの手法としては、通常のUpcyclingと同様、FFN Layerの重みをn個のexpertsの数だけreplicateする。その後、re-initializationを実施する比率rに基づいて、[1, intermediate size d_f]の範囲からr*d_f個のindexをサンプリングする。最終的にSwiGLU、およびFFNにおける3つのWeight W_{gate, up, down}において、サンプリングされたindexと対応するrow/columnと対応する重みをre-initializeする。<br><br>re-initializeする際には、各W_{gate, up, down}中のサンプリングされたindexと対応するベクトルの平均と分散をそれぞれ独立して求め、それらの平均と分散を持つ正規分布からサンプリングする。<br><br>学習の初期から高い性能を発揮し、long termでの性能も向上している。また、learning curveの形状もscratchから学習した場合と同様の形状となっており、知識の転移とexpertsのspecializationがうまく進んだことが示唆される。<br><img src="https://github.com/user-attachments/assets/945e5ae5-05cd-4117-80e8-078b47f0e53c" alt="image" loading="lazy" width="400" height="300"></p>
<p>解説:


<a href="https://llm-jp.nii.ac.jp/news/post-566/" target="_blank" rel="noopener noreferrer">https://llm-jp.nii.ac.jp/news/post-566/</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="near$^2$-a-2087" class="title-link">[Paper Note] NEAR$^2$: A Nested Embedding Approach to Efficient Product Retrieval and  Ranking, Shenbin Qian+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2506.19743" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2087" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="InformationRetrieval.html" target="_blank" rel="noopener noreferrer">#InformationRetrieval</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="RepresentationLearning.html" target="_blank" rel="noopener noreferrer">#RepresentationLearning</a>
<span class="issue_date">Issue Date: 2025-06-25</span>
<span class="snippet"><span>GPT Summary</span>- Eコマース情報検索システムは、ユーザーの意図を正確に理解しつつ、大規模な商品カタログを効率的に処理することが難しい。本論文では、NEAR$^2$というネストされた埋め込みアプローチを提案し、推論時の埋め込みサイズを最大12倍効率化し、トレーニングコストを増やさずにトランスフォーマーモデルの精度を向上させる。さまざまなIR課題に対して異なる損失関数を用いて検証した結果、既存モデルよりも小さな埋め込み次元での性能向上を達成した。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_reachsumit/status/1937697219387490566?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="mercury-ultra-fast-2085" class="title-link">[Paper Note] Mercury: Ultra-Fast Language Models Based on Diffusion, Inception Labs+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2506.17298" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2085" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2025-06-25</span>
<span class="snippet"><span>GPT Summary</span>- 新しい拡散型大規模言語モデルMercuryを発表。特にコーディングアプリケーション向けのMercury Coderは、MiniとSmallの2サイズで提供され、速度と品質で最先端を達成。独立評価では、Mercury Coder Miniが1109トークン/秒、Smallが737トークン/秒を記録し、他のモデルを大幅に上回る性能を示す。さらに、実世界での検証結果や公開API、無料プレイグラウンドも提供。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/arankomatsuzaki/status/1937360864262389786?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>スループット（モデルのトークン生成速度）が、SoTAらしいdLLMモデル</p>
<p>解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1938026627642101858?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="wait-2053" class="title-link">[Paper Note] Wait, We Don't Need to "Wait" Removing Thinking Tokens Improves  Reasoning Efficiency, Chenlong Wang+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2506.08343" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2053" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<span class="issue_date">Issue Date: 2025-06-18</span>
<span class="snippet"><span>GPT Summary</span>- 自己反省を抑制する「NoWait」アプローチを提案し、推論の効率を向上。10のベンチマークで最大27%-51%の思考の連鎖の長さを削減し、有用性を維持。マルチモーダル推論のための効果的なソリューションを提供。</span>
<span class="snippet"><span>Comment</span><p>Wait, Hmmといったlong CoTを誘導するようなtokenを抑制することで、Accはほぼ変わらずに生成されるトークン数を削減可能、といった図に見える。Reasoningモデルでデコーディング速度を向上したい場合に効果がありそう。<br><img src="https://github.com/user-attachments/assets/c0abd2b4-f019-435e-b72f-f588fa0eb782" alt="image" loading="lazy" width="400" height="300"></p>
<p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/huggingpapers/status/1935130111608492060?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="overclocking-llm-2048" class="title-link">[Paper Note] Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path  Lengths in LLMs, Roy Eisenstadt+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2506.07240" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2048" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<span class="issue_date">Issue Date: 2025-06-17</span>
<span class="snippet"><span>GPT Summary</span>- LLMの推論プロセスにおける思考段階の長さを調整するメカニズムを探求。進捗をエンコードし、可視化することで計画ダイナミクスを明らかにし、不要なステップを減らす「オーバークロッキング」手法を提案。これにより、考えすぎを軽減し、回答精度を向上させ、推論のレイテンシを減少させることを実証。コードは公開。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1934357202619310559?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="resa-transparent-2035" class="title-link">[Paper Note] Resa: Transparent Reasoning Models via SAEs, Shangshang Wang+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2506.09967" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2035" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-06-13</span>
<span class="snippet"><span>GPT Summary</span>- Resaという1.5Bの推論モデル群を提案し、効率的なスパースオートエンコーダーチューニング（SAE-Tuning）手法を用いて訓練。これにより、97%以上の推論性能を保持しつつ、訓練コストを2000倍以上削減し、訓練時間を450倍以上短縮。軽いRL訓練を施したモデルで高い推論性能を実現し、抽出された推論能力は一般化可能かつモジュール化可能であることが示された。全ての成果物はオープンソース。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1933101904529363112?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>著者ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/upupwang/status/1933207676663865482?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>論文中で利用されているSource Modelの一つ:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1935" target="_blank" rel="noopener noreferrer">[Paper Note] Tina: Tiny Reasoning Models via LoRA, Shangshang Wang+, arXiv'25</a>
</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="text-to-lora-instant-2033" class="title-link">[Paper Note] Text-to-LoRA: Instant Transformer Adaption, Rujikorn Charakorn+, ICML'25</h3>
<br><a href="https://arxiv.org/abs/2506.06105" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2033" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="PEFT(Adaptor_LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<span class="issue_date">Issue Date: 2025-06-12</span>
<span class="snippet"><span>GPT Summary</span>- Text-to-LoRA（T2L）は、自然言語による説明に基づいて大規模言語モデル（LLMs）を迅速に適応させる手法で、従来のファインチューニングの高コストと時間を克服します。T2Lは、LoRAを安価なフォワードパスで構築するハイパーネットワークを使用し、タスク特有のアダプターと同等のパフォーマンスを示します。また、数百のLoRAインスタンスを圧縮し、新しいタスクに対してゼロショットで一般化可能です。このアプローチは、基盤モデルの専門化を民主化し、計算要件を最小限に抑えた言語ベースの適応を実現します。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/roberttlange/status/1933074366603919638?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>な、なるほど、こんな手が…！</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="saffron-1-towards-2027" class="title-link">[Paper Note] Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety  Assurance, Ruizhong Qiu+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2506.06444" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2027" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Alignment.html" target="_blank" rel="noopener noreferrer">#Alignment</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="Safety.html" target="_blank" rel="noopener noreferrer">#Safety</a>
<span class="issue_date">Issue Date: 2025-06-11</span>
<span class="snippet"><span>GPT Summary</span>- 既存のLLMの安全保証研究は主にトレーニング段階に焦点を当てているが、脱獄攻撃に対して脆弱であることが明らかになった。本研究では、推論スケーリングを用いた新たな安全性向上手法SAFFRONを提案し、計算オーバーヘッドを削減する多分岐報酬モデル（MRM）を導入。これにより、報酬モデル評価の数を減らし、探索-効率性のジレンマを克服する。実験により手法の有効性を確認し、訓練済みモデルと安全報酬データセットを公開。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gaotangli/status/1932289294657626189?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="log-linear-attention-2025" class="title-link">[Paper Note] Log-Linear Attention, Han Guo+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2506.04761" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2025" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-06-10</span>
<span class="snippet"><span>GPT Summary</span>- 対数線形注意を提案し、線形注意の効率性とソフトマックス注意の表現力を両立。固定サイズの隠れ状態を対数的に成長する隠れ状態に置き換え、計算コストを対数線形に抑える。Mamba-2とGated DeltaNetの対数線形バリアントが線形時間のバリアントと比較して優れた性能を示すことを確認。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1932194773559107911?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>解説ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theturingpost/status/1931432543766847887?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="unleashing-the-2013" class="title-link">[Paper Note] Unleashing the Reasoning Potential of Pre-trained LLMs by Critique  Fine-Tuning on One Problem, Yubo Wang+, EMNLP'25</h3>
<br><a href="https://arxiv.org/abs/2506.03295" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2013" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="EMNLP.html" target="_blank" rel="noopener noreferrer">#EMNLP</a>
<span class="issue_date">Issue Date: 2025-06-05</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、強力な大規模言語モデル（LLM）の推論能力を引き出すために、批評微調整（CFT）が効果的であることを示します。CFTは、単一の問題に対する多様な解を収集し、教師LLMによる批評データを構築する手法です。QwenおよびLlamaモデルを微調整した結果、数学や論理推論のベンチマークで顕著な性能向上を観察しました。特に、わずか5時間のトレーニングで、Qwen-Math-7B-CFTは他の手法と同等以上の成果を上げました。CFTは計算効率が高く、現代のLLMの推論能力を引き出すためのシンプルなアプローチであることが示されました。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenhuchen/status/1930447298527670662?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>関連:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1832" target="_blank" rel="noopener noreferrer">Critique Fine-Tuning: Learning to Critique is More Effective than   Learning to Imitate, Yubo Wang+, COLM'25</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1938" target="_blank" rel="noopener noreferrer">Reinforcement Learning for Reasoning in Large Language Models with One   Training Example, Yiping Wang+, NeurIPS'25</a>
</p>
<p>参考:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/weiliu99/status/1930826904522875309?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="dkv-cache-the-1984" class="title-link">dKV-Cache: The Cache for Diffusion Language Models, Xinyin Ma+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2505.15781" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1984" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2025-05-24</span>
<span class="snippet"><span>GPT Summary</span>- 拡散言語モデル（DLM）の遅い推論を改善するために、遅延KVキャッシュを提案。これは、異なるトークンの表現ダイナミクスに基づくキャッシング戦略で、2つのバリアントを設計。dKV-Cache-Decodeは損失の少ない加速を提供し、dKV-Cache-Greedyは高いスピードアップを実現。最終的に、推論速度を2〜10倍向上させ、DLMの性能を強化することを示した。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/arankomatsuzaki/status/1925384029718946177?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>提案手法を適用した場合、ARなモデルとDiffusion Modelで、実際のところどの程度のdecoding速度の差があるのだろうか？そういった分析はざーーっと見た感じ見当たらなかったように思える。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="parallel-scaling-1981" class="title-link">Parallel Scaling Law for Language Models, Mouxiang Chen+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2505.10475" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1981" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Scaling%20Laws.html" target="_blank" rel="noopener noreferrer">#Scaling Laws</a>
<span class="issue_date">Issue Date: 2025-05-21</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、言語モデルのスケーリングにおいて、並列計算を増加させる新しい手法「ParScale」を提案。これにより、モデルの前方パスを並列に実行し、出力を動的に集約することで、推論効率を向上させる。ParScaleは、少ないメモリ増加とレイテンシで同等の性能向上を実現し、既存のモデルを再利用することでトレーニングコストも削減可能。新しいスケーリング法則は、リソースが限られた状況での強力なモデル展開を促進する。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1924959706331939099?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/405" target="_blank" rel="noopener noreferrer">[Paper Note] Prefix-Tuning: Optimizing Continuous Prompts for Generation, Xiang Lisa Li+, arXiv'21, 2021.01</a>
<br><br>と考え方が似ている</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="adacot-pareto-optimal-1980" class="title-link">AdaCoT: Pareto-Optimal Adaptive Chain-of-Thought Triggering via  Reinforcement Learning, Chenwei Lou+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2505.11896" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1980" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<a class="button" href="Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<span class="issue_date">Issue Date: 2025-05-21</span>
<span class="snippet"><span>GPT Summary</span>- AdaCoT（Adaptive Chain-of-Thought）は、LLMsが推論を適応的に行う新しいフレームワークで、CoTの呼び出しタイミングを最適化します。強化学習を用いて、クエリの複雑さに基づいてCoTの必要性を判断し、計算コストを削減します。実験では、AdaCoTがCoTトリガー率を3.18%に低下させ、応答トークンを69.06%減少させつつ、高い性能を維持することが示されました。</span>
<span class="snippet"><span>Comment</span><p>RLのRewardにおいて、bassのリワードだけでなく、<br>- reasoningをなくした場合のペナルティ項<br>- reasoningをoveruseした場合のペナルティ項<br>- formattingに関するペナルティ項<br>を設定し、reasoningの有無を適切に判断できた場合にrewardが最大化されるような形にしている。(2.2.2)<br><br>が、multi-stageのRLでは（stageごとに利用するデータセットを変更するが）、データセットの分布には歪みがあり、たとえば常にCoTが有効なデータセットも存在しており（数学に関するデータなど）、その場合常にCoTをするような分布を学習してしまい、AdaptiveなCoT decisionが崩壊したり、不安定になってしまう（decision boundary collapseと呼ぶ）。特にこれがfinal stageで起きると最悪で、これまでAdaptiveにCoTされるよう学習されてきたものが全て崩壊してしまう。これを防ぐために、Selective Loss Maskingというlossを導入している。具体的には、decision token [^1]のlossへの貢献をマスキングするようにすることで、CoTが生じるratioにバイアスがかからないようにする。今回は、Decision tokenとして、`<think>`トークン直後のトークンをdecision tokenとみなし、lossに対する貢献をマスクしている（Selective Loss Masking）。<br><br>[^1]: CoTするかどうかは多くの場合このDecision Tokenによって決まる、といったことがどっかの研究に示されていたはず</think></p>
<p>いつか必要になったらしっかり読むが、全てのステージでSelective Loss Maskingをしたら、SFTでwarm upした段階からあまりCoTのratioが変化しないような学習のされ方になる気がするが、どのステージに対してapplyするのだろうか。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="insights-into-1977" class="title-link">Insights into DeepSeek-V3: Scaling Challenges and Reflections on  Hardware for AI Architectures, Chenggang Zhao+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2505.09343" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1977" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<span class="issue_date">Issue Date: 2025-05-20</span>
<span class="snippet"><span>GPT Summary</span>- DeepSeek-V3は、2,048台のNVIDIA H800 GPUでトレーニングされ、ハードウェア制約に対処するための共同設計を示す。メモリ効率向上のためのマルチヘッド潜在注意や、計算と通信の最適化を図る専門家の混合アーキテクチャ、FP8混合精度トレーニングなどの革新を強調。ハードウェアのボトルネックに基づく将来の方向性について議論し、AIワークロードに応えるためのハードウェアとモデルの共同設計の重要性を示す。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/deedydas/status/1924512147947848039?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="faster-cascades-1961" class="title-link">Faster Cascades via Speculative Decoding, Harikrishna Narasimhan+, ICLR'25</h3>
<br><a href="https://arxiv.org/abs/2405.19261" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1961" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="Test-Time%20Scaling.html" target="_blank" rel="noopener noreferrer">#Test-Time Scaling</a>
<a class="button" href="Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<a class="button" href="Verification.html" target="_blank" rel="noopener noreferrer">#Verification</a>
<a class="button" href="SpeculativeDecoding.html" target="_blank" rel="noopener noreferrer">#SpeculativeDecoding</a>
<span class="issue_date">Issue Date: 2025-05-13</span>
<span class="snippet"><span>GPT Summary</span>- カスケードと推測デコーディングは、言語モデルの推論効率を向上させる手法であり、異なるメカニズムを持つ。カスケードは難しい入力に対して大きなモデルを遅延的に使用し、推測デコーディングは並行検証で大きなモデルを活用する。新たに提案する推測カスケーディング技術は、両者の利点を組み合わせ、最適な遅延ルールを特定する。実験結果は、提案手法がカスケードおよび推測デコーディングのベースラインよりも優れたコスト品質トレードオフを実現することを示した。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1922059828429832259?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>OpenReview: 


<a href="https://openreview.net/forum?id=vo9t20wsmd" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=vo9t20wsmd</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="nemotron-cc-transforming-1944" class="title-link">Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon   Pretraining Dataset, Dan Su+, ACL'25</h3>
<br><a href="https://arxiv.org/abs/2412.02595" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1944" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-05-10</span>
<span class="snippet"><span>GPT Summary</span>- FineWeb-EduとDCLMは、モデルベースのフィルタリングによりデータの90%を削除し、トレーニングに適さなくなった。著者は、アンサンブル分類器や合成データの言い換えを用いて、精度とデータ量のトレードオフを改善する手法を提案。1Tトークンで8Bパラメータモデルをトレーニングし、DCLMに対してMMLUを5.6ポイント向上させた。新しい6.3Tトークンデータセットは、DCLMと同等の性能を持ちながら、4倍のユニークなトークンを含み、長トークンホライズンでのトレーニングを可能にする。15Tトークンのためにトレーニングされた8Bモデルは、Llama 3.1の8Bモデルを上回る性能を示した。データセットは公開されている。</span>
</article>
<article class="paper-entry">
<h3 id="reinforcement-learning-1938" class="title-link">Reinforcement Learning for Reasoning in Large Language Models with One   Training Example, Yiping Wang+, NeurIPS'25</h3>
<br><a href="https://arxiv.org/abs/2504.20571" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1938" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-05-09</span>
<span class="snippet"><span>GPT Summary</span>- 1-shot RLVRを用いることで、LLMの数学的推論能力が大幅に向上することを示した。Qwen2.5-Math-1.5Bモデルは、MATH500でのパフォーマンスが36.0%から73.6%に改善され、他の数学的ベンチマークでも同様の向上が見られた。1-shot RLVR中には、クロスドメイン一般化や持続的なテストパフォーマンスの改善が観察され、ポリシー勾配損失が主な要因であることが確認された。エントロピー損失の追加も重要で、結果報酬なしでもパフォーマンスが向上した。これらの成果は、RLVRのデータ効率に関するさらなる研究を促進する。</span>
<span class="snippet"><span>Comment</span><p><img src="https://github.com/user-attachments/assets/03cd9200-7fed-4c6d-a5a6-2379d2c8950a" alt="image" loading="lazy" width="400" height="300"></p>
<p>下記ポストでQwenに対してpromptを適切に与えることで、追加のpost training無しで高い数学に関する能力を引き出せたという情報がある。おそらく事前学習時に数学のQAデータによって継続事前学習されており、この能力はその際に身についているため、数学に対する高い能力は実は簡単に引き出すことができるのかもしれない（だから1サンプルでも性能が向上したのではないか？）といった考察がある。<br><br>参考:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/weiliu99/status/1930826904522875309?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2011" target="_blank" rel="noopener noreferrer">[Paper Note] ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in  Large Language Models, Mingjie Liu+, NeurIPS'25</a>
<br><br>とはどのような関係性があるだろうか？</p>
<p>著者ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/ypwang61/status/1968834328508379563?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="tina-tiny-1935" class="title-link">[Paper Note] Tina: Tiny Reasoning Models via LoRA, Shangshang Wang+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2504.15777" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1935" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="PEFT(Adaptor_LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="GRPO.html" target="_blank" rel="noopener noreferrer">#GRPO</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-05-07</span>
<span class="snippet"><span>GPT Summary</span>- Tinaは、コスト効率よく強力な推論能力を実現する小型の推論モデルファミリーであり、1.5Bパラメータのベースモデルに強化学習を適用することで高い推論性能を示す。Tinaは、従来のSOTAモデルと競争力があり、AIME24で20%以上の性能向上を達成し、トレーニングコストはわずか9ドルで260倍のコスト削減を実現。LoRAを通じた効率的なRL推論の効果を検証し、すべてのコードとモデルをオープンソース化している。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rasbt/status/1920107023980462575?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>（おそらく）Reasoningモデルに対して、LoRAとRLを組み合わせて、reasoning能力を向上させた初めての研究</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="bitnet-b1.58-1898" class="title-link">BitNet b1.58 2B4T Technical Report, Shuming Ma+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2504.12285" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1898" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="Quantization.html" target="_blank" rel="noopener noreferrer">#Quantization</a>
<a class="button" href="SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<span class="issue_date">Issue Date: 2025-04-19</span>
<span class="snippet"><span>GPT Summary</span>- BitNet b1.58 2B4Tは、20億パラメータを持つオープンソースの1ビット大規模言語モデルで、4兆トークンで訓練されました。言語理解や数学的推論などのベンチマークで評価され、同サイズのフルプレシジョンLLMと同等の性能を示しつつ、計算効率が向上しています。メモリ、エネルギー消費、デコーディングレイテンシが削減され、モデルの重みはHugging Faceで公開されています。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iscienceluvr/status/1912783876365177235?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>圧倒的省メモリかつcpuでのinference速度も早そう<br><img src="https://github.com/user-attachments/assets/dacf05e4-9cb3-48b4-9a98-532f7245eb8e" alt="image" loading="lazy" width="400" height="300"></p>
<p>- アーキテクチャはTransformerを利用<br>- Linear layerとしてBitLinear Layerを利用<br>  - 重みは{1, 0, -1}の3値をとる<br>  - activationは8bitのintegerに量子化<br>  - Layer Normalizationはsubln normalization <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1899" target="_blank" rel="noopener noreferrer">Foundation Transformers, Hongyu Wang+, PMLR'23</a>
 を利用</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="scalable-softmax-is-1866" class="title-link">Scalable-Softmax Is Superior for Attention, Ken M. Nakanishi, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2501.19399" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1866" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<span class="issue_date">Issue Date: 2025-04-06</span>
<span class="snippet"><span>GPT Summary</span>- SSMaxを提案し、Softmaxの代替としてTransformerモデルに統合。これにより、長いコンテキストでの重要情報の取得が向上し、事前学習中の損失減少が速くなる。SSMaxは注意スコアを改善し、長さの一般化を促進する。</span>
<span class="snippet"><span>Comment</span><p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1863" target="_blank" rel="noopener noreferrer">Llama 4 Series, Meta, 2025.04</a>
<br><br>で採用されている手法で、ブログポスト中で引用されている。Long Contextになった場合にsoftmaxの分布が均一になる（＝重要な情報にattendする能力が削がれる）ことを防ぐための手法を提案している。</p>
<p>解説ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/nrehiew_/status/1908613993998045534"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="demystifying-llm-based-1847" class="title-link">[Paper Note] Demystifying LLM-based Software Engineering Agents, Chunqiu Steven Xia+, FSE'25, 2024.07</h3>
<br><a href="https://arxiv.org/abs/2407.01489" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1847" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-04-02</span>
<span class="snippet"><span>GPT Summary</span>- 最近のLLMの進展により、ソフトウェア開発タスクの自動化が進んでいるが、複雑なエージェントアプローチの必要性に疑問が生じている。これに対し、Agentlessというエージェントレスアプローチを提案し、シンプルな三段階プロセスで問題を解決。SWE-bench Liteベンチマークで最高のパフォーマンスと低コストを達成。研究は自律型ソフトウェア開発におけるシンプルで解釈可能な技術の可能性を示し、今後の研究の方向性を刺激することを目指している。</span>
<span class="snippet"><span>Comment</span><p>日本語解説:


<a href="https://note.com/ainest/n/nac1c795e3825" target="_blank" rel="noopener noreferrer">https://note.com/ainest/n/nac1c795e3825</a>


</p>
<p>LLMによる計画の立案、環境からのフィードバックによる意思決定などの複雑なワークフローではなく、Localization（階層的に問題のある箇所を同定する）とRepair（LLMで複数のパッチ候補を生成する）、PatchValidation(再現テストと回帰テストの両方を通じて結果が良かったパッチを選ぶ）のシンプルなプロセスを通じてIssueを解決する。<br><img src="https://github.com/user-attachments/assets/6d042dfe-9780-4410-9077-b265af5456d1" alt="image" loading="lazy" width="400" height="300"><br><br>これにより、低コストで高い性能を達成している、といった内容な模様。</p>
<p>Agentlessと呼ばれ手法だが、preprint版にあったタイトルの接頭辞だった同呼称がproceeding版では無くなっている。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="stop-overthinking-1819" class="title-link">Stop Overthinking: A Survey on Efficient Reasoning for Large Language  Models, Yang Sui+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2503.16419" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1819" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<span class="issue_date">Issue Date: 2025-03-22</span>
<span class="snippet"><span>GPT Summary</span>- 本論文では、LLMsにおける効率的な推論の進展を体系的に調査し、以下の主要な方向に分類します：(1) モデルベースの効率的推論、(2) 推論出力ベースの効率的推論、(3) 入力プロンプトベースの効率的推論。特に、冗長な出力による計算オーバーヘッドを軽減する方法を探求し、小規模言語モデルの推論能力や評価方法についても議論します。</span>
<span class="snippet"><span>Comment</span><p>Reasoning Modelにおいて、Over Thinking現象（不要なreasoning stepを生成してしまう）を改善するための手法に関するSurvey。<br><img src="https://github.com/user-attachments/assets/a411f2cf-2ba1-4e58-8dc7-ac1ae2ff0de6" alt="image" loading="lazy" width="400" height="300"><br><br>下記Figure2を見るとよくまとまっていて、キャプションを読むとだいたい分かる。なるほど。<br>Length Rewardについては、<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1746" target="_blank" rel="noopener noreferrer">[Paper Note] Demystifying Long Chain-of-Thought Reasoning in LLMs, Edward Yeo+, arXiv'25</a>
<br><br>で考察されている通り、Reward Hackingが起きるので設計の仕方に気をつける必要がある。<br><br><img src="https://github.com/user-attachments/assets/fd6880bd-95e1-4ca6-9593-8ffc9390962a" alt="image" loading="lazy" width="400" height="300"></p>
<p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/_reachsumit/status/1902977896685396275?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>各カテゴリにおけるliteratureも見やすくまとめられている。必要に応じて参照したい。<br><img src="https://github.com/user-attachments/assets/b6be0d79-35c5-45a8-878b-2b6be67c2f76" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="the-first-1813" class="title-link">The First Few Tokens Are All You Need: An Efficient and Effective  Unsupervised Prefix Fine-Tuning Method for Reasoning Models, Ke Ji+, arXiv'25</h3>
<br><a href="https://arxiv.org/abs/2503.02875" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1813" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="PEFT(Adaptor_LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<span class="issue_date">Issue Date: 2025-03-19</span>
<span class="snippet"><span>GPT Summary</span>- 非教師ありプレフィックスファインチューニング（UPFT）を提案し、LLMの推論効率を向上。初期のプレフィックス部分文字列に基づいて訓練し、ラベル付きデータやサンプリングを不要に。UPFTは、教師あり手法と同等の性能を維持しつつ、訓練時間を75%、サンプリングコストを99%削減。最小限の非教師ありファインチューニングで大幅な推論向上を実現し、リソース効率の良い代替手段を提供。</span>
<span class="snippet"><span>Comment</span><p>斜め読みだが、reasoning traceの冒頭部分は重要な役割を果たしており、サンプリングした多くのresponseのreasoning traceにおいて共通しているものは重要という直感から（Prefix Self-Consistency）、reasoning traceの冒頭部分を適切に生成できるようにモデルをFinetuningする。従来のRejection Samplingを用いた手法では、複数のresponseを生成させて、最終的なanswerが正解のものをサンプリングするため正解ラベルが必要となるが、提案手法ではreasoning traceの冒頭部分の共通するsubsequenceをmajority voteするだけなのでラベルが不要である。<br><img src="https://github.com/user-attachments/assets/ff3938da-dcd0-4b6c-a764-26d2e8caa63a" alt="image" loading="lazy" width="400" height="300"><br><br>reasoning prefixを学習する際は下記のようなテンプレートを用いる。このときに、prefixのspanのみを利用して学習することで大幅に学習時間を削減できる。<br><img src="https://github.com/user-attachments/assets/63aabe47-9c40-41cb-8d5a-5039900f6edc" alt="image" loading="lazy" width="400" height="300"><br><br>また、そのような学習を行うとcatastrophic forgettingのリスクが非常に高いが、これを防ぐために、マルチタスクラーニングを実施する。具体的には学習データのp%については全体のreasoning traceを生成して学習に利用する。このときに、最終的な回答の正誤を気にせずtraceを生成して学習に利用することで、ラベルフリーな特性を維持できる（つまり、こちらのデータは良いreasoning traceを学習することを目的としているわけではなく、あくまでcatastrophic forgettingを防ぐためにベースモデルのようなtraceもきちんと生成できれば良い、という感覚だと思われる）。<br><img src="https://github.com/user-attachments/assets/6e83c686-5bcf-4db8-9b8a-63c39570a4dc" alt="image" loading="lazy" width="400" height="300"><br><br>AppendixにQwenを用いてtemperature 0.7で16個のresponseをサンプリングし、traceの冒頭部分が共通している様子が示されている。</p>
<p>下記論文でlong-CoTを学習させる際のlong-CoTデータとして、reasoningモデルから生成したtraceと非reasoning modelから生成したtraceによるlong-CoTデータを比較したところ前者の方が一貫して学習性能が良かったとあるが、この研究でもreasoning traceをつよつよモデルで生成したら性能上がるんだろうか。<br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1746" target="_blank" rel="noopener noreferrer">[Paper Note] Demystifying Long Chain-of-Thought Reasoning in LLMs, Edward Yeo+, arXiv'25</a>
</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="transformers-without-1795" class="title-link">[Paper Note] Transformers without Normalization, Jiachen Zhu+, CVPR'25</h3>
<br><a href="https://arxiv.org/abs/2503.10622" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1795" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="CVPR.html" target="_blank" rel="noopener noreferrer">#CVPR</a>
<a class="button" href="Normalization.html" target="_blank" rel="noopener noreferrer">#Normalization</a>
<span class="issue_date">Issue Date: 2025-03-14</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、正規化層なしのトランスフォーマーがDynamic Tanh（DyT）を用いることで、同等またはそれ以上のパフォーマンスを達成できることを示します。DyTは、レイヤー正規化の代替として機能し、ハイパーパラメータの調整なしで効果を発揮します。多様な設定での実験により、正規化層の必要性に対する新たな洞察を提供します。</span>
<span class="snippet"><span>Comment</span><p>なん…だと…。LayerNormalizationを下記アルゴリズムのようなtanhを用いた超絶シンプルなレイヤー（parameterized thnh [Lecun氏ポスト](



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/ylecun/status/1900610590315249833?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q)）に置換するだけっぽい？"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<br><img src="https://github.com/user-attachments/assets/474d3ee4-4c08-4b00-9a41-126ca5d5207e" alt="image" loading="lazy" width="400" height="300"><br><img src="https://github.com/user-attachments/assets/5aea9f93-85d9-4e0b-b9db-bb407d596493" alt="image" loading="lazy" width="400" height="300"><br><br>同等以上の性能を維持しながらモデル全体のinference, trainingの時間を8%程度削減。<br><img src="https://github.com/user-attachments/assets/98f8caa3-3ef2-4594-a45a-ae0aa2cf2ef6" alt="image" loading="lazy" width="400" height="300"></span><br><br>
</article>
<article class="paper-entry">
<h3 id="native-sparse-1775" class="title-link">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse   Attention, Jingyang Yuan+, ACL'25</h3>
<br><a href="https://arxiv.org/abs/2502.11089" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1775" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-03-02</span>
<span class="snippet"><span>GPT Summary</span>- 長文コンテキストモデリングのために、計算効率を改善するスパースアテンションメカニズム「NSA」を提案。NSAは動的な階層スパース戦略を用い、トークン圧縮と選択を組み合わせてグローバルなコンテキスト認識とローカルな精度を両立。実装最適化によりスピードアップを実現し、エンドツーエンドのトレーニングを可能にすることで計算コストを削減。NSAはフルアテンションモデルと同等以上の性能を維持しつつ、長シーケンスに対して大幅なスピードアップを達成。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dair_ai/status/1893698286545969311?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>ACL'25のBest Paperの一つ:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1950644063952052643?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="mixture-of-transformers-a-1505" class="title-link">Mixture-of-Transformers: A Sparse and Scalable Architecture for   Multi-Modal Foundation Models, Weixin Liang+, TMLR'25</h3>
<br><a href="https://arxiv.org/abs/2411.04996" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1505" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="TMLR.html" target="_blank" rel="noopener noreferrer">#TMLR</a>
<a class="button" href="UMM.html" target="_blank" rel="noopener noreferrer">#UMM</a>
<span class="issue_date">Issue Date: 2024-11-12</span>
<span class="snippet"><span>GPT Summary</span>- 大規模言語モデル（LLMs）のマルチモーダル処理を効率化するために、Mixture-of-Transformers（MoT）を提案。MoTは計算コストを削減し、モダリティごとにパラメータを分離して特化した処理を実現。Chameleon 7B設定では、55.8%のFLOPsで密なベースラインに匹敵する性能を示し、音声を含む場合も37.2%のFLOPsで同様の結果を達成。さらに、Transfusion設定では、7BのMoTモデルが密なベースラインの画像性能に対してFLOPsの3分の1で匹敵し、760Mのモデルは主要な画像生成指標で上回る結果を得た。MoTは実用的な利点も示し、画像品質を47.2%、テキスト品質を75.6%の経過時間で達成。</span>
</article>
<article class="paper-entry">
<h3 id="mantis-interleaved-3862" class="title-link">[Paper Note] MANTIS: Interleaved Multi-Image Instruction Tuning, Dongfu Jiang+, TMLR'24 Outstanding Certification, 2024.05</h3>
<br><a href="https://arxiv.org/abs/2405.01483" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3862" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="InstructionTuning.html" target="_blank" rel="noopener noreferrer">#InstructionTuning</a>
<a class="button" href="MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="TMLR.html" target="_blank" rel="noopener noreferrer">#TMLR</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<a class="button" href="2D%20(Image).html" target="_blank" rel="noopener noreferrer">#2D (Image)</a>
<span class="issue_date">Issue Date: 2025-12-02</span>
<span class="snippet"><span>GPT Summary</span>- Mantisモデルは、721Kの複数画像指示データを用いた指示調整により、複数画像の視覚言語タスクで最先端の性能を達成。特に、Idefics2-8Bを平均13ポイント上回り、一般化能力も示す。大規模な事前学習に依存せず、低コストの指示調整で複数画像能力を向上できることを示した。</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=skLtdUVaJa" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=skLtdUVaJa</a>


</p>
<p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/wenhuchen/status/1995554059122868735?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="depth-anything-3709" class="title-link">[Paper Note] Depth Anything V2, Lihe Yang+, NeurIPS'24, 2024.06</h3>
<br><a href="https://arxiv.org/abs/2406.09414" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3709" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="SyntheticData.html" target="_blank" rel="noopener noreferrer">#SyntheticData</a>
<a class="button" href="2D%20(Image).html" target="_blank" rel="noopener noreferrer">#2D (Image)</a>
<a class="button" href="DepthEstimation.html" target="_blank" rel="noopener noreferrer">#DepthEstimation</a>
<span class="issue_date">Issue Date: 2025-11-18</span>
<span class="snippet"><span>GPT Summary</span>- Depth Anything V2を提案し、合成画像の使用、教師モデルの能力拡大、擬似ラベル付き実画像を用いた学生モデルの教育を通じて、より細かく堅牢な深度推定を実現。最新のStable Diffusionモデルと比較して、効率的かつ正確であり、異なるスケールのモデルを提供。多様なシーンを考慮した評価ベンチマークも構築。</span>
<span class="snippet"><span>Comment</span><p>pj page:


<a href="https://depth-anything-v2.github.io" target="_blank" rel="noopener noreferrer">https://depth-anything-v2.github.io</a>


</p>
<p>openreview:


<a href="https://openreview.net/forum?id=cFTi3gLJ1X&referrer=%5Bthe%20profile%20of%20Hengshuang%20Zhao%5D(%2Fprofile%3Fid%3D~Hengshuang_Zhao2)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=cFTi3gLJ1X&referrer=%5Bthe%20profile%20of%20Hengshuang%20Zhao%5D(%2Fprofile%3Fid%3D~Hengshuang_Zhao2)</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="back-to-3009" class="title-link">[Paper Note] Back to Basics: Revisiting REINFORCE Style Optimization for Learning   from Human Feedback in LLMs, Arash Ahmadian+, ACL'24, 2024.02</h3>
<br><a href="https://arxiv.org/abs/2402.14740" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3009" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Alignment.html" target="_blank" rel="noopener noreferrer">#Alignment</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-09-27</span>
<span class="snippet"><span>GPT Summary</span>- RLHFにおける整合性の重要性を考慮し、PPOの高コストとハイパーパラメータ調整の問題を指摘。シンプルなREINFORCEスタイルの最適化手法がPPOや新提案の手法を上回ることを示し、LLMの整合性特性に適応することで低コストのオンラインRL最適化が可能であることを提案。</span>
</article>
<article class="paper-entry">
<h3 id="minicpm-unveiling-2540" class="title-link">[Paper Note] MiniCPM: Unveiling the Potential of Small Language Models with Scalable   Training Strategies, Shengding Hu+, COLM'24</h3>
<br><a href="https://arxiv.org/abs/2404.06395" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2540" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="COLM.html" target="_blank" rel="noopener noreferrer">#COLM</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="Scheduler.html" target="_blank" rel="noopener noreferrer">#Scheduler</a>
<span class="issue_date">Issue Date: 2025-08-25</span>
<span class="snippet"><span>GPT Summary</span>- 急成長する大規模言語モデル（LLMs）の開発におけるコストの懸念から、小規模言語モデル（SLMs）の可能性が注目されている。本研究では、MiniCPMという1.2Bおよび2.4Bの非埋め込みパラメータバリアントを紹介し、これらが7B-13BのLLMsと同等の能力を持つことを示す。モデルのスケーリングには広範な実験を、データのスケーリングにはWarmup-Stable-Decay（WSD）学習率スケジューラを導入し、効率的なデータ-モデルスケーリング法を研究した。MiniCPMファミリーにはMiniCPM-DPO、MiniCPM-MoE、MiniCPM-128Kが含まれ、優れたパフォーマンスを発揮している。MiniCPMモデルは公開されている。</span>
<span class="snippet"><span>Comment</span><p>Warmup-Stable-Decay (WSD)</p>
<p>openreview:


<a href="https://openreview.net/forum?id=3X2L2TFr0f&noteId=QvwPc5chyd" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=3X2L2TFr0f&noteId=QvwPc5chyd</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="better-&amp;-2444" class="title-link">[Paper Note] Better &amp; Faster Large Language Models via Multi-token Prediction, Fabian Gloeckle+, ICML'24</h3>
<br><a href="https://arxiv.org/abs/2404.19737" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2444" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Coding.html" target="_blank" rel="noopener noreferrer">#Coding</a>
<a class="button" href="ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-08-16</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、大規模言語モデルを複数の将来のトークンを同時に予測するように訓練する手法を提案し、サンプル効率の向上を図る。具体的には、n個の独立した出力ヘッドを用いて次のnトークンを予測し、訓練時間にオーバーヘッドをかけずに下流の能力を向上させる。特に、コーディングタスクにおいて、提案モデルは強力なベースラインを上回る性能を示し、推論時に最大3倍の速度向上も実現。</span>
<span class="snippet"><span>Comment</span><p>next tokenだけでなく、next 4-tokenを予測して学習することで、MBPP/HumanEvalにおいて、モデルのパラメータサイズが1.3Bを超えた時点でベースライン（=同じパラメータサイズとなるように調整されたnext-token prediction）をoutperformしはじめ、モデルサイズが大きくなるにつれて性能の差が顕著に表れることを示した。コーディングドメインにおいて事前学習、およびfinetuningの双方で効果がある。ただし、3.7節で示されている通り、これはコーディングドメインでのみこのような顕著な改善がみられており、自然言語データに対してはここまで顕著な改善はしていないように見える（5.1節で考察されていそう; 昨今のLLMでは事前学習データにコーディングなどのデータが入るのが普通なので利用する恩恵はありそう; Abstractive Summarizationでは性能が改善している(Figure6); GSM8Kでは200Bまではnext 2 tokenを予測すると性能が改善しているが500B token学習するとnext token predictionの方が性能が良くなる）。全体的にperplexityの改善（=次のトークンにおいて正解トークンの生成確率を改善する）というよりは、モデルの"最終的な生成結果”にフォーカスした評価となっている。<br><br>モデルは共有のトランクf_s (おそらくhead間でパラメータを共有している一連のtransformerブロック) を持っておりinput x_t:1に対応するlatent representation z_t:1を生成する。latent representationをoutput headにinputすることで、それぞれのheadが合計でn個のnext tokenを予測する。<br><img width="608" height="1021" alt="Image" src="&lt;a%20href=" https: target="_blank" rel="noopener noreferrer">https://github.com/user-attachments/assets/433d69cb-5593-483b-b591-6445c482ed2e"


/&gt;<br><br>next n-tokenを予測する際には、GPUメモリを大幅に食ってしまう （logitsのshapeが(n, V)となりそれらの勾配も保持しなければならない) ことがボトルネックとなるが、f_sまでforward passを実行したら、各headに対してforward/backward passを順番に実行して、logitsの値は破棄し勾配の情報だけf_sに蓄積することで、長期的に保持する情報を各headのから逆伝搬された勾配情報のみにすることでこれを解決している。<br><img width="597" height="478" alt="Image" src="&lt;a%20href=" https: target="_blank" rel="noopener noreferrer">https://github.com/user-attachments/assets/3f5ff3fc-5934-4f12-9327-23b689526464"


/&gt;<br><br>実際にinferenceをするときはnext tokenを予測するヘッドの出力を活用することを前提としているが、全てのヘッドを活用することで、t時点でt+nトークンの予測を可能なため、self-speculative decodingを実施しinference timeを短縮することができる。<br><br>3.4で示されているように、nの値は大きければ大きいほど良いというわけではなく、4程度（byte levelなモデルの場合は8 bytes）が最適なようである。が、Table1を見ると、データによってはn=6が良かったり（i.e., 最適なnは学習データ依存）複数エポック学習するとmulti token predictionの効果が薄くなっていそう（i.e., 同じトークンの予測を複数回学習するので実質multi token predictionと似たようなことをやっている。言い換えると、multi token predictionは複数epochの学習を先取りしているとみなせる？）なのは注意が必要そう。</p>
<p>全体的に複数epochを学習すると恩恵がなくなっていく（コーディング） or next token predictionよりも性能が悪化する（自然言語）ので、LLMの事前学習において、複数epochを学習するような当たり前みたいな世界線が訪れたら、このアーキテクチャを採用すると性能はむしろ悪化しそうな気はする。</p>
<p>MBPP/HumanEval:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2439" target="_blank" rel="noopener noreferrer">[Paper Note] Program Synthesis with Large Language Models, Jacob Austin+, arXiv'21</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2438" target="_blank" rel="noopener noreferrer">[Paper Note] Evaluating Large Language Models Trained on Code, Mark Chen+, arXiv'21</a>
</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="densing-law-1992" class="title-link">Densing Law of LLMs, Chaojun Xiao+, arXiv'24</h3>
<br><a href="https://arxiv.org/abs/2412.04315" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1992" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Scaling%20Laws.html" target="_blank" rel="noopener noreferrer">#Scaling Laws</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-05-27</span>
<span class="snippet"><span>GPT Summary</span>- 大規模言語モデル（LLMs）の性能向上に伴うトレーニングと推論の効率の課題を解決するために、「キャパシティ密度」という新しい指標を提案。これは、ターゲットLLMの有効パラメータサイズと実際のパラメータサイズの比率を用いて、モデルの効果と効率を評価するフレームワークを提供する。分析により、LLMsのキャパシティ密度は約3か月ごとに倍増する傾向があることが示され、今後のLLM開発における重要性が強調される。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1926785750277693859?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p><img src="https://github.com/user-attachments/assets/8cdcfe78-6682-481b-a6b0-a175b84d735c" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="the-fineweb-1942" class="title-link">The FineWeb Datasets: Decanting the Web for the Finest Text Data at   Scale, Guilherme Penedo+, NeurIPS'24</h3>
<br><a href="https://arxiv.org/abs/2406.17557" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1942" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-05-10</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、15兆トークンからなるFineWebデータセットを紹介し、LLMの性能向上に寄与することを示します。FineWebは高品質な事前学習データセットのキュレーション方法を文書化し、重複排除やフィルタリング戦略を詳細に調査しています。また、FineWebから派生した1.3兆トークンのFineWeb-Eduを用いたLLMは、MMLUやARCなどのベンチマークで優れた性能を発揮します。データセット、コードベース、モデルは公開されています。</span>
<span class="snippet"><span>Comment</span><p>日本語解説:


<a href="https://zenn.dev/deepkawamura/articles/da9aeca6d6d9f9" target="_blank" rel="noopener noreferrer">https://zenn.dev/deepkawamura/articles/da9aeca6d6d9f9</a>


</p>
<p>openreview:


<a href="https://openreview.net/forum?id=n6SCkn2QaG#discussion" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=n6SCkn2QaG#discussion</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="full-parameter-1785" class="title-link">Full Parameter Fine-tuning for Large Language Models with Limited Resources, Lv+, ACL'24, 2024.08</h3>
<br><a href="https://aclanthology.org/2024.acl-long.445/" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1785" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<span class="issue_date">Issue Date: 2025-03-06</span>
<span class="snippet"><span>GPT Summary</span>- 新しいオプティマイザ「LOMO」を提案し、勾配計算とパラメータ更新を1ステップで融合することでメモリ使用量を削減。これにより、24GBのメモリを持つ8台のRTX 3090で65Bモデルの全パラメータファインチューニングが可能に。メモリ使用量は標準的なアプローチと比較して10.8%削減。</span>
</article>
<article class="paper-entry">
<h3 id="a-survey-1627" class="title-link">A Survey on LLM Inference-Time Self-Improvement, Xiangjue Dong+, arXiv'24</h3>
<br><a href="https://arxiv.org/abs/2412.14352" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1627" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2024-12-31</span>
<span class="snippet"><span>GPT Summary</span>- LLM推論における自己改善技術を三つの視点から検討。独立した自己改善はデコーディングやサンプリングに焦点、文脈に応じた自己改善は追加データを活用、モデル支援の自己改善はモデル間の協力を通じて行う。関連研究のレビューと課題、今後の研究への洞察を提供。</span>
</article>
<article class="paper-entry">
<h3 id="observational-scaling-1540" class="title-link">Observational Scaling Laws and the Predictability of Language Model  Performance, Yangjun Ruan+, arXiv'24</h3>
<br><a href="https://arxiv.org/abs/2405.10938" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1540" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2024-11-22</span>
<span class="snippet"><span>GPT Summary</span>- 言語モデルの性能を理解するために、約100の公開モデルからスケーリング法則を構築する新しい観察アプローチを提案。モデルファミリー間の能力変動を考慮し、性能が低次元の能力空間の関数であることを示す。これにより、複雑なスケーリング現象の予測可能性を示し、GPT-4のエージェント性能を非エージェント的ベンチマークから予測できることを明らかにし、Chain-of-ThoughtやSelf-Consistencyの影響を予測する方法を示す。</span>
<span class="snippet"><span>Comment</span><p>縦軸がdownstreamタスクの主成分（のうち最も大きい80%を説明する成分）の変化（≒LLMの性能）で、横軸がlog scaleの投入計算量。<br>Qwenも頑張っているが、投入データ量に対する性能（≒データの品質）では、先駆け的な研究であるPhiがやはり圧倒的?<br><img src="https://github.com/user-attachments/assets/c38286df-37c1-4c72-832f-676832845c0e" alt="image" loading="lazy" width="400" height="300"></p>
<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/766" target="_blank" rel="noopener noreferrer">Textbooks Are All You Need, Suriya Gunasekar+, N/A, arXiv'23</a>
<br><br>も参照のこと</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="balancing-speed-1524" class="title-link">Balancing Speed and Stability: The Trade-offs of FP8 vs. BF16 Training  in LLMs, Kazuki Fujii+, arXiv'24</h3>
<br><a href="https://arxiv.org/abs/2411.08719" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1524" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Analysis.html" target="_blank" rel="noopener noreferrer">#Analysis</a>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="Japanese.html" target="_blank" rel="noopener noreferrer">#Japanese</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2024-11-17</span>
<span class="snippet"><span>GPT Summary</span>- 大規模言語モデル（LLMs）は、その言語理解能力と適用可能性から注目を集めており、特にLlama 3シリーズは4050億パラメータを持つ。トレーニングの効率化が求められる中、NVIDIAのH100 GPUはFP8フォーマットを導入し、トレーニング時間を短縮する可能性がある。初期研究ではFP8が性能を損なわずに効率を向上させることが示唆されているが、トレーニングの安定性や下流タスクへの影響はまだ不明である。本研究は、LLMsのトレーニングにおけるBF16とFP8のトレードオフを探る。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/okoge_kaz/status/1857639065421754525?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>FP8で継続的事前学習をするとスループットは向上するが、lossのスパイクを生じたり、downstreamタスクの性能がBF16よりも低下したりする（日本語と英語の両方）との報告のようである。現状アブストと付録しか記載がないが、内容はこれから更新されるのだろうか。<br><br><img src="https://github.com/user-attachments/assets/8d60d59b-de00-483a-bff0-04a4145715c1" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="understanding-llms-1523" class="title-link">Understanding LLMs: A Comprehensive Overview from Training to Inference, Yiheng Liu+, arXiv'24</h3>
<br><a href="https://arxiv.org/abs/2401.02038" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1523" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2024-11-17</span>
<span class="snippet"><span>GPT Summary</span>- ChatGPTの普及に伴い、LLMsのコスト効率の良いトレーニングとデプロイメントへの関心が高まっている。本論文では、LLMsのトレーニング技術と推論デプロイメント技術の進化をレビューし、データ前処理やモデル圧縮などのさまざまな側面を議論する。また、LLMsの利用方法と将来の発展についての洞察も提供する。</span>
<span class="snippet"><span>Comment</span><p>[Perplexity（参考;Hallucinationに注意）](


<a href="https://www.perplexity.ai/search/yi-xia-nolun-wen-wodu-minei-ro-7vGwDK_AQX.HDO7j9H8iNA)" target="_blank" rel="noopener noreferrer">https://www.perplexity.ai/search/yi-xia-nolun-wen-wodu-minei-ro-7vGwDK_AQX.HDO7j9H8iNA)</a>


</p>
<p>単なるLLMの理論的な説明にとどまらず、実用的に必要な各種並列処理技術、Mixed Precision、Offloadingなどのテクニックもまとまっているのがとても良いと思う。</p>
<p>LLM Frameworkのところに、メジャーなものが網羅されていないように感じる。たとえば、UnslothやLiger-KernelなどはTransformersの部分で言及されてても良いのでは、と感じる。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="delift-data-1504" class="title-link">DELIFT: Data Efficient Language model Instruction Fine Tuning, Ishika Agarwal+, arXiv'24</h3>
<br><a href="https://arxiv.org/abs/2411.04425" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1504" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="InstructionTuning.html" target="_blank" rel="noopener noreferrer">#InstructionTuning</a>
<span class="issue_date">Issue Date: 2024-11-12</span>
<span class="snippet"><span>GPT Summary</span>- DELIFTという新しいアルゴリズムを提案し、ファインチューニングの各ステージでデータ選択を最適化。ペアワイズユーティリティメトリックを用いてデータの有益性を定量化し、最大70%のデータ削減を実現。計算コストを大幅に節約し、既存の方法を上回る効率性と効果を示す。</span>
</article>
<article class="paper-entry">
<h3 id="scaling-llm-1501" class="title-link">Scaling LLM Test-Time Compute Optimally can be More Effective than  Scaling Model Parameters, Charlie Snell+, arXiv'24</h3>
<br><a href="https://arxiv.org/abs/2408.03314" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1501" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Test-Time%20Scaling.html" target="_blank" rel="noopener noreferrer">#Test-Time Scaling</a>
<span class="issue_date">Issue Date: 2024-11-12</span>
<span class="snippet"><span>GPT Summary</span>- LLMの推論時の計算をスケーリングすることで、挑戦的なプロンプトに対するパフォーマンスを改善する方法を研究。特に、密なプロセスベースの検証者報酬モデルとプロンプトに応じた応答の適応的更新を分析。プロンプトの難易度によって効果が変化し、計算最適戦略を適用することで効率を4倍以上向上。さらに、テスト時計算を用いることで小さなモデルが大きなモデルを上回ることが示された。</span>
<span class="snippet"><span>Comment</span><p><img src="https://github.com/user-attachments/assets/0562a65e-b2f1-4ff4-b806-107313fc255e" alt="image" loading="lazy" width="400" height="300"></p>
<p>[Perplexity（参考;Hallucinationに注意）](


<a href="https://www.perplexity.ai/search/yi-xia-noyan-jiu-wodu-mi-nei-r-1e1euXgLTH.G0Wlp.V2iqA)" target="_blank" rel="noopener noreferrer">https://www.perplexity.ai/search/yi-xia-noyan-jiu-wodu-mi-nei-r-1e1euXgLTH.G0Wlp.V2iqA)</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="what-matters-1467" class="title-link">What Matters in Transformers? Not All Attention is Needed, Shwai He+, N_A, arXiv'24</h3>
<br><a href="https://arxiv.org/abs/2406.15786" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1467" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<span class="issue_date">Issue Date: 2024-10-22</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、トランスフォーマー内のBlocks、MLP、Attention層間の冗長性を調査し、Attention層の高い類似性によりプルーニングが可能であることを示しました。具体的には、Llama-2-70BではAttention層の半分を削除することで48.4%のスピードアップを達成し、パフォーマンスはわずか2.4%低下しました。また、Attention層とMLP層を同時に削除する手法を提案し、31層削除してもLlama-2-13Bは90%のパフォーマンスを維持しました。これにより、今後のネットワークアーキテクチャ設計に貴重な洞察を提供します。</span>
<span class="snippet"><span>Comment</span><p>通常LLMはtransformer decoderのブロックをstackすることで形成されるが、積み上げたブロック、あるいはlayerってほんとに全部必要なの?という疑問に答えてくれる論文のようである。<br><br>transformer blockそのもの、あるいはMLP layerを削除するとpeformanceは大幅に低下するが、attention layerを削除してもperformanceの低下が起きなかった模様。これにより高速化が実現可能。<br><br>削除するブロックやlayerはinputとoutputのコサイン類似度が高いものを削除することによって実現。<br><br><img src="https://github.com/user-attachments/assets/da1e6a56-1bc4-4206-9423-acd7512300c8" alt="image" loading="lazy" width="400" height="300"><br><br><img src="https://github.com/user-attachments/assets/724ddf50-cd63-437d-9df2-73423dd77a6e" alt="image" loading="lazy" width="400" height="300"><br><br>比較的パラメータサイズが小さい7B, 13Bモデルでの実験結果<br><img src="https://github.com/user-attachments/assets/19253c9e-7eae-4084-a8c2-e99680b34649" alt="image" loading="lazy" width="400" height="300"><br><br>より大きなモデルでの実験結果<br><img src="https://github.com/user-attachments/assets/18eef07e-623c-482c-9a6b-9ea65450ecea" alt="image" loading="lazy" width="400" height="300"></p>
<p>パフォーマンスが変わらない範囲だと、attention layer dropにより、7B, 13Bモデルの場合は23%程度、70Bの場合は35%のスループット向上</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="addition-is-1459" class="title-link">Addition is All You Need for Energy-efficient Language Models, Hongyin Luo+, N_A, arXiv'24</h3>
<br><a href="https://arxiv.org/abs/2410.00907" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1459" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<span class="issue_date">Issue Date: 2024-10-20</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、浮動小数点乗算を高精度で整数加算器によって近似するL-Mulアルゴリズムを提案。これにより、8ビット浮動小数点乗算に比べて計算リソースを大幅に削減しつつ、より高い精度を実現。L-Mulをテンソル処理ハードウェアに適用することで、エネルギーコストを95％（要素ごとの乗算）および80％（ドット積）削減可能。実験結果は理論的誤差推定と一致し、L-Mulは従来の浮動小数点乗算と同等またはそれ以上の精度を達成。トランスフォーマーモデル内の浮動小数点乗算をL-Mulに置き換えることで、ファインチューニングと推論において高い精度を維持できることを示した。</span>
</article>
<article class="paper-entry">
<h3 id="enhancing-performance-1420" class="title-link">Enhancing Performance and Scalability of Large-Scale Recommendation  Systems with Jagged Flash Attention, Rengan Xu+, N_A, arXiv'24</h3>
<br><a href="https://arxiv.org/abs/2409.15373" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1420" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<span class="issue_date">Issue Date: 2024-09-25</span>
<span class="snippet"><span>GPT Summary</span>- ハードウェアアクセラレーターの統合により、推薦システムの能力が向上する一方で、GPU計算コストが課題となっている。本研究では、カテゴリ特徴の長さによるGPU利用の複雑さに対処するため、「Jagged Feature Interaction Kernels」を提案し、動的サイズのテンソルを効率的に扱う手法を開発。さらに、JaggedテンソルをFlash Attentionと統合し、最大9倍のスピードアップと22倍のメモリ削減を実現。実際のモデルでは、10%のQPS改善と18%のメモリ節約を確認し、複雑な推薦システムのスケーリングを可能にした。</span>
</article>
<article class="paper-entry">
<h3 id="from-decoding-1386" class="title-link">From Decoding to Meta-Generation: Inference-time Algorithms for Large  Language Models, Sean Welleck+, N_A, arXiv'24</h3>
<br><a href="https://arxiv.org/abs/2406.16838" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1386" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2024-09-10</span>
<span class="snippet"><span>GPT Summary</span>- 推論時の計算リソース拡大の利点に焦点を当て、トークンレベル生成、メタ生成、効率的生成の3つのアプローチを統一的に探求。トークンレベル生成はデコーディングアルゴリズムを用い、メタ生成はドメイン知識や外部情報を活用し、効率的生成はコスト削減と速度向上を目指す。従来の自然言語処理、現代のLLMs、機械学習の視点を統合した調査。</span>
<span class="snippet"><span>Comment</span><p>元ツイート: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gneubig/status/1833522477605261799?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>CMUのチームによるinference timeの高速化に関するサーベイ</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="flashattention-3-fast-1338" class="title-link">[Paper Note] FlashAttention-3: Fast and Accurate Attention with Asynchrony and   Low-precision, Jay Shah+, NeurIPS'24</h3>
<br><a href="https://arxiv.org/abs/2407.08608" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1338" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2024-07-30</span>
<span class="snippet"><span>GPT Summary</span>- FlashAttention-3は、Hopper GPU上でAttentionを高速化するために、3つの技術を開発し、H100 GPUで1.5-2.0倍の速度向上を実現。FP16で740 TFLOPs/s、FP8で約1.2 PFLOPs/sに達し、FP8では数値誤差が2.6倍低いことを確認。</span>
<span class="snippet"><span>Comment</span><p>openreview:


<a href="https://openreview.net/forum?id=tVConYid20&referrer=%5Bthe%20profile%20of%20Tri%20Dao%5D(%2Fprofile%3Fid%3D~Tri_Dao1)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=tVConYid20&referrer=%5Bthe%20profile%20of%20Tri%20Dao%5D(%2Fprofile%3Fid%3D~Tri_Dao1)</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="phi-3-technical-1293" class="title-link">Phi-3 Technical Report: A Highly Capable Language Model Locally on Your  Phone, Marah Abdin+, N_A, arXiv'24</h3>
<br><a href="https://arxiv.org/abs/2404.14219" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1293" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<span class="issue_date">Issue Date: 2024-04-23</span>
<span class="snippet"><span>GPT Summary</span>- phi-3-miniは38億パラメータの言語モデルであり、3.3兆トークンで訓練されています。Mixtral 8x7BやGPT-3.5などの大規模モデルに匹敵する総合的なパフォーマンスを持ちながら、スマートフォンにデプロイ可能なサイズです。このモデルは、厳密にフィルタリングされたWebデータと合成データで構成されており、堅牢性、安全性、およびチャット形式に適合しています。また、phi-3-smallとphi-3-mediumというより大規模なモデルも紹介されています。</span>
<span class="snippet"><span>Comment</span><p><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1039" target="_blank" rel="noopener noreferrer">Textbooks Are All You Need II: phi-1.5 technical report, Yuanzhi Li+, N/A, arXiv'23</a>
 の次の次（Phi2.0についてはメモってなかった）。スマホにデプロイできるレベルのサイズで、GPT3.5Turbo程度の性能を実現したらしい</p>
<p>Llama2と同じブロックを利用しているため、アーキテクチャはLlama2と共通。<br><br></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="the-unreasonable-1292" class="title-link">The Unreasonable Ineffectiveness of the Deeper Layers, Andrey Gromov+, N_A, arXiv'24</h3>
<br><a href="https://arxiv.org/abs/2403.17887" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1292" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Pruning.html" target="_blank" rel="noopener noreferrer">#Pruning</a>
<span class="issue_date">Issue Date: 2024-04-22</span>
<span class="snippet"><span>GPT Summary</span>- 一般的なオープンウェイトの事前学習されたLLMのレイヤー剪定戦略を研究し、異なる質問応答ベンチマークでのパフォーマンスの低下を最小限に抑えることを示しました。レイヤーの最大半分を削除することで、最適なブロックを特定し、微調整して損傷を修復します。PEFT手法を使用し、実験を単一のA100 GPUで実行可能にします。これにより、計算リソースを削減し、推論のメモリとレイテンシを改善できることが示唆されます。また、LLMがレイヤーの削除に対して堅牢であることは、浅いレイヤーが知識を格納する上で重要な役割を果たしている可能性を示唆しています。</span>
<span class="snippet"><span>Comment</span><p>下記ツイートによると、学習済みLLMから、コサイン類似度で入出力間の類似度が高い層を除いてもタスクの精度が落ちず、特に深い層を2-4割削除しても精度が落ちないとのこと。<br><br>参考:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1773110076502368642?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<br><br>VRAMに載せるのが大変なので、このような枝刈り技術が有効だと分かるのはありがたい。LoRAや量子化も利用しているっぽい。</span><br><br>
</article>
<article class="paper-entry">
<h3 id="mixture-of-depths-dynamically-1273" class="title-link">Mixture-of-Depths: Dynamically allocating compute in transformer-based  language models, David Raposo+, N_A, arXiv'24</h3>
<br><a href="https://arxiv.org/abs/2404.02258" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1273" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<span class="snippet"><span>GPT Summary</span>- Transformerベースの言語モデルは、入力シーケンス全体に均等にFLOPsを分散させる代わりに、特定の位置にFLOPsを動的に割り当てることを学習できることを示す。モデルの深さにわたって割り当てを最適化するために、異なるレイヤーで計算を動的に割り当てる。この手法は、トークンの数を制限することで合計計算予算を強制し、トークンはtop-kルーティングメカニズムを使用して決定される。この方法により、FLOPsを均等に消費しつつ、計算の支出が予測可能であり、動的かつコンテキストに敏感である。このようにトレーニングされたモデルは、計算を動的に割り当てることを学習し、効率的に行うことができる。</span>
<span class="snippet"><span>Comment</span><p>参考: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theseamouse/status/1775782800362242157?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="dynamic-memory-1270" class="title-link">Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference, Piotr Nawrot+, N_A, arXiv'24</h3>
<br><a href="https://arxiv.org/abs/2403.09636" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1270" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<span class="snippet"><span>GPT Summary</span>- トランスフォーマーの生成効率を向上させるために、Dynamic Memory Compression（DMC）が提案された。DMCは、異なるヘッドとレイヤーで異なる圧縮率を適用する方法を学習し、事前学習済みLLMsに適用される。DMCは、元の下流パフォーマンスを最大4倍のキャッシュ圧縮で維持しつつ、スループットを向上させることができる。DMCは、GQAと組み合わせることでさらなる利益をもたらす可能性があり、長いコンテキストと大きなバッチを処理する際に有用である。</span>
<span class="snippet"><span>Comment</span><p>参考: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1776755029581676943?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>論文中のFigure1が非常にわかりやすい。<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/d416547e-f9ca-4c6c-8ebb-7d164bef5283" alt="image" loading="lazy" width="400" height="300"><br><br></p>
<p>GQA <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1271" target="_blank" rel="noopener noreferrer">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head
  Checkpoints, Joshua Ainslie+, N/A, arXiv'23</a>
 と比較して、2~4倍キャッシュを圧縮しつつ、より高い性能を実現。70Bモデルの場合は、GQAで8倍キャッシュを圧縮した上で、DMCで追加で2倍圧縮をかけたところ、同等のパフォーマンスを実現している。<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/7b131f07-5eab-4830-88cc-5f6fd0508958" alt="image" loading="lazy" width="400" height="300"><br><br></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="lora+-efficient-1245" class="title-link">LoRA+: Efficient Low Rank Adaptation of Large Models, Soufiane Hayou+, N_A, ICML'24</h3>
<br><a href="https://arxiv.org/abs/2402.12354" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1245" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="PEFT(Adaptor_LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<span class="issue_date">Issue Date: 2024-03-05</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、Huら（2021）によって導入されたLow Rank Adaptation（LoRA）が、大埋め込み次元を持つモデルの適切な微調整を妨げることを指摘します。この問題は、LoRAのアダプターマトリックスAとBが同じ学習率で更新されることに起因します。我々は、AとBに同じ学習率を使用することが効率的な特徴学習を妨げることを示し、異なる学習率を設定することでこの問題を修正できることを示します。修正されたアルゴリズムをLoRA$+$と呼び、幅広い実験により、LoRA$+$は性能を向上させ、微調整速度を最大2倍高速化することが示されました。</span>
<span class="snippet"><span>Comment</span><p>LoRAで導入される低ランク行列AとBを異なる学習率で学習することで、LoRAと同じ計算コストで、2倍以上の高速化、かつ高いパフォーマンスを実現する手法<br><br><img src="https://github.com/user-attachments/assets/cde925fa-bfe8-4385-ae55-d80f7bf034f5" alt="image" loading="lazy" width="400" height="300"><br><img src="https://github.com/user-attachments/assets/c054c5a6-56a2-4aa5-b7f2-0ae87a808f58" alt="image" loading="lazy" width="400" height="300"><br><img src="https://github.com/user-attachments/assets/f32a7aba-e4b1-4d28-920d-00f81e9b85e8" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="efficient-memory-2474" class="title-link">[Paper Note] Efficient Memory Management for Large Language Model Serving with  PagedAttention, Woosuk Kwon+, SOSP'23</h3>
<br><a href="https://arxiv.org/abs/2309.06180" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2474" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="python.html" target="_blank" rel="noopener noreferrer">#python</a>
<a class="button" href="LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-08-19</span>
<span class="snippet"><span>GPT Summary</span>- PagedAttentionを用いたvLLMシステムを提案し、KVキャッシュメモリの無駄を削減し、リクエスト間での柔軟な共有を実現。これにより、同レベルのレイテンシでLLMのスループットを2-4倍向上。特に長いシーケンスや大規模モデルで効果が顕著。ソースコードは公開中。</span>
<span class="snippet"><span>Comment</span><p>（今更ながら）vLLMはこちら:<br>


<a href="https://github.com/vllm-project/vllm" target="_blank" rel="noopener noreferrer">https://github.com/vllm-project/vllm</a>


<br><br>現在の主要なLLM Inference/Serving Engineのひとつ。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="sarathi-efficient-2028" class="title-link">[Paper Note] SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked  Prefills, Amey Agrawal+, arXiv'23</h3>
<br><a href="https://arxiv.org/abs/2308.16369" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2028" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="python.html" target="_blank" rel="noopener noreferrer">#python</a>
<a class="button" href="LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Inference.html" target="_blank" rel="noopener noreferrer">#Inference</a>
<span class="issue_date">Issue Date: 2025-06-12</span>
<span class="snippet"><span>GPT Summary</span>- SARATHIは、LLMの推論効率を向上させる手法で、プレフィルリクエストをチャンクに分割し、デコードマキシマルバッチを構築することで計算利用率を最大化します。これにより、デコードスループットを最大10倍向上させ、エンドツーエンドスループットも改善。特に、A6000 GPU上のLLaMA-13Bモデルで顕著な性能向上を示し、パイプラインバブルを大幅に削減しました。</span>
<span class="snippet"><span>Comment</span><p>vLLMでも採用されている `Chunked Prefills` と `Decode-Maximal Batching` を提案している。<br>![Image](https://github.com/user-attachments/assets/4db0f73d-bdf4-4c2b-a765-2c9b242904f1)</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="sequence-parallelism-1970" class="title-link">Sequence Parallelism: Long Sequence Training from System Perspective, Li+, ACL'23</h3>
<br><a href="https://aclanthology.org/2023.acl-long.134/" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1970" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="Parallelism.html" target="_blank" rel="noopener noreferrer">#Parallelism</a>
<span class="issue_date">Issue Date: 2025-05-16</span>
<span class="snippet"><span>Comment</span><p>入力系列をチャンクに分割して、デバイスごとに担当するチャンクを決めることで原理上無限の長さの系列を扱えるようにした並列化手法。系列をデバイス間で横断する場合attention scoreをどのように計算するかが課題になるが、そのためにRing Self attentionと呼ばれるアルゴリズムを提案している模様。また、MLPブロックとMulti Head Attentonブロックの計算も、BatchSize * Sequence Lengthの大きさが、それぞれ32*Hidden Size, 16*Attention Head size * 
<strong># of Attention Headよりも大きくなった場合に、Tensor Parallelismよりもメモリ効率が良くなるらしい。<br><img src="https://github.com/user-attachments/assets/f3ba9010-da3a-4c3a-8515-d3715466ff59" alt="image" loading="lazy" width="400" height="300"></strong></p>
<p>Data Parallel, Pipeline Parallel, Tensor Parallel、全てに互換性があるとのこと（併用可能）</p>
<p>そのほかの並列化の解説については<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1184" target="_blank" rel="noopener noreferrer">大規模モデルを支える分散並列学習のしくみ Part1</a>

<br>
<br><br>を参照のこと。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="the-impact-1865" class="title-link">The Impact of Positional Encoding on Length Generalization in   Transformers, Amirhossein Kazemnejad+, NeurIPS'23</h3>
<br><a href="https://arxiv.org/pdf/2305.19466" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1865" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="PositionalEncoding.html" target="_blank" rel="noopener noreferrer">#PositionalEncoding</a>
<a class="button" href="NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<a class="button" href="Surface-level%20Note.html" target="_blank" rel="noopener noreferrer">#Surface-level Note</a>
<span class="issue_date">Issue Date: 2025-04-06</span>
<span class="snippet"><span>GPT Summary</span>- 長さ一般化はTransformerベースの言語モデルにおける重要な課題であり、位置エンコーディング（PE）がその性能に影響を与える。5つの異なるPE手法（APE、T5の相対PE、ALiBi、Rotary、NoPE）を比較した結果、ALiBiやRotaryなどの一般的な手法は長さ一般化に適しておらず、NoPEが他の手法を上回ることが明らかになった。NoPEは追加の計算を必要とせず、絶対PEと相対PEの両方を表現可能である。さらに、スクラッチパッドの形式がモデルの性能に影響を与えることも示された。この研究は、明示的な位置埋め込みが長いシーケンスへの一般化に必須でないことを示唆している。</span>
<span class="snippet"><span>Comment</span><p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1863" target="_blank" rel="noopener noreferrer">Llama 4 Series, Meta, 2025.04</a>
<br><br>において、Llama4 Scoutが10Mコンテキストウィンドウを実現できる理由の一つとのこと。<br><br>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/drjimfan/status/1908615861650547081?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<br><br>Llama4のブログポストにもその旨記述されている:<br>&gt;A key innovation in the Llama 4 architecture is the use of interleaved attention layers without positional embeddings. Additionally, we employ inference time temperature scaling of attention to enhance length generalization.<br><br>[The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation](


<a href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/?utm_source=twitter&utm_medium=organic_social&utm_content=image&utm_campaign=llama4)" target="_blank" rel="noopener noreferrer">https://ai.meta.com/blog/llama-4-multimodal-intelligence/?utm_source=twitter&utm_medium=organic_social&utm_content=image&utm_campaign=llama4)</a>


<p>斜め読みだが、length generalizationを評価する上でdownstream taskに焦点を当て、3つの代表的なカテゴリに相当するタスクで評価したところ、この観点においてはT5のrelative positinal encodingとNoPE（位置エンコードディング無し）のパフォーマンスが良く、<br><br><img src="https://github.com/user-attachments/assets/dddadfff-ab28-4073-96c3-831eb16845a0" alt="image" loading="lazy" width="400" height="300"><br><img src="https://github.com/user-attachments/assets/c6ec8e0e-7abb-4330-be23-2261486a477c" alt="image" loading="lazy" width="400" height="300"><br><br>NoPEは絶対位置エンコーディングと相対位置エンコーディングを理論上実現可能であり[^1]<br><img src="https://github.com/user-attachments/assets/bbcf797a-d394-42d4-b017-08d7dba4261c" alt="image" loading="lazy" width="400" height="300"><br><br>実際に学習された異なる2つのモデルに対して同じトークンをそれぞれinputし、同じ深さのLayerの全てのattention distributionの組み合わせからJensen Shannon Divergenceで距離を算出し、最も小さいものを2モデル間の当該layerの距離として可視化すると下記のようになり、NoPEとT5のrelative positional encodingが最も類似していることから、NoPEが学習を通じて（実用上は）相対位置エンコーディングのようなものを学習することが分かった。<br><img src="https://github.com/user-attachments/assets/9619c7e5-0612-45de-8717-1634bee509b7" alt="image" loading="lazy" width="400" height="300"><br><br>[^1]:深さ1のLayerのHidden State H^1から絶対位置の復元が可能であり（つまり、当該レイヤーのHが絶対位置に関する情報を保持している）、この前提のもと、後続のLayerがこの情報を上書きしないと仮定した場合に、相対位置エンコーディングを実現できる。</p>
<p>また、CoT/Scratchpadはlong sequenceに対する汎化性能を向上させることがsmall scaleではあるが先行研究で示されており、Positional Encodingを変化させた時にCoT/Scratchpadの性能にどのような影響を与えるかを調査。<br><br>具体的には、CoT/Scratchpadのフォーマットがどのようなものが有効かも明らかではないので、5種類のコンポーネントの組み合わせでフォーマットを構成し、mathematical reasoningタスクで以下のような設定で訓練し<br><br>- さまざまなコンポーネントの組み合わせで異なるフォーマットを作成し、<br>- 全ての位置エンコーディングあり/なしモデルを訓練<br><br>これらを比較した。この結果、CoT/Scratchpadはフォーマットに関係なく、特定のタスクでのみ有効（有効かどうかはタスク依存）であることが分かった。このことから、CoT/Scratcpad（つまり、モデルのinputとoutputの仕方）単体で、long contextに対する汎化性能を向上させることができないので、Positional Encoding（≒モデルのアーキテクチャ）によるlong contextに対する汎化性能の向上が非常に重要であることが浮き彫りになった。<br><img src="https://github.com/user-attachments/assets/e23c4fbf-84de-4344-a01e-1e7e9e66fa7e" alt="image" loading="lazy" width="400" height="300"><br><br>また、CoT/Scratchpadが有効だったAdditionに対して各Positional Embeddingモデルを学習し、生成されたトークンのattentionがどの位置のトークンを指しているかを相対距離で可視化したところ（0が当該トークン、つまり現在のScratchpadに着目しており、1が遠いトークン、つまりinputに着目していることを表すように正規化）、NoPEとRelative Positional Encodingがshort/long rangeにそれぞれフォーカスするようなbinomialな分布なのに対し、他のPositional Encodingではよりuniformな分布であることが分かった。このタスクにおいてはNoPEとRelative POの性能が高かったため、binomialな分布の方がより最適であろうことが示唆された。<br><img src="https://github.com/user-attachments/assets/833e6a81-8611-4e79-9d2e-473f7ebee2d0" alt="image" loading="lazy" width="400" height="300"><br></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="loftq-lora-fine-tuning-aware-1407" class="title-link">LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models, Yixiao Li+, N_A, arXiv'23</h3>
<br><a href="https://arxiv.org/abs/2310.08659" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1407" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="Quantization.html" target="_blank" rel="noopener noreferrer">#Quantization</a>
<a class="button" href="PEFT(Adaptor_LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<span class="issue_date">Issue Date: 2024-09-24</span>
<span class="snippet"><span>GPT Summary</span>- LoftQという新しい量子化フレームワークを提案し、LLMにおける量子化とLoRAファインチューニングを同時に適用。これにより、量子化モデルとフル精度モデルの不一致を軽減し、下流タスクの一般化を改善。自然言語理解や質問応答などのタスクで、特に難易度の高い条件下で既存手法を上回る性能を示した。</span>
</article>
<article class="paper-entry">
<h3 id="gqa-training-1271" class="title-link">GQA: Training Generalized Multi-Query Transformer Models from Multi-Head  Checkpoints, Joshua Ainslie+, N_A, arXiv'23</h3>
<br><a href="https://arxiv.org/abs/2305.13245" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1271" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<span class="snippet"><span>GPT Summary</span>- Multi-query attention（MQA）は、単一のkey-value headのみを使用しており、デコーダーの推論を劇的に高速化しています。ただし、MQAは品質の低下を引き起こす可能性があり、さらには、より速い推論のためだけに別個のモデルをトレーニングすることが望ましくない場合もあります。既存のマルチヘッド言語モデルのチェックポイントを、オリジナルの事前トレーニング計量の5%を使用してMQAを持つモデルにアップトレーニングするためのレシピを提案し、さらに、複数のkey-value headを使用するマルチクエリアテンションの一般化であるグループ化クエリアテンション（GQA）を紹介します。アップトレーニングされたGQAが、MQAと同等の速度でマルチヘッドアテンションに匹敵する品質を達成することを示しています。</span>
<span class="snippet"><span>Comment</span><p>通常のMulti-Head AttentionがQKVが1対1対応なのに対し、Multi Query Attention (MQA) <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1272" target="_blank" rel="noopener noreferrer">Fast Transformer Decoding: One Write-Head is All You Need, Noam Shazeer, N/A, arXiv'19</a>
  は全てのQに対してKVを共有する。一方、GQAはグループごとにKVを共有する点で異なる。MQAは大幅にInfeerence` speedが改善するが、精度が劣化する問題があった。この研究では通常のMulti-Head Attentionに対して、オリジナルの事前学習に対して追加の5%の計算量でGQAモデルを学習する手法を提案している。<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/70ec2179-428c-47b8-af53-cb3cc0e4f022" alt="image" loading="lazy" width="400" height="300"><br><br></p>
<p>Main Result. Multi-Head Attentionに対して、inference timeが大幅に改善しているが、Multi-Query Attentionよりも高い性能を維持している。<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/3687aeb4-90b8-403d-853b-740121dd5f98" alt="image" loading="lazy" width="400" height="300"><br><br></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="vera-vector-based-1211" class="title-link">VeRA: Vector-based Random Matrix Adaptation, Dawid J. Kopiczko+, N_A, arXiv'23</h3>
<br><a href="https://arxiv.org/abs/2310.11454" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1211" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="PEFT(Adaptor_LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<span class="issue_date">Issue Date: 2024-01-17</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、大規模な言語モデルのfine-tuningにおいて、訓練可能なパラメータの数を削減するための新しい手法であるベクトルベースのランダム行列適応（VeRA）を提案する。VeRAは、共有される低ランク行列と小さなスケーリングベクトルを使用することで、同じ性能を維持しながらパラメータ数を削減する。GLUEやE2Eのベンチマーク、画像分類タスクでの効果を示し、言語モデルのインストラクションチューニングにも応用できることを示す。</span>
</article>
<article class="paper-entry">
<h3 id="exponentially-faster-1163" class="title-link">Exponentially Faster Language Modelling, Peter Belcak+, N_A, arXiv'23</h3>
<br><a href="https://arxiv.org/abs/2311.10770" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1163" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-11-23</span>
<span class="snippet"><span>GPT Summary</span>- UltraFastBERTは、推論時にわずか0.3%のニューロンしか使用せず、同等の性能を発揮することができる言語モデルです。UltraFastBERTは、高速フィードフォワードネットワーク（FFF）を使用して、効率的な実装を提供します。最適化されたベースラインの実装に比べて78倍の高速化を実現し、バッチ処理された推論に対しては40倍の高速化を実現します。トレーニングコード、ベンチマークのセットアップ、およびモデルの重みも公開されています。</span>
</article>
<article class="paper-entry">
<h3 id="fast-chain-of-thought-1135" class="title-link">Fast Chain-of-Thought: A Glance of Future from Parallel Decoding Leads  to Answers Faster, Hongxuan Zhang+, N_A, arXiv'23</h3>
<br><a href="https://arxiv.org/abs/2311.08263" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1135" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Chain-of-Thought.html" target="_blank" rel="noopener noreferrer">#Chain-of-Thought</a>
<a class="button" href="Prompting.html" target="_blank" rel="noopener noreferrer">#Prompting</a>
<span class="issue_date">Issue Date: 2023-11-15</span>
<span class="snippet"><span>GPT Summary</span>- この研究では、FastCoTというフレームワークを提案します。FastCoTは、LLMを使用して並列デコーディングと自己回帰デコーディングを同時に行い、計算リソースを最大限に活用します。また、FastCoTは推論時間を約20%節約し、性能の低下がほとんどないことを実験で示しました。さらに、異なるサイズのコンテキストウィンドウに対しても頑健性を示すことができました。</span>
<span class="snippet"><span>Comment</span><p>論文中の図を見たが、全くわからなかった・・・。ちゃんと読まないとわからなそうである。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="longlora-efficient-1045" class="title-link">LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models, Yukang Chen+, N_A, arXiv'23</h3>
<br><a href="https://arxiv.org/abs/2309.12307" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1045" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Dataset.html" target="_blank" rel="noopener noreferrer">#Dataset</a>
<a class="button" href="QuestionAnswering.html" target="_blank" rel="noopener noreferrer">#QuestionAnswering</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="PEFT(Adaptor_LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<span class="issue_date">Issue Date: 2023-09-30</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、計算コストを制限しながら大規模言語モデル（LLMs）のコンテキストサイズを拡張する効率的なファインチューニング手法であるLongLoRAを提案します。従来の方法では、LLMsの長いコンテキストサイズでのトレーニングには高い計算コストとGPUリソースが必要でしたが、提案手法ではコンテキスト拡張を高速化し、非自明な計算コストの削減を実現します。また、パラメータ効率的なファインチューニング手法も再評価し、LongLoRAはさまざまなタスクで強力な実験結果を示しています。さらに、教師ありファインチューニングのためのデータセットであるLongQAも収集されました。</span>
<span class="snippet"><span>Comment</span><p># 概要<br><br>context長が大きい場合でも効率的にLoRAする手法。通常のLoRAではcontext lengthが大きくなるにつれてperplexityが大きくなってしまう。一方、通常のFinetuningではperplexityは高い性能を維持するが、計算コストとVRAMの消費量が膨大になってしまう。LongLoRAでは、perplexityを通常のFinetuningと同等に抑えつつ、VRAM消費量もLoRAと同等、かつより小さな計算量でFinetuningを実現している。<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/fc3d17c7-b1ac-4741-9895-bce70cf0b356" alt="image" loading="lazy" width="400" height="300"><br><br><br><br># 手法概要<br><br>attentionをcontext length全体で計算するとinput長の二乗の計算量がかかるため、contextをいくつかのグループに分割しグループごとにattentionを計算することで計算量削減。さらに、グループ間のattentionの間の依存関係を捉えるために、グループをshiftさせて計算したものと最終的に組み合わせている。また、embedding, normalization layerもtrainableにしている。<br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/2b443a4c-73da-4610-8ee2-cccdeab21efa" alt="image" loading="lazy" width="400" height="300"><br><br></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="textbooks-are-1039" class="title-link">Textbooks Are All You Need II: phi-1.5 technical report, Yuanzhi Li+, N_A, arXiv'23</h3>
<br><a href="https://arxiv.org/abs/2309.05463" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1039" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2023-09-13</span>
<span class="snippet"><span>GPT Summary</span>- 私たちは、小さなTransformerベースの言語モデルであるTinyStoriesと、大規模な言語モデルであるphi-1の能力について調査しました。また、phi-1を使用して教科書の品質のデータを生成し、学習プロセスを改善する方法を提案しました。さらに、phi-1.5という新しいモデルを作成し、自然言語のタスクにおいて性能が向上し、複雑な推論タスクにおいて他のモデルを上回ることを示しました。phi-1.5は、良い特性と悪い特性を持っており、オープンソース化されています。</span>
<span class="snippet"><span>Comment</span><p><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/766" target="_blank" rel="noopener noreferrer">Textbooks Are All You Need, Suriya Gunasekar+, N/A, arXiv'23</a>
 に続く論文</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="skeleton-of-thought-large-921" class="title-link">Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding, Xuefei Ning+, N_A, arXiv'23</h3>
<br><a href="https://arxiv.org/abs/2307.15337" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/921" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-08-08</span>
<span class="snippet"><span>GPT Summary</span>- この研究では、大規模言語モデル（LLMs）の生成遅延を減らすために、思考の骨組み（SoT）という手法を提案しています。SoTは、回答の骨組みをまず生成し、その後に内容を並列で処理することで高速化を実現します。また、回答品質の向上も期待されます。SoTはデータ中心の最適化の初めの試みであり、LLMsの人間らしい思考を可能にする可能性があります。</span>
<span class="snippet"><span>Comment</span><p>最初に回答の枠組みだけ生成して、それぞれの内容を並列で出力させることでデコーディングを高速化しましょう、という話。<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/fb25d8ba-dff7-4f6f-be25-0973488f6e8a" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="frugalgpt-how-905" class="title-link">FrugalGPT: How to Use Large Language Models While Reducing Cost and  Improving Performance, Lingjiao Chen+, N_A, arXiv'23</h3>
<br><a href="https://arxiv.org/abs/2305.05176" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/905" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-07-26</span>
<span class="snippet"><span>GPT Summary</span>- 大規模言語モデル（LLMs）の使用には高いコストがかかるため、LLMsの推論コストを削減するための3つの戦略（プロンプトの適応、LLMの近似、LLMのカスケード）を提案する。FrugalGPTという具体的な手法を紹介し、最大98％のコスト削減と4％の精度向上を実現することを示す。これにより、LLMsの持続可能な使用が可能となる。</span>
<span class="snippet"><span>Comment</span><p>限られた予算の中で、いかに複数のLLM APIを使い、安いコストで高い性能を達成するかを追求した研究。<br><br>LLM Cascadeなどはこの枠組みでなくても色々と使い道がありそう。Question Concatenationは実質Batch Prompting。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="batch-prompting-900" class="title-link">Batch Prompting: Efficient Inference with Large Language Model APIs, Zhoujun Cheng+, N_A, arXiv'23</h3>
<br><a href="https://arxiv.org/abs/2301.08721" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/900" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="Prompting.html" target="_blank" rel="noopener noreferrer">#Prompting</a>
<span class="issue_date">Issue Date: 2023-07-24</span>
<span class="snippet"><span>GPT Summary</span>- 大規模な言語モデル（LLMs）を効果的に使用するために、バッチプロンプティングという手法を提案します。この手法は、LLMが1つのサンプルではなくバッチで推論を行うことを可能にし、トークンコストと時間コストを削減しながらパフォーマンスを維持します。さまざまなデータセットでの実験により、バッチプロンプティングがLLMの推論コストを大幅に削減し、良好なパフォーマンスを達成することが示されました。また、バッチプロンプティングは異なる推論方法にも適用できます。詳細はGitHubのリポジトリで確認できます。</span>
<span class="snippet"><span>Comment</span><p><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/16aaed9b-da2b-4c38-86df-e223bdbec826" alt="image" loading="lazy" width="400" height="300"><br><br>10種類のデータセットで試した結果、バッチにしても性能は上がったり下がったりしている。著者らは類似した性能が出ているので、コスト削減になると結論づけている。</p>
<p>Batch sizeが大きくなるに連れて性能が低下し、かつタスクの難易度が高いとパフォーマンスの低下が著しいことが報告されている。また、contextが長ければ長いほど、バッチサイズを大きくした際のパフォーマンスの低下が著しい。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="qlora-efficient-881" class="title-link">QLoRA: Efficient Finetuning of Quantized LLMs, Tim Dettmers+, N_A, NeurIPS'23</h3>
<br><a href="https://arxiv.org/abs/2305.14314" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/881" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="Quantization.html" target="_blank" rel="noopener noreferrer">#Quantization</a>
<a class="button" href="PEFT(Adaptor_LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2023-07-22</span>
<span class="snippet"><span>GPT Summary</span>- 私たちは、QLoRAという効率的なファインチューニング手法を提案します。この手法は、メモリ使用量を削減し、48GBの単一のGPU上で65Bパラメータモデルをファインチューニングすることができます。また、16ビットのファインチューニングタスクのパフォーマンスを維持します。QLoRAは、凍結された4ビット量子化された事前学習済み言語モデルの勾配をLow Rank Adapters（LoRA）に逆伝播させます。私たちの最良のモデルファミリーであるGuanacoは、Vicunaベンチマークで以前に公開されたすべてのモデルを上回り、ChatGPTのパフォーマンスレベルの99.3%に達します。また、単一のGPU上でのファインチューニングには24時間しかかかりません。QLoRAは、パフォーマンスを犠牲にすることなくメモリを節約するためのいくつかの革新を導入しています。具体的には、4ビットNormalFloat（NF4）という情報理論的に最適な新しいデータ型、ダブル量子化による平均メモリフットプリントの削減、およびページドオプティマイザによるメモリスパイクの管理です。私たちはQLoRAを使用して1,000以上のモデルをファインチューニングし、8つの命令データセット、複数のモデルタイプ（LLaMA、T5）、および従来のファインチューニングでは実行不可能なモデルスケール（33Bおよび65Bパラメータモデル）にわたる命令の追跡とチャットボットのパフォーマンスの詳細な分析を提供します。私たちの結果は、QLoRAを使用して小規模な高品質のデータセットでのファインチューニングが、以前のSoTAよりも小さいモデルを使用しても最先端の結果をもたらすことを示しています。また、人間の評価とGPT-4の評価に基づいたチャットボットのパフォーマンスの詳細な分析を提供し、GPT-4の評価が安価で合理的な人間の評価の代替手段であることを示します。さらに、現在のチャットボットのベンチマークは、チャットボットのパフォーマンスレベルを正確に評価するためには信頼性がないことがわかります。GuanacoがChatGPTと比較してどこで失敗するかを示す分析も行っています。私たちは、4ビットトレーニングのためのCUDAカーネルを含む、すべてのモデルとコードを公開しています。</span>
<span class="snippet"><span>Comment</span><p>実装: 


<a href="https://github.com/artidoro/qlora" target="_blank" rel="noopener noreferrer">https://github.com/artidoro/qlora</a>


<br>PEFTにもある</p>
<p>参考: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1662946722690236417?s=46&t=TDHYK31QiXKxggPzhZbcAQ"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>OpenReview:


<a href="https://openreview.net/forum?id=OUIFPHEgJU&referrer=%5Bthe%20profile%20of%20Ari%20Holtzman%5D(%2Fprofile%3Fid%3D~Ari_Holtzman1)" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=OUIFPHEgJU&referrer=%5Bthe%20profile%20of%20Ari%20Holtzman%5D(%2Fprofile%3Fid%3D~Ari_Holtzman1)</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="pad-net-an-856" class="title-link">PAD-Net: An Efficient Framework for Dynamic Networks, ACL'23</h3>
<br><a href="https://virtual2023.aclweb.org/paper_P1843.html" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/856" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="DynamicNetworks.html" target="_blank" rel="noopener noreferrer">#DynamicNetworks</a>
<a class="button" href="Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<span class="issue_date">Issue Date: 2023-07-18</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、ダイナミックネットワークの一般的な問題点を解決するために、部分的にダイナミックなネットワーク（PAD-Net）を提案します。PAD-Netは、冗長なダイナミックパラメータを静的なパラメータに変換することで、展開コストを削減し、効率的なネットワークを実現します。実験結果では、PAD-Netが画像分類と言語理解のタスクで高い性能を示し、従来のダイナミックネットワークを上回ることを示しました。</span>
</article>
<article class="paper-entry">
<h3 id="parameter-efficient-weight-822" class="title-link">Parameter-efficient Weight Ensembling Facilitates Task-level Knowledge Transfer, ACL'23</h3>
<br><a href="https://virtual2023.aclweb.org/paper_P2877.html" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/822" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Ensemble.html" target="_blank" rel="noopener noreferrer">#Ensemble</a>
<a class="button" href="TransferLearning.html" target="_blank" rel="noopener noreferrer">#TransferLearning</a>
<span class="issue_date">Issue Date: 2023-07-14</span>
<span class="snippet"><span>GPT Summary</span>- 最近の研究では、大規模な事前学習済み言語モデルを特定のタスクに効果的に適応させることができることが示されています。本研究では、軽量なパラメータセットを使用してタスク間で知識を転送する方法を探求し、その有効性を検証しました。実験結果は、提案手法がベースラインに比べて5％〜8％の改善を示し、タスクレベルの知識転送を大幅に促進できることを示しています。</span>
</article>
<article class="paper-entry">
<h3 id="fid-icl-a-817" class="title-link">FiD-ICL: A Fusion-in-Decoder Approach for Efficient In-Context Learning, ACL'23</h3>
<br><a href="https://virtual2023.aclweb.org/paper_P4254.html" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/817" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Zero_Few_ManyShotPrompting.html" target="_blank" rel="noopener noreferrer">#Zero/Few/ManyShotPrompting</a>
<a class="button" href="In-ContextLearning.html" target="_blank" rel="noopener noreferrer">#In-ContextLearning</a>
<span class="issue_date">Issue Date: 2023-07-13</span>
<span class="snippet"><span>GPT Summary</span>- 大規模な事前学習モデルを使用したfew-shot in-context learning（ICL）において、fusion-in-decoder（FiD）モデルを適用することで効率とパフォーマンスを向上させることができることを検証する。FiD-ICLは他のフュージョン手法と比較して優れたパフォーマンスを示し、推論時間も10倍速くなる。また、FiD-ICLは大規模なメタトレーニングモデルのスケーリングも可能にする。</span>
</article>
<article class="paper-entry">
<h3 id="full-parameter-769" class="title-link">Full Parameter Fine-tuning for Large Language Models with Limited  Resources, Kai Lv+, N_A, arXiv'23</h3>
<br><a href="https://arxiv.org/abs/2306.09782" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/769" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<span class="issue_date">Issue Date: 2023-06-26</span>
<span class="snippet"><span>GPT Summary</span>- LLMsのトレーニングには膨大なGPUリソースが必要であり、既存のアプローチは限られたリソースでの全パラメーターの調整に対処していない。本研究では、LOMOという新しい最適化手法を提案し、メモリ使用量を削減することで、8つのRTX 3090を搭載した単一のマシンで65Bモデルの全パラメーターファインチューニングが可能になる。</span>
<span class="snippet"><span>Comment</span><p>8xRTX3090 24GBのマシンで65Bモデルの全パラメータをファインチューニングできる手法。LoRAのような（新たに追加しれた）一部の重みをアップデートするような枠組みではない。勾配計算とパラメータのアップデートをone stepで実施することで実現しているとのこと。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="textbooks-are-766" class="title-link">Textbooks Are All You Need, Suriya Gunasekar+, N_A, arXiv'23</h3>
<br><a href="https://arxiv.org/abs/2306.11644" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/766" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2023-06-25</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、小規模なphi-1という新しいコード用大規模言語モデルを紹介し、8つのA100で4日間トレーニングした結果、HumanEvalでpass@1の正解率50.6％、MBPPで55.5％を達成したことを報告しています。また、phi-1は、phi-1-baseやphi-1-smallと比較して、驚くべき新しい性質を示しています。phi-1-smallは、HumanEvalで45％を達成しています。</span>
<span class="snippet"><span>Comment</span><p>参考: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hillbig/status/1671643297616654342?s=46&t=JYDYid2m0v7vYaL7jhZYjQ"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>教科書のような品質の良いテキストで事前学習すると性能が向上し（グラフ真ん中）、さらに良質なエクササイズでFinetuningするとより性能が向上する（グラフ右）<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/9f0b945a-f965-42ae-b5d8-ac464359af35" alt="image" loading="lazy" width="400" height="300"></p>
<p>日本語解説: 


<a href="https://dalab.jp/archives/journal/introduction-textbooks-are-all-you-need/" target="_blank" rel="noopener noreferrer">https://dalab.jp/archives/journal/introduction-textbooks-are-all-you-need/</a>


</p>
<p>ざっくり言うと、教科書で事前学習し、エクササイズでFinetuningすると性能が向上する（= より大きいモデルと同等の性能が得られる）。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="megabyte-predicting-682" class="title-link">[Paper Note] MEGABYTE: Predicting Million-byte Sequences with Multiscale Transformers, Lili Yu+, NeurIPS'23, 2023.05</h3>
<br><a href="https://arxiv.org/abs/2305.07185" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/682" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="SpeechProcessing.html" target="_blank" rel="noopener noreferrer">#SpeechProcessing</a>
<a class="button" href="LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="Byte-level.html" target="_blank" rel="noopener noreferrer">#Byte-level</a>
<span class="issue_date">Issue Date: 2023-05-15</span>
<span class="snippet"><span>GPT Summary</span>- Megabyteというマルチスケールデコーダーアーキテクチャを提案し、長いシーケンスのエンドツーエンドのモデリングを可能にする。シーケンスをパッチに分割し、ローカルサブモデルとグローバルモデルを使用することで、計算効率を向上させつつコストを削減。実験により、Megabyteは長いコンテキストの言語モデリングで競争力を持ち、最先端の密度推定を達成した。トークン化なしの自己回帰シーケンスモデリングの実現可能性を示す。</span>
<span class="snippet"><span>Comment</span><p>byte列のsequenceからpatch embeddingを作成することで、tokenizer freeなtransformerを提案。<br>byte列で表現されるデータならなんでも入力できる。つまり、理論上なんでも入力できる。</p>
<p>openreview: 


<a href="https://openreview.net/forum?id=JTmO2V9Xpz" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=JTmO2V9Xpz</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="efficiently-scaling-601" class="title-link">Efficiently Scaling Transformer Inference, Reiner Pope+, N_A, MLSys'23</h3>
<br><a href="https://arxiv.org/abs/2211.05102" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/601" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="Inference.html" target="_blank" rel="noopener noreferrer">#Inference</a>
<span class="issue_date">Issue Date: 2023-04-30</span>
<span class="snippet"><span>GPT Summary</span>- - 大規模Transformerベースのモデルの推論のエンジニアリングのトレードオフを理解するために、最適な多次元分割技術を選択するための単純な解析モデルを開発- 低レベルの最適化と組み合わせることで、500B+パラメータモデルのレイテンシーとモデルFLOPS利用率のトレードオフにおいて、FasterTransformerベンチマークスイートを上回る新しいParetoフロンティアを実現- 適切な分割により、マルチクエリアテンションの低いメモリ要件により、32倍の大きなコンテキスト長にスケーリング可能- int8ウェイト量子化を使用した生成中の低バッチサイズレイテンシーは、トークンあたり29msであり、入力トークンの大バッチサイズ処理において76％のMFUを実現し、PaLM 540Bパラメータモデルにおいて2048トークンの長いコンテキスト長をサポートしている。</span>
<span class="snippet"><span>Comment</span><p>特にMultiquery Attentionという技術がTransformerのinferenceのコスト削減に有効らしい</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="efficient-methods-525" class="title-link">Efficient Methods for Natural Language Processing: A Survey, Treviso+, TACL'23</h3>
<br><a href="https://arxiv.org/pdf/2209.00099.pdf" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/525" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="TACL.html" target="_blank" rel="noopener noreferrer">#TACL</a>
<span class="issue_date">Issue Date: 2023-04-25</span>
<span class="snippet"><span>GPT Summary</span>- NLPのパフォーマンス向上にはスケールの拡大が重要だが、リソース消費も増加する。限られたリソースで効率的にNLPを実施する方法を統合し、指針を提供。効率的な手法の開発に向けた研究方向を示唆。</span>
<span class="snippet"><span>Comment</span><p>パラメータ数でゴリ押すような方法ではなく、"Efficient"に行うための手法をまとめている<br><br><img src="https://user-images.githubusercontent.com/12249301/234287218-2d42766f-5c5c-4cf9-859e-c2b0a5dfd4c3.jpeg" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="bert2bert-towards-3927" class="title-link">[Paper Note] bert2BERT: Towards Reusable Pretrained Language Models, Cheng Chen+, ACL'22, 2021.10</h3>
<br><a href="https://arxiv.org/abs/2110.07143" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3927" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="Encoder.html" target="_blank" rel="noopener noreferrer">#Encoder</a>
<a class="button" href="Decoder.html" target="_blank" rel="noopener noreferrer">#Decoder</a>
<span class="issue_date">Issue Date: 2025-12-11</span>
<span class="snippet"><span>GPT Summary</span>- bert2BERTは、既存の小規模事前学習モデルの知識を大規模モデルに転送し、事前学習効率を向上させる手法。二段階の事前学習を提案し、トレーニングコストを大幅に削減。BERT_BASEとGPT_BASEの事前学習で約45%および47%の計算コストを節約。</span>
</article>
<article class="paper-entry">
<h3 id="efficient-transformers-3854" class="title-link">[Paper Note] Efficient Transformers: A Survey, Yi Tay+, ACM Computing Surveys'22, 2022.12</h3>
<br><a href="https://arxiv.org/abs/2009.06732" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3854" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Survey.html" target="_blank" rel="noopener noreferrer">#Survey</a>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="Sparse.html" target="_blank" rel="noopener noreferrer">#Sparse</a>
<a class="button" href="SparseAttention.html" target="_blank" rel="noopener noreferrer">#SparseAttention</a>
<span class="issue_date">Issue Date: 2025-11-30</span>
<span class="snippet"><span>GPT Summary</span>- 本論文では、計算効率やメモリ効率を向上させることに焦点を当てた「X-former」モデル（Reformer、Linformer、Performer、Longformerなど）の大規模なセレクションを紹介し、最近の研究を体系的かつ包括的にまとめる。Transformersは自然言語処理を含む多くの分野で重要な役割を果たしている。</span>
<span class="snippet"><span>Comment</span><p>関連:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3854" target="_blank" rel="noopener noreferrer">[Paper Note] Efficient Transformers: A Survey, Yi Tay+, ACM Computing Surveys'22, 2022.12</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3855" target="_blank" rel="noopener noreferrer">[Paper Note] Big Bird: Transformers for Longer Sequences, Manzil Zaheer+, NIPS'20, 2020.07</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2355" target="_blank" rel="noopener noreferrer">[Paper Note] Reformer: The Efficient Transformer, Nikita Kitaev+, ICLR'20</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3853" target="_blank" rel="noopener noreferrer">[Paper Note] Generating Long Sequences with Sparse Transformers, Rewon Child+, arXiv'19, 2019.04</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2388" target="_blank" rel="noopener noreferrer">[Paper Note] Longformer: The Long-Document Transformer, Iz Beltagy+, arXiv'20</a>
</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="stablemoe-stable-2644" class="title-link">[Paper Note] StableMoE: Stable Routing Strategy for Mixture of Experts, Damai Dai+, arXiv'22</h3>
<br><a href="https://arxiv.org/abs/2204.08396" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2644" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="Stability.html" target="_blank" rel="noopener noreferrer">#Stability</a>
<span class="issue_date">Issue Date: 2025-09-02</span>
<span class="snippet"><span>GPT Summary</span>- StableMoEは、ルーティングの変動問題に対処するために2つのトレーニングステージを持つMixture-of-Experts手法を提案。最初のステージで一貫したルーティング戦略を学習し、軽量ルーターに蒸留。第二のステージでそのルーターを用いてエキスパートへの割り当てを固定。言語モデリングと多言語機械翻訳での実験により、StableMoEは収束速度と性能で既存手法を上回ることが示された。</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/vikhyatk/status/1962225296314429543?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="switch-transformers-1754" class="title-link">Switch Transformers: Scaling to Trillion Parameter Models with Simple  and Efficient Sparsity, William Fedus+, JMLR'22</h3>
<br><a href="https://arxiv.org/abs/2101.03961" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1754" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-02-11</span>
<span class="snippet"><span>GPT Summary</span>- Switch Transformerを提案し、Mixture of Experts (MoE)の複雑さや通信コスト、トレーニングの不安定性を改善。これにより、低精度フォーマットでの大規模スパースモデルのトレーニングが可能になり、最大7倍の事前トレーニング速度向上を実現。さらに、1兆パラメータのモデルを事前トレーニングし、T5-XXLモデルに対して4倍の速度向上を達成。</span>
</article>
<article class="paper-entry">
<h3 id="few-shot-parameter-efficient-1000" class="title-link">Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than  In-Context Learning, Haokun Liu+, N_A, arXiv'22</h3>
<br><a href="https://arxiv.org/abs/2205.05638" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1000" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<span class="issue_date">Issue Date: 2023-08-16</span>
<span class="snippet"><span>GPT Summary</span>- Few-shot in-context learning（ICL）とパラメータ効率の良いファインチューニング（PEFT）を比較し、PEFTが高い精度と低い計算コストを提供することを示す。また、新しいPEFTメソッドである（IA）^3を紹介し、わずかな新しいパラメータしか導入しないまま、強力なパフォーマンスを達成する。さらに、T-Fewというシンプルなレシピを提案し、タスク固有のチューニングや修正なしに新しいタスクに適用できる。RAFTベンチマークでT-Fewを使用し、超人的なパフォーマンスを達成し、最先端を6％絶対的に上回る。</span>
</article>
<article class="paper-entry">
<h3 id="flashattention-fast-688" class="title-link">FlashAttention: Fast and Memory-Efficient Exact Attention with  IO-Awareness, Tri Dao+, N_A, arXiv'22</h3>
<br><a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/688" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2023-05-20</span>
<span class="snippet"><span>GPT Summary</span>- トランスフォーマーは、長いシーケンスに対して遅く、メモリを多く消費するため、注意アルゴリズムを改善する必要がある。FlashAttentionは、タイリングを使用して、GPUの高帯域幅メモリ（HBM）とGPUのオンチップSRAM間のメモリ読み取り/書き込みの数を減らし、トランスフォーマーを高速にトレーニングできる。FlashAttentionは、トランスフォーマーでより長い文脈を可能にし、より高品質なモデルや、完全に新しい機能を提供する。</span>
<span class="snippet"><span>Comment</span><p>より高速なGPU上のSRAM上で計算できるようにQKVをブロック単位に分割して計算することで、より高い計算効率を実現するFlashAttentionを提案[^1]<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/e3cb11b7-f413-4831-bea6-97886b683ff7" alt="image" loading="lazy" width="400" height="300"><br><br>[^1]: （2025.05.24追記)下記日本語ブログを参考に一部文言を訂正しました。ありがとうございます。</p>
<p>日本語解説:


<a href="https://zenn.dev/sinchir0/articles/21bb6e96c7b05b" target="_blank" rel="noopener noreferrer">https://zenn.dev/sinchir0/articles/21bb6e96c7b05b</a>


<br>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/sinchir0/status/1926199436406849786?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>日本語解説:


<a href="https://zenn.dev/uchiiii/articles/306d0bb7ef67a7" target="_blank" rel="noopener noreferrer">https://zenn.dev/uchiiii/articles/306d0bb7ef67a7</a>


<br>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/aquarobot0202/status/1957109068797018545?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="gram-fast-463" class="title-link">GRAM: Fast Fine-tuning of Pre-trained Language Models for Content-based   Collaborative Filtering, Yoonseok Yang+, NAACL'22</h3>
<br><a href="https://arxiv.org/abs/2204.04179" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/463" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="CollaborativeFiltering.html" target="_blank" rel="noopener noreferrer">#CollaborativeFiltering</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="EducationalDataMining.html" target="_blank" rel="noopener noreferrer">#EducationalDataMining</a>
<a class="button" href="KnowledgeTracing.html" target="_blank" rel="noopener noreferrer">#KnowledgeTracing</a>
<a class="button" href="Contents-based.html" target="_blank" rel="noopener noreferrer">#Contents-based</a>
<a class="button" href="NAACL.html" target="_blank" rel="noopener noreferrer">#NAACL</a>
<span class="issue_date">Issue Date: 2022-08-01</span>
<span class="snippet"><span>GPT Summary</span>- コンテンツベースの協調フィルタリング（CCF）において、PLMを用いたエンドツーエンドのトレーニングはリソースを消費するため、GRAM（勾配蓄積手法）を提案。Single-step GRAMはアイテムエンコーディングの勾配を集約し、Multi-step GRAMは勾配更新の遅延を増加させてメモリを削減。これにより、Knowledge TracingとNews Recommendationのタスクでトレーニング効率を最大146倍改善。</span>
<span class="snippet"><span>Comment</span><p>RiiiDがNAACL'22に論文通してた</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="tensor-programs-2582" class="title-link">[Paper Note] Tensor Programs V: Tuning Large Neural Networks via Zero-Shot  Hyperparameter Transfer, Greg Yang+, NeurIPS'21</h3>
<br><a href="https://arxiv.org/abs/2203.03466" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2582" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="ZeroshotHyperparameterTransfer.html" target="_blank" rel="noopener noreferrer">#ZeroshotHyperparameterTransfer</a>
<span class="issue_date">Issue Date: 2025-08-28</span>
<span class="snippet"><span>GPT Summary</span>- ハイパーパラメータチューニングは高コストであり、特に大規模なニューラルネットワークにおいて負担が大きい。新たに提案するmuTransferは、最大更新パラメータ化（muP）を利用し、小さなモデルでチューニングしたHPをフルサイズモデルにゼロショットで転送する手法である。実験により、1300万パラメータのモデルからBERT-largeを超える性能を達成し、4000万パラメータからはGPT-3を上回る結果を得た。チューニングコストはそれぞれ事前学習コストの同等または7%に抑えられた。</span>
<span class="snippet"><span>Comment</span><p>openreview: 


<a href="https://openreview.net/forum?id=Bx6qKuBM2AD" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=Bx6qKuBM2AD</a>


</p>
<p>小規模なモデルに対してハイパーパラメータのチューニングを実施し、同様のベースモデルで、**各layerのwidthが大きいもの**に対しても、小規模モデルで最適であったハイパーパラメータをzero-shotで転移することで near optimalなハイパーパラメータで学習できるmu Transferを提案。<br><br>モデルの深さ（以外にも下表中の*印のパラメータ）に対しても限定的に転移可能な模様。Post-Layer NormのTransformerやではあまりうまくいかないことが11節に記述されている（実験はpre-Layer Norm Transformer, ResNetに対して行われている模様）。<br>また、6.1節では、（実験的に）利用する小規模モデルのスケールとして幅256, 深さ4, バッチサイズ32, sequence長128, 訓練ステップ数5000を最低満たしており、かつスケールさせる幅が妥当な範囲内である必要がある、といった話が記述されている。<br><br>前提知識（muP）や条件が多そうな気がするので、しっかり確認した方がよさそう。<br>たとえば、muPで初期化されている必要があることや、転送可能なハイパーパラメータに限りがある（e.g. 学習率）、異なるデータに対するfinetuningなどは転送できないなど。<br><br><br><img width="872" height="336" alt="Image" src="&lt;a%20href=" https: target="_blank" rel="noopener noreferrer">https://github.com/user-attachments/assets/e5aeb152-5c9e-4ba2-9152-4bfef0d7c27c"


/&gt;</p>
<p>muP:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2583" target="_blank" rel="noopener noreferrer">[Paper Note] Feature Learning in Infinite-Width Neural Networks, Greg Yang+, PMLR'21</a>
</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="vilt-vision-and-language-1009" class="title-link">ViLT: Vision-and-Language Transformer Without Convolution or Region   Supervision, Wonjae Kim+, N_A, ICML'21</h3>
<br><a href="https://arxiv.org/abs/2102.03334" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1009" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<span class="issue_date">Issue Date: 2023-08-22</span>
<span class="snippet"><span>GPT Summary</span>- VLP（Vision-and-Language Pre-training）のアプローチは、ビジョンと言語のタスクでのパフォーマンスを向上させているが、現在の方法は効率性と表現力の面で問題がある。そこで、本研究では畳み込みフリーのビジョンと言語のトランスフォーマ（ViLT）モデルを提案する。ViLTは高速でありながら競争力のあるパフォーマンスを示し、コードと事前学習済みの重みはGitHubで利用可能である。</span>
<span class="snippet"><span>Comment</span><p>日本語解説:


<a href="https://tech.fusic.co.jp/posts/2021-12-29-vilt/" target="_blank" rel="noopener noreferrer">https://tech.fusic.co.jp/posts/2021-12-29-vilt/</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="big-bird-3855" class="title-link">[Paper Note] Big Bird: Transformers for Longer Sequences, Manzil Zaheer+, NIPS'20, 2020.07</h3>
<br><a href="https://arxiv.org/abs/2007.14062" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3855" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="Sparse.html" target="_blank" rel="noopener noreferrer">#Sparse</a>
<a class="button" href="SparseAttention.html" target="_blank" rel="noopener noreferrer">#SparseAttention</a>
<span class="issue_date">Issue Date: 2025-11-30</span>
<span class="snippet"><span>GPT Summary</span>- BigBirdは、Transformersモデルのシーケンス長に対する二次的依存性を線形に削減するスパース注意メカニズムを提案。これにより、長いシーケンスを最大8倍処理可能となり、質問応答や要約などのNLPタスクでの性能が向上。さらに、ゲノムデータへの新たな応用も示唆。</span>
<span class="snippet"><span>Comment</span><p>日本語解説: 


<a href="https://www.docswell.com/s/DeepLearning2023/KVV8VP-dlvisual-grounding-of-learned-physical-models-238500048" target="_blank" rel="noopener noreferrer">https://www.docswell.com/s/DeepLearning2023/KVV8VP-dlvisual-grounding-of-learned-physical-models-238500048</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="denoising-diffusion-3206" class="title-link">[Paper Note] Denoising Diffusion Probabilistic Models, Jonathan Ho+, NeurIPS'20, 2020.06</h3>
<br><a href="https://arxiv.org/abs/2006.11239" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3206" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="NeurIPS.html" target="_blank" rel="noopener noreferrer">#NeurIPS</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="Encoder-Decoder.html" target="_blank" rel="noopener noreferrer">#Encoder-Decoder</a>
<a class="button" href="ScoreMatching.html" target="_blank" rel="noopener noreferrer">#ScoreMatching</a>
<a class="button" href="ImageSynthesis.html" target="_blank" rel="noopener noreferrer">#ImageSynthesis</a>
<a class="button" href="U-Net.html" target="_blank" rel="noopener noreferrer">#U-Net</a>
<span class="issue_date">Issue Date: 2025-10-10</span>
<span class="snippet"><span>GPT Summary</span>- 拡散確率モデルを用いた高品質な画像合成を提案。新しい重み付き変分境界でのトレーニングにより、優れた結果を得る。無条件CIFAR10で9.46のInceptionスコア、256x256のLSUNでProgressiveGANに匹敵する品質を達成。実装はGitHubで公開。</span>
<span class="snippet"><span>Comment</span><p>日本語解説: 


<a href="https://qiita.com/ground0state/items/565de257807b12dba52a" target="_blank" rel="noopener noreferrer">https://qiita.com/ground0state/items/565de257807b12dba52a</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="longformer-the-2388" class="title-link">[Paper Note] Longformer: The Long-Document Transformer, Iz Beltagy+, arXiv'20</h3>
<br><a href="https://arxiv.org/abs/2004.05150" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2388" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="Sparse.html" target="_blank" rel="noopener noreferrer">#Sparse</a>
<a class="button" href="SparseAttention.html" target="_blank" rel="noopener noreferrer">#SparseAttention</a>
<span class="issue_date">Issue Date: 2025-08-09</span>
<span class="snippet"><span>GPT Summary</span>- Longformerは、長いシーケンスを線形に処理できる注意機構を持つTransformerベースのモデルで、数千トークンの文書を扱える。局所的なウィンドウ注意とタスクに基づくグローバル注意を組み合わせ、文字レベルの言語モデリングで最先端の結果を達成。事前学習とファインチューニングを行い、長文タスクでRoBERTaを上回る性能を示した。また、Longformer-Encoder-Decoder（LED）を導入し、長文生成タスクにおける効果を確認した。</span>
<span class="snippet"><span>Comment</span><p>（固定された小さめのwindowsサイズの中でのみattentionを計算する）sliding window attentionを提案。Figure2を見ると、通常のAttentionと比較して、現在のトークンの周辺のトークンにしか注目しない特性が図示されており、イメージが掴みやすい。<br><br><img width="795" height="231" alt="Image" src="&lt;a%20href=" https: target="_blank" rel="noopener noreferrer">https://github.com/user-attachments/assets/d1eccdaf-5b5b-4444-ad31-44c54c345d79"


/&gt;</p>
<p>OpenLLMの文脈だと、Mistralに採用されて話題になったかも？<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1309" target="_blank" rel="noopener noreferrer">Mistral 7B, Albert Q. Jiang+, N/A, arXiv'23</a>
</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="transformers-are-2356" class="title-link">[Paper Note] Transformers are RNNs: Fast Autoregressive Transformers with Linear  Attention, Angelos Katharopoulos+, ICML'20</h3>
<br><a href="https://arxiv.org/pdf/2006.16236" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2356" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<span class="snippet"><span>GPT Summary</span>- 自己注意をカーネル特徴マップの線形ドット積として表現することで、Transformersの複雑性を$\mathcal{O}\left(N^2\right)$から$\mathcal{O}\left(N\right)$に削減。これにより、自己回帰型Transformersの速度が最大4000倍向上し、従来のパフォーマンスを維持。</span>
<span class="snippet"><span>Comment</span><p>関連:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1210" target="_blank" rel="noopener noreferrer">Transformers are Multi-State RNNs, Matanel Oren+, N/A, EMNLP'24</a>
 </p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="reformer-the-2355" class="title-link">[Paper Note] Reformer: The Efficient Transformer, Nikita Kitaev+, ICLR'20</h3>
<br><a href="https://arxiv.org/abs/2001.04451" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2355" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="ICLR.html" target="_blank" rel="noopener noreferrer">#ICLR</a>
<a class="button" href="Sparse.html" target="_blank" rel="noopener noreferrer">#Sparse</a>
<a class="button" href="SparseAttention.html" target="_blank" rel="noopener noreferrer">#SparseAttention</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<span class="snippet"><span>GPT Summary</span>- 本研究では、トランスフォーマーモデルの効率を向上させるために、局所感度ハッシュを用いた注意機構と可逆残差層を提案。これにより、計算量をO($L^2$)からO($L\log L$)に削減し、メモリ効率と速度を向上させたReformerモデルを実現。トランスフォーマーと同等の性能を維持。</span>
<span class="snippet"><span>Comment</span><p>openreview: 


<a href="https://openreview.net/forum?id=rkgNKkHtvB" target="_blank" rel="noopener noreferrer">https://openreview.net/forum?id=rkgNKkHtvB</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="linformer-self-attention-2354" class="title-link">[Paper Note] Linformer: Self-Attention with Linear Complexity, Sinong Wang+, arXiv'20</h3>
<br><a href="https://arxiv.org/abs/2006.04768" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2354" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<span class="snippet"><span>GPT Summary</span>- 大規模トランスフォーマーモデルは自然言語処理で成功を収めているが、長いシーケンスに対しては高コスト。自己注意メカニズムを低ランク行列で近似し、複雑さを$O(n^2)$から$O(n)$に削減する新しいメカニズムを提案。これにより、メモリと時間効率が向上した線形トランスフォーマー「Linformer」が標準モデルと同等の性能を示す。</span>
</article>
<article class="paper-entry">
<h3 id="transformer-xl-attentive-2359" class="title-link">[Paper Note] Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context, Zihang Dai+, ACL'19</h3>
<br><a href="https://arxiv.org/abs/1901.02860" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2359" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="LongSequence.html" target="_blank" rel="noopener noreferrer">#LongSequence</a>
<a class="button" href="PositionalEncoding.html" target="_blank" rel="noopener noreferrer">#PositionalEncoding</a>
<a class="button" href="ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<span class="snippet"><span>GPT Summary</span>- Transformer-XLは、固定長のコンテキストを超えた長期的な依存関係を学習する新しいニューラルアーキテクチャで、セグメントレベルの再帰メカニズムと新しい位置エンコーディングを採用。これにより、RNNより80%、従来のTransformersより450%長い依存関係を学習し、評価時には最大1,800倍の速度向上を実現。enwiki8やWikiText-103などで最先端のパフォーマンスを達成し、数千トークンの一貫したテキスト生成も可能。コードとモデルはTensorflowとPyTorchで利用可能。</span>
<span class="snippet"><span>Comment</span><p>日本語解説:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/329" target="_blank" rel="noopener noreferrer">事前学習言語モデルの動向 / Survey of Pretrained Language Models, Kyosuke Nishida, 2019</a>
</p>
<p>3.2節の定式化を見ると、一つ前のセグメントのトークン・layerごとのhidden stateを、現在のセグメントの対応するトークンとlayerのhidden stateにconcatし（過去のセグメントに影響を与えないように勾配を伝搬させないStop-Gradientを適用する）、QKVのうち、KVの計算に活用している。また、絶対位置エンコーディングを利用するとモデルがセグメント間の時系列的な関係を認識できなくなるため、位置エンコーディングには相対位置エンコーディングを利用する。これにより、現在のセグメントのKVが一つ前のセグメントによって条件づけられ、contextとして考慮することが可能となり、セグメント間を跨いだ依存関係の考慮が実現される。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="efficientnet-rethinking-1957" class="title-link">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks, Mingxing Tan+, ICML'19</h3>
<br><a href="https://arxiv.org/abs/1905.11946" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1957" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="ICML.html" target="_blank" rel="noopener noreferrer">#ICML</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="Backbone.html" target="_blank" rel="noopener noreferrer">#Backbone</a>
<span class="issue_date">Issue Date: 2025-05-12</span>
<span class="snippet"><span>GPT Summary</span>- 本論文では、ConvNetsのスケーリングを深さ、幅、解像度のバランスを考慮して体系的に研究し、新しいスケーリング手法を提案。これにより、MobileNetsやResNetのスケールアップを実証し、EfficientNetsという新しいモデルファミリーを設計。特にEfficientNet-B7は、ImageNetで84.3%のトップ1精度を達成し、従来のConvNetsよりも小型かつ高速である。CIFAR-100やFlowersなどのデータセットでも最先端の精度を記録。ソースコードは公開されている。</span>
<span class="snippet"><span>Comment</span><p>元論文をメモってなかったので追加。<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/346" target="_blank" rel="noopener noreferrer">EfficientNet解説, omiita (オミータ), 2019</a>
<br><br>も参照のこと。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="fast-transformer-1272" class="title-link">Fast Transformer Decoding: One Write-Head is All You Need, Noam Shazeer, N_A, arXiv'19</h3>
<br><a href="https://arxiv.org/abs/1911.02150v1" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1272" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2024-04-07</span>
<span class="snippet"><span>GPT Summary</span>- マルチヘッドアテンションレイヤーのトレーニングは高速かつ簡単だが、増分推論は大きな"keys"と"values"テンソルを繰り返し読み込むために遅くなることがある。そこで、キーと値を共有するマルチクエリアテンションを提案し、メモリ帯域幅要件を低減する。実験により、高速なデコードが可能で、わずかな品質の低下しかないことが確認された。</span>
<span class="snippet"><span>Comment</span><p>Multi Query Attention論文。KVのsetに対して、単一のQueryのみでMulti-Head Attentionを代替する。劇的にDecoderのInferenceが早くなりメモリ使用量が減るが、論文中では言及されていない？ようだが、性能と学習の安定性が課題となるようである。<br><br><br><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/e2d77b43-70c3-4922-a822-bf95d6b4704f" alt="image" loading="lazy" width="400" height="300"><br><br></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="efficient-attention-2353" class="title-link">[Paper Note] Efficient Attention: Attention with Linear Complexities, Zhuoran Shen+, arXiv'18</h3>
<br><a href="https://arxiv.org/abs/1812.01243" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2353" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2025-08-05</span>
<span class="snippet"><span>GPT Summary</span>- 新しい効率的なアテンションメカニズムを提案し、ドット積アテンションと同等の性能を維持しつつ、メモリと計算コストを大幅に削減。これにより、アテンションモジュールの柔軟な統合が可能となり、精度向上を実現。実験結果では、MS-COCO 2017での物体検出やインスタンスセグメンテーションでの性能向上が確認され、Scene Flowデータセットでは最先端の精度を達成。コードは公開されている。</span>
<span class="snippet"><span>Comment</span><p>Figure1を見るとコンセプトが一目でわかり、非常にわかりやすい<br><img width="1068" height="580" alt="Image" src="&lt;a%20href=" https: target="_blank" rel="noopener noreferrer">https://github.com/user-attachments/assets/18e6a7da-fc07-495f-bda6-bcef4acab321"


/&gt;</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="learning-to-82" class="title-link">[Paper Note] Learning to Skim Text, Adams Wei Yu+, ACL'17, 2017.04</h3>
<br><a href="https://arxiv.org/abs/1704.06877" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/82" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<a class="button" href="Decoder.html" target="_blank" rel="noopener noreferrer">#Decoder</a>
<a class="button" href="KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<a class="button" href="Sparse.html" target="_blank" rel="noopener noreferrer">#Sparse</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
<span class="snippet"><span>GPT Summary</span>- 再帰型ニューラルネットワーク（RNN）は自然言語処理での可能性を示すが、長文の処理が遅い。本論文では、無関係な情報をスキップしながらテキストを読むアプローチを提案。モデルは、入力テキストの数語を読んだ後にジャンプする距離を学習し、ポリシー勾配法で訓練。数値予測や自動Q&amp;Aなど4つのタスクで、提案モデルは標準LSTMに比べて最大6倍の速度向上を達成し、精度も維持。</span>
<span class="snippet"><span>Comment</span><p>解説スライド：


<a href="http://www.lr.pi.titech.ac.jp/~haseshun/acl2017suzukake/slides/07.pdf" target="_blank" rel="noopener noreferrer">http://www.lr.pi.titech.ac.jp/~haseshun/acl2017suzukake/slides/07.pdf</a>


<br><br>Reinforceにおける勾配の更新式の導出が丁寧に記述されており大変ありがたい。</p>
<p>RNNにおいて重要な部分以外は読み飛ばすことで効率を向上させる研究。いくつ読み飛ばすかも潜在変数として一緒に学習する。潜在変数（離散変数）なので、普通に尤度最大化するやり方では学習できず、おまけに離散変数なのでバックプロパゲーション使えないので、強化学習で学習する。<br><br><br><br>Vanilla LSTMと比較し、色々なタスクで実験した結果、性能も（少し）上がるし、スピードアップもする。</p>
<p>うーんこの研究は今改めて見返すと非常に面白いな…（8年も経ったのか）。ざっくり言うと必要のない部分は読み飛ばして考慮しないという話であり、最近のLLMでもこういった話はよくやられている印象。一番近いのはSparse Attentionだろうか。<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3854" target="_blank" rel="noopener noreferrer">[Paper Note] Efficient Transformers: A Survey, Yi Tay+, ACM Computing Surveys'22, 2022.12</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3855" target="_blank" rel="noopener noreferrer">[Paper Note] Big Bird: Transformers for Longer Sequences, Manzil Zaheer+, NIPS'20, 2020.07</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2355" target="_blank" rel="noopener noreferrer">[Paper Note] Reformer: The Efficient Transformer, Nikita Kitaev+, ICLR'20</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3853" target="_blank" rel="noopener noreferrer">[Paper Note] Generating Long Sequences with Sparse Transformers, Rewon Child+, arXiv'19, 2019.04</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2388" target="_blank" rel="noopener noreferrer">[Paper Note] Longformer: The Long-Document Transformer, Iz Beltagy+, arXiv'20</a>
<br><br>トークン単位などはなくlayerをスキップするとかもある（Layer Skip）。<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2610" target="_blank" rel="noopener noreferrer">[Paper Note] Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs, Ziyue Li+, arXiv'25</a>
</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="mini-sglang-a-4086" class="title-link">mini-sglang: A compact implementation of SGLang, designed to demystify the complexities of modern LLM serving systems, sgl-project, 2025</h3>
<br><a href="https://github.com/sgl-project/mini-sglang" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/4086" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="python.html" target="_blank" rel="noopener noreferrer">#python</a>
<a class="button" href="Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<a class="button" href="LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="MinimalCode.html" target="_blank" rel="noopener noreferrer">#MinimalCode</a>
<span class="issue_date">Issue Date: 2025-12-28</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/theahmadosman/status/2004761075284127861?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>めっちゃ勉強したい</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="optimizing-large-scale-4049" class="title-link">Optimizing Large-Scale Pretraining at Character.ai, character.ai, 2025.12</h3>
<br><a href="https://blog.character.ai/squinch/" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/4049" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-12-24</span>
<span class="snippet"><span>Comment</span><p>元ポスト: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/simon_mo_/status/2003608325624406482?s=20"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="hot-topics-4039" class="title-link">Hot topics in RL, Kimbo, X, 2025.12</h3>
<br><a href="https://x.com/kimbochen/status/2002817512723869940?s=20" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/4039" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="Post.html" target="_blank" rel="noopener noreferrer">#Post</a>
<a class="button" href="Diversity.html" target="_blank" rel="noopener noreferrer">#Diversity</a>
<a class="button" href="train-inference-gap.html" target="_blank" rel="noopener noreferrer">#train-inference-gap</a>
<span class="issue_date">Issue Date: 2025-12-22</span>
<span class="snippet"><span>Comment</span><p>ロールアウト側のエンジンと、学習側のエンジンのトークンのlogprobのミスマッチによりon-policy RLを実施しているつもりが実はoff policyになってしまっているという話と <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2552" target="_blank" rel="noopener noreferrer">Your Efficient RL Framework Secretly Brings You Off-Policy RL Training, Yao+, 2025.08</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3388" target="_blank" rel="noopener noreferrer">[Paper Note] Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale
  Thinking Model, Ling Team+, arXiv'25, 2025.10</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3261" target="_blank" rel="noopener noreferrer">[Paper Note] Stabilizing MoE Reinforcement Learning by Aligning Training and
  Inference Routers, Wenhan Ma+, arXiv'25, 2025.10</a>
 <br><br>長いロールアウトを待っている間がアイドルタイムとなり学習が非常に遅くなる問題を、長すぎるロールアウトは待たないでモデルの重みをロールアウトの途中でもかけてしまい、新しいポリシーでロールアウトを継続すると学習は崩壊せずに高速化できるよ（=in flight updates）という話と<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3615" target="_blank" rel="noopener noreferrer">[Paper Note] PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence
  Generation, Alexandre Piché+, arXiv'25, 2025.09</a>
 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3130" target="_blank" rel="noopener noreferrer">PipelineRL, Piche+, ServiceNow, 2025.04</a>
 <br><br>RLVRはもともとモデルが事前学習時に保持しているReasoningの能力を広げるわけではなく効率化するだけだよ、という主張と、<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3589" target="_blank" rel="noopener noreferrer">[Paper Note] Does Reinforcement Learning Really Incentivize Reasoning Capacity in   LLMs Beyond the Base Model?, Yang Yue+, NeurIPS'25, 2025.04</a>
<br><br>効率化するだけという主張と、Reasoning能力を拡大しているよ、という相反する主張がコミュニティでされているがそれらをphysics of language modelsに則り完全にコントロールされた条件下で実験し、どのような条件でどのような挙動になるかを明らかにしたよ、という話 <br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3912" target="_blank" rel="noopener noreferrer">[Paper Note] On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models, Charlie Zhang+, arXiv'25, 2025.12</a>
 <br><br>RLVRはPass@1を報酬としているとみなせるが、それをPass@kにすることで、モデルがRL中に探索する能力が向上し、downstreamタスクのPass@kが向上するよ<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2477" target="_blank" rel="noopener noreferrer">[Paper Note] Pass@k Training for Adaptively Balancing Exploration and Exploitation of
  Large Reasoning Models, Zhipeng Chen+, arXiv'25</a>
<br><br>といったこの辺の話がホットトピックとして挙げられている。 </p>
<p>train-inference-mismatchについては、以下もおもしろかった:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3947" target="_blank" rel="noopener noreferrer">SID-1 Technical Report: Test-Time Compute for Retrieval, SID Research, 2025.12</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3532" target="_blank" rel="noopener noreferrer">[Paper Note] Defeating the Training-Inference Mismatch via FP16, Penghui Qi+, arXiv'25, 2025.10</a>
</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="opentinker--4036" class="title-link">OpenTinker  Democratizing Agentic Reinforcement Learning as a Service, Zhu+, University of Illinois Urbana-Champaign, 2025.12</h3>
<br><a href="https://open-tinker.github.io/opentinker-page/" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/4036" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="Tools.html" target="_blank" rel="noopener noreferrer">#Tools</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-12-22</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/youjiaxuan/status/2002838551319253281?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>code:


<a href="https://github.com/open-tinker/OpenTinker" target="_blank" rel="noopener noreferrer">https://github.com/open-tinker/OpenTinker</a>


</p>
<p>関連:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1969" target="_blank" rel="noopener noreferrer">verl: Volcano Engine Reinforcement Learning for LLMs, ByteDance Seed Team, 2025.04</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3077" target="_blank" rel="noopener noreferrer">Tinker is a training API for {developers, builders, researchers}, THINKING MACHINES, 2025.10</a>
</p>
<p>Tinkerに着想を得てクライアントとサーバを分離した設計になっており、バックエンド側のGPUクラスタでサーバを一度起動するだけでクライアント側がスケジューラにジョブを送ればRLが実行される（ローカルにGPUは不要）。クライアント側はRLを実施したい環境のみをローカルで定義しコンフィグをロードしfitを呼び出すだけ。verlよりもよりも手間が省けているらしい。<br><br>リポジトリを見る限りは、verlをRLのコアエンジンとして使ってる模様。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="performance-hints-4016" class="title-link">Performance Hints, Jeff Dean+, 2025.12</h3>
<br><a href="https://abseil.io/fast/hints#performance-hints" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/4016" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="Coding.html" target="_blank" rel="noopener noreferrer">#Coding</a>
<a class="button" href="SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="reading.html" target="_blank" rel="noopener noreferrer">#reading</a>
<span class="issue_date">Issue Date: 2025-12-21</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/pmddomingos/status/2002376790216507778?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="[paper-notes]-3526" class="title-link">[Paper Notes] KIMI LINEAR: AN EXPRESSIVE, EFFICIENT ATTENTION ARCHITECTURE, Kimi Team, 2025.10</h3>
<br><a href="https://github.com/MoonshotAI/Kimi-Linear/blob/master/tech_report.pdf" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3526" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="Architecture.html" target="_blank" rel="noopener noreferrer">#Architecture</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Hybrid.html" target="_blank" rel="noopener noreferrer">#Hybrid</a>
<span class="issue_date">Issue Date: 2025-10-31</span>
<span class="snippet"><span>Comment</span><p>HF:


<a href="https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct" target="_blank" rel="noopener noreferrer">https://huggingface.co/moonshotai/Kimi-Linear-48B-A3B-Instruct</a>


</p>
<p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/kimi_moonshot/status/1983937694360322136?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>所見:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/nrehiew_/status/1983891931823505518?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>所見:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1983992979153985676?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>アーキテクチャ解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rasbt/status/1984617030356451642?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="nanochat-3377" class="title-link">nanochat, karpathy, 2025.10</h3>
<br><a href="https://github.com/karpathy/nanochat" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3377" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="ChatGPT.html" target="_blank" rel="noopener noreferrer">#ChatGPT</a>
<a class="button" href="Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<a class="button" href="mid-training.html" target="_blank" rel="noopener noreferrer">#mid-training</a>
<a class="button" href="GRPO.html" target="_blank" rel="noopener noreferrer">#GRPO</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="Inference.html" target="_blank" rel="noopener noreferrer">#Inference</a>
<a class="button" href="MinimalCode.html" target="_blank" rel="noopener noreferrer">#MinimalCode</a>
<a class="button" href="KV%20Cache.html" target="_blank" rel="noopener noreferrer">#KV Cache</a>
<span class="issue_date">Issue Date: 2025-10-22</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/karpathy/status/1977755427569111362?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>新たなスピードランが...!!</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="modded-nanogpt-medium-3338" class="title-link">modded-nanogpt medium world record: Re-using intermediate activations in the output latents, shimu's blog, 2025.10</h3>
<br><a href="https://snimu.github.io/2025/10/19/modded-nanogpt-backout.html" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3338" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<span class="issue_date">Issue Date: 2025-10-20</span>
<span class="snippet"><span>Comment</span><p>元ポスト:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/stochasticchasm/status/1979985191356731663?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="introducing-swe-grep-3302" class="title-link">Introducing SWE-grep and SWE-grep-mini: RL for Multi-Turn, Fast Context Retrieval, Cognition, 2025.10</h3>
<br><a href="https://cognition.ai/blog/swe-grep" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3302" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="Multi.html" target="_blank" rel="noopener noreferrer">#Multi</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="Proprietary.html" target="_blank" rel="noopener noreferrer">#Proprietary</a>
<a class="button" href="Parallelism.html" target="_blank" rel="noopener noreferrer">#Parallelism</a>
<a class="button" href="ContextEngineering.html" target="_blank" rel="noopener noreferrer">#ContextEngineering</a>
<a class="button" href="KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-10-18</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/walden_yan/status/1978884662601617859?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>最大で4 turnの間8つのツールコール（guessingとしては従来モデルは1--2, Sonnet-4.5は1--4)を並列する（3 turnは探索、最後の1 turnをanswerのために使う) parallel tool calls を効果的に実施できるように、on policy RLでマルチターンのRLを実施することで、高速で正確なcontext retrievalを実現した、という感じらしい。<br><br>従来のembedding-basedなdense retrieverは速いが正確性に欠け、Agenticなsearchは正確だが遅いという双方の欠点を補う形。</p>
<p>parallel tool callというのは具体的にどういうtrajectoryになるのか…？</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="pipelinerl-3130" class="title-link">PipelineRL, Piche+, ServiceNow, 2025.04</h3>
<br><a href="https://huggingface.co/blog/ServiceNow/pipelinerl" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3130" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="ReinforcementLearning.html" target="_blank" rel="noopener noreferrer">#ReinforcementLearning</a>
<a class="button" href="AIAgents.html" target="_blank" rel="noopener noreferrer">#AIAgents</a>
<a class="button" href="Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="KeyPoint%20Notes.html" target="_blank" rel="noopener noreferrer">#KeyPoint Notes</a>
<span class="issue_date">Issue Date: 2025-10-05</span>
<span class="snippet"><span>Comment</span><p>code:


<a href="https://github.com/ServiceNow/PipelineRL" target="_blank" rel="noopener noreferrer">https://github.com/ServiceNow/PipelineRL</a>


</p>
<p>元ポスト:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/vllm_project/status/1974732295627301254?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>Inflight Weight Updates</p>
<p>（この辺の細かい実装の話はあまり詳しくないので誤りがある可能性が結構あります）<br>通常のon-policy RLでは全てのGPU上でのsequenceのロールアウトが終わるまで待ち、全てのロールアウト完了後にモデルの重みを更新するため、長いsequenceのデコードをするGPUの処理が終わるまで、短いsequenceの生成で済んだGPUは待機しなければならない。一方、PipelineRLはsequenceのデコードの途中でも重みを更新し、生成途中のsequenceは古いKV Cacheを保持したまま新しい重みでsequenceのデコードを継続する。これによりGPU Utilizationを最大化できる（ロールアウト完了のための待機時間が無くなる）。また、一見古いKV Cacheを前提に新たな重みで継続して部分sequenceを継続するとポリシーのgapにより性能が悪化するように思えるが、性能が悪化しないことが実験的に示されている模様。<br><img src="https://github.com/user-attachments/assets/6794927a-3fa9-4a68-ba48-d112d495e0ab" alt="image" loading="lazy" width="400" height="300"><br><br>Conventional RLの疑似コード部分を見るととてもわかりやすくて参考になる。Conventional RL（PPOとか）では、実装上は複数のバッチに分けて重みの更新が行われる（らしい）。このとき、GPUの利用を最大化しようとするとバッチサイズを大きくせざるを得ない。このため、逐次更新をしたときのpolicyのgapがどんどん蓄積していき大きくなる（=ロールアウトで生成したデータが、実際に重み更新するときにはlagが蓄積されていきどんどんoff-policyデータに変化していってしまう）という弊害がある模様。かといってlagを最小にするために小さいバッチサイズにするとgpuの効率を圧倒的に犠牲にするのでできない。Inflight Weight Updatesではこのようなトレードオフを解決できる模様。<br><br>また、trainerとinference部分は完全に独立させられ、かつplug-and-playで重みを更新する、といった使い方も想定できる模様。</p>
<p>あとこれは余談だが、引用ポストの主は下記研究でattentionメカニズムを最初に提案したBahdanau氏である。<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1954" target="_blank" rel="noopener noreferrer">Neural Machine Translation by Jointly Learning to Align and Translate, Dzmitry Bahdanau+, ICLR'15</a>
</p>
<p>続報:<br>



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/dbahdanau/status/1974889569607876747?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>論文:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3615" target="_blank" rel="noopener noreferrer">[Paper Note] PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence
  Generation, Alexandre Piché+, arXiv'25, 2025.09</a>
</p>
<p>続報:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/finbarrtimbers/status/1986481494823739581?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="we-reverse-engineered-3010" class="title-link">We reverse-engineered Flash Attention 4, Modal Blog, 2025.09</h3>
<br><a href="https://modal.com/blog/reverse-engineer-flash-attention-4" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/3010" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="SoftwareEngineering.html" target="_blank" rel="noopener noreferrer">#SoftwareEngineering</a>
<a class="button" href="One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<span class="issue_date">Issue Date: 2025-09-28</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/iwashi86/status/1972085451055157725?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>Flash Attention4は数学的なトリックよりも非同期処理の複雑なパイプライン、Blackwellに最適化、とのこと</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="fast-dllm-v2-2720" class="title-link">Fast-dLLM v2: Efficient Block-Diffusion Large Language Model, Wu+, 2025.09</h3>
<br><a href="https://nvlabs.github.io/Fast-dLLM/v2/" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2720" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2025-09-07</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/songhan_mit/status/1964375581761388828?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="longcat-flash-chat-2620" class="title-link">LongCat-Flash-Chat, meituan-longcat, 2025.08</h3>
<br><a href="https://huggingface.co/meituan-longcat/LongCat-Flash-Chat" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2620" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<a class="button" href="One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<a class="button" href="Reference%20Collection.html" target="_blank" rel="noopener noreferrer">#Reference Collection</a>
<span class="issue_date">Issue Date: 2025-08-31</span>
<span class="snippet"><span>Comment</span><p>テクニカルレポート:


<a href="https://github.com/meituan-longcat/LongCat-Flash-Chat/blob/main/tech_report.pdf" target="_blank" rel="noopener noreferrer">https://github.com/meituan-longcat/LongCat-Flash-Chat/blob/main/tech_report.pdf</a>


</p>
<p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rosinality/status/1961955926136832381?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>Agent周りのベンチで高性能なnon thinkingモデル。毎秒100+トークンの生成速度で、MITライセンス。Dynamic Activation...?</p>
<p>関連:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2621" target="_blank" rel="noopener noreferrer">[Paper Note] Shortcut-connected Expert Parallelism for Accelerating   Mixture-of-Experts, Weilin Cai+, ICLR'25</a>
</p>
<p>Dynamic Activation (activation paramが入力に応じて変化(全てのトークンをMoEにおいて均一に扱わない）することで効率化）は、下記を利用することで実現している模様<br><br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2622" target="_blank" rel="noopener noreferrer">[Paper Note] MoE++: Accelerating Mixture-of-Experts Methods with Zero-Computation   Experts, Peng Jin+, ICLR'25</a>
</p>
<p>しかし中国は本当に次々に色々な企業から基盤モデルが出てくるなぁ…すごい</p>
<p>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2623" target="_blank" rel="noopener noreferrer">[Paper Note] Scaling Exponents Across Parameterizations and Optimizers, Katie Everett+, ICML'24</a>
 </p>
<p>解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/nrehiew_/status/1962186876099739767?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>解説:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/gm8xx8/status/1962980770550628841?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="fastvlm-webgpu-2606" class="title-link">fastvlm-webgpu, Apple, 2025.08</h3>
<br><a href="https://huggingface.co/spaces/apple/fastvlm-webgpu" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2606" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="SmallModel.html" target="_blank" rel="noopener noreferrer">#SmallModel</a>
<a class="button" href="VisionLanguageModel.html" target="_blank" rel="noopener noreferrer">#VisionLanguageModel</a>
<span class="issue_date">Issue Date: 2025-08-30</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/fartashfg/status/1961441954157244448?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>pj page:


<a href="https://fastvlm.net" target="_blank" rel="noopener noreferrer">https://fastvlm.net</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="vllmのspeculative-decodingによる推論高速化を試す-2499" class="title-link">vLLMのSpeculative Decodingによる推論高速化を試す, Aratako, 2025.05</h3>
<br><a href="https://zenn.dev/aratako_lm/articles/efeadd6d1e0f4b" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2499" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="python.html" target="_blank" rel="noopener noreferrer">#python</a>
<a class="button" href="Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<a class="button" href="SpeculativeDecoding.html" target="_blank" rel="noopener noreferrer">#SpeculativeDecoding</a>
<span class="issue_date">Issue Date: 2025-08-21</span>
</article>
<article class="paper-entry">
<h3 id="simple-paged-attention-2472" class="title-link">simple-paged-attention, torotoki, 2025.06</h3>
<br><a href="https://github.com/torotoki/simple-paged-attention" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2472" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<a class="button" href="python.html" target="_blank" rel="noopener noreferrer">#python</a>
<a class="button" href="Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<a class="button" href="read-later.html" target="_blank" rel="noopener noreferrer">#read-later</a>
<a class="button" href="MinimalCode.html" target="_blank" rel="noopener noreferrer">#MinimalCode</a>
<span class="issue_date">Issue Date: 2025-08-19</span>
<span class="snippet"><span>Comment</span><p>CUDA + C++によるミニマルなpaged-attentionの実装。アルゴリズムの理解+実装理解の参考に非常に良さそう。</p>
<p>PagedAttentionは 現在の主要なLLM Inference/Serving EngineのひとつであるvLLM で（提案|実装）された技術であり、元論文は下記:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2474" target="_blank" rel="noopener noreferrer">[Paper Note] Efficient Memory Management for Large Language Model Serving with  PagedAttention, Woosuk Kwon+, SOSP'23</a>
</p>
<p>この辺もあわせて読むとおもしろいかもしれない:<br>


<a href="https://nttdocomo-developers.jp/entry/2024/12/19/090000_6" target="_blank" rel="noopener noreferrer">https://nttdocomo-developers.jp/entry/2024/12/19/090000_6</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="lmcache-2349" class="title-link">LMCache, LMCache, 2025.07</h3>
<br><a href="https://github.com/LMCache/LMCache" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2349" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="python.html" target="_blank" rel="noopener noreferrer">#python</a>
<a class="button" href="LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<span class="issue_date">Issue Date: 2025-08-03</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/akshay_pachaar/status/1951626977213059406?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>KV Cacheを色々なところにキャッシュしておいて、prefixだけでなく全てのreused可能なものをキャッシュすることで、TTFTとスループットを大幅に向上するらしい。特にlong contextなタスクで力を発揮し、vLLMと組み合わせると下記のようなパフォーマンス向上結果<br><img src="https://github.com/user-attachments/assets/d40301c3-f7a9-4f2a-a66e-65c3357ce2b9" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="qwen3-coder-30b-a3b-instruct-2333" class="title-link">Qwen3-Coder-30B-A3B-Instruct, QwenTeam, 2025.08</h3>
<br><a href="https://huggingface.co/Qwen/Qwen3-Coder-30B-A3B-Instruct" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2333" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Coding.html" target="_blank" rel="noopener noreferrer">#Coding</a>
<a class="button" href="Reasoning.html" target="_blank" rel="noopener noreferrer">#Reasoning</a>
<a class="button" href="MoE(Mixture-of-Experts).html" target="_blank" rel="noopener noreferrer">#MoE(Mixture-of-Experts)</a>
<span class="issue_date">Issue Date: 2025-08-02</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/alibaba_qwen/status/1950925444057792808?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p><img src="https://github.com/user-attachments/assets/d2c30b64-10df-40b2-bcac-f029bdc9f1f1" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="seed-diffusion-2331" class="title-link">Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference, ByteDance Seed,</h3>
<br><a href="https://lf3-static.bytednsdoc.com/obj/eden-cn/hyvsmeh7uhobf/sdiff_updated.pdf" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2331" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<span class="issue_date">Issue Date: 2025-08-01</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/jiqizhixin/status/1951092714164101590?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p><img src="https://github.com/user-attachments/assets/b7ba3b05-760d-4820-b685-0058706286ff" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="speculative-decoding：faster-2291" class="title-link">Speculative Decoding：Faster Inference Without Paying for More GPU, ELYZA, 2025.07</h3>
<br><a href="https://zenn.dev/elyza/articles/4e0b45a8c11220" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2291" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<a class="button" href="Decoding.html" target="_blank" rel="noopener noreferrer">#Decoding</a>
<a class="button" href="SpeculativeDecoding.html" target="_blank" rel="noopener noreferrer">#SpeculativeDecoding</a>
<span class="issue_date">Issue Date: 2025-07-24</span>
</article>
<article class="paper-entry">
<h3 id="modded-nanogpt-2223" class="title-link">Modded-NanoGPT, KellerJordan, 2024.05</h3>
<br><a href="https://github.com/KellerJordan/modded-nanogpt" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2223" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<a class="button" href="Optimizer.html" target="_blank" rel="noopener noreferrer">#Optimizer</a>
<a class="button" href="Decoder.html" target="_blank" rel="noopener noreferrer">#Decoder</a>
<span class="issue_date">Issue Date: 2025-07-15</span>
<span class="snippet"><span>Comment</span><p>NanoGPT speedrun</p>
<p>関連:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2118" target="_blank" rel="noopener noreferrer">[Paper Note] The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT  Improvements, Bingchen Zhao+, arXiv'25</a>
<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2208" target="_blank" rel="noopener noreferrer">きみはNanoGPT speedrunを知っているか？, PredNext, 2025.07</a>
</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="日経電子版のアプリトップ「おすすめ」をtwo-towerモデルでリプレースしました-2113" class="title-link">日経電子版のアプリトップ「おすすめ」をTwo Towerモデルでリプレースしました, NIKKEI, 2025.05</h3>
<br><a href="https://hack.nikkei.com/blog/replace_ai_rec_by_two_tower/" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2113" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="AWS.html" target="_blank" rel="noopener noreferrer">#AWS</a>
<a class="button" href="MLOps.html" target="_blank" rel="noopener noreferrer">#MLOps</a>
<a class="button" href="Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="A_B%20Testing.html" target="_blank" rel="noopener noreferrer">#A/B Testing</a>
<a class="button" href="TwoTowerModel.html" target="_blank" rel="noopener noreferrer">#TwoTowerModel</a>
<span class="issue_date">Issue Date: 2025-06-29</span>
<span class="snippet"><span>Comment</span><p>リアルタイム推薦をするユースケースにおいて、ルールベース+協調フィルタリング(Jubatus)からTwo Towerモデルに切り替えた際にレイテンシが300ms増えてしまったため、ボトルネックを特定し一部をパッチ処理にしつつもリアルタイム性を残すことで解決したという話。AWSの構成、A/Bテストや負荷テストの話もあり、実用的で非常に興味深かった。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="nemo-rl-2095" class="title-link">Nemo-RL, Nvidia, 2025.05</h3>
<br><a href="https://github.com/NVIDIA-NeMo/RL" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2095" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<a class="button" href="PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<span class="issue_date">Issue Date: 2025-06-25</span>
</article>
<article class="paper-entry">
<h3 id="nemotron-h-a-1830" class="title-link">Nemotron-H: A Family of Accurate, Efficient Hybrid Mamba-Transformer Models, Nvidia, 2025.03</h3>
<br><a href="https://research.nvidia.com/labs/adlr/nemotronh/" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1830" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="SSM%20(StateSpaceModel).html" target="_blank" rel="noopener noreferrer">#SSM (StateSpaceModel)</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2025-03-24</span>
<span class="snippet"><span>Comment</span><p>関連:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1820" target="_blank" rel="noopener noreferrer">Hunyuan T1, Tencent, 2025.03</a>
</p>
<p>TransformerのSelf-attention LayerをMamba2 Layerに置換することで、様々なベンチマークで同等の性能、あるいは上回る性能で3倍程度のInference timeの高速化をしている（65536 input, 1024 output）。<br><br>56B程度のmediumサイズのモデルと、8B程度の軽量なモデルについて述べられている。特に、8BモデルでMambaとTransformerのハイブリッドモデルと、通常のTransformerモデルを比較している。学習データに15 Trillion Tokenを利用しており、このデータ量でのApple to Appleのアーキテクチャ間の比較は、現状では最も大規模なものとのこと。性能は多くのベンチマークでハイブリッドにしても同等、Commonsense Understandingでは上回っている。<br><br>また、学習したNemotron-Hをバックボーンモデルとして持つVLMについてもモデルのアーキテクチャが述べられている。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="modernbert-1606" class="title-link">ModernBERT, AnswerDotAI, 2024.12</h3>
<br><a href="https://github.com/AnswerDotAI/ModernBERT" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1606" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="pretrained-LM.html" target="_blank" rel="noopener noreferrer">#pretrained-LM</a>
<span class="issue_date">Issue Date: 2024-12-20</span>
<span class="snippet"><span>GPT Summary</span>- ModernBERTは、エンコーダ専用のトランスフォーマーモデルで、従来のBERTに比べて大幅なパレート改善を実現。2兆トークンで訓練され、8192シーケンス長を持ち、分類タスクやリトリーバルで最先端の結果を示す。速度とメモリ効率も優れており、一般的なGPUでの推論に最適化されている。</span>
<span class="snippet"><span>Comment</span><p>最近の進化しまくったTransformer関連のアーキテクチャをEncodnr-OnlyモデルであるBERTに取り込んだら性能上がるし、BERTの方がコスパが良いタスクはたくさんあるよ、系の話、かつその実装だと思われる。<br>テクニカルペーパー中に記載はないが、評価データと同じタスクでのDecoder-Onlyモデル（SFT有り無し両方）との性能を比較したらどの程度の性能なのだろうか？</p>
<p>そもそも学習データが手元にあって、BERTをFinetuningするだけで十分な性能が出るのなら（BERTはGPU使うのでそもそもxgboostとかでも良いが）、わざわざLLM使う必要ないと思われる。BERTのFinetuningはそこまで時間はかからないし、inferenceも速い。<br><br>参考:<br>- <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1024" target="_blank" rel="noopener noreferrer">Prompt2Model: Generating Deployable Models from Natural Language   Instructions, Vijay Viswanathan+, N/A, EMNLP'23</a>
</p>
<p>日本語解説:


<a href="https://zenn.dev/dev_commune/articles/3f5ab431abdea1?utm_source=substack&utm_medium=email" target="_blank" rel="noopener noreferrer">https://zenn.dev/dev_commune/articles/3f5ab431abdea1?utm_source=substack&utm_medium=email</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="fast-llm-1599" class="title-link">Fast LLM Inference From Scratch, Andrew Chan, 2024.12</h3>
<br><a href="https://andrewkchan.dev/posts/yalm.html" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1599" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2024-12-17</span>
<span class="snippet"><span>Comment</span><p>ライブラリを使用せずにC++とCUDAを利用してLLMの推論を実施する方法の解説記事</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="tensorrt-llmによる推論高速化-1518" class="title-link">TensorRT-LLMによる推論高速化, Hiroshi Matsuda, NVIDIA AI Summit 2024.11 </h3>
<br><a href="https://docs.google.com/presentation/d/1IcBX1DuqnLwhV0JLH1BHxoDou6rgJFZMifmYiSexsqM/mobilepresent?slide=id.g3014aeec0cb_0_5" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1518" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Slide.html" target="_blank" rel="noopener noreferrer">#Slide</a>
<span class="issue_date">Issue Date: 2024-11-14</span>
<span class="snippet"><span>Comment</span><p>元ポスト:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hmtd223/status/1856887876665184649?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>非常に興味深いので後で読む</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="zero-deepspeedの紹介-1487" class="title-link">ZeRO: DeepSpeedの紹介, レトリバ, 2021.07 </h3>
<br><a href="https://tech.retrieva.jp/entry/2021/07/26/102514" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1487" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="Pretraining.html" target="_blank" rel="noopener noreferrer">#Pretraining</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="One-Line%20Notes.html" target="_blank" rel="noopener noreferrer">#One-Line Notes</a>
<a class="button" href="Reference%20Collection.html" target="_blank" rel="noopener noreferrer">#Reference Collection</a>
<span class="issue_date">Issue Date: 2024-11-07</span>
<span class="snippet"><span>Comment</span><p>ZeROの説明がわかりやすい</p>
<p>こちらの記事もわかりやすい<br><br>


<a href="https://zenn.dev/turing_motors/articles/d00c46a79dc976" target="_blank" rel="noopener noreferrer">https://zenn.dev/turing_motors/articles/d00c46a79dc976</a>


</p>
<p>DeepSpeedのコンフィグの一覧<br><br>


<a href="https://www.deepspeed.ai/docs/config-json/" target="_blank" rel="noopener noreferrer">https://www.deepspeed.ai/docs/config-json/</a>


</p>
<p>transformersにおけるdeepspeedのドキュメント:<br>


<a href="https://huggingface.co/transformers/v4.9.2/main_classes/deepspeed.html" target="_blank" rel="noopener noreferrer">https://huggingface.co/transformers/v4.9.2/main_classes/deepspeed.html</a>


</p>
<p>参考: deepspeedの使い方まとめ<br>


<a href="https://note.com/fukudawataru/n/n5152e6f587c8" target="_blank" rel="noopener noreferrer">https://note.com/fukudawataru/n/n5152e6f587c8</a>


</p>
<p>ZeRO Stage3を使う場合、ページ後方にしれっととんでもなく重要なことが書いてあるので気をつけましょう。。。。<br><br>


<a href="https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/deepspeed#constructing-massive-models" target="_blank" rel="noopener noreferrer">https://huggingface.co/docs/transformers/v4.17.0/en/main_classes/deepspeed#constructing-massive-models</a>


<br><br><br><br><img src="https://github.com/user-attachments/assets/677b6656-1302-4b1b-8be6-ca954c7edda6" alt="image" loading="lazy" width="400" height="300"><br><br></p>
<p>ZeROはparameterとoptimizerのmemory footprintの最適化を頑張っていて、activation memory footprint（バッチをforward passに流す時に消費されるメモリ）の削減は、tiling, activation/gradient checkpointingとかで頑張ってねという<br><br><br><br>という話が本家issueの4047に記載されている。</p>
<p>結論: つまづいたらDeepSpeedのIssueをエラーメッセージで検索かけるのが一番効果的</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="lingua-1479" class="title-link">Lingua, Meta</h3>
<br><a href="https://github.com/facebookresearch/lingua/" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1479" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<a class="button" href="MinimalCode.html" target="_blank" rel="noopener noreferrer">#MinimalCode</a>
<span class="issue_date">Issue Date: 2024-11-05</span>
<span class="snippet"><span>Comment</span><p>研究目的のための、minimal、かつ高速なLLM training/inferenceのコードが格納されたリポジトリ。独自のモデルやデータ、ロスなどが簡単に実装できる模様。<br><br><img src="https://github.com/user-attachments/assets/47f70515-3de0-455f-9fc4-0e2e17442eed" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="introducing-quantized-1471" class="title-link">Introducing quantized Llama models with increased speed and a reduced memory footprint, Meta, 2024.10</h3>
<br><a href="https://ai.meta.com/blog/meta-llama-quantized-lightweight-models/" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1471" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Quantization.html" target="_blank" rel="noopener noreferrer">#Quantization</a>
<a class="button" href="Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2024-10-26</span>
</article>
<article class="paper-entry">
<h3 id="unsloth-1450" class="title-link">Unsloth</h3>
<br><a href="https://github.com/unslothai/unsloth" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1450" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="InstructionTuning.html" target="_blank" rel="noopener noreferrer">#InstructionTuning</a>
<span class="issue_date">Issue Date: 2024-10-08</span>
<span class="snippet"><span>Comment</span><p>single-GPUで、LLMのLoRA/QLoRAを高速/省メモリに実行できるライブラリ</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="llmの効率化・高速化を支えるアルゴリズム-1419" class="title-link">LLMの効率化・高速化を支えるアルゴリズム, Tatsuya Urabe, 2024.09</h3>
<br><a href="https://speakerdeck.com/taturabe/llmnoxiao-lu-hua-wozhi-eruarugorizumu" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1419" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="Pocket.html" target="_blank" rel="noopener noreferrer">#Pocket</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2024-09-25</span>
</article>
<article class="paper-entry">
<h3 id="sohu-1399" class="title-link">Sohu, etched, 2024.06</h3>
<br><a href="https://www.etched.com/announcing-etched" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1399" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Chip.html" target="_blank" rel="noopener noreferrer">#Chip</a>
<span class="issue_date">Issue Date: 2024-09-18</span>
<span class="snippet"><span>Comment</span><p>&gt;By burning the transformer architecture into our chip, we can’t run most traditional AI models: the DLRMs powering Instagram ads, protein-folding models like AlphaFold 2, or older image models like Stable Diffusion 2. We can’t run CNNs, RNNs, or LSTMs either.<br><br>transformer以外の大抵のモデルでは動作しないが、代わりにH-100よりも20倍早いinferenceを実現できるチップらしい。<br><img src="https://github.com/user-attachments/assets/1fb2c6b4-3837-4bec-9a64-f8b4878e5941" alt="image" loading="lazy" width="400" height="300"><br><br>&gt;With over 500,000 tokens per second in Llama 70B throughput, Sohu lets you build products impossible on GPUs.<br><br>いやいやいやLlama-70Bで0.5M Token/secは早すぎる！！！</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="liger-kernel-1356" class="title-link">Liger-Kernel, 2024.08</h3>
<br><a href="https://github.com/linkedin/Liger-Kernel" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1356" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<span class="issue_date">Issue Date: 2024-08-25</span>
<span class="snippet"><span>Comment</span><p>LLMを学習する時に、ワンライン追加するだけで、マルチGPUトレーニングのスループットを20%改善し、メモリ使用量を60%削減するらしい<br><br>元ツイート:



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/hsu_byron/status/1827072737673982056?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


<p>これだけでいい<br><img src="https://github.com/user-attachments/assets/abce24ed-f979-43db-ac51-e850f2ae877a" alt="image" loading="lazy" width="400" height="300"></p>
<p>Unsloth <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1450" target="_blank" rel="noopener noreferrer">Unsloth</a>
 はLoRA/QLoRAが可能な一方でまだMulti-GPUはサポートしていない。一方、Liger-KernelはLoRAよりもfull-parameter tuningとMulti-GPUにフォーカスしており、目的に応じて使い分けが必要。<br><br><br><br>


<a href="https://github.com/linkedin/Liger-Kernel/issues/57" target="_blank" rel="noopener noreferrer">https://github.com/linkedin/Liger-Kernel/issues/57</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="deepspeed-1343" class="title-link">DeepSpeed, vLLM, CTranslate2 で rinna 3.6b の生成速度を比較する, 2024.06</h3>
<br><a href="https://zenn.dev/rinna/articles/5fd4f3cc12f7c5" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1343" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="python.html" target="_blank" rel="noopener noreferrer">#python</a>
<a class="button" href="Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="OpenWeight.html" target="_blank" rel="noopener noreferrer">#OpenWeight</a>
<a class="button" href="LLMServing.html" target="_blank" rel="noopener noreferrer">#LLMServing</a>
<span class="issue_date">Issue Date: 2024-08-05</span>
<span class="snippet"><span>Comment</span><p>[vllm](


<a href="https://github.com/vllm-project/vllm)%E3%82%92%E4%BD%BF%E3%81%86%E3%81%AE%E3%81%8C%E4%B8%80%E7%95%AA%E3%81%8A%E6%89%8B%E8%BB%BD%E3%81%A7%E3%80%81inference%E9%80%9F%E5%BA%A6%E3%81%8C%E9%80%9F%E3%81%9D%E3%81%86%E3%80%82PagedAttention%E3%81%A8%E5%91%BC%E3%81%B0%E3%82%8C%E3%82%8B%E3%82%AD%E3%83%A3%E3%83%83%E3%82%B7%E3%83%A5%E3%82%92%E5%88%A9%E7%94%A8%E3%81%97%E3%81%A6%E9%AB%98%E9%80%9F%E5%8C%96%E3%81%97%E3%81%A6%E3%81%84%E3%82%8B%E3%81%A3%E3%81%BD%E3%81%84%E3%80%82" target="_blank" rel="noopener noreferrer">https://github.com/vllm-project/vllm)を使うのが一番お手軽で、inference速度が速そう。PagedAttentionと呼ばれるキャッシュを利用して高速化しているっぽい。</a>


<br><br>（図はブログ中より引用）<br><br><br><br><img src="https://github.com/user-attachments/assets/36c83605-f925-4e19-b06c-2059c837e359" alt="image" loading="lazy" width="400" height="300"><br><br></p>
<p>こちらも参照のこと<br><br>vLLMの仕組みをざっくりと理解する：


<a href="https://dalab.jp/archives/journal/vllm/#PagedAttention" target="_blank" rel="noopener noreferrer">https://dalab.jp/archives/journal/vllm/#PagedAttention</a>


</p>
<p>vLLMでReasoning ModelをServingするときは、`--enable-reasoning`等の追加オプションを指定する必要がある点に注意<br>


<a href="https://docs.vllm.ai/en/stable/features/reasoning_outputs.html" target="_blank" rel="noopener noreferrer">https://docs.vllm.ai/en/stable/features/reasoning_outputs.html</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="airllm-1297" class="title-link">AirLLM, 2024.04</h3>
<br><a href="https://github.com/lyogavin/Anima/tree/main/air_llm" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1297" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<span class="issue_date">Issue Date: 2024-04-28</span>
<span class="snippet"><span>Comment</span><p>4GBのSingle GPUで、70Bモデルのinferenceを実現できるライブラリ。トークンの生成速度は検証する必要がある。transformer decoderの各layerの演算は独立しているため、GPUに全てのlayerを載せず、必要な分だけ載せてinferenceするといった操作を繰り返す模様。<br><br>元ツイート: 



</p>
<div class="tweet-embed" style="min-height:400px; max-width:550px; margin:1em auto;" data-embed='&lt;blockquote class="twitter-tweet"&gt;&lt;a href="https://twitter.com/rohanpaul_ai/status/1784349737899982943?s=46&t=Y6UuIHB0Lv0IpmFAjlc2-Q"&gt;&lt;/a&gt;&lt;/blockquote&gt;'>
  <div class="tweet-placeholder">Loading…</div>
</div>


</span><br><br>
</article>
<article class="paper-entry">
<h3 id="optimize-llm-1188" class="title-link">optimize-llm, HuggingFace</h3>
<br><a href="https://huggingface.co/blog/optimize-llm" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1188" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<span class="issue_date">Issue Date: 2023-12-15</span>
<span class="snippet"><span>Comment</span><p>LLMをoptimizeする実用的なチュートリアル</p>
<p>こちらも有用なので参照のこと<br><br><br><br>【GPU inference】<br><br>


<a href="https://huggingface.co/docs/transformers/main/perf_infer_gpu_one" target="_blank" rel="noopener noreferrer">https://huggingface.co/docs/transformers/main/perf_infer_gpu_one</a>


<br><br></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="【続】flash-attentionを使ってllmの推論を高速・軽量化できるか？-1187" class="title-link">【続】Flash Attentionを使ってLLMの推論を高速・軽量化できるか？</h3>
<br><a href="https://qiita.com/jovyan/items/5716cd83e246df4a158e" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1187" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2023-12-14</span>
<span class="snippet"><span>Comment</span><p>use_cacheがTrue/Falseの場合のFlashAttention2のinference timeとVRAM使用量の傾向をsequence_lengthごとに考察している。<br><br>use_cacheはKey Value cacheのオンオフを切り替えられるオプションである。autoregressiveなモデルのinference時には、何度も同じinput tokenに対するKVの計算が生じるため（M番目のトークンを生成した後、M+1番目のトークンの生成をする場合、M-1番目までのトークンのKVを再計算せねばならない）、cacheをすることで大幅に計算速度が改善される。<br><br>use_cacheをTrueにできるならFlashAttention2の恩恵は小さい（inference timeが少し早くなるのみ）ため、潤沢なVRAMがあるなら得られる恩恵は小さい。<br>逆にVRAM節約してuse_cacheをFalseにせざるを得ないのであれば、FlashAttention2によりVRAM使用量をsequence_legthの線形に抑えることができ、かつinference timeも短くなる。<br><br>↑上記はあくまでinferenceをする場合のみの話であり（train時はautoregressive modelではcausal maskを用い、teacher forcingで並列にトークンを生成するためそもそもKV-cacheする意味がない）、trainingをする場合FlashAttention2で大幅にVRAM使用量を減らせるので、そこは分けて考えること。<br>


<a href="https://qiita.com/jovyan/items/ff3d0a49163c7afa33ce" target="_blank" rel="noopener noreferrer">https://qiita.com/jovyan/items/ff3d0a49163c7afa33ce</a>


</p>
<p>Flash Attentionを使ってLLMの推論を高速・軽量化できるか？<br>


<a href="https://qiita.com/jovyan/items/11deb9d4601e4705a60d" target="_blank" rel="noopener noreferrer">https://qiita.com/jovyan/items/11deb9d4601e4705a60d</a>


<br><br>こちらの記事も非常に勉強になる</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="gpt4all-1150" class="title-link">GPT4All, 2023</h3>
<br><a href="https://github.com/nomic-ai/gpt4all" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1150" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="Tools.html" target="_blank" rel="noopener noreferrer">#Tools</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Repository.html" target="_blank" rel="noopener noreferrer">#Repository</a>
<span class="issue_date">Issue Date: 2023-11-21</span>
<span class="snippet"><span>Comment</span><p>ローカルマシンでChatGPT likeなUIでチャットボットを動作させられるOpensource。<br>Mistral7BやGGUFフォーマットのモデルのよつな（おそらく量子化されたものも含む）ローカルマシンで動作させられる規模感のモデルがサポートされている。<br>


<a href="https://gpt4all.io/index.html" target="_blank" rel="noopener noreferrer">https://gpt4all.io/index.html</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="tsuzumi-1111" class="title-link">tsuzumi, NTT’23</h3>
<br><a href="https://www.rd.ntt/research/LLM_tsuzumi.html" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1111" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="MultiModal.html" target="_blank" rel="noopener noreferrer">#MultiModal</a>
<a class="button" href="FoundationModel.html" target="_blank" rel="noopener noreferrer">#FoundationModel</a>
<a class="button" href="Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2023-11-01</span>
<span class="snippet"><span>Comment</span><p>NTT製のLLM。パラメータ数は7Bと軽量だが高性能。<br>MTBenchのようなGPT4に勝敗を判定させるベンチマークで、地理、歴史、政治、社会に関する質問応答タスク（図6）でgpt3.5turboと同等、国産LLMの中でトップの性能。GPT3.5turboには、コーディングや数学などの能力では劣るとのこと。<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/d064e0dc-b598-4853-9466-f56f39986acc" alt="image" loading="lazy" width="400" height="300"><br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/c8251b2e-f865-4069-a3b7-9bfb848554bb" alt="image" loading="lazy" width="400" height="300"><br>&gt; ＊6 Rakudaベンチマーク<br>日本語の言語モデルの性能を評価するベンチマークの一つで、日本の地理・政治・歴史・社会に関する質問応答タスクによって評価を行う。<br>URL：


<a href="https://yuzuai.jp/benchmark" target="_blank" rel="noopener noreferrer">https://yuzuai.jp/benchmark</a>


<br><br>&gt;＊7 Japanese Vicuna QAベンチマーク<br>Rakudaよりもさらに幅広いカテゴリで言語モデルのQAや指示遂行の能力を問う評価方法。一般知識、ロールプレイなど多数の質問から構成される。<br>URL：


<a href="https://github.com/hitoshizuku7/LLM_Judge_ku/blob/main/README.md" target="_blank" rel="noopener noreferrer">https://github.com/hitoshizuku7/LLM_Judge_ku/blob/main/README.md</a>


</p>
<p>tsuzumiはアダプタを追加することで、モデル全体のパラメータを更新することなく、さまざまな知識を持たせたり、振る舞いを変えたりできるようになるとのこと（LoRAアダプタのようなものだと思われる）。<br>まて、将来的に視覚や聴覚などのマルチモーダル対応も実施。</p>
<p>思想がLoRA Hub <a href="https://github.com/AkihikoWatanabe/paper_notes/issues/917" target="_blank" rel="noopener noreferrer">LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA   Composition, Chengsong Huang+, N/A, COLM'24</a>
 に近く、アダプタを着脱すれば柔軟に生成を変えられるのは有用だと思う。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="大規模言語モデルのfine-tuningによるドメイン知識獲得の検討-1109" class="title-link">大規模言語モデルのFine-tuningによるドメイン知識獲得の検討, PFN Blog, 2023.10</h3>
<br><a href="https://tech.preferred.jp/ja/blog/llm-fine-tuning-for-domain-knowledge/" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1109" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="PEFT(Adaptor_LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="Catastrophic%20Forgetting.html" target="_blank" rel="noopener noreferrer">#Catastrophic Forgetting</a>
<span class="issue_date">Issue Date: 2023-10-29</span>
<span class="snippet"><span>Comment</span><p>以下記事中で興味深かった部分を引用<br>&gt; まとめると、LoRAは、[3]で言われている、事前学習モデルは大量のパラメータ数にもかかわらず低い固有次元を持ち、Fine-tuningに有効な低次元のパラメータ化も存在する、という主張にインスパイアされ、ΔWにおける重みの更新の固有次元も低いという仮説のもとで、低ランク行列で学習する手法になります。<br><br>LoRAが拠り所とする仮説が説明されており、勉強になった。<br><br>&gt; こうしたニューラルネットワークを圧縮する他の技術には枝刈りや知識蒸留がありますが、量子化は、ほとんどの場合に枝刈りより優れているとされ[5]、蒸留よりも手軽に高精度なモデルが得られる可能性が高く、LLMにおいても有力な技術と考えられます。<br><br>これも知らなかったし、文献付きで記述されていることが大変ありがたい。<br><br>&gt; QLoRA以外のLoRAの派生手法としては、ランクを適応的に定めるAdaLoRA[7] やDyLoRA[8]、コンテキスト長を拡大できるLongLoRA[9]、行列Aの重みをfreezeすることでさらに軽量化を行うLoRA-FA、行列積をアダマール積やクロネッカー積で計算するLoHAやLoKRなどがあります（一部はLLMではなくStable Diffusionの学習で用いられる手法の通称です）。<br><br>この辺は実際にLoRAを使うことになったら勉強したい。<br><br>&gt; 言語モデルの学習は通常、Causal LMの場合は、Next Token PredictionにおけるPerplexityの最小化による教師なし学習によって最適化されます。<br><br>HuggingFaceの実装の話だと思うが、そうだろうなと思ってはいたがソースを確認できていなかったので勉強になった。<br><br>&gt; 7Bのモデルでは、以下のグラフのように、データの件数を増やすと学習がうまくいかないという結果が得られました。また、LoRAのランクは低い方が学習が安定することがわかりました。正答率が著しく低いものは、学習時のロス（交差エントロピー）が非常に大きくなっており、選択肢を間違えるというよりは言語モデルとしての機能が失われていました。<br><br>&gt; 他には、Instructionデータ（1つのクイズのQ&amp;A）が2500件を超えるとロスが悪化することや、2000件でも2epoch繰り返すとcatastrophic forgettingが見られ、言語モデルそのものの性能が失われ意味のない出力をしていました。[17] でも言及されていますが、日本語の学習では、数BのモデルにおけるLoRAによるInstruction Tuningはあまり効果が得られない可能性が高いと考えられます。<br><br>&gt; 一方、13Bのモデルでは、8、16、32、64いずれのランクでも大きな差は見られませんでした。<br>&gt; これらから、Addtional Trainingで学習させるデータがInstruction Tuningに対して膨大である場合には先に学習した方がよく、少数の場合は後に学習させてもInstruction Tuningの効果には悪影響がないということが示唆されました。<br><br>&gt; また学習は、初期学習率を小さくした方が安定する可能性が高いと思われます。LoRAの論文[2] ではGPTのFine-tuneは2e-4で行われており、hugging faceの実装でもデフォルトでは2e-4となっていますが、他の論文やブログでは3e-5での例などもあります。しかし、単に下げれば安定するということでもなく、１回の試行における計算コストとチューニングがトレードオフになる可能性はあります。<br><br>Additional TrainingとはFinetuningのことで便宜上の本ブログでの呼称。実際の文書中では図が複数個挟まれている。<br>こうした実際に手を動かした上でないと得られない知見を公開してくれるのは非常にありがたいことだし、日本語データでLoRAをする際に非常に参考になりそう。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="stablediffusion-1107" class="title-link">StableDiffusion, LLMのGPUメモリ削減のあれこれ</h3>
<br><a href="https://qiita.com/nishiha/items/c1f31b4bd0a732a57985" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1107" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="DiffusionModel.html" target="_blank" rel="noopener noreferrer">#DiffusionModel</a>
<a class="button" href="Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2023-10-29</span>
<span class="snippet"><span>Comment</span><p>Gradient Accumulation, Gradient Checkpointingの説明が丁寧でわかりやすかった。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="flashattention-2-faster-899" class="title-link">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning, 2023</h3>
<br><a href="https://tridao.me/publications/flash2/flash2.pdf" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/899" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="MachineLearning.html" target="_blank" rel="noopener noreferrer">#MachineLearning</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="Attention.html" target="_blank" rel="noopener noreferrer">#Attention</a>
<span class="issue_date">Issue Date: 2023-07-23</span>
<span class="snippet"><span>GPT Summary</span>- FlashAttention-2は、長いシーケンス長におけるTransformerのスケーリングの問題に対処するために提案された手法です。FlashAttention-2は、非対称なGPUメモリ階層を利用してメモリの節約とランタイムの高速化を実現し、最適化された行列乗算に比べて約2倍の高速化を達成します。また、FlashAttention-2はGPTスタイルのモデルのトレーニングにおいても高速化を実現し、最大225 TFLOPs/sのトレーニング速度に達します。</span>
<span class="snippet"><span>Comment</span><p>Flash Attention1よりも2倍高速なFlash Attention 2</p>
<p>Flash Attention1はこちらを参照<br>


<a href="https://arxiv.org/pdf/2205.14135.pdf" target="_blank" rel="noopener noreferrer">https://arxiv.org/pdf/2205.14135.pdf</a>


<br><br>QK Matrixの計算をブロックに分けてSRAMに送って処理することで、3倍高速化し、メモリ効率を10-20倍を達成。<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/935f61f3-97ce-4e76-826b-040f92ca567c" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="llama2を3行で訓練-886" class="title-link">LLaMA2を3行で訓練</h3>
<br><a href="https://huggingface.co/docs/trl/main/en/lora_tuning_peft#finetuning-llama2-model" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/886" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="Quantization.html" target="_blank" rel="noopener noreferrer">#Quantization</a>
<a class="button" href="PEFT(Adaptor_LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<span class="issue_date">Issue Date: 2023-07-22</span>
<span class="snippet"><span>Comment</span><p>LLaMA2を3行で、1つのA100GPU、QLoRAで、自前のデータセットで訓練する方法</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="assisted-generation-675" class="title-link">Assisted Generation: a new direction toward low-latency text generation, 2023</h3>
<br><a href="https://huggingface.co/blog/assisted-generation" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/675" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="python.html" target="_blank" rel="noopener noreferrer">#python</a>
<span class="issue_date">Issue Date: 2023-05-11</span>
<span class="snippet"><span>Comment</span><p>1 line加えるとtransformerのgenerationが最大3倍程度高速化されるようになったらしい</p>
<p><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/fecc1c5e-b9e5-4844-af96-ba48c3d60fae" alt="image" loading="lazy" width="400" height="300"><br><br>assistant modelをロードしgenerateに引数として渡すだけ<br><img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/7dabf3bf-cd32-469c-abba-f1269318576d" alt="image" loading="lazy" width="400" height="300"></p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="training-a-531" class="title-link">Training a recommendation model with dynamic embeddings</h3>
<br><a href="https://blog.tensorflow.org/2023/04/training-recommendation-model-with-dynamic-embeddings.html?m=1" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/531" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="RecommenderSystems.html" target="_blank" rel="noopener noreferrer">#RecommenderSystems</a>
<a class="button" href="Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="Embeddings.html" target="_blank" rel="noopener noreferrer">#Embeddings</a>
<a class="button" href="Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<span class="issue_date">Issue Date: 2023-04-25</span>
<span class="snippet"><span>Comment</span><p>dynamic embeddingを使った推薦システムの構築方法の解説</p>
<p>（理解が間違っているかもしれないが）推薦システムは典型的にはユーザとアイテムをベクトル表現し、関連度を測ることで推薦をしている。この枠組みをめっちゃスケールさせるととんでもない数のEmbeddingを保持することになり、メモリ上にEmbeddingテーブルを保持して置けなくなる。特にこれはonline machine learning（たとえばユーザのセッションがアイテムのsequenceで表現されたとき、そのsequenceを表すEmbeddingを計算し保持しておき、アイテムとの関連度を測ることで推薦するアイテムを決める、みたいなことが必要）では顕著である（この辺の理解が浅い）。しかし、ほとんどのEmbeddingはrarely seenなので、厳密なEmbeddingを保持しておくことに実用上の意味はなく、それらを単一のベクトルでできるとメモリ節約になって嬉しい（こういった処理をしてもtopNの推薦結果は変わらないと思われるので）。<br>これがdynamic embeddingのモチベであり、どうやってそれをTFで実装するか解説している。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="lora論文解説-528" class="title-link">LoRA論文解説, Hayato Tsukagoshi, 2023.04</h3>
<br><a href="https://speakerdeck.com/hpprc/lun-jiang-zi-liao-lora-low-rank-adaptation-of-large-language-models" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/528" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="LanguageModel.html" target="_blank" rel="noopener noreferrer">#LanguageModel</a>
<a class="button" href="Supervised-FineTuning%20(SFT).html" target="_blank" rel="noopener noreferrer">#Supervised-FineTuning (SFT)</a>
<a class="button" href="PEFT(Adaptor_LoRA).html" target="_blank" rel="noopener noreferrer">#PEFT(Adaptor/LoRA)</a>
<a class="button" href="Slide.html" target="_blank" rel="noopener noreferrer">#Slide</a>
<a class="button" href="PostTraining.html" target="_blank" rel="noopener noreferrer">#PostTraining</a>
<a class="button" href="Selected%20Papers_Blogs.html" target="_blank" rel="noopener noreferrer">#Selected Papers/Blogs</a>
<span class="issue_date">Issue Date: 2023-04-25</span>
<span class="snippet"><span>Comment</span><p>ベースとなる事前学習モデルの一部の線形層の隣に、低ランク行列A,Bを導入し、A,Bのパラメータのみをfinetuningの対象とすることで、チューニングするパラメータ数を激減させた上で同等の予測性能を達成し、推論速度も変わらないようにするfinetuning手法の解説</p>
<p>LoRAを使うと、でかすぎるモデルだと、そもそもGPUに載らない問題や、ファインチューニング後のモデルファイルでかすぎワロタ問題が回避できる。<br><br>前者は事前学習済みモデルのBPのための勾配を保存しておく必要がなくなるため学習時にメモリ節約になる。後者はA,Bのパラメータだけ保存すればいいので、ストレージの節約になる。<br><br>かつ、学習速度が25%程度早くなる。</p>
<p>既存研究であるAdapter（transformerの中に学習可能なMLPを差し込む手法）は推論コストが増加し、prefix tuningは学習が非常に難しく、高い性能を達成するためにprefixとして128 token入れたりしなければならない。</p>
<p>huggingfaceがすでにLoRAを実装している<br>


<a href="https://github.com/huggingface/peft" target="_blank" rel="noopener noreferrer">https://github.com/huggingface/peft</a>


</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="fastseq-make-384" class="title-link">FastSeq: Make Sequence Generation Faster, Yan+, ACL’21</h3>
<br><a href="https://arxiv.org/abs/2106.04718" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/384" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="NLP.html" target="_blank" rel="noopener noreferrer">#NLP</a>
<a class="button" href="Transformer.html" target="_blank" rel="noopener noreferrer">#Transformer</a>
<a class="button" href="ACL.html" target="_blank" rel="noopener noreferrer">#ACL</a>
<span class="issue_date">Issue Date: 2021-06-10</span>
<span class="snippet"><span>Comment</span><p>BART, DistilBART, T5, GPT2等のさまざまなTransformer-basedな手法で、4-9倍Inference speedを向上させる手法を提案。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="intel-mkl-373" class="title-link">intel MKL</h3>
<br><a href="https://qiita.com/f0o0o/items/69d9b766008091a6e698" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/373" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="Library.html" target="_blank" rel="noopener noreferrer">#Library</a>
<a class="button" href="python.html" target="_blank" rel="noopener noreferrer">#python</a>
<a class="button" href="Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<span class="issue_date">Issue Date: 2021-06-03</span>
<span class="snippet"><span>Comment</span><p>intel CPUでpythonの数値計算を高速化するライブラリ(numpyとかはやくなるらしい; Anacondaだとデフォルトで入ってるとかなんとか)</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="efficientnet解説-346" class="title-link">EfficientNet解説, omiita （オミータ）, 2019</h3>
<br><a href="https://qiita.com/omiita/items/83643f78baabfa210ab1" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/346" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<a class="button" href="ComputerVision.html" target="_blank" rel="noopener noreferrer">#ComputerVision</a>
<a class="button" href="Blog.html" target="_blank" rel="noopener noreferrer">#Blog</a>
<a class="button" href="ImageClassification.html" target="_blank" rel="noopener noreferrer">#ImageClassification</a>
<span class="issue_date">Issue Date: 2021-05-24</span>
<span class="snippet"><span>Comment</span><p>既存画像認識モデルの構造は変化させず、広さ、深さ、解像度を複合スケーリングすることで、従来よりも少ないパラメータ数、かつ学習速度でSoTAを達成。広さ、深さ、解像度はそれぞれ性能に互いに影響しあっており、従来のように別々にスケーリングするのではなく、3つのバランスをとりながらスケーリングする。スケーリングする際は、結果的にはそれぞれをある値で定数倍すれば良く、そのある値は最大メモリや最大FLOPS数以下（およびFLOPSが2のΦ乗で増加するような）といった制約下でAccuracyが最大化される値をグリッドサーチで見つける（らしい。ざっくりとした理解）。<br>転移学習しても多くのタスクでSoTA達成した。</p></span><br><br>
</article>
<article class="paper-entry">
<h3 id="efficient-methods-81" class="title-link">Efficient Methods and Hardware for Deep Learning, Song Han, Stanford University, 2017.05</h3>
<br><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture15.pdf" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
Paper/Blog Link
</a><a href="https://github.com/AkihikoWatanabe/paper_notes/issues/81" target="_blank" rel="noopener noreferrer" class="external-link-badge"><svg width="14" height="14" viewbox="0 0 24 24" aria-hidden="true">
  <path d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3z"></path>
  <path d="M5 5h5V3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2v-5h-2v5H5V5z"></path>
</svg>
My Issue
</a><br>
<a class="button" href="Article.html" target="_blank" rel="noopener noreferrer">#Article</a>
<a class="button" href="NeuralNetwork.html" target="_blank" rel="noopener noreferrer">#NeuralNetwork</a>
<a class="button" href="Tutorial.html" target="_blank" rel="noopener noreferrer">#Tutorial</a>
<span class="issue_date">Issue Date: 2017-12-31</span>
</article>
</div>
<script>
document.addEventListener("DOMContentLoaded", function() {
  // Twitterのwidgets.jsを動的に一度だけ読み込む関数
  let twitterScriptLoaded = false;
  function loadTwitterScript() {
    if (!twitterScriptLoaded) {
      const script = document.createElement('script');
      script.src = "https://platform.twitter.com/widgets.js";
      script.charset = "utf-8";
      script.async = true;
      document.body.appendChild(script);
      twitterScriptLoaded = true;
    }
  }

  // Intersection Observerの設定
  const observer = new IntersectionObserver((entries, obs) => {
    entries.forEach(entry => {
      if (entry.isIntersecting) {
        // 画面に入った時だけスクリプトをロード開始
        loadTwitterScript();

        const container = entry.target;
        const embedHtml = container.getAttribute('data-embed');
        
        if (embedHtml) {
          container.innerHTML = embedHtml;
          container.removeAttribute('data-embed');
          
          // ウィジェットの再スキャン（twttrオブジェクトが準備できていれば実行）
          if (window.twttr && window.twttr.widgets) {
            window.twttr.widgets.load(container);
          }
        }
        obs.unobserve(container);
      }
    });
  }, { rootMargin: '200px', threshold: 0.01 }); // 少し早めに読み込む

  document.querySelectorAll('.tweet-embed').forEach(el => observer.observe(el));
});
</script>


    </div>

</article>
<div class="post-nav">
<a class="previous" href="/paper_notes/articles/ContextEngineering/" title="ContextEngineeringに関する論文・技術記事メモの一覧">ContextEngineeringに関する論文・技術記事メモの一覧</a><a class="next" href="/paper_notes/articles/EmbodiedAI/" title="EmbodiedAIに関する論文・技術記事メモの一覧">EmbodiedAIに関する論文・技術記事メモの一覧</a>
</div>
<div class="post-related">
      <div>Related Articles</div>
      <ul>
        <li class="">
          <a class="post-link" href="/paper_notes/articles/SpatialUnderstanding/" title="SpatialUnderstandingに関する論文・技術記事メモの一覧">
            SpatialUnderstandingに関する論文・技術記事メモの一覧

<span class="post-badges"><span class="post-badge badge-top">📝 11</span> 
  <span class="post-badge badge-new">📝 11</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/NaturalLanguageUnderstanding/" title="NaturalLanguageUnderstandingに関する論文・技術記事メモの一覧">
            NaturalLanguageUnderstandingに関する論文・技術記事メモの一覧

<span class="post-badges"><span class="post-badge badge-top">📝 3</span> 
  <span class="post-badge badge-new">📝 3</span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/Test-time-Learning/" title="Test-time Learningに関する論文・技術記事メモの一覧">
            Test-time Learningに関する論文・技術記事メモの一覧

<span class="post-badges"><span class="post-badge badge-top">📝 </span> 
  <span class="post-badge badge-new">📝 </span>
</span>
</a>
        </li>
<li class="">
          <a class="post-link" href="/paper_notes/articles/Trustfulness/" title="Trustfulnessに関する論文・技術記事メモの一覧">
            Trustfulnessに関する論文・技術記事メモの一覧

<span class="post-badges"><span class="post-badge badge-top">📝 8</span> 
  <span class="post-badge badge-new">📝 8</span>
</span>
</a>
        </li>
</ul>
    </div>
<div class="post-comments"></div></section>
</div>


  </section>
  <section class="sidebar" style="margin-left: 15px;">
    <!-- Get sidebar items --><style type="text/css" media="screen">
/* --- レイアウト用（前回と同じ） --- */
.post-menu {
  position: -webkit-sticky;
  position: sticky;
  top: 20px;
  max-height: calc(100vh - 40px);
  display: flex;
  flex-direction: column;
}

.post-menu-title {
  flex-shrink: 0;
  margin-bottom: 10px;
  font-weight: bold;
}

.post-menu-content {
  overflow-y: auto;
  scrollbar-width: thin;
}

.post-menu ul {
  list-style: none;
  padding: 0;
  margin: 0;
}

/* --- 開閉アニメーションとアイコン用 --- */

/* h2のスタイル：クリックできるようにする */
.post-menu li.h-h2 {
  cursor: pointer;
  position: relative;
  padding-left: 15px; /* アイコン用のスペース */
  font-weight: bold;
  margin-top: 5px;
}

/* 開閉アイコン（▼） */
.post-menu li.h-h2::before {
  content: '';
  display: inline-block;
  width: 0;
  height: 0;
  border-style: solid;
  border-width: 5px 0 5px 6px; /* 三角形 */
  border-color: transparent transparent transparent #555;
  position: absolute;
  left: 0;
  top: 50%;
  transform: translateY(-50%);
  transition: transform 0.2s ease;
}

.post-menu li.h-h2.no-icon::before {
  content: none; /* 擬似要素の中身をなしにする */
  /* または display: none; でもOKです */
}

/* 開いている時のアイコン（下向きにする） */
.post-menu li.h-h2.open::before {
  transform: translateY(-50%) rotate(90deg);
}

/* h3（子要素）のスタイル */
.post-menu li.h-h3 {
  margin-left: 15px;
  font-size: 0.9em;
  /* 初期状態はJSで制御しますが、念のため */
}

/* アクティブな項目の色 */
.post-menu li.active > a {
  color: #d9534f;
  font-weight: bold;
}

/* リンク自体のスタイル調整 */
.post-menu li a {
  text-decoration: none;
  color: inherit;
  display: inline-block;
  width: 100%;
}
</style>

<div class="post-menu">
  <div class="post-menu-title">TOC</div>
  <div class="post-menu-content"></div>
</div>

<script>
  function generateContent() {
    var menu = document.querySelector(".post-menu");
    var menuContent = menu.querySelector(".post-menu-content");
    var headings = document.querySelector(".post-content").querySelectorAll("h2, h3");

    if (headings.length === 0) {
      return menu.style.display = "none";
    }

    // --- HTML生成 ---
    var menuHTML = '';
    for (var i = 0; i < headings.length; i++) {
      var h = headings[i];
      // h-h2 クラスの要素には初期状態で open クラスをつけるか、つけないかで「最初から開いているか」を決められます
      // ここでは閉じた状態をデフォルトとします
      menuHTML += (
        '<li class="h-' + h.tagName.toLowerCase() + '">'
        + '<a href="#' + h.getAttribute('id') + '">' + h.textContent + '</a></li>');
    }
    menuContent.innerHTML = '<ul>' + menuHTML + '</ul>';


    // --- 開閉ロジックの実装 ---
    var listItems = menuContent.querySelectorAll('li');

    // h2要素にクリックイベントを追加
    listItems.forEach(function(item, index) {
      if (item.classList.contains('h-h2')) {
        
        // クリックイベント
        item.addEventListener('click', function(e) {
          // リンクをクリックした場合はページ内遷移させたいので、イベントを止めない
          // ただし、アイコン付近をクリックした等の挙動を統一するため、
          // 開閉処理を行います。
          
          // クラスの付け替え（アイコンの回転用）
          item.classList.toggle('open');

          // 次のh2が出てくるまで、h3を表示/非表示切り替え
          for (var i = index + 1; i < listItems.length; i++) {
            var sibling = listItems[i];
            if (sibling.classList.contains('h-h2')) {
              break; // 次のh2に来たら終了
            }
            if (sibling.classList.contains('h-h3')) {
              if (item.classList.contains('open')) {
                sibling.style.display = 'block';
              } else {
                sibling.style.display = 'none';
              }
            }
          }
        });
      }
    });

    // --- 初期状態の設定（すべて閉じる） ---
    // もし最初から開いておきたい場合は、このブロックを削除するか調整してください
    listItems.forEach(function(item) {
      if (item.classList.contains('h-h3')) {
        item.style.display = 'none';
      }
    });


    // --- スクロール連動（ハイライト機能のみ残す） ---
    var header = document.querySelector('header.site-header');
    
    window.addEventListener('scroll', function (event) {
      var lastActive = menuContent.querySelector('.active');
      var changed = true;
      var activeIndex = -1;
      
      for (var i = headings.length - 1; i >= 0; i--) {
        var h = headings[i];
        var headingRect = h.getBoundingClientRect();
        var headerRect = header ? header.getBoundingClientRect() : {top:0, height:0}; // headerがない場合の安全策
        var headerTop = Math.floor(headerRect.top);
        var headerHeight = Math.floor(headerRect.height);
        var offset = headerTop + headerHeight + 20;

        if (headingRect.top <= offset) {
          var id = h.getAttribute('id');
          var a = menuContent.querySelector('a[href="#' + id  + '"]');
          var curActive = a.parentNode;
          
          if (curActive) {
            // もしアクティブになった項目が閉じているh2の中にあった場合、
            // 自動で開く処理を追加したい場合はここに記述します。
            // 今回は「手動開閉」を優先し、自動オープンはあえて行いません。
            
            curActive.classList.add('active');
            activeIndex = i;
          }
          if (lastActive == curActive) {
            changed = false;
          }
          break;
        }
      }

      if (changed) {
        if (lastActive) {
          lastActive.classList.remove('active');
        }
      }
    });
  }
  generateContent();
</script>
</section>
</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/paper_notes/"></data>

  <div class="wrapper">
    <div class="site-footer-inner">
<div>Copyright © 2023-current AkihikoWATANABE. The header images and any thumbnail images for the posts were generated by ChatGPT's DALL-E3.</div>
      <div>Powered by <a title="Jekyll is a simple, blog-aware, static site
      generator." href="https://jekyllrb.com/">Jekyll</a> &amp; <a title="Yat, yet
      another theme." href="https://github.com/jeffreytse/jekyll-theme-yat">Yat Theme</a>.</div>
      <div class="footer-col rss-subscribe">Subscribe <a href="/paper_notes/feed.xml">via RSS</a>
</div>
    </div>
  </div>
</footer>
</body>
  </html>
