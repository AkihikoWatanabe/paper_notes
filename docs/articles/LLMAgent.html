<!DOCTYPE html>
<html lang="ja"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="google-translate-customization" content="108d9124921d80c3-80e20d618ff053c8-g4f02ec6f3dba68b7-c"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>LLMAgentに関する論文・技術記事メモの一覧 | わたしのべんきょうノート</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="LLMAgentに関する論文・技術記事メモの一覧" />
<meta name="author" content="AkihikoWATANABE" />
<meta property="og:locale" content="ja" />
<meta name="description" content="LLMAgent #Survey#NLP#LanguageModel#ContextEngineeringIssue Date: 2025-07-19 Paper Note A Survey of Context Engineering for Large Language Models, Lingrui Mei+, arXiv25 Summary本調査では、LLMsの性能を向上させる「コンテキストエンジニアリング」を提案し、その要素と実装方法を体系的に分類。コンテキストの取得、生成、処理、管理を検討し、洗練されたシステム実装を探る。1300以上の研究を分析し、モデルの能力の非対称性を明らかにし、複雑な文脈理解と長文出力生成のギャップに対処する重要性を強調。研究者とエンジニアのための統一フレームワークを提供。 CommentもうContext Engineeringという切り口の体系化されたSurveyが出てきた。早すぎ。 ![image](https://github.com/user-attachments/assets/9577c3f8-8fd5-49e0-b80f-19c0d4f22064)元ポスト:https://x.com/neural_avb/status/1946288694882685317?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q #EfficiencyImprovement#Pocket#NLP#Dataset#Evaluation#SoftwareEngineeringIssue Date: 2025-07-18 Paper Note SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?, Xinyi He+, arXiv25 Summaryコードのパフォーマンス最適化は重要であり、LLMsのリポジトリレベルでの能力は未探求。これに対処するため、SWE-Perfという初のベンチマークを導入。140のインスタンスを用いて、LLMsと専門家の最適化パフォーマンスのギャップを評価し、研究機会を示す。 Comment元ポスト:https://x.com/sivil_taram/status/1945855374336446577?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-QこれまでのSWE系のベンチマークはBug Fixなどにフォーカスされてきたが、こちらのベンチマークはソフトウェアのパフォーマンス（i.e., 実行時間）を改善させられるかにフォーカスしているとのこと。 実際にリポジトリからPRを収集し、パッチ前後の実行時間を比較。20回のrunを通じて統計的に有意な実行時間の差があるもののみにフィルタリングをしているとのこと。" />
<meta property="og:description" content="LLMAgent #Survey#NLP#LanguageModel#ContextEngineeringIssue Date: 2025-07-19 Paper Note A Survey of Context Engineering for Large Language Models, Lingrui Mei+, arXiv25 Summary本調査では、LLMsの性能を向上させる「コンテキストエンジニアリング」を提案し、その要素と実装方法を体系的に分類。コンテキストの取得、生成、処理、管理を検討し、洗練されたシステム実装を探る。1300以上の研究を分析し、モデルの能力の非対称性を明らかにし、複雑な文脈理解と長文出力生成のギャップに対処する重要性を強調。研究者とエンジニアのための統一フレームワークを提供。 CommentもうContext Engineeringという切り口の体系化されたSurveyが出てきた。早すぎ。 ![image](https://github.com/user-attachments/assets/9577c3f8-8fd5-49e0-b80f-19c0d4f22064)元ポスト:https://x.com/neural_avb/status/1946288694882685317?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q #EfficiencyImprovement#Pocket#NLP#Dataset#Evaluation#SoftwareEngineeringIssue Date: 2025-07-18 Paper Note SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?, Xinyi He+, arXiv25 Summaryコードのパフォーマンス最適化は重要であり、LLMsのリポジトリレベルでの能力は未探求。これに対処するため、SWE-Perfという初のベンチマークを導入。140のインスタンスを用いて、LLMsと専門家の最適化パフォーマンスのギャップを評価し、研究機会を示す。 Comment元ポスト:https://x.com/sivil_taram/status/1945855374336446577?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-QこれまでのSWE系のベンチマークはBug Fixなどにフォーカスされてきたが、こちらのベンチマークはソフトウェアのパフォーマンス（i.e., 実行時間）を改善させられるかにフォーカスしているとのこと。 実際にリポジトリからPRを収集し、パッチ前後の実行時間を比較。20回のrunを通じて統計的に有意な実行時間の差があるもののみにフィルタリングをしているとのこと。" />
<link rel="canonical" href="http://akihikowatanabe.github.io/paper_notes/articles/LLMAgent.html" />
<meta property="og:url" content="http://akihikowatanabe.github.io/paper_notes/articles/LLMAgent.html" />
<meta property="og:site_name" content="わたしのべんきょうノート" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-07-22T16:47:57+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="LLMAgentに関する論文・技術記事メモの一覧" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"AkihikoWATANABE"},"dateModified":"2025-07-22T16:47:57+00:00","datePublished":"2025-07-22T16:47:57+00:00","description":"LLMAgent #Survey#NLP#LanguageModel#ContextEngineeringIssue Date: 2025-07-19 Paper Note A Survey of Context Engineering for Large Language Models, Lingrui Mei+, arXiv25 Summary本調査では、LLMsの性能を向上させる「コンテキストエンジニアリング」を提案し、その要素と実装方法を体系的に分類。コンテキストの取得、生成、処理、管理を検討し、洗練されたシステム実装を探る。1300以上の研究を分析し、モデルの能力の非対称性を明らかにし、複雑な文脈理解と長文出力生成のギャップに対処する重要性を強調。研究者とエンジニアのための統一フレームワークを提供。 CommentもうContext Engineeringという切り口の体系化されたSurveyが出てきた。早すぎ。 ![image](https://github.com/user-attachments/assets/9577c3f8-8fd5-49e0-b80f-19c0d4f22064)元ポスト:https://x.com/neural_avb/status/1946288694882685317?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q #EfficiencyImprovement#Pocket#NLP#Dataset#Evaluation#SoftwareEngineeringIssue Date: 2025-07-18 Paper Note SWE-Perf: Can Language Models Optimize Code Performance on Real-World Repositories?, Xinyi He+, arXiv25 Summaryコードのパフォーマンス最適化は重要であり、LLMsのリポジトリレベルでの能力は未探求。これに対処するため、SWE-Perfという初のベンチマークを導入。140のインスタンスを用いて、LLMsと専門家の最適化パフォーマンスのギャップを評価し、研究機会を示す。 Comment元ポスト:https://x.com/sivil_taram/status/1945855374336446577?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-QこれまでのSWE系のベンチマークはBug Fixなどにフォーカスされてきたが、こちらのベンチマークはソフトウェアのパフォーマンス（i.e., 実行時間）を改善させられるかにフォーカスしているとのこと。 実際にリポジトリからPRを収集し、パッチ前後の実行時間を比較。20回のrunを通じて統計的に有意な実行時間の差があるもののみにフィルタリングをしているとのこと。","headline":"LLMAgentに関する論文・技術記事メモの一覧","mainEntityOfPage":{"@type":"WebPage","@id":"http://akihikowatanabe.github.io/paper_notes/articles/LLMAgent.html"},"url":"http://akihikowatanabe.github.io/paper_notes/articles/LLMAgent.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="icon" href="">
  <link rel="canonical" href="http://akihikowatanabe.github.io">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-noto-sans@0.0.72/index.min.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/main.css">
  <link rel="stylesheet" href="/paper_notes/assets/css/custom_button.css">
  <script src="/paper_notes/assets/js/main.js"></script>
  <script src="https://d3js.org/d3.v5.min.js"></script><link type="application/atom+xml" rel="alternate" href="http://akihikowatanabe.github.io/paper_notes/feed.xml" title="わたしのべんきょうノート" /><script>
  function showMore(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "block";
          
      // このボタンの参照を取得して非表示にします
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "none";
      }
          
      // hideボタンの参照を取得して表示します
      const hideButton = contentDiv.querySelector('button[onclick^="hideContent"]');
      if (hideButton) {
        hideButton.style.display = "block";
      }
    }
  }

  function hideContent(index) {
    const contentDivs = document.getElementsByClassName('hidden-content');
    const contentDiv = contentDivs[index];

    if (contentDiv) {
      contentDiv.style.display = "none";
  
      // moreボタンの参照を取得して表示します
      const moreButtons = document.querySelectorAll('button[onclick^="showMore"]');
      const moreButton = moreButtons[index];
      if (moreButton) {
        moreButton.style.display = "block";
      }
  
      // このボタンを隠します
      const hideButtons = document.querySelectorAll('button[onclick^="hideContent"]');
      const hideButton = hideButtons[index];
      if (hideButton) {
        hideButton.style.display = "none";
      }
    }
  }
</script><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
<!-- and it's easy to individually load additional languages -->
<script charset="UTF-8"
        src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/go.min.js"
        async></script>



















<script>
// Init highlight js
document.addEventListener('DOMContentLoaded', function(event) {
  var els = document.querySelectorAll('pre code')

  function addLangData(block) {
    var outer = block.parentElement.parentElement.parentElement;
    var lang = block.getAttribute('data-lang');
    for (var i = 0; i < outer.classList.length; i++) {
      var cls = outer.classList[i];
      if (cls.startsWith('language-')) {
        lang = cls;
        break;
      }
    }
    if (!lang) {
      cls = block.getAttribute('class');
      lang = cls ? cls.replace('hljs ', '') : '';
    }
    if (lang.startsWith('language-')) {
      lang = lang.substr(9);
    }
    block.setAttribute('class', 'hljs ' + lang);
    block.parentNode.setAttribute('data-lang', lang);
  }

  function addBadge(block) {
    var enabled = ('true' || 'true').toLowerCase();
    if (enabled == 'true') {
      var pre = block.parentElement;
      pre.classList.add('badge');
    }
  }

  function handle(block) {
    addLangData(block);
    addBadge(block)
    hljs.highlightBlock(block);
  }

  for (var i = 0; i < els.length; i++) {
    var el = els[i];
    handle(el);
  }
});
</script>

<style>
  /* code language badge */
  pre.badge::before {
    content: attr(data-lang);
    color: #fff;
    background-color: #ff4e00;
    padding: 0 .5em;
    border-radius: 0 2px;
    text-transform: uppercase;
    text-align: center;
    min-width: 32px;
    display: inline-block;
    position: absolute;
    right: 0;
  }

  /* fix wrong badge display for firefox browser */
  code > table pre::before {
    display: none;
  }
</style>
<script
  src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe-lightbox.umd.min.js" async></script>
<script
  src="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/umd/photoswipe.umd.min.js" async></script>
<link
  href="//cdnjs.cloudflare.com/ajax/libs/photoswipe/5.3.7/photoswipe.min.css"
  rel="stylesheet"
/>
<style>
  .pswp .pswp__container .pswp__img {
    background-color: white;
  }
</style>

<script>
  function initPhotoSwipe() {
    let customOptions = {};

    try {
      const data = `{"gallery"=>"section.main", "children"=>"a.photo-swipe", "bgOpacity"=>0.8, "padding"=>{"top"=>20, "bottom"=>40, "left"=>100, "right"=>100}}`.replaceAll("=>", ":");
      customOptions = JSON.parse(data);
    } catch (e) {
      console.info("Invalid custom photo previewer options! " + e.message);
    }

    // Define object and gallery options
    const options = Object.assign(
      {
        gallery: "section.main",
        children: "a.photo-swipe",
        photo_scale: 2,
        // dynamic import is not supported in UMD version
        pswpModule: PhotoSwipe,
      },
      customOptions
    );

    const galleryEl = document.querySelector(options.gallery);
    if (!galleryEl) {
      return;
    }

    const imgEls = [];
    const els = galleryEl.querySelectorAll("img:not(.emoji)");
    els.forEach((el) => {
      if (el.src.trim() == "") {
        return;
      }
      if (!imgEls.includes(el)) {
        imgEls.push(el);
      }
    });

    if (imgEls.length === 0) {
      return;
    }

    imgEls.forEach((imgEl) => {
      imgEl.outerHTML = `
      <a class="photo-swipe"
        href="${imgEl.src}"
        data-pswp-width="${
          Math.max(imgEl.naturalWidth, imgEl.width) * options.photo_scale
        }"
        data-pswp-height="${
          Math.max(imgEl.naturalHeight, imgEl.height) * options.photo_scale
        }"
        data-pswp-caption="${imgEl.getAttribute("caption") || imgEl.alt}"
        target="_blank">
        ${imgEl.outerHTML}
      </a>`;
    });

    // Initialize PhotoSwipe 5
    var lightbox = new PhotoSwipeLightbox(options);

    lightbox.init();
  }

  window.addEventListener("load", initPhotoSwipe);
</script>
<meta name="google-site-verification" content="u_DTTPcCZ806iq51zgirHyWq3556HUKGq8AQfH91iFI" />
</head><body>





































































































































<header class="site-header site-header-transparent" role="banner">

  <div class="wrapper">
    <div class="site-header-inner"><span class="site-brand"><a class="site-brand-inner" rel="author" href="/paper_notes/">
  <img class="site-favicon" title="わたしのべんきょうノート" src="" onerror="this.style.display='none'">
  わたしのべんきょうノート
</a>
</span><nav class="site-nav">
          <input type="checkbox" id="nav-trigger" class="nav-trigger" />
          <label for="nav-trigger">
            <span class="menu-icon">
              <svg viewBox="0 0 18 15" width="18px" height="15px">
                <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
              </svg>
            </span>
          </label>

          <div class="trigger">









<span class="page-link">



<div id="google_translate_element" style="display: none;">
</div>

<span class="ct-language">
  <ul class="list-unstyled ct-language-dropdown">
    
      <li>
        <a href="#" class="lang-select" data-lang="en">
          
          <img src="https://cdn.countryflags.com/thumbs/united-states-of-america/flag-400.png" title="English">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="fr">
          
          <img src="https://cdn.countryflags.com/thumbs/france/flag-400.png" title="French">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="zh-CN">
          
          <img src="https://cdn.countryflags.com/thumbs/china/flag-400.png" title="Chinese(Simple)">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ja">
          
          <img src="https://cdn.countryflags.com/thumbs/japan/flag-400.png" title="Japanese">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ko">
          
          <img src="https://cdn.countryflags.com/thumbs/south-korea/flag-400.png" title="Korean">
          
        </a>
      </li>
    
      <li>
        <a href="#" class="lang-select" data-lang="ru">
          
          <img src="https://cdn.countryflags.com/thumbs/russia/flag-400.png" title="Russian">
          
        </a>
      </li>
    
  </ul>
</span>

<script type="text/javascript">
function googleTranslateElementInit() {
  new google.translate.TranslateElement({
    pageLanguage: 'ja',
    autoDisplay: false,
    layout: google.translate.TranslateElement.InlineLayout.VERTICAL
  }, 'google_translate_element');

  // Links to cross-origin destinations are unsafe
  var gll = document.getElementsByClassName('goog-logo-link')[0];
  if (gll) {
    gll.setAttribute('rel', 'noopener');
  }

  function restoreLang() {
    var iframe = document.getElementsByClassName('goog-te-banner-frame')[0];
    if (!iframe) return;

    var innerDoc = iframe.contentDocument || iframe.contentWindow.document;
    var restore_el = innerDoc.getElementsByTagName("button");

    for (var i = 0; i < restore_el.length; i++) {
      if (restore_el[i].id.indexOf("restore") >= 0) {
        restore_el[i].click();
        var close_el = innerDoc.getElementsByClassName("goog-close-link");
        close_el[0].click();
        return;
      }
    }
  }

  function triggerHtmlEvent(element, eventName) {
    var event;
    if (document.createEvent) {
      event = document.createEvent('HTMLEvents');
      event.initEvent(eventName, true, true);
      element.dispatchEvent(event);
    } else {
      event = document.createEventObject();
      event.eventType = eventName;
      element.fireEvent('on' + event.eventType, event);
    }
  }

  var googleCombo = document.querySelector("select.goog-te-combo");
  var langSelect = document.querySelector('.ct-language');
  langSelect.addEventListener('click', function(event) {
    if (!event.target) {
      return;
    }

    var selected = document.querySelector('.ct-language .ct-language-selected');
    if (selected) {
      selected.classList.remove('ct-language-selected');
    }

    var target = event.target;
    while (target && target !== langSelect ) {
      if (target.matches('.lang-select')) {
        break;
      }
      target = target.parentElement;
    }

    if (target && target.matches('.lang-select')) {
      var lang = target.getAttribute('data-lang');
      if (googleCombo.value == lang) {
        restoreLang();
      } else {
        target.parentElement.classList.add('ct-language-selected');
        googleCombo.value = lang;
        triggerHtmlEvent(googleCombo, 'change');
      }
    }

    event.preventDefault();
  });
}
</script>

<script type="text/javascript" src="https://translate.google.com/translate_a/element.js?cb=googleTranslateElementInit" async></script>
</span></div>
        </nav></div>
  </div>
</header>

<script>
  function initHeader() {
    var lastScrollY = getScrollPos().y;
    var documentElement = document.documentElement;

    function storeScrollData() {
      var y = getScrollPos().y;documentElement.setAttribute("data-header-transparent", "");var scrollStatus = "";

      if (y <= 0) {
        scrollStatus = "top";
      } else if ((window.innerHeight + y) >= document.body.offsetHeight) {
        scrollStatus = "bottom";
      } else {
        var isScrollDown = (y - lastScrollY > 0) ? true : false;
        scrollStatus = isScrollDown ? "down" : "up";
      }

      lastScrollY = y;
      documentElement.setAttribute("data-scroll-status", scrollStatus);
    }

    window.addEventListener('scroll', function(e) {
      storeScrollData();
    });

    storeScrollData();
  }
  document.addEventListener('DOMContentLoaded', initHeader);
</script>


























































































































































<style>
    html .page-banner .page-banner-img > *:first-child {
      opacity: 0.4;
    }

    html[data-theme="dark"] .page-banner .page-banner-img > *:first-child {
      opacity: 0.2872;
    }
  </style><section class="page-banner">
    <div class="page-banner-img"><div style="background-image: url(/paper_notes/assets/images/banner.png)"></div>
        <img class="img-placeholder" src="/paper_notes/assets/images/banner.png"></div>
    <div class="wrapper">
      <div class="page-banner-inner"><header class="post-header">
  <h1 class="post-title p-name" itemprop="name headline">わたしのべんきょうノート</h1>
  <h2 class="post-subtitle">勉強した論文や技術等の情報をGithubのIssueにメモっているひとのブログ。
それなりにメモの量が蓄積されてきたので、一度整理したいなと思いブログはじめてみました！
自然言語処理(NLP), 推薦システム(RecommenderSystem), Educational Data Mining (EDM), Learning Analytics (LA)などの分野のメモが多いと思います。
最近は特にLLMの勉強が多めです :)</h2>

  <div class="post-meta">
    <time class="dt-published" datetime="2025-07-22T16:47:57+00:00" itemprop="datePublished"><i class="fa fa-calendar"></i> Jul 22, 2025
    </time><span class="post-author left-vsplit"><i class="fa fa-pencil"></i> AkihikoWATANABE</span>
    
































    <span class="post-reading-time left-vsplit"><i class="fa fa-clock-o"></i> About 1 hour 38 mins</span>
  </div></header>
</div>
    </div>
  </section><script>
  function hashLocate(hashValue) {
    hashValue = hashValue.replace(/^.*#h-/, '');
    hashValue = decodeURIComponent(hashValue);
    var element = document.getElementById(hashValue);

    if (!element) {
      return;
    }

    var header = document.querySelector('header.site-header');
    var headerRect = header.getBoundingClientRect();
    var headerTop = Math.floor(headerRect.top);
    var headerHeight = Math.floor(headerRect.height);
    var scrollPos = getScrollPos();
    var offsetY = element.offsetTop - (headerTop + headerHeight + 20);

    if (offsetY == scrollPos.y) {
      return;
    }

    if (headerTop == 0  && offsetY > scrollPos.y) {
      offsetY += headerHeight + 2;
    } else if (headerTop < 0  && offsetY < scrollPos.y) {
      offsetY -= headerHeight - 2;
    }

    smoothScrollTo(offsetY);
  }

  // The first event occurred
  window.addEventListener('load', function(event) {
    if (window.location.hash) {
      hashLocate(window.location.hash);
    }
  });

  // The first event occurred
  window.addEventListener('click', function(event) {
    if (event.target.tagName.toLowerCase() == 'a') {
      hashLocate(event.target.getAttribute('href'));
    }
  });
</script>
<div class="theme-toggle">
  <input type="checkbox" id="theme-switch">
  <label for="theme-switch">
    <div class="toggle"></div>
    <div class="names">
      <p class="light">Light</p>
      <p class="dark">Dark</p>
    </div>
  </label>
</div>




<script>
  (function() {
    var sw = document.getElementById('theme-switch');
    var html = document.getElementsByTagName('html')[0];
    var nightModeOption = ('off' || 'auto').toLowerCase();
    var storage = nightModeOption === 'manual'
        ? localStorage
        : sessionStorage;
    var themeData = loadThemeData();

    function saveThemeData(data) {
      storage.setItem('theme', JSON.stringify(data));
    }

    function loadThemeData() {
      var data = storage.getItem('theme');
      try {
        data = JSON.parse(data ? data : '');
      } catch(e) {
        data = { nightShift: undefined, autoToggleAt: 0 };
        saveThemeData(data);
      }
      return data;
    }

    function handleThemeToggle(nightShift) {
      themeData.nightShift = nightShift;
      saveThemeData(themeData);
      html.dataset.theme = nightShift ? 'dark' : 'light';
      setTimeout(function() {
        sw.checked = nightShift ? true : false;
      }, 50);
    }

    function autoThemeToggle() {
      // Next time point of theme toggle
      var now = new Date();
      var toggleAt = new Date();
      var hours = now.getHours();
      var nightShift = hours >= 19 || hours <=7;

      if (nightShift) {
        if (hours > 7) {
          toggleAt.setDate(toggleAt.getDate() + 1);
        }
        toggleAt.setHours(7);
      } else {
        toggleAt.setHours(19);
      }

      toggleAt.setMinutes(0);
      toggleAt.setSeconds(0);
      toggleAt.setMilliseconds(0)

      var delay = toggleAt.getTime() - now.getTime();

      // auto toggle theme mode
      setTimeout(function() {
        handleThemeToggle(!nightShift);
      }, delay);

      return {
        nightShift: nightShift,
        toggleAt: toggleAt.getTime()
      };
    }

    // Listen the theme toggle event
    sw.addEventListener('change', function(event) {
      handleThemeToggle(event.target.checked);
    });

    if (nightModeOption == 'auto') {
      var data = autoThemeToggle();

      // Toggle theme by local setting
      if (data.toggleAt > themeData.autoToggleAt) {
        themeData.autoToggleAt = data.toggleAt;
        handleThemeToggle(data.nightShift);
      } else {
        handleThemeToggle(themeData.nightShift);
      }
    } else if (nightModeOption == 'manual') {
      handleThemeToggle(themeData.nightShift);
    } else {
      var nightShift = themeData.nightShift;
      if (nightShift === undefined) {
        nightShift = nightModeOption === 'on';
      }
      handleThemeToggle(nightShift);
    }
  })();
</script>
<div id="click-to-top" class="click-to-top">
  <i class="fa fa-arrow-up"></i>
</div>
<script>
  (function () {
    const clickToTop = document.getElementById('click-to-top');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 100) {
        clickToTop.classList.add('show')
      }else {
        clickToTop.classList.remove('show')
      }
    });
    clickToTop.addEventListener('click', () => {
      window.smoothScrollTo(0);
    });
  })();
</script>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="framework">
  <section class="main">

     <div class="post">
  <section>









<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

    <div class="post-content e-content" itemprop="articleBody">

      <h2 id="llmagent">LLMAgent</h2>
<div class="visible-content">
<a class="button" href="articles/Survey.html">#Survey</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/ContextEngineering.html">#ContextEngineering</a><br /><span class="issue_date">Issue Date: 2025-07-19</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2256">Paper Note A Survey of Context Engineering for Large Language Models, Lingrui Mei+, arXiv25</a>
<span class="snippet"><span>Summary</span>本調査では、LLMsの性能を向上させる「コンテキストエンジニアリング」を提案し、その要素と実装方法を体系的に分類。コンテキストの取得、生成、処理、管理を検討し、洗練されたシステム実装を探る。1300以上の研究を分析し、モデルの能力の非対称性を明らかにし、複雑な文脈理解と長文出力生成のギャップに対処する重要性を強調。研究者とエンジニアのための統一フレームワークを提供。</span>
<span class="snippet"><span>Comment</span>もうContext Engineeringという切り口の体系化されたSurveyが出てきた。早すぎ。
![image](https://github.com/user-attachments/assets/9577c3f8-8fd5-49e0-b80f-19c0d4f22064)元ポスト:https://x.com/neural_avb/status/1946288694882685317?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/Evaluation.html">#Evaluation</a><a class="button" href="articles/SoftwareEngineering.html">#SoftwareEngineering</a><br /><span class="issue_date">Issue Date: 2025-07-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2251">Paper Note SWE-Perf: Can Language Models Optimize Code Performance on Real-World  Repositories?, Xinyi He+, arXiv25</a>
<span class="snippet"><span>Summary</span>コードのパフォーマンス最適化は重要であり、LLMsのリポジトリレベルでの能力は未探求。これに対処するため、SWE-Perfという初のベンチマークを導入。140のインスタンスを用いて、LLMsと専門家の最適化パフォーマンスのギャップを評価し、研究機会を示す。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/sivil_taram/status/1945855374336446577?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-QこれまでのSWE系のベンチマークはBug Fixなどにフォーカスされてきたが、こちらのベンチマークはソフトウェアのパフォーマンス（i.e., 実行時間）を改善させられるかにフォーカスしているとのこと。
実際にリポジトリからPRを収集し、パッチ前後の実行時間を比較。20回のrunを通じて統計的に有意な実行時間の差があるもののみにフィルタリングをしているとのこと。

Human Expertsは平均10.9%のgainを得たが、エージェントは2.3%にとどまっており、ギャップがあるとのこと。

傾向として、LLMはlow levelなインフラストラクチャ（環境構築, 依存関係のハンドリング, importのロジック）を改善するが、Human Expertsはhigh levelなロジックやデータ構造を改善する（e.g., アルゴリズムや、データハンドリング）。</span>
<a class="button" href="articles/GraphBased.html">#GraphBased</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/ScientificDiscovery.html">#ScientificDiscovery</a><br /><span class="issue_date">Issue Date: 2025-07-08</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2150">Paper Note AI Research Agents for Machine Learning: Search, Exploration, and  Generalization in MLE-bench, Edan Toledo+, arXiv25</a>
<span class="snippet"><span>Summary</span>AI研究エージェントは、機械学習の自動化を通じて科学の進展を促進する可能性がある。本研究では、MLE-benchというKaggleコンペティションを用いてエージェントの性能向上に取り組み、検索ポリシーとオペレーターを用いて候補解の空間を探索する方法を提案。異なる検索戦略とオペレーターの組み合わせが高いパフォーマンスに寄与することを示し、MLE-bench liteでの結果を向上させ、Kaggleメダル獲得率を39.6%から47.7%に引き上げた。自動化された機械学習の進展には、これらの要素を共同で考慮することが重要である。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/martinjosifoski/status/1942238775305699558?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q関連:
- #1457グラフ中の各ノードはartifacts（i.e., エージェントが生成したコード)で、先行研究がiterativeな実験に加え、潜在的なsolutionに対してtree searchをすることでSoTAを達成しており、これをグラフを用いてより一般化することで異なるデザインのエージェントでも適用できるようにしている。
![image](https://github.com/user-attachments/assets/a1f417c1-f5a0-4e51-a17e-6e5a3fcce75d)

あとで追記する</span>
</div>
<p><button onclick="showMore(0)">more</button></p>

<div class="hidden-content">
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/Investigation.html">#Investigation</a><br /><span class="issue_date">Issue Date: 2025-07-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2123">Paper Note Future of Work with AI Agents: Auditing Automation and Augmentation  Potential across the U.S. Workforce, Yijia Shao+, arXiv25</a>
<span class="snippet"><span>Summary</span>本論文では、労働者がAIエージェントに自動化または補完してほしい職業タスクを評価する新しい監査フレームワークを提案し、労働者の希望と技術的能力の一致を分析します。音声強化ミニインタビューを用いて「人間主体性スケール（HAS）」を導入し、米国労働省のO*NETデータベースを基にしたWORKBankデータベースを構築しました。タスクを自動化のゾーンに分類し、AIエージェント開発におけるミスマッチと機会を明らかにします。結果は職業ごとの多様なHASプロファイルを示し、AIエージェントの統合がスキルのシフトを促す可能性を示唆しています。これにより、AIエージェントの開発を労働者の希望に整合させる重要性が強調されます。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/hillbig/status/1939806172061868173?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Evaluation.html">#Evaluation</a><a class="button" href="articles/ScientificDiscovery.html">#ScientificDiscovery</a><a class="button" href="articles/Reproducibility.html">#Reproducibility</a><br /><span class="issue_date">Issue Date: 2025-06-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2118">Paper Note The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT  Improvements, Bingchen Zhao+, arXiv25</a>
<span class="snippet"><span>Summary</span>大規模言語モデル（LLMs）の進展を活用し、AIエージェントの研究再現能力を評価するために、LLMスピードランベンチマークを導入。19のタスクで訓練スクリプトとヒントを提供し、迅速な実行を促進。既知の革新の再実装が難しいことを発見し、科学的再現を自動化するための指標を提供。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/karpathy/status/1939709449956126910?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/Evaluation.html">#Evaluation</a><a class="button" href="articles/Programming.html">#Programming</a><a class="button" href="articles/LongSequence.html">#LongSequence</a><br /><span class="issue_date">Issue Date: 2025-06-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2045">Paper Note ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm  Engineering, Yuki Imajuku+, arXiv25</a>
<span class="snippet"><span>Summary</span>AIシステムの最適化問題に対するパフォーマンスを評価する新しいベンチマークALE-Benchを提案。ALE-Benchは実際のタスクに基づき、長期的な解決策の洗練を促進する。大規模言語モデル（LLM）の評価では特定の問題で高いパフォーマンスを示すが、一貫性や長期的な問題解決能力において人間とのギャップが残ることが明らかになり、今後のAI進展に向けた必要性を示唆している。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/sakanaailabs/status/1934767254715117812?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q関連ポスト:https://x.com/iwiwi/status/1934830621756674499?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/x-Use.html">#x-Use</a><br /><span class="issue_date">Issue Date: 2025-06-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2032">Paper Note Go-Browse: Training Web Agents with Structured Exploration, Apurva Gandhi+, arXiv25</a>
<span class="snippet"><span>Summary</span>Go-Browseを提案し、ウェブ環境の構造的探索を通じて多様なデータを自動収集。グラフ探索を用いて効率的なデータ収集を実現し、WebArenaベンチマークで成功率21.7%を達成。これはGPT-4o miniを2.4%上回り、10B未満のモデルでの最先端結果を2.9%上回る。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/gneubig/status/1932786231542493553?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-QWebArena:
- #1849</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a><a class="button" href="articles/Coding.html">#Coding</a><br /><span class="issue_date">Issue Date: 2025-06-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2018">Paper Note Training Language Models to Generate Quality Code with Program Analysis  Feedback, Feng Yao+, arXiv25</a>
<span class="snippet"><span>Summary</span>プログラム分析に基づくフィードバックを用いた強化学習フレームワーク「REAL」を提案。セキュリティや保守性の欠陥を検出し、機能的正確性を保証することで、LLMsによる高品質なコード生成を促進。手動介入不要でスケーラブルな監視を実現し、実験により最先端の手法を上回る性能を示した。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/fengyao1909/status/1930377346693116350?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q現在のCoding LLMはUnitTestを通るように学習されるが、UnitTestに通るからといってコードの品質が良いわけでは無いので、UnitTestに通るか否かのReward（Functionality)に加えて、RL中に生成されたコードを制御フローグラフ[^1]に変換し汚染解析[^2]をした結果をRewardに組み込むことで、FunctionalityとQualityを両立したよ、という話のようである。

Figure1のグラフの縦軸は、Functionalityと（UnitTestが通ったか否か）と、Quailty(セキュリティや保守性に関する問題が検出されなかった)、という両方の条件を満たした割合である点に注意。

![image](https://github.com/user-attachments/assets/b843e416-8c96-40ca-ac1f-0318eb1ae40c)

![image](https://github.com/user-attachments/assets/6beeea63-571b-4ce6-bef8-ac8e0cfffee2)

[^1]:プログラムを実行したときに通る可能性のある経路のすべてをグラフとして表したもの[引用元](https://qiita.com/uint256_t/items/7d4556cb8f5997b9e95c)
[^2]:信頼できない汚染されたデータがプログラム中でどのように処理されるかを分析すること</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/SelfImprovement.html">#SelfImprovement</a><a class="button" href="articles/read-later.html">#read-later</a><br /><span class="issue_date">Issue Date: 2025-06-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2012">Paper Note Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents, Jenny Zhang+, arXiv25</a>
<span class="snippet"><span>Summary</span>ダーヴィン・ゴーデルマシン（DGM）は、自己改善するAIシステムであり、コードを反復的に修正し、コーディングベンチマークで変更を検証します。進化とオープンエンドな研究に基づき、生成されたエージェントのアーカイブを維持し、新しいバージョンを作成することで多様なエージェントを育成します。DGMはコーディング能力を自動的に向上させ、SWE-benchでのパフォーマンスを20.0%から50.0%、Polyglotでのパフォーマンスを14.2%から30.7%に改善しました。安全対策を講じた実験により、自己改善を行わないベースラインを大幅に上回る成果を示しました。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://www.linkedin.com/posts/omarsar_new-paper-open-ended-evolution-of-self-improving-activity-7334610178832556033-8dA-?utm_source=share&amp;utm_medium=member_ios&amp;rcm=ACoAACzQvjwB2FeLVE3yukDiUYtr5J4k-6nlNG4- #1212

あたりの研究とはどう違うのだろうか、という点が気になる。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/SelfImprovement.html">#SelfImprovement</a><br /><span class="issue_date">Issue Date: 2025-06-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2008">Paper Note Self-Challenging Language Model Agents, Yifei Zhou+, arXiv25</a>
<span class="snippet"><span>Summary</span>Self-Challengingフレームワークを提案し、エージェントが自ら生成した高品質なタスクで訓練。エージェントは挑戦者としてタスクを生成し、実行者として強化学習を用いて訓練。M3ToolEvalとTauBenchでLlama-3.1-8B-Instructが2倍以上の改善を達成。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/jaseweston/status/1929719473952497797?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q解説ポスト:https://x.com/omarsar0/status/1930748591242424439?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/SoftwareEngineering.html">#SoftwareEngineering</a><a class="button" href="articles/read-later.html">#read-later</a><br /><span class="issue_date">Issue Date: 2025-06-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2004">Paper Note Satori-SWE: Evolutionary Test-Time Scaling for Sample-Efficient Software  Engineering, Guangtao Zeng+, arXiv25</a>
<span class="snippet"><span>Summary</span>EvoScaleを提案し、進化的プロセスを用いて小型言語モデルの性能を向上させる手法を開発。選択と突然変異を通じて出力を洗練し、サンプル数を減少させる。強化学習を用いて自己進化を促進し、SWE-Bench-Verifiedで32Bモデルが100B以上のモデルと同等以上の性能を示す。コード、データ、モデルはオープンソースとして公開予定。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/gan_chuang/status/1928963872188244400?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Multi.html">#Multi</a><a class="button" href="articles/Analysis.html">#Analysis</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><br /><span class="issue_date">Issue Date: 2025-04-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1904">Why Do Multi-Agent LLM Systems Fail?, Mert Cemri+, arXiv25</a>
<span class="snippet"><span>Summary</span>MASの性能向上が単一エージェントと比較して限定的であることを受け、MAST（Multi-Agent System Failure Taxonomy）を提案。200以上のタスクを分析し、14の失敗モードを特定し、3つの大カテゴリに整理。Cohenのカッパスコア0.88を達成し、LLMを用いた評価パイプラインを開発。ケーススタディを通じて失敗分析とMAS開発の方法を示し、今後の研究のためのロードマップを提示。データセットとLLMアノテーターをオープンソース化予定。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/mertcemri/status/1915567789714329799?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q7つのメジャーなマルチエージェントフレームワークに対して200以上のタスクを実施し、6人の専門家がtraceをアノテーション。14種類の典型的なfailure modeを見つけ、それらを3つにカテゴライズ。これを考慮してマルチエージェントシステムの失敗に関するTaxonomy（MAS）を提案
![image](https://github.com/user-attachments/assets/21d45bc7-cc6c-4561-b991-098f8d068627)</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Hallucination.html">#Hallucination</a><br /><span class="issue_date">Issue Date: 2025-04-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1882">Hallucination Mitigation using Agentic AI Natural Language-Based  Frameworks, Diego Gosmar+, arXiv25</a>
<span class="snippet"><span>Summary</span>本研究では、複数のAIエージェントを調整し、自然言語処理を活用して幻覚を軽減する方法を探求。300以上の幻覚を誘発するプロンプトを用いたパイプラインを設計し、出力を第二および第三レベルのエージェントがレビュー。新たに設計したKPIで幻覚スコアを評価し、OVONフレームワークを通じてエージェント間で文脈情報を転送。結果として、相互運用可能なエージェントを活用することで幻覚の軽減に成功し、AIへの信頼を強化することが示された。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Evaluation.html">#Evaluation</a><a class="button" href="articles/QuestionGeneration.html">#QuestionGeneration</a><br /><span class="issue_date">Issue Date: 2025-04-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1853">Interactive Agents to Overcome Ambiguity in Software Engineering, Sanidhya Vijayvargiya+, arXiv25</a>
<span class="snippet"><span>Summary</span>AIエージェントはあいまいな指示に基づくタスク自動化に利用されるが、誤った仮定や質問不足がリスクを生む。本研究では、LLMエージェントのあいまいな指示処理能力を評価し、インタラクティビティを活用したパフォーマンス向上、あいまいさの検出、目標を絞った質問の実施を検討。結果、モデルは明確な指示と不十分な指示を区別するのが難しいが、インタラクションを通じて重要な情報を取得し、パフォーマンスが向上することが示された。これにより、現在のモデルの限界と改善のための評価手法の重要性が明らかになった。</span>
<span class="snippet"><span>Comment</span>曖昧なユーザメッセージに対する、エージェントが"質問をする能力を測る"ベンチマーク

<img width="422" alt="Image" src="https://github.com/user-attachments/assets/3d201ebf-9ca1-4333-9d27-e33a9028066f" /></span>
<a class="button" href="articles/Tools.html">#Tools</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Reasoning.html">#Reasoning</a><a class="button" href="articles/NAACL.html">#NAACL</a><br /><span class="issue_date">Issue Date: 2025-02-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1770">OctoTools: An Agentic Framework with Extensible Tools for Complex   Reasoning, Pan Lu+, NAACL25</a>
<span class="snippet"><span>Summary</span>複雑な推論タスクに対応するためのオープンソースエージェントフレームワーク「OctoTools」を提案。トレーニング不要で拡張可能なこのフレームワークは、標準化されたツールカードやプランナー、エグゼキューターを備え、16の多様なタスクでGPT-4oに対して平均9.3%の精度向上を達成。さらに、他の手法を最大10.6%上回る性能を示した。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/lupantech/status/1892260474320015861?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-QNAACL'25でベストペーパーに選出:
https://x.com/lupantech/status/1919495362102100365?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br /><span class="issue_date">Issue Date: 2025-02-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1750">Rethinking Mixture-of-Agents: Is Mixing Different Large Language Models  Beneficial?, Wenzhe Li+, arXiv25</a>
<span class="snippet"><span>Summary</span>Self-MoAは、単一の高性能LLMからの出力を集約するアンサンブル手法であり、従来のMoAを上回る性能を示す。AlpacaEval 2.0で6.6%の改善を達成し、MMLUやCRUXなどでも平均3.8%の向上を記録。出力の多様性と品質のトレードオフを調査し、異なるLLMの混合が品質を低下させることを確認。Self-MoAの逐次バージョンも効果的であることを示した。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/dair_ai/status/1888658770059816968?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Alignment.html">#Alignment</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><a class="button" href="articles/COLING.html">#COLING</a><a class="button" href="articles/PostTraining.html">#PostTraining</a><br /><span class="issue_date">Issue Date: 2024-12-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1577">Towards Adaptive Mechanism Activation in Language Agent, Ziyang Huang+, COLING25</a>
<span class="snippet"><span>Summary</span>自己探索によるメカニズム活性化学習（ALAMA）を提案し、固定されたメカニズムに依存せずに適応的なタスク解決を目指す。調和のとれたエージェントフレームワーク（UniAct）を構築し、タスク特性に応じてメカニズムを自動活性化。実験結果は、動的で文脈に敏感なメカニズム活性化の有効性を示す。</span>
<span class="snippet"><span>Comment</span>元ポスト: https://x.com/omarsar0/status/1863956776623747433?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q手法としては、SFTとKTOを活用しpost trainingするようである
![image](https://github.com/user-attachments/assets/0eab8029-124d-4ac1-b906-2463472b90b2)

- #1472</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br /><span class="issue_date">Issue Date: 2025-04-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1854">Agent Workflow Memory, Zora Zhiruo Wang+, arXiv24</a>
<span class="snippet"><span>Summary</span>エージェントワークフローメモリ（AWM）を導入し、エージェントが再利用可能なタスクワークフローを学習することで、複雑なウェブナビゲーションタスクを効率的に解決。Mind2WebとWebArenaのベンチマークで、成功率をそれぞれ24.6%および51.1%向上させ、必要なステップ数を削減。オンラインAWMは、タスクやドメインに対しても堅牢に一般化し、ベースラインを大幅に上回る性能を示した。</span>
<span class="snippet"><span>Comment</span>過去のワークフローをエージェントがprompt中で利用することができ、利用すればするほど賢くなるような仕組みの提案
<img width="873" alt="Image" src="https://github.com/user-attachments/assets/6160cfa5-9dbd-44c6-926c-a56eb698d78d" /></span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br /><span class="issue_date">Issue Date: 2025-04-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1852">CoAct: A Global-Local Hierarchy for Autonomous Agent Collaboration, Xinming Hou+, arXiv24</a>
<span class="snippet"><span>Summary</span>CoActフレームワークを提案し、2つのエージェント（グローバル計画エージェントとローカル実行エージェント）を用いて、LLMの複雑なタスクへの対応力を向上させる。実験では、WebArenaベンチマークにおいて優れた性能を示し、失敗時のプロセス再編成能力を確認。コードは公開中。</span>
<span class="snippet"><span>Comment</span>Planningエージェントと実行エージェントを活用するソフトウェアエージェント

<img width="632" alt="Image" src="https://github.com/user-attachments/assets/55db47b8-15f8-4a9c-b641-ce906994897f" />

ReActより性能向上
-  #518 
<img width="325" alt="Image" src="https://github.com/user-attachments/assets/79ac984a-1aa4-4d27-8a3f-860ed2c3abf7" /></span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/SoftwareEngineering.html">#SoftwareEngineering</a><br /><span class="issue_date">Issue Date: 2025-04-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1851">Training Software Engineering Agents and Verifiers with SWE-Gym, Jiayi Pan+, arXiv24</a>
<span class="snippet"><span>Summary</span>SWE-Gymを提案し、2,438件の実世界のPythonタスクを含む環境を構築。言語モデルに基づくSWEエージェントを訓練し、SWE-Benchで最大19%の解決率向上を達成。微調整されたエージェントは新たな最先端の性能を示し、SWE-Gymやモデル、エージェントの軌跡を公開。</span>
<span class="snippet"><span>Comment</span>SWE-Benchとは完全に独立したより広範な技術スタックに関連するタスクに基づくSWEベンチマーク
- #1848 SWE-Benchと比べて実行可能な環境と単体テストが提供されており、単なるベンチマークではなくエージェントを訓練できる環境が提供されている点が大きく異なるように感じる。
![image](https://github.com/user-attachments/assets/8c96df84-d211-4035-8337-1ab624d30a4f)
![image](https://github.com/user-attachments/assets/d25687d9-6f1a-44f6-8235-09be1ff4890f)</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/ICLR.html">#ICLR</a><br /><span class="issue_date">Issue Date: 2025-04-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1849">WebArena: A Realistic Web Environment for Building Autonomous Agents, Shuyan Zhou+, ICLR24</a>
<span class="snippet"><span>Summary</span>生成AIの進展により、自律エージェントが自然言語コマンドで日常タスクを管理する可能性が生まれたが、現行のエージェントは簡略化された環境でのテストに限られている。本研究では、ウェブ上でタスクを実行するエージェントのための現実的な環境を構築し、eコマースやソーシャルフォーラムなどのドメインを含む完全なウェブサイトを提供する。この環境を基に、タスクの正確性を評価するベンチマークを公開し、実験を通じてGPT-4ベースのエージェントの成功率が14.41%であり、人間の78.24%には及ばないことを示した。これにより、実生活のタスクにおけるエージェントのさらなる開発の必要性が強調される。</span>
<span class="snippet"><span>Comment</span>Webにおけるさまざまなrealisticなタスクを評価するためのベンチマーク
![image](https://github.com/user-attachments/assets/8895fc29-e997-4cce-a43e-65b928dc1d78)実際のexample。スタート地点からピッツバーグのmuseumを巡る最短の経路を見つけるといった複雑なタスクが含まれる。
![image](https://github.com/user-attachments/assets/5b7bebea-34c7-4c6f-bbe5-3928544e6c13)

人間とGPT4,GPT-3.5の比較結果
![image](https://github.com/user-attachments/assets/390fee31-85d0-4d83-969a-57a7f1548ca8)</span>
<a class="button" href="articles/EfficiencyImprovement.html">#EfficiencyImprovement</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/SoftwareEngineering.html">#SoftwareEngineering</a><br /><span class="issue_date">Issue Date: 2025-04-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1847">Agentless: Demystifying LLM-based Software Engineering Agents, Chunqiu Steven Xia+, arXiv24</a>
<span class="snippet"><span>Summary</span>最近のLLMの進展により、ソフトウェア開発タスクの自動化が進んでいるが、複雑なエージェントアプローチの必要性に疑問が生じている。これに対し、Agentlessというエージェントレスアプローチを提案し、シンプルな三段階プロセスで問題を解決。SWE-bench Liteベンチマークで最高のパフォーマンスと低コストを達成。研究は自律型ソフトウェア開発におけるシンプルで解釈可能な技術の可能性を示し、今後の研究の方向性を刺激することを目指している。</span>
<span class="snippet"><span>Comment</span>日本語解説:https://note.com/ainest/n/nac1c795e3825LLMによる計画の立案、環境からのフィードバックによる意思決定などの複雑なワークフローではなく、Localization（階層的に問題のある箇所を同定する）とRepair（LLMで複数のパッチ候補を生成する）、PatchValidation(再現テストと回帰テストの両方を通じて結果が良かったパッチを選ぶ）のシンプルなプロセスを通じてIssueを解決する。
![image](https://github.com/user-attachments/assets/6d042dfe-9780-4410-9077-b265af5456d1)

これにより、低コストで高い性能を達成している、といった内容な模様。
![image](https://github.com/user-attachments/assets/3934126f-3a4d-406c-8860-c3ed35a351c4)</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/AutomaticPromptEngineering.html">#AutomaticPromptEngineering</a><br /><span class="issue_date">Issue Date: 2025-02-10</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1752">PromptWizard: Task-Aware Prompt Optimization Framework, Eshaan Agarwal+, arXiv24</a>
<span class="snippet"><span>Summary</span>PromptWizardは、完全自動化された離散プロンプト最適化フレームワークであり、自己進化的かつ自己適応的なメカニズムを利用してプロンプトの質を向上させる。フィードバック駆動の批評を通じて、タスク特有のプロンプトを生成し、45のタスクで優れたパフォーマンスを実現。限られたデータや小規模なLLMでも効果を発揮し、コスト分析により効率性とスケーラビリティの利点が示された。</span>
<span class="snippet"><span>Comment</span>Github:https://github.com/microsoft/PromptWizard?tab=readme-ov-file
元ポスト:https://x.com/tom_doerr/status/1888178173684199785?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q初期に提案された
- #1034

と比較すると大分性能が上がってきているように見える。
![image](https://github.com/user-attachments/assets/5f7a329e-e83b-46da-9213-af8877201572)
![image](https://github.com/user-attachments/assets/857b3526-4f56-4e31-8a69-a4193657b286)reasoning modelではfewshot promptingをすると性能が落ちるという知見があるので、reasoningモデル向けのAPE手法もそのうち出現するのだろう（既にありそう）。OpenReview: https://openreview.net/forum?id=VZC9aJoI6a
ICLR'25にrejectされている</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Blog.html">#Blog</a><a class="button" href="articles/NeurIPS.html">#NeurIPS</a><br /><span class="issue_date">Issue Date: 2025-01-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1729">Paper Note Chain of Agents: Large language models collaborating on long-context tasks, Google Research, 2025.01, NeurIPS24</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/googleai/status/1882554959272849696?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-QLLMがどこまでいってもcontext長の制約に直面する問題に対してLLM Agentを組み合わせて対処しました、的な話な模様ブログ中にアプローチを解説した動画があるのでわかりやすいIs the experimental code open source?Thank you for your comment. I tried to find an official open-source implementation provided by the authors, but I was not able to locate one. In fact, I also checked the personal webpage of the first author, but there was no link to any released code.

Is seems that an unofficial implementation is listed under the “Code” tab on the NeurIPS page. I hope this is helpful. Thank you.

NeurIPS link: https://nips.cc/virtual/2024/poster/95563
openreview: https://openreview.net/forum?id=LuCLf4BJsr</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/SyntheticData.html">#SyntheticData</a><a class="button" href="articles/Evaluation.html">#Evaluation</a><a class="button" href="articles/SyntheticDataGeneration.html">#SyntheticDataGeneration</a><br /><span class="issue_date">Issue Date: 2025-01-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1648">MAG-V: A Multi-Agent Framework for Synthetic Data Generation and  Verification, Saptarshi Sengupta+, arXiv24</a>
<span class="snippet"><span>Summary</span>MAG-Vというマルチエージェントフレームワークを提案し、顧客クエリを模倣したデータセットを生成してエージェントのパフォーマンスを向上させる。軌跡の検証手法は従来のMLモデルを上回り、GPT-4と同等の性能を示す。多様なタスクエージェントを統一するアプローチを提供。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/dair_ai/status/1868299921117630528?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Evaluation.html">#Evaluation</a><br /><span class="issue_date">Issue Date: 2025-01-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1645">TheAgentCompany: Benchmarking LLM Agents on Consequential Real World  Tasks, Frank F. Xu+, arXiv24</a>
<span class="snippet"><span>Summary</span>日常生活や仕事におけるAIエージェントの効果を測定するため、TheAgentCompanyというベンチマークを導入。AIエージェントは、ウェブブラウジングやコード実行などのタスクを自律的に行う能力を評価。テストの結果、最も競争力のあるエージェントはタスクの24%を自律的に完了できることが判明。簡単なタスクは自動化可能だが、難しい長期的なタスクは現行システムでは対応できないことが示された。</span>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/dair_ai/status/1870821189809217921?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Qソフトウェアエンジニアリングの企業の設定で現実に起こりうるな　175種類のタスクを定義してAI Agentを評価できるベンチマークTheAgentCompanyを提案。

![image](https://github.com/user-attachments/assets/ef7b51d3-b4af-4171-a692-48fb2c2552ef)

既存のベンチマークより、多様で、実際のソフトウェアエンジニアリング企業でで起こりうる幅広いタスクを持ち、タスクの遂行のために同僚に対して何らかのインタラクションが必要で、達成のために多くのステップが必要でかつ個々のステップ（サブタスク）を評価可能で、多様なタスクを遂行するために必要な様々なインタフェースをカバーし、self hostingして結果を完全に再現可能なベンチマークとなっている模様。
![image](https://github.com/user-attachments/assets/e5fbd6da-75d7-49e1-8c66-dc7950d443e4)

https://x.com/gneubig/status/1869735196700062089?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q
（画像は著者ツイートより引用）プロプライエタリなモデルとOpenWeightなモデルでAI Agentとしての能力を評価した結果、Claude-3.5-sonnetは約24%のタスクを解決可能であり、他モデルと比べて性能が明らかに良かった。また、Gemini-2.0-flashなコストパフォーマンスに優れている。OpenWeightなモデルの中ではLlama3.3-70Bのコストパフォーマンスが良かった。タスクとしては具体的に評価可能なタスクのみに焦点を当てており、Open Endなタスクでは評価していない点に注意とのこと。
https://x.com/gneubig/status/1869735209404682706?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q
https://x.com/gneubig/status/1869735213976432764?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q
![image](https://github.com/user-attachments/assets/3bdcabef-70da-4f09-8366-efe29f7ab371)まだまだAI Agentが完全に'同僚'として機能することとは現時点ではなさそうだが、このベンチマークのスコアが今後どこまで上がっていくだろうか。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><br /><span class="issue_date">Issue Date: 2024-11-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1548">Generative Agent Simulations of 1,000 People, Joon Sung Park+, arXiv24</a>
<span class="snippet"><span>Summary</span>新しいエージェントアーキテクチャを提案し、1,052人の実在の個人の態度と行動を85%の精度で再現。大規模言語モデルを用いた質的インタビューに基づき、参加者の回答を正確にシミュレート。人口統計的説明を用いたエージェントと比較して、精度バイアスを軽減。個人および集団の行動調査の新しいツールを提供。</span>
<a class="button" href="articles/Survey.html">#Survey</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br /><span class="issue_date">Issue Date: 2024-11-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1503">GUI Agents with Foundation Models: A Comprehensive Survey, Shuai Wang+, arXiv24</a>
<span class="snippet"><span>Summary</span>(M)LLMを活用したGUIエージェントの研究を統合し、データセット、フレームワーク、アプリケーションの革新を強調。重要なコンポーネントをまとめた統一フレームワークを提案し、商業アプリケーションを探求。課題を特定し、今後の研究方向を示唆。</span>
<span class="snippet"><span>Comment</span>![image](https://github.com/user-attachments/assets/999adca8-f0d7-483c-ae5a-b6f78fe9da4b)
![image](https://github.com/user-attachments/assets/b69dc991-3e15-4965-a183-cc7909ad9eba)Referenceやページ数はサーベイにしては少なめに見える。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/API.html">#API</a><br /><span class="issue_date">Issue Date: 2024-11-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1499">Beyond Browsing: API-Based Web Agents, Yueqi Song+, arXiv24</a>
<span class="snippet"><span>Summary</span>APIを利用するAIエージェントの研究を行い、従来のウェブブラウジングエージェントと比較。API呼び出しエージェントはオンラインタスクをAPI経由で実行し、ハイブリッドエージェントはウェブブラウジングとAPIの両方を活用。実験結果では、ハイブリッドエージェントが他のエージェントを上回り、タスク非依存の最先端パフォーマンスを達成。APIの利用がウェブブラウジングよりも優れた選択肢であることを示唆。</span>
<span class="snippet"><span>Comment</span>![image](https://github.com/user-attachments/assets/f4beb58b-f6da-4536-87e6-3d746cb7c586)CMUの研究。後で読みたい</span>
<a class="button" href="articles/Pretraining.html">#Pretraining</a><a class="button" href="articles/Tools.html">#Tools</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Supervised-FineTuning (SFT).html">#Supervised-FineTuning (SFT)</a><br /><span class="issue_date">Issue Date: 2024-10-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1458">ToolGen: Unified Tool Retrieval and Calling via Generation, Renxi Wang+, N_A, arXiv24</a>
<span class="snippet"><span>Summary</span>ToolGenは、外部ツールとの直接対話を可能にする新しいフレームワークで、各ツールをユニークなトークンとして表現し、LLMのパラメータに統合します。これにより、LLMはツール呼び出しや引数を自然言語生成の一部としてシームレスに生成でき、情報取得ステップなしで多くのツールにアクセス可能になります。実験結果は、ToolGenが自律的なタスク完了と情報取得で優れた性能を示し、より効率的で自律的なAIシステムの基盤を築くことを示しています。</span>
<span class="snippet"><span>Comment</span>昔からよくある特殊トークンを埋め込んで、特殊トークンを生成したらそれに応じた処理をする系の研究。今回はツールに対応するトークンを仕込む模様。斜め読みだが、3つのstepでFoundation Modelを訓練する。まずはツールのdescriptionからツールトークンを生成する。これにより、モデルにツールの情報を覚えさせる（memorization）。斜め読みなので読めていないが、ツールトークンをvocabに追加してるのでここは継続的事前学習をしているかもしれない。続いて、（おそらく）人手でアノテーションされたクエリ-必要なツールのペアデータから、クエリに対して必要なツールを生成するタスクを学習させる。最後に、（おそらく人手で作成された）クエリ-タスクを解くためのtrajectoryペアのデータで学習させる。
![image](https://github.com/user-attachments/assets/eebe4260-2e4f-4be7-9b59-a0b84913e667)
![image](https://github.com/user-attachments/assets/d03ed971-e5c9-49f3-8385-cfb00505907c)学習データのサンプル。Appendix中に記載されているものだが、本文のデータセット節とAppendixの双方に、データの作り方の詳細は記述されていなかった。どこかに書いてあるのだろうか。
![image](https://github.com/user-attachments/assets/41975d34-dc9d-405d-aaca-062a3ee1a4b0)![image](https://github.com/user-attachments/assets/41e80988-5770-420e-bc80-a4cc0a724994)最終的な性能
![image](https://github.com/user-attachments/assets/a247cc99-10eb-4346-9f0d-b406a022c3b4)特殊トークンを追加のvocabとして登録し、そのトークンを生成できるようなデータで学習し、vocabに応じて何らかの操作を実行するという枠組み、その学習手法は色々なタスクで役立ちそう。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/ScientificDiscovery.html">#ScientificDiscovery</a><br /><span class="issue_date">Issue Date: 2024-08-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1350">The AI Scientist: Towards Fully Automated Open-Ended Scientific  Discovery, Chris Lu+, N_A, arXiv24</a>
<span class="snippet"><span>Summary</span>最先端の大規模言語モデルを使用して、完全自動の科学的発見を可能にする包括的なフレームワークが提案された。AI Scientistは新しい研究アイデアを生成し、コードを記述し、実験を実行し、結果を可視化し、完全な科学論文を執筆し、査読プロセスを実行することができる。このアプローチは、機械学習における科学的発見の新しい時代の始まりを示しており、AIエージェントの変革的な利点をAI自体の研究プロセス全体にもたらし、世界で最も難しい問題に無限の手頃な価格の創造性とイノベーションを解き放つことに近づいています。</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/SoftwareEngineering.html">#SoftwareEngineering</a><br /><span class="issue_date">Issue Date: 2025-04-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1848">SWE-bench: Can Language Models Resolve Real-World GitHub Issues?, Carlos E. Jimenez+, arXiv23</a>
<span class="snippet"><span>Summary</span>SWE-benchは、12の人気Pythonリポジトリから得られた2,294のソフトウェアエンジニアリング問題を評価するフレームワークで、言語モデルがコードベースを編集して問題を解決する能力を測定します。評価の結果、最先端の商用モデルや微調整されたモデルSWE-Llamaも最も単純な問題しか解決できず、Claude 2はわずか1.96%の問題を解決するにとどまりました。SWE-benchは、より実用的で知的な言語モデルへの進展を示しています。</span>
<span class="snippet"><span>Comment</span>ソフトウェアエージェントの最もpopularなベンチマーク

<img width="693" alt="Image" src="https://github.com/user-attachments/assets/ac905221-d3b1-4d16-b447-3bdd4d5e97bb" />

主にpythonライブラリに関するリポジトリに基づいて構築されている。
<img width="731" alt="Image" src="https://github.com/user-attachments/assets/14d26dd1-6b4a-4337-a652-4e48e36d633b" />SWE-Bench, SWE-Bench Lite, SWE-Bench Verifiedの3種類がありソフトウェアエージェントではSWE-Bench Verifiedを利用して評価することが多いらしい。Verifiedでは、issueの記述に曖昧性がなく、適切なunittestのスコープが適切なもののみが採用されているとのこと（i.e., 人間の専門家によって問題がないと判断されたもの）。
https://www.swebench.com/</span>
<a class="button" href="articles/MachineLearning.html">#MachineLearning</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Evaluation.html">#Evaluation</a><a class="button" href="articles/AutoML.html">#AutoML</a><br /><span class="issue_date">Issue Date: 2023-10-09</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1067">Benchmarking Large Language Models As AI Research Agents, Qian Huang+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>本研究では、AI研究エージェントを構築し、科学的な実験のタスクを実行するためのベンチマークとしてMLAgentBenchを提案する。エージェントはファイルの読み書きやコードの実行などのアクションを実行し、実験を実行し、結果を分析し、機械学習パイプラインのコードを変更することができる。GPT-4ベースの研究エージェントは多くのタスクで高性能なモデルを実現できるが、成功率は異なる。また、LLMベースの研究エージェントにはいくつかの課題がある。</span>
<span class="snippet"><span>Comment</span>GPT4がMLモデルをどれだけ自動的に構築できるかを調べた模様。また、ベンチマークデータを作成した模様。結果としては、既存の有名なデータセットでの成功率は90%程度であり、未知のタスク（新たなKaggle Challenge等）では30%程度とのこと。</span>
<a class="button" href="articles/Survey.html">#Survey</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br /><span class="issue_date">Issue Date: 2023-09-01</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1028">A Survey on Large Language Model based Autonomous Agents, Lei Wang+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>自律エージェントの研究は、以前は限られた知識を持つエージェントに焦点を当てていましたが、最近では大規模言語モデル（LLMs）を活用した研究が増えています。本論文では、LLMに基づく自律エージェントの研究を包括的に調査し、統一されたフレームワークを提案します。さらに、LLMに基づくAIエージェントの応用や評価戦略についてもまとめています。将来の方向性や課題についても議論し、関連する参考文献のリポジトリも提供しています。</span>
<span class="snippet"><span>Comment</span>![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/c921a960-02f7-44e6-8c24-bb578f599bbe)
![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/73c4662b-ca74-41cc-8be5-c76c4aad36c8)良いサーベイ</span>
<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/c921a960-02f7-44e6-8c24-bb578f599bbe" alt="image" loading="lazy" /><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Evaluation.html">#Evaluation</a><br /><span class="issue_date">Issue Date: 2023-08-27</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1020">AgentBench: Evaluating LLMs as Agents, Xiao Liu+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>本研究では、大規模言語モデル（LLMs）をエージェントとして評価するための多次元の進化するベンチマーク「AgentBench」を提案しています。AgentBenchは、8つの異なる環境でマルチターンのオープンエンドの生成設定を提供し、LLMの推論と意思決定能力を評価します。25のLLMsに対するテストでは、商用LLMsは強力な能力を示していますが、オープンソースの競合他社との性能には差があります。AgentBenchのデータセット、環境、および評価パッケージは、GitHubで公開されています。</span>
<span class="snippet"><span>Comment</span>エージェントとしてのLLMの推論能力と意思決定能力を評価するためのベンチマークを提案。
トップの商用LLMとOpenSource LLMの間に大きな性能差があることを示した。</span>
<a class="button" href="articles/ComputerVision.html">#ComputerVision</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br /><span class="issue_date">Issue Date: 2023-07-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/883">Towards A Unified Agent with Foundation Models, Norman Di Palo+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>本研究では、言語モデルとビジョン言語モデルを強化学習エージェントに組み込み、効率的な探索や経験データの再利用などの課題に取り組む方法を調査しました。スパースな報酬のロボット操作環境でのテストにおいて、ベースラインに比べて大幅な性能向上を実証し、学習済みのスキルを新しいタスクの解決や人間の専門家のビデオの模倣に活用する方法を示しました。</span>
<span class="snippet"><span>Comment</span>
![image](https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/aa40d0e3-9499-4804-9046-a9ad795c2d52)</span>
<img src="https://github.com/AkihikoWatanabe/paper_notes/assets/12249301/aa40d0e3-9499-4804-9046-a9ad795c2d52" alt="image" loading="lazy" /><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/Evaluation.html">#Evaluation</a><br /><span class="issue_date">Issue Date: 2023-07-03</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/783">Mind2Web: Towards a Generalist Agent for the Web, Xiang Deng+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>Mind2Webという新しいデータセットを紹介します。このデータセットは、任意のウェブサイト上で複雑なタスクを実行するための言語の指示に従うウェブエージェントを開発・評価するために作成されました。従来のデータセットでは一般的なウェブエージェントには適していなかったため、Mind2Webはより多様なドメイン、実世界のウェブサイト、幅広いユーザーの相互作用パターンを提供します。また、大規模言語モデル（LLMs）を使用して一般的なウェブエージェントを構築するための初期の探索も行われます。この研究は、ウェブエージェントのさらなる研究を促進するためにデータセット、モデルの実装、およびトレーニング済みモデルをオープンソース化します。</span>
<span class="snippet"><span>Comment</span>Webにおけるgeneralistエージェントを評価するためのデータセットを構築。31ドメインの137件のwebサイトにおける2350個のタスクが含まれている。

タスクは、webサイトにおける多様で実用的なユースケースを反映し、チャレンジングだが現実的な問題であり、エージェントの環境やタスクをまたいだ汎化性能を評価できる。

プロジェクトサイト:
https://osu-nlp-group.github.io/Mind2Web/</span>
<a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Transformer.html">#Transformer</a><br /><span class="issue_date">Issue Date: 2023-06-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/760">Think Before You Act: Decision Transformers with Internal Working Memory, Jikun Kang+, N_A, arXiv23</a>
<span class="snippet"><span>Summary</span>大規模言語モデル（LLM）の性能は、トレーニング中にパラメータに振る舞いを記憶する「忘却現象」によって低下する可能性がある。人間の脳は分散型のメモリストレージを利用しており、忘却現象を軽減している。そこで、我々は、内部作業メモリモジュールを提案し、Atariゲームとメタワールドオブジェクト操作タスクの両方でトレーニング効率と汎化性を向上させることを示した。</span>
<a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Admin'sPick.html">#Admin'sPick</a><br /><span class="issue_date">Issue Date: 2023-04-13</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/518">REACT : SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS, Yao+, Princeton University and Google brain, ICLR23</a>
<span class="snippet"><span>Comment</span># 概要
人間は推論と行動をシナジーさせることで、さまざまな意思決定を行える。近年では言語モデルにより言語による推論を意思決定に組み合わせる可能性が示されてきた。たとえば、タスクをこなすための推論トレースをLLMが導けることが示されてきた（Chain-of-Thought）が、CoTは外部リソースにアクセスできないため知識がアップデートできず、事後的に推論を行うためhallucinationやエラーの伝搬が生じる。一方で、事前学習言語モデルをinteractiveな環境において計画と行動に利用する研究が行われているが、これらの研究では、高レベルの目標について抽象的に推論したり、行動をサポートするための作業記憶を維持したりするために言語モデルを利用していない。推論と行動を一般的な課題解決のためにどのようにシナジーできるか、またそのようなシナジーが単独で推論や行動を実施した場合と比較してどのような利益をもたらすかについて研究されていない。
そこで、REACTを提案。REACTは推論と行動をLLMと組み合わせて、多様な推論や意思決定タスクを実現するための一般的な枠組みであり、推論トレースとアクションを交互に生成するため、動的に推論を実行して行動するための大まかな計画を作成、維持、調整できると同時に、wikipediaなどの外部ソースとやりとりして追加情報を収集し、推論プロセスに組み込むことが可能となる。

- 要はいままではGeneralなタスク解決モデルにおいては、推論とアクションの生成は独立にしかやられてこなかったけど、推論とアクションを交互作用させることについて研究したよ
- そしたら性能がとってもあがったよ
- reasoningを人間が編集すれば、エージェントのコントロールもできるよ　という感じ

# イントロ
人間は推論と行動の緊密なシナジーによって、不確実な状況に遭遇しても適切な意思決定が行える。たとえば、任意の2つの特定のアクションの間で、進行状況をトレースするために言語で推論したり（すべて切り終わったからお湯を沸かす必要がある）、例外を処理したり、状況に応じて計画を調整したりする（塩がないから代わりに醤油と胡椒を使おう）。また、推論をサポートし、疑問（いまどんな料理を作ることができるだろうか？）を解消するために、行動（料理本を開いてレシピを読んで、冷蔵庫を開いて材料を確確認したり）をすることもある。

近年の研究では言語での推論を、インタラクティブな意思決定を組み合わせる可能性についてのヒントが得られてきた。一つは、適切にPromptingされたLLMが推論トレースを実行できることを示している。推論トレースとは、解決策に到達するための一連のステップを経て推論をするためのプロセスのことである。しかしながらChain-of-thoughytは、このアプローチでは、モデルが外界対してgroundingできず、内部表現のみに基づい思考を生成するため限界がある。これによりモデルが事後対応的に推論したり、外部情報に基づいて知識を更新したりできないため、推論プロセス中にhallucinationやエラーの伝搬などの問題が発生する可能性が生じる。
一方、近年の研究では事前学習言語モデルをinteractiveな環境において計画と行動に利用する研究が行われている。これらの研究では、通常マルチモーダルな観測結果をテキストに変換し、言語モデルを使用してドメイン固有のアクション、またはプランを生成し、コントローラーを利用してそれらを選択または実行する。ただし、これらのアプローチは高レベルの目標について抽象的に推論したり、行動をサポートするための作業記憶を維持したりするために言語モデルを利用していない。
推論と行動を一般的な課題解決のためにどのようにシナジーできるか、またそのようなシナジーが単独で推論や行動を実施した場合と比較してどのような利益をもたらすかについて研究されていない。

LLMにおける推論と行動を組み合わせて、言語推論と意思決定タスクを解決するREACTと呼ばれる手法を提案。REACTでは、推論と行動の相乗効果を高めることが可能。推論トレースによりアクションプランを誘発、追跡、更新するのに役立ち、アクションでは外部ソースと連携して追加情報を収集できる。

REACTは推論と行動をLLMと組み合わせて、多様な推論や意思決定タスクを実現するための一般的な枠組みである。REACTのpromptはLLMにverbalな推論トレースとタスクを実行するためのアクションを交互に生成する。これにより、モデルは動的な推論を実行して行動するための大まかな計画を作成、維持、調整できると同時に、wikipediaなどの外部ソースとやりとりして追加情報を収集し、推論プロセスに組み込むことが可能となる。

# 手法
変数を以下のように定義する：
- O_t: Observertion on time t
- a_t: Action on time t
- c_t: context, i.e. (o_1, a_1, o_2, a_2, ..., a_t-1, o_t)
- policy pi(a_t | c_t): Action Spaceからアクションを選択するポリシー
- A: Action Space
- O: Observation Space

普通はc_tが与えられたときに、ポリシーに従いAからa_tを選択しアクションを行い、アクションの結果o_tを得て、c_t+1を構成する、といったことを繰り返していく。

このとき、REACTはAをA ∪ Lに拡張しする。ここで、LはLanguage spaceである。LにはAction a_hatが含まれ、a_hatは環境に対して作用をしない。単純にthought, あるいは reasoning traceを実施し、現在のcontext c_tをアップデートするために有用な情報を構成することを目的とする。Lはunlimitedなので、事前学習された言語モデルを用いる。今回はPaLM-540B（c.f. GPT3は175Bパラメータ）が利用され、few-shotのin-context exampleを与えることで推論を行う。それぞれのin-context exampleは、action, thoughtsそしてobservationのtrajectoryを与える。

推論が重要なタスクでは、thoughts-action-observationステップから成るtask-solving trajectoryを生成する。一方、多数のアクションを伴う可能性がある意思決定タスクでは、thoughtsのみを行うことをtask-solving trajectory中の任意のタイミングで、自分で判断して行うことができる。

意思決定と推論能力がLLMによってもたらされているため、REACTは4つのuniqueな特徴を持つ：
- 直感的で簡単なデザイン
  - REACTのpromptは人間のアノテータがアクションのトップに思考を言語で記述するようなストレートなものであり、ad-hocなフォーマットの選択、思考のデザイン、事例の選定などが必要ない。
- 一般的で柔軟性が高い
  - 柔軟な thought spaceと thought-actionのフォーマットにより、REACTはさまざまなタスクにも柔軟に対応できる
- 高性能でロバスト
  - REACTは1-6個の事例によって、新たなタスクに対する強力な汎化を示す。そして推論、アクションのみを行うベースラインよりも高い性能を示している。REACTはfinetuningの斧系も得ることができ、promptの選択に対してREACTの性能はrobustである。
- 人間による調整と操作が可能
  - REACTは、解釈可能な意思決定と推論のsequenceを前提としているため、人間は簡単に推論や事実の正しさを検証できる。加えて、thoughtsを編集することによって、m人間はエージェントの行動を制御、あるいは修正できる。

# KNOWLEDGE INTENSIVE REASONING TASKS</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Programming.html">#Programming</a><a class="button" href="articles/Slide.html">#Slide</a><a class="button" href="articles/SoftwareEngineering.html">#SoftwareEngineering</a><a class="button" href="articles/ContextEngineering.html">#ContextEngineering</a><br /><span class="issue_date">Issue Date: 2025-07-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2148">Claude Code の Context Engineering, schroneko, 2025.07</a>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Blog.html">#Blog</a><a class="button" href="articles/SoftwareEngineering.html">#SoftwareEngineering</a><a class="button" href="articles/ContextEngineering.html">#ContextEngineering</a><br /><span class="issue_date">Issue Date: 2025-07-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2138">Context Engineering - What it is, and techniques to consider, llamaindex, 2025.07</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/llama_index/status/1940810514227196236?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Blog.html">#Blog</a><a class="button" href="articles/SoftwareEngineering.html">#SoftwareEngineering</a><a class="button" href="articles/ContextEngineering.html">#ContextEngineering</a><br /><span class="issue_date">Issue Date: 2025-07-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2135">The New Skill in AI is Not Prompting, Its Context Engineering, PHLSCHMID, 2025.06</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/akiratosei/status/1940960253233058198?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Library.html">#Library</a><a class="button" href="articles/ReinforcementLearning.html">#ReinforcementLearning</a><a class="button" href="articles/PostTraining.html">#PostTraining</a><br /><span class="issue_date">Issue Date: 2025-07-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2134">rLLM, Agentica, 2025.06</a>
<span class="snippet"><span>Comment</span>&gt;rLLM is an open-source framework for post-training language agents via reinforcement learning. With rLLM, you can easily build their custom agents and environments, train them with reinforcement learning, and deploy them for real-world workloads.
なるほど。


バックボーンにはverlが採用されており、シンプルかつ統一的なインタフェースでカスタムエージェントが学習できる模様？

https://rllm-project.readthedocs.io/en/latest/#key-features元ポスト:https://x.com/chenguangwang/status/1940585022010122692?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q関連:
- #1969</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Blog.html">#Blog</a><a class="button" href="articles/Programming.html">#Programming</a><a class="button" href="articles/SoftwareEngineering.html">#SoftwareEngineering</a><br /><span class="issue_date">Issue Date: 2025-06-23</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2072">AI Agent Manager （AAM） として生きていく : 作業環境とワークフローの設計, icoxfog417, 2025.06</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/icoxfog417/status/1936929479324319807?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Blog.html">#Blog</a><a class="button" href="articles/Programming.html">#Programming</a><a class="button" href="articles/read-later.html">#read-later</a><br /><span class="issue_date">Issue Date: 2025-06-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2067">AI-assisted coding for teams that cant get away with vibes, Atharva Raykar, 2025.05</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/deedydas/status/1936090859319259321?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Blog.html">#Blog</a><a class="button" href="articles/read-later.html">#read-later</a><br /><span class="issue_date">Issue Date: 2025-06-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2065">Single vs Multi-Agent System?, PHILSCHMID, 2025.06</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/_philschmid/status/1935985099171840140?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q関連:
- #2050</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Multi.html">#Multi</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Blog.html">#Blog</a><a class="button" href="articles/read-later.html">#read-later</a><br /><span class="issue_date">Issue Date: 2025-06-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/2050">Don’t Build Multi-Agents, Cognition, 2025.06</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/ngo275/status/1934819225111285852?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Blog.html">#Blog</a><a class="button" href="articles/Coding.html">#Coding</a><br /><span class="issue_date">Issue Date: 2025-05-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1972">OpenAI-Codex, OpenAI, 2025.05</a>
<span class="snippet"><span>Comment</span>OpenHandsのNeubig氏が、OpenAIのブログポスト中で報告されているSWE-Bench Verifiedのスコアについて、言及している。OpenAIは23個サンプルについて(internal infrastructureで動作させられないため)除外しているので、その分スコアに下駄が履かれているようで、ブログ中のpassNのスコアを他のリーダーボードのスコアと比較する際には注意が必要っぽい。
https://x.com/gneubig/status/1923893277519962287?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/ScientificDiscovery.html">#ScientificDiscovery</a><a class="button" href="articles/Coding.html">#Coding</a><br /><span class="issue_date">Issue Date: 2025-05-17</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1971">AlphaEvolve: A coding agent for scientific and algorithmic discovery, Novikov+, Google DeepMind, 2025.05</a>
<span class="snippet"><span>Comment</span>blog post:https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Analysis.html">#Analysis</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Library.html">#Library</a><a class="button" href="articles/Blog.html">#Blog</a><br /><span class="issue_date">Issue Date: 2025-05-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1927">Agent Frameworkはどれを使うべきか タスク性能編, はち, 2025.05</a>
<span class="snippet"><span>Comment</span>各フレームワーク毎の性能の違いや消費したトークン数、実装の微妙や違いがまとめられており、太字でtakeawayが記述されているので非常にわかりやすい。元ポスト:https://x.com/curveweb/status/1919301208096866660?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Slide.html">#Slide</a><a class="button" href="articles/SoftwareEngineering.html">#SoftwareEngineering</a><br /><span class="issue_date">Issue Date: 2025-04-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1906">Cursor_Devin全社導入の理想と現実, Ryoichi Saito, 2025.04</a>
<span class="snippet"><span>Comment</span>Devinの思わぬ挙動のくだりが非常に面白かった。まだまだ使いづらいところが多そうだなあ…。</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Blog.html">#Blog</a><a class="button" href="articles/Repository.html">#Repository</a><br /><span class="issue_date">Issue Date: 2025-04-26</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1903">Deepwiki, Cognition, 2025.04</a>
<span class="snippet"><span>Comment</span>githubリポジトリに関するリッチなドキュメントに対してDevinを通じて対話的に質問ができる模様。サインアップ不要で、githubリポジトリのドメインをdeepwikiに変えるだけで利用可能</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/ComputerVision.html">#ComputerVision</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/MulltiModal.html">#MulltiModal</a><a class="button" href="articles/Blog.html">#Blog</a><a class="button" href="articles/Reasoning.html">#Reasoning</a><a class="button" href="articles/OpenWeight.html">#OpenWeight</a><a class="button" href="articles/x-Use.html">#x-Use</a><br /><span class="issue_date">Issue Date: 2025-04-18</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1896">Introducing UI-TARS-1.5, ByteDance, 2025.04</a>
<span class="snippet"><span>Summary</span>UI-TARSは、スクリーンショットを入力として人間のようにインタラクションを行うネイティブGUIエージェントモデルであり、従来の商業モデルに依存せず、エンドツーエンドで優れた性能を発揮します。実験では、10以上のベンチマークでSOTA性能を達成し、特にOSWorldやAndroidWorldで他のモデルを上回るスコアを記録しました。UI-TARSは、強化された知覚、統一アクションモデリング、システム-2推論、反射的オンライントレースによる反復トレーニングなどの革新を取り入れ、最小限の人間の介入で適応し続ける能力を持っています。</span>
<span class="snippet"><span>Comment</span>paper:https://arxiv.org/abs/2501.12326色々と書いてあるが、ざっくり言うとByteDanceによる、ImageとTextをinputとして受け取り、TextをoutputするマルチモーダルLLMによるComputer Use Agent (CUA)関連
- #1794元ポスト:https://x.com/_akhaliq/status/1912913195607663049?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Library.html">#Library</a><br /><span class="issue_date">Issue Date: 2025-03-16</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1805">The TypeScript Agent Framework, mastra, 2025.03</a>
<span class="snippet"><span>Comment</span>日本語解説:https://zenn.dev/yosh1/articles/mastra-ai-agent-framework-guide</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Blog.html">#Blog</a><br /><span class="issue_date">Issue Date: 2025-03-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1801">Model Context Protocol （MCP）, Anthropic</a>
<span class="snippet"><span>Comment</span>下記リンクのMCPサーバ/クライアントの作り方を読むとだいぶ理解が捗る:
https://modelcontextprotocol.io/quickstart/server
https://modelcontextprotocol.io/quickstart/client</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Blog.html">#Blog</a><a class="button" href="articles/x-Use.html">#x-Use</a><br /><span class="issue_date">Issue Date: 2025-03-15</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1800">browser-useの基礎理解, むさし, 2024.12</a>
<span class="snippet"><span>Comment</span>公式リポジトリ:https://github.com/browser-use/browser-useBrowserUseはDoMを解析するということは内部的にテキストをLLMで処理してアクションを生成するのだろうか。OpenAIのComputer useがスクリーンショットからアクションを生成するのとは対照的だと感じた（小並感）。

- #1794</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Slide.html">#Slide</a><br /><span class="issue_date">Issue Date: 2025-03-14</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1796">AI_Agent_の作り方_近藤憲児, Kenji KONDO, 2025.03</a>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Blog.html">#Blog</a><a class="button" href="articles/x-Use.html">#x-Use</a><br /><span class="issue_date">Issue Date: 2025-03-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1794">OpenAI API での Computer use の使い方, npaka, 2025.03</a>
<span class="snippet"><span>Comment</span>OpenAIのCompute Useがどのようなものかコンパクトにまとまっている。勉強になりました。公式:https://platform.openai.com/docs/guides/tools-computer-use</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br /><span class="issue_date">Issue Date: 2025-03-12</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1793">GAIA, gaia-bemchmark, 2023.11</a>
<span class="snippet"><span>Summary</span>GAIAは一般的なAIアシスタントのためのベンチマークで、推論やマルチモーダル処理などの基本能力を必要とする現実世界の質問を提案します。人間は92％の正答率を達成する一方、GPT-4は15％の正答率にとどまり、このパフォーマンス差はAIの限界を示しています。GAIAは人間にとって難しいタスクをターゲットにし、AGIの出現にはこのような質問に対する堅牢性が必要と考えています。466の質問と回答を作成し、300の質問の回答を公開したリーダーボードを提供しています。</span>
<span class="snippet"><span>Comment</span>paper:https://arxiv.org/abs/2311.12983- #1792

で言及されているLLM Agentの評価で最も有名なベンチマークな模様- #1158

見たことあるなと思ったら上記で既にメモっていた</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Library.html">#Library</a><br /><span class="issue_date">Issue Date: 2025-03-06</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1784">smolagents, HuggingFace, 2025.03</a>
<span class="snippet"><span>Summary</span>smolagentsは、数行のコードで強力なエージェントを構築できるライブラリで、シンプルなロジック、コードエージェントのサポート、安全な実行環境、ハブ統合、モデルやモダリティに依存しない設計が特徴。テキスト、視覚、動画、音声入力をサポートし、さまざまなツールと統合可能。詳細はローンチブログ記事を参照。</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><br /><span class="issue_date">Issue Date: 2025-03-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1777">Introducing the SWE-Lancer benchmark, OpenAI, 2025.02</a>
<span class="snippet"><span>Comment</span>元ポスト:https://x.com/dair_ai/status/1893698290174108113?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q1400以上のフリーランスソフトウェアエンジニアリングタスクを集めたベンチマーク。タスクはバグ修正から機能実装まで多岐にわたり、経験豊富なエンジニアによって評価されたもの。</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Library.html">#Library</a><a class="button" href="articles/RAG(RetrievalAugmentedGeneration).html">#RAG(RetrievalAugmentedGeneration)</a><br /><span class="issue_date">Issue Date: 2025-01-25</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1726">Llama Stack, Meta, 2024.11</a>
<span class="snippet"><span>Comment</span>Llamaを用いたLLM Agentを構築するための標準化されたフレームワーク。Quick StartではRAG Agentを構築している。</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Blog.html">#Blog</a><br /><span class="issue_date">Issue Date: 2025-01-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1660">AI Agents 2024 Rewind - A Year of Building and Learning, VICTOR DIBIA, 2025.01</a>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Blog.html">#Blog</a><br /><span class="issue_date">Issue Date: 2025-01-05</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1659">AI Agent Era,  福島良典 | LayerX, 2024.12</a>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/python.html">#python</a><a class="button" href="articles/Blog.html">#Blog</a><a class="button" href="articles/API.html">#API</a><a class="button" href="articles/x-Use.html">#x-Use</a><br /><span class="issue_date">Issue Date: 2025-01-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1652">browser-use やばいです, Syoitu, 2024.12</a>
<span class="snippet"><span>Comment</span>すごい手軽に使えそうだが、クローリング用途に使おうとするとhallucinationが起きた時に困るのでうーんと言ったところ。</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Dataset.html">#Dataset</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Evaluation.html">#Evaluation</a><br /><span class="issue_date">Issue Date: 2024-10-20</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1457">MLE-Bench, OpenAI, 2024.10</a>
<span class="snippet"><span>Summary</span>MLE-benchを紹介し、AIエージェントの機械学習エンジニアリング能力を測定するためのベンチマークを構築。75のKaggleコンペを基に多様なタスクを作成し、人間のベースラインを確立。最前線の言語モデルを評価した結果、OpenAIのo1-previewが16.9%のコンペでKaggleのブロンズメダル相当の成果を達成。AIエージェントの能力理解を促進するため、ベンチマークコードをオープンソース化。</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Repository.html">#Repository</a><a class="button" href="articles/Conversation.html">#Conversation</a><br /><span class="issue_date">Issue Date: 2024-10-02</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1440">AutoGen, Microsoft, 2024.10</a>
<span class="snippet"><span>Summary</span>AutoGenは、AIエージェントの構築と協力を促進するオープンソースのプログラミングフレームワークで、エージェント間の相互作用や多様なLLMの使用をサポートします。これにより、次世代LLMアプリケーションの開発が容易になり、複雑なワークフローのオーケストレーションや最適化が簡素化されます。カスタマイズ可能なエージェントを用いて多様な会話パターンを構築でき、強化されたLLM推論や高度なユーティリティ機能も提供します。AutoGenは、Microsoftや大学との共同研究から生まれました。</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Pocket.html">#Pocket</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/QuestionAnswering.html">#QuestionAnswering</a><a class="button" href="articles/GenerativeAI.html">#GenerativeAI</a><a class="button" href="articles/RAG(RetrievalAugmentedGeneration).html">#RAG(RetrievalAugmentedGeneration)</a><a class="button" href="articles/Repository.html">#Repository</a><br /><span class="issue_date">Issue Date: 2024-09-11</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1387">PaperQA2, 2023.02</a>
<span class="snippet"><span>Comment</span>元ポスト: https://x.com/sgrodriques/status/1833908643856818443?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-Q</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NaturalLanguageGeneration.html">#NaturalLanguageGeneration</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Repository.html">#Repository</a><br /><span class="issue_date">Issue Date: 2024-07-04</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1325">OpenDevin: Code Less, Make More, 2024</a>
<span class="snippet"><span>Comment</span>LLMによるOpenSourceなソフトウェア生成エージェントプラットフォームfull timeのスタッフを雇用しworldクラスのUXを目指すとのこと。楽しみ。
参考: https://x.com/gneubig/status/1808493521315496229?s=46&amp;t=Y6UuIHB0Lv0IpmFAjlc2-QOpen化される前の最初のDevinのツイート
https://x.com/cognition_labs/status/1767548763134964000</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Library.html">#Library</a><br /><span class="issue_date">Issue Date: 2023-09-30</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/1049">Agents: An opensource framework for autonomous language agents</a>
<span class="snippet"><span>Comment</span>以下の特徴を持つLLMAgent開発のためのフレームワーク

- long-short term memory
- tool usage
- web navigation
- multi-agent communication
- human-agent interaction
- symbolic control

また、他のAgent frameworkと違い、ゴールを達成するだの細かいプランニングを策定（SOP; サブタスクとサブゴールを定義）することで、エージェントに対してきめ細かなワークフローを定義できる。</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Tools.html">#Tools</a><a class="button" href="articles/InformationRetrieval.html">#InformationRetrieval</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/Library.html">#Library</a><br /><span class="issue_date">Issue Date: 2023-04-22</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/521">Llamaindex</a>
<span class="snippet"><span>Comment</span>- LlamaIndexのインデックスを更新し、更新前後で知識がアップデートされているか確認してみた
  - https://dev.classmethod.jp/articles/llama-index-insert-index/</span>
<a class="button" href="articles/Article.html">#Article</a><a class="button" href="articles/Tools.html">#Tools</a><a class="button" href="articles/InformationRetrieval.html">#InformationRetrieval</a><a class="button" href="articles/NLP.html">#NLP</a><a class="button" href="articles/LanguageModel.html">#LanguageModel</a><a class="button" href="articles/Library.html">#Library</a><br /><span class="issue_date">Issue Date: 2023-04-21</span>
<a href="https://github.com/AkihikoWatanabe/paper_notes/issues/520">LangChain</a>
<span class="snippet"><span>Comment</span>- LangChain の Googleカスタム検索 連携を試す
  - https://note.com/npaka/n/nd9a4a26a8932- LangChainのGetting StartedをGoogle Colaboratoryでやってみる ④Agents
    - https://zenn.dev/kun432/scraps/8216511783e3da</span>
<button onclick="hideContent(0)" style="display: none;">hide</button>
</div>


    </div>

</article>
<div class="post-nav"><a class="previous" href="/paper_notes/articles/LLM-as-a-Judge.html" title="LLM-as-a-Judgeに関する論文・技術記事メモの一覧">LLM-as-a-Judgeに関する論文・技術記事メモの一覧</a><a class="next" href="/paper_notes/articles/LLMServing.html" title="LLMServingに関する論文・技術記事メモの一覧">LLMServingに関する論文・技術記事メモの一覧</a></div><div class="post-related">
      <div>Related Articles</div>
      <ul>
        <li class="">
          <a class="post-link"
            href="/paper_notes/articles/GenerativeAI.html"
            title="GenerativeAIに関する論文・技術記事メモの一覧">
            GenerativeAIに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li><li class="">
          <a class="post-link"
            href="/paper_notes/articles/QuestionGeneration.html"
            title="QuestionGenerationに関する論文・技術記事メモの一覧">
            QuestionGenerationに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li><li class="">
          <a class="post-link"
            href="/paper_notes/articles/OpenSource.html"
            title="OpenSourceに関する論文・技術記事メモの一覧">
            OpenSourceに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li><li class="">
          <a class="post-link"
            href="/paper_notes/articles/COLT.html"
            title="COLTに関する論文・技術記事メモの一覧">
            COLTに関する論文・技術記事メモの一覧<span class="post-badges">
  <span class="post-badge badge-top">TOP</span>
  <span class="post-badge badge-new">NEW</span>
</span>
</a>
        </li></ul>
    </div><div class="post-comments"></div></section>
</div>


  </section>
  <section class="sidebar" style="margin-left: 15px;">
    <!-- Get sidebar items --><style type="text/css" media="screen">
.post-menu ul {
  list-style: none;
  padding: 0;
  margin: 0;
}
</style>

<div class="post-menu">
  <div class="post-menu-title">TOC</div>
  <div class="post-menu-content"></div>
</div>

<script>
  function generateContent() {
    var menu = document.querySelector(".post-menu");
    var menuContent =  menu.querySelector(".post-menu-content");
    var headings = document.querySelector(".post-content").querySelectorAll("h2, h3, h4, h5, h6");

    // Hide menu when no headings
    if (headings.length === 0) {
      return menu.style.display = "none";
    }

    // Generate post menu
    var menuHTML = '';
    for (var i = 0; i < headings.length; i++) {
      var h = headings[i];
      menuHTML += (
        '<li class="h-' + h.tagName.toLowerCase() + '">'
        + '<a href="#h-' + h.getAttribute('id') + '">' + h.textContent + '</a></li>');
    }

    menuContent.innerHTML = '<ul>' + menuHTML + '</ul>';

    // The header element
    var header = document.querySelector('header.site-header');

    function doMenuCollapse(index, over_items) {
      var items = menuContent.firstChild.children;

      if (over_items == undefined) {
        over_items = 20;
      }

      if (items.length < over_items) {
        return;
      }

      var activeItem = items[index];
      var beginItem = activeItem
      var endItem = activeItem
      var beginIndex = index;
      var endIndex = index + 1;
      while (beginIndex >= 0
        && !items[beginIndex].classList.contains('h-h2')) {
        beginIndex -= 1;
      }
      while (endIndex < items.length
        && !items[endIndex].classList.contains('h-h2')) {
        endIndex += 1;
      }
      for (var i = 0; i < beginIndex; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
      for (var i = beginIndex + 1; i < endIndex; i++) {
        item = items[i]
        // if (!item.classList.contains('h-h2')) {
          item.style.display = '';
        // }
      }
      for (var i = endIndex; i < items.length; i++) {
        item = items[i]
        if (!item.classList.contains('h-h2')) {
          item.style.display = 'none';
        }
      }
    }

    // Init menu collapsed
    doMenuCollapse(-1);

    // Active the menu item
    window.addEventListener('scroll', function (event) {
      var lastActive = menuContent.querySelector('.active');
      var changed = true;
      var activeIndex = -1;
      for (var i = headings.length - 1; i >= 0; i--) {
        var h = headings[i];
        var headingRect = h.getBoundingClientRect();
        var headerRect = header.getBoundingClientRect();
        var headerTop = Math.floor(headerRect.top);
        var headerHeight = Math.floor(headerRect.height);
        var headerHeight = headerTop + headerHeight + 20;
        if (headingRect.top <= headerHeight) {
          var id = 'h-' + h.getAttribute('id');
          var a = menuContent.querySelector('a[href="#' + id  + '"]');
          var curActive = a.parentNode;
          if (curActive) {
            curActive.classList.add('active');
            activeIndex = i;
          }
          if (lastActive == curActive) {
            changed = false;
          }
          break;
        }
      }
      if (changed) {
        if (lastActive) {
          lastActive.classList.remove('active');
        }
        doMenuCollapse(activeIndex);
      }
      event.preventDefault();
    });
  }
  generateContent();
</script>
</section>
</div>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/paper_notes/"></data>

  <div class="wrapper">
    <div class="site-footer-inner"><div>Copyright © 2023-current AkihikoWATANABE. The header images and any thumbnail images for the posts were generated by ChatGPT's DALL-E3.</div>
      <div>Powered by <a title="Jekyll is a simple, blog-aware, static site
      generator." href="https://jekyllrb.com/">Jekyll</a> &amp; <a title="Yat, yet
      another theme." href="https://github.com/jeffreytse/jekyll-theme-yat">Yat Theme</a>.</div>
      <div class="footer-col rss-subscribe">Subscribe <a href="/paper_notes/feed.xml">via RSS</a></div>
    </div>
  </div>
</footer>
</body>
</html>
